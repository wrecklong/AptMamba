CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
| distributed init (rank 6): env://
| distributed init (rank 2): env://
| distributed init (rank 0): env://
| distributed init (rank 5): env://
| distributed init (rank 1): env://
| distributed init (rank 3): env://
| distributed init (rank 4): env://
| distributed init (rank 7): env://
Namespace(batch_size=128, epochs=100, bce_loss=False, update_freq=1, unscale_lr=False, model='vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.1, sched='cosine', lr=5e-06, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='/cluster/work/cvl/guosun/shangye/pretrained/vim_t_midclstok_76p1acc.pth', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, cosub=False, finetune='', attn_only=False, data_path='/scratch/tmp.13735801.guosun/datasets/imagenet_full_size/', data_set='IMNET', inat_category='name', output_dir='/cluster/work/cvl/guosun/shangye/output/Vim_new/vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_300einit_100e_batch_size128_p0.9_lr0.000005_min_lr1e-6_decoder_pruning_loss3stage_weight0.1_mse_weight0.02_sort_keep_policy_pretrain_mae', log_dir=None, device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=32, pin_mem=True, distributed=True, world_size=8, dist_url='env://', if_amp=False, if_continue_inf=True, if_nan2num=False, if_random_cls_token_position=False, if_random_token_rank=False, base_rate=0.9, lr_scale=0.01, ratio_weight=2.0, pruning_weight=0.1, mse_weight=0.02, pretrained_mae_path='/cluster/work/cvl/guosun/shangye/output/pretrain_mae_vim/vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_mae_300e/checkpoint.pth', local_rank=0, rank=0, gpu=0, dist_backend='nccl')
Creating model: vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2
keep_rate [0.9, 0.81, 0.7290000000000001]
VisionMambaPrunning(
  (token_merge_module): TPSModule()
  (score_predictor): ModuleList(
    (0-2): 3 x PredictorLG(
      (in_conv_local): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (in_conv_cls): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (out_conv): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=96, out_features=48, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=48, out_features=2, bias=True)
        (5): LogSoftmax(dim=-1)
      )
    )
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=192, out_features=1000, bias=True)
  (drop_path): Identity()
  (layers): ModuleList(
    (0-23): 24 x Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=192, out_features=768, bias=False)
        (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (act): SiLU()
        (x_proj): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj): Linear(in_features=12, out_features=384, bias=True)
        (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
        (out_proj): Linear(in_features=384, out_features=192, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): Identity()
    )
  )
  (decoder): MAE_Decoder(
    (decoder_embed): Linear(in_features=192, out_features=512, bias=True)
    (decoder_blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)
  )
  (norm_f): RMSNorm()
)
Weights of VisionMambaPrunning not initialized from pretrained model: ['score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'decoder.mask_token', 'decoder.decoder_pos_embed', 'decoder.decoder_embed.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_blocks.0.norm1.weight', 'decoder.decoder_blocks.0.norm1.bias', 'decoder.decoder_blocks.0.attn.qkv.weight', 'decoder.decoder_blocks.0.attn.qkv.bias', 'decoder.decoder_blocks.0.attn.proj.weight', 'decoder.decoder_blocks.0.attn.proj.bias', 'decoder.decoder_blocks.0.norm2.weight', 'decoder.decoder_blocks.0.norm2.bias', 'decoder.decoder_blocks.0.mlp.fc1.weight', 'decoder.decoder_blocks.0.mlp.fc1.bias', 'decoder.decoder_blocks.0.mlp.fc2.weight', 'decoder.decoder_blocks.0.mlp.fc2.bias', 'decoder.decoder_blocks.1.norm1.weight', 'decoder.decoder_blocks.1.norm1.bias', 'decoder.decoder_blocks.1.attn.qkv.weight', 'decoder.decoder_blocks.1.attn.qkv.bias', 'decoder.decoder_blocks.1.attn.proj.weight', 'decoder.decoder_blocks.1.attn.proj.bias', 'decoder.decoder_blocks.1.norm2.weight', 'decoder.decoder_blocks.1.norm2.bias', 'decoder.decoder_blocks.1.mlp.fc1.weight', 'decoder.decoder_blocks.1.mlp.fc1.bias', 'decoder.decoder_blocks.1.mlp.fc2.weight', 'decoder.decoder_blocks.1.mlp.fc2.bias', 'decoder.decoder_blocks.2.norm1.weight', 'decoder.decoder_blocks.2.norm1.bias', 'decoder.decoder_blocks.2.attn.qkv.weight', 'decoder.decoder_blocks.2.attn.qkv.bias', 'decoder.decoder_blocks.2.attn.proj.weight', 'decoder.decoder_blocks.2.attn.proj.bias', 'decoder.decoder_blocks.2.norm2.weight', 'decoder.decoder_blocks.2.norm2.bias', 'decoder.decoder_blocks.2.mlp.fc1.weight', 'decoder.decoder_blocks.2.mlp.fc1.bias', 'decoder.decoder_blocks.2.mlp.fc2.weight', 'decoder.decoder_blocks.2.mlp.fc2.bias', 'decoder.decoder_blocks.3.norm1.weight', 'decoder.decoder_blocks.3.norm1.bias', 'decoder.decoder_blocks.3.attn.qkv.weight', 'decoder.decoder_blocks.3.attn.qkv.bias', 'decoder.decoder_blocks.3.attn.proj.weight', 'decoder.decoder_blocks.3.attn.proj.bias', 'decoder.decoder_blocks.3.norm2.weight', 'decoder.decoder_blocks.3.norm2.bias', 'decoder.decoder_blocks.3.mlp.fc1.weight', 'decoder.decoder_blocks.3.mlp.fc1.bias', 'decoder.decoder_blocks.3.mlp.fc2.weight', 'decoder.decoder_blocks.3.mlp.fc2.bias', 'decoder.decoder_blocks.4.norm1.weight', 'decoder.decoder_blocks.4.norm1.bias', 'decoder.decoder_blocks.4.attn.qkv.weight', 'decoder.decoder_blocks.4.attn.qkv.bias', 'decoder.decoder_blocks.4.attn.proj.weight', 'decoder.decoder_blocks.4.attn.proj.bias', 'decoder.decoder_blocks.4.norm2.weight', 'decoder.decoder_blocks.4.norm2.bias', 'decoder.decoder_blocks.4.mlp.fc1.weight', 'decoder.decoder_blocks.4.mlp.fc1.bias', 'decoder.decoder_blocks.4.mlp.fc2.weight', 'decoder.decoder_blocks.4.mlp.fc2.bias', 'decoder.decoder_blocks.5.norm1.weight', 'decoder.decoder_blocks.5.norm1.bias', 'decoder.decoder_blocks.5.attn.qkv.weight', 'decoder.decoder_blocks.5.attn.qkv.bias', 'decoder.decoder_blocks.5.attn.proj.weight', 'decoder.decoder_blocks.5.attn.proj.bias', 'decoder.decoder_blocks.5.norm2.weight', 'decoder.decoder_blocks.5.norm2.bias', 'decoder.decoder_blocks.5.mlp.fc1.weight', 'decoder.decoder_blocks.5.mlp.fc1.bias', 'decoder.decoder_blocks.5.mlp.fc2.weight', 'decoder.decoder_blocks.5.mlp.fc2.bias', 'decoder.decoder_blocks.6.norm1.weight', 'decoder.decoder_blocks.6.norm1.bias', 'decoder.decoder_blocks.6.attn.qkv.weight', 'decoder.decoder_blocks.6.attn.qkv.bias', 'decoder.decoder_blocks.6.attn.proj.weight', 'decoder.decoder_blocks.6.attn.proj.bias', 'decoder.decoder_blocks.6.norm2.weight', 'decoder.decoder_blocks.6.norm2.bias', 'decoder.decoder_blocks.6.mlp.fc1.weight', 'decoder.decoder_blocks.6.mlp.fc1.bias', 'decoder.decoder_blocks.6.mlp.fc2.weight', 'decoder.decoder_blocks.6.mlp.fc2.bias', 'decoder.decoder_blocks.7.norm1.weight', 'decoder.decoder_blocks.7.norm1.bias', 'decoder.decoder_blocks.7.attn.qkv.weight', 'decoder.decoder_blocks.7.attn.qkv.bias', 'decoder.decoder_blocks.7.attn.proj.weight', 'decoder.decoder_blocks.7.attn.proj.bias', 'decoder.decoder_blocks.7.norm2.weight', 'decoder.decoder_blocks.7.norm2.bias', 'decoder.decoder_blocks.7.mlp.fc1.weight', 'decoder.decoder_blocks.7.mlp.fc1.bias', 'decoder.decoder_blocks.7.mlp.fc2.weight', 'decoder.decoder_blocks.7.mlp.fc2.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_pred.bias']
Weights of VisionMambaPrunning not initialized from pretrained model: ['cls_token', 'pos_embed', 'score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'head.weight', 'head.bias', 'layers.0.mixer.A_log', 'layers.0.mixer.D', 'layers.0.mixer.A_b_log', 'layers.0.mixer.D_b', 'layers.0.mixer.in_proj.weight', 'layers.0.mixer.conv1d.weight', 'layers.0.mixer.conv1d.bias', 'layers.0.mixer.x_proj.weight', 'layers.0.mixer.dt_proj.weight', 'layers.0.mixer.dt_proj.bias', 'layers.0.mixer.conv1d_b.weight', 'layers.0.mixer.conv1d_b.bias', 'layers.0.mixer.x_proj_b.weight', 'layers.0.mixer.dt_proj_b.weight', 'layers.0.mixer.dt_proj_b.bias', 'layers.0.mixer.out_proj.weight', 'layers.0.norm.weight', 'layers.1.mixer.A_log', 'layers.1.mixer.D', 'layers.1.mixer.A_b_log', 'layers.1.mixer.D_b', 'layers.1.mixer.in_proj.weight', 'layers.1.mixer.conv1d.weight', 'layers.1.mixer.conv1d.bias', 'layers.1.mixer.x_proj.weight', 'layers.1.mixer.dt_proj.weight', 'layers.1.mixer.dt_proj.bias', 'layers.1.mixer.conv1d_b.weight', 'layers.1.mixer.conv1d_b.bias', 'layers.1.mixer.x_proj_b.weight', 'layers.1.mixer.dt_proj_b.weight', 'layers.1.mixer.dt_proj_b.bias', 'layers.1.mixer.out_proj.weight', 'layers.1.norm.weight', 'layers.2.mixer.A_log', 'layers.2.mixer.D', 'layers.2.mixer.A_b_log', 'layers.2.mixer.D_b', 'layers.2.mixer.in_proj.weight', 'layers.2.mixer.conv1d.weight', 'layers.2.mixer.conv1d.bias', 'layers.2.mixer.x_proj.weight', 'layers.2.mixer.dt_proj.weight', 'layers.2.mixer.dt_proj.bias', 'layers.2.mixer.conv1d_b.weight', 'layers.2.mixer.conv1d_b.bias', 'layers.2.mixer.x_proj_b.weight', 'layers.2.mixer.dt_proj_b.weight', 'layers.2.mixer.dt_proj_b.bias', 'layers.2.mixer.out_proj.weight', 'layers.2.norm.weight', 'layers.3.mixer.A_log', 'layers.3.mixer.D', 'layers.3.mixer.A_b_log', 'layers.3.mixer.D_b', 'layers.3.mixer.in_proj.weight', 'layers.3.mixer.conv1d.weight', 'layers.3.mixer.conv1d.bias', 'layers.3.mixer.x_proj.weight', 'layers.3.mixer.dt_proj.weight', 'layers.3.mixer.dt_proj.bias', 'layers.3.mixer.conv1d_b.weight', 'layers.3.mixer.conv1d_b.bias', 'layers.3.mixer.x_proj_b.weight', 'layers.3.mixer.dt_proj_b.weight', 'layers.3.mixer.dt_proj_b.bias', 'layers.3.mixer.out_proj.weight', 'layers.3.norm.weight', 'layers.4.mixer.A_log', 'layers.4.mixer.D', 'layers.4.mixer.A_b_log', 'layers.4.mixer.D_b', 'layers.4.mixer.in_proj.weight', 'layers.4.mixer.conv1d.weight', 'layers.4.mixer.conv1d.bias', 'layers.4.mixer.x_proj.weight', 'layers.4.mixer.dt_proj.weight', 'layers.4.mixer.dt_proj.bias', 'layers.4.mixer.conv1d_b.weight', 'layers.4.mixer.conv1d_b.bias', 'layers.4.mixer.x_proj_b.weight', 'layers.4.mixer.dt_proj_b.weight', 'layers.4.mixer.dt_proj_b.bias', 'layers.4.mixer.out_proj.weight', 'layers.4.norm.weight', 'layers.5.mixer.A_log', 'layers.5.mixer.D', 'layers.5.mixer.A_b_log', 'layers.5.mixer.D_b', 'layers.5.mixer.in_proj.weight', 'layers.5.mixer.conv1d.weight', 'layers.5.mixer.conv1d.bias', 'layers.5.mixer.x_proj.weight', 'layers.5.mixer.dt_proj.weight', 'layers.5.mixer.dt_proj.bias', 'layers.5.mixer.conv1d_b.weight', 'layers.5.mixer.conv1d_b.bias', 'layers.5.mixer.x_proj_b.weight', 'layers.5.mixer.dt_proj_b.weight', 'layers.5.mixer.dt_proj_b.bias', 'layers.5.mixer.out_proj.weight', 'layers.5.norm.weight', 'layers.6.mixer.A_log', 'layers.6.mixer.D', 'layers.6.mixer.A_b_log', 'layers.6.mixer.D_b', 'layers.6.mixer.in_proj.weight', 'layers.6.mixer.conv1d.weight', 'layers.6.mixer.conv1d.bias', 'layers.6.mixer.x_proj.weight', 'layers.6.mixer.dt_proj.weight', 'layers.6.mixer.dt_proj.bias', 'layers.6.mixer.conv1d_b.weight', 'layers.6.mixer.conv1d_b.bias', 'layers.6.mixer.x_proj_b.weight', 'layers.6.mixer.dt_proj_b.weight', 'layers.6.mixer.dt_proj_b.bias', 'layers.6.mixer.out_proj.weight', 'layers.6.norm.weight', 'layers.7.mixer.A_log', 'layers.7.mixer.D', 'layers.7.mixer.A_b_log', 'layers.7.mixer.D_b', 'layers.7.mixer.in_proj.weight', 'layers.7.mixer.conv1d.weight', 'layers.7.mixer.conv1d.bias', 'layers.7.mixer.x_proj.weight', 'layers.7.mixer.dt_proj.weight', 'layers.7.mixer.dt_proj.bias', 'layers.7.mixer.conv1d_b.weight', 'layers.7.mixer.conv1d_b.bias', 'layers.7.mixer.x_proj_b.weight', 'layers.7.mixer.dt_proj_b.weight', 'layers.7.mixer.dt_proj_b.bias', 'layers.7.mixer.out_proj.weight', 'layers.7.norm.weight', 'layers.8.mixer.A_log', 'layers.8.mixer.D', 'layers.8.mixer.A_b_log', 'layers.8.mixer.D_b', 'layers.8.mixer.in_proj.weight', 'layers.8.mixer.conv1d.weight', 'layers.8.mixer.conv1d.bias', 'layers.8.mixer.x_proj.weight', 'layers.8.mixer.dt_proj.weight', 'layers.8.mixer.dt_proj.bias', 'layers.8.mixer.conv1d_b.weight', 'layers.8.mixer.conv1d_b.bias', 'layers.8.mixer.x_proj_b.weight', 'layers.8.mixer.dt_proj_b.weight', 'layers.8.mixer.dt_proj_b.bias', 'layers.8.mixer.out_proj.weight', 'layers.8.norm.weight', 'layers.9.mixer.A_log', 'layers.9.mixer.D', 'layers.9.mixer.A_b_log', 'layers.9.mixer.D_b', 'layers.9.mixer.in_proj.weight', 'layers.9.mixer.conv1d.weight', 'layers.9.mixer.conv1d.bias', 'layers.9.mixer.x_proj.weight', 'layers.9.mixer.dt_proj.weight', 'layers.9.mixer.dt_proj.bias', 'layers.9.mixer.conv1d_b.weight', 'layers.9.mixer.conv1d_b.bias', 'layers.9.mixer.x_proj_b.weight', 'layers.9.mixer.dt_proj_b.weight', 'layers.9.mixer.dt_proj_b.bias', 'layers.9.mixer.out_proj.weight', 'layers.9.norm.weight', 'layers.10.mixer.A_log', 'layers.10.mixer.D', 'layers.10.mixer.A_b_log', 'layers.10.mixer.D_b', 'layers.10.mixer.in_proj.weight', 'layers.10.mixer.conv1d.weight', 'layers.10.mixer.conv1d.bias', 'layers.10.mixer.x_proj.weight', 'layers.10.mixer.dt_proj.weight', 'layers.10.mixer.dt_proj.bias', 'layers.10.mixer.conv1d_b.weight', 'layers.10.mixer.conv1d_b.bias', 'layers.10.mixer.x_proj_b.weight', 'layers.10.mixer.dt_proj_b.weight', 'layers.10.mixer.dt_proj_b.bias', 'layers.10.mixer.out_proj.weight', 'layers.10.norm.weight', 'layers.11.mixer.A_log', 'layers.11.mixer.D', 'layers.11.mixer.A_b_log', 'layers.11.mixer.D_b', 'layers.11.mixer.in_proj.weight', 'layers.11.mixer.conv1d.weight', 'layers.11.mixer.conv1d.bias', 'layers.11.mixer.x_proj.weight', 'layers.11.mixer.dt_proj.weight', 'layers.11.mixer.dt_proj.bias', 'layers.11.mixer.conv1d_b.weight', 'layers.11.mixer.conv1d_b.bias', 'layers.11.mixer.x_proj_b.weight', 'layers.11.mixer.dt_proj_b.weight', 'layers.11.mixer.dt_proj_b.bias', 'layers.11.mixer.out_proj.weight', 'layers.11.norm.weight', 'layers.12.mixer.A_log', 'layers.12.mixer.D', 'layers.12.mixer.A_b_log', 'layers.12.mixer.D_b', 'layers.12.mixer.in_proj.weight', 'layers.12.mixer.conv1d.weight', 'layers.12.mixer.conv1d.bias', 'layers.12.mixer.x_proj.weight', 'layers.12.mixer.dt_proj.weight', 'layers.12.mixer.dt_proj.bias', 'layers.12.mixer.conv1d_b.weight', 'layers.12.mixer.conv1d_b.bias', 'layers.12.mixer.x_proj_b.weight', 'layers.12.mixer.dt_proj_b.weight', 'layers.12.mixer.dt_proj_b.bias', 'layers.12.mixer.out_proj.weight', 'layers.12.norm.weight', 'layers.13.mixer.A_log', 'layers.13.mixer.D', 'layers.13.mixer.A_b_log', 'layers.13.mixer.D_b', 'layers.13.mixer.in_proj.weight', 'layers.13.mixer.conv1d.weight', 'layers.13.mixer.conv1d.bias', 'layers.13.mixer.x_proj.weight', 'layers.13.mixer.dt_proj.weight', 'layers.13.mixer.dt_proj.bias', 'layers.13.mixer.conv1d_b.weight', 'layers.13.mixer.conv1d_b.bias', 'layers.13.mixer.x_proj_b.weight', 'layers.13.mixer.dt_proj_b.weight', 'layers.13.mixer.dt_proj_b.bias', 'layers.13.mixer.out_proj.weight', 'layers.13.norm.weight', 'layers.14.mixer.A_log', 'layers.14.mixer.D', 'layers.14.mixer.A_b_log', 'layers.14.mixer.D_b', 'layers.14.mixer.in_proj.weight', 'layers.14.mixer.conv1d.weight', 'layers.14.mixer.conv1d.bias', 'layers.14.mixer.x_proj.weight', 'layers.14.mixer.dt_proj.weight', 'layers.14.mixer.dt_proj.bias', 'layers.14.mixer.conv1d_b.weight', 'layers.14.mixer.conv1d_b.bias', 'layers.14.mixer.x_proj_b.weight', 'layers.14.mixer.dt_proj_b.weight', 'layers.14.mixer.dt_proj_b.bias', 'layers.14.mixer.out_proj.weight', 'layers.14.norm.weight', 'layers.15.mixer.A_log', 'layers.15.mixer.D', 'layers.15.mixer.A_b_log', 'layers.15.mixer.D_b', 'layers.15.mixer.in_proj.weight', 'layers.15.mixer.conv1d.weight', 'layers.15.mixer.conv1d.bias', 'layers.15.mixer.x_proj.weight', 'layers.15.mixer.dt_proj.weight', 'layers.15.mixer.dt_proj.bias', 'layers.15.mixer.conv1d_b.weight', 'layers.15.mixer.conv1d_b.bias', 'layers.15.mixer.x_proj_b.weight', 'layers.15.mixer.dt_proj_b.weight', 'layers.15.mixer.dt_proj_b.bias', 'layers.15.mixer.out_proj.weight', 'layers.15.norm.weight', 'layers.16.mixer.A_log', 'layers.16.mixer.D', 'layers.16.mixer.A_b_log', 'layers.16.mixer.D_b', 'layers.16.mixer.in_proj.weight', 'layers.16.mixer.conv1d.weight', 'layers.16.mixer.conv1d.bias', 'layers.16.mixer.x_proj.weight', 'layers.16.mixer.dt_proj.weight', 'layers.16.mixer.dt_proj.bias', 'layers.16.mixer.conv1d_b.weight', 'layers.16.mixer.conv1d_b.bias', 'layers.16.mixer.x_proj_b.weight', 'layers.16.mixer.dt_proj_b.weight', 'layers.16.mixer.dt_proj_b.bias', 'layers.16.mixer.out_proj.weight', 'layers.16.norm.weight', 'layers.17.mixer.A_log', 'layers.17.mixer.D', 'layers.17.mixer.A_b_log', 'layers.17.mixer.D_b', 'layers.17.mixer.in_proj.weight', 'layers.17.mixer.conv1d.weight', 'layers.17.mixer.conv1d.bias', 'layers.17.mixer.x_proj.weight', 'layers.17.mixer.dt_proj.weight', 'layers.17.mixer.dt_proj.bias', 'layers.17.mixer.conv1d_b.weight', 'layers.17.mixer.conv1d_b.bias', 'layers.17.mixer.x_proj_b.weight', 'layers.17.mixer.dt_proj_b.weight', 'layers.17.mixer.dt_proj_b.bias', 'layers.17.mixer.out_proj.weight', 'layers.17.norm.weight', 'layers.18.mixer.A_log', 'layers.18.mixer.D', 'layers.18.mixer.A_b_log', 'layers.18.mixer.D_b', 'layers.18.mixer.in_proj.weight', 'layers.18.mixer.conv1d.weight', 'layers.18.mixer.conv1d.bias', 'layers.18.mixer.x_proj.weight', 'layers.18.mixer.dt_proj.weight', 'layers.18.mixer.dt_proj.bias', 'layers.18.mixer.conv1d_b.weight', 'layers.18.mixer.conv1d_b.bias', 'layers.18.mixer.x_proj_b.weight', 'layers.18.mixer.dt_proj_b.weight', 'layers.18.mixer.dt_proj_b.bias', 'layers.18.mixer.out_proj.weight', 'layers.18.norm.weight', 'layers.19.mixer.A_log', 'layers.19.mixer.D', 'layers.19.mixer.A_b_log', 'layers.19.mixer.D_b', 'layers.19.mixer.in_proj.weight', 'layers.19.mixer.conv1d.weight', 'layers.19.mixer.conv1d.bias', 'layers.19.mixer.x_proj.weight', 'layers.19.mixer.dt_proj.weight', 'layers.19.mixer.dt_proj.bias', 'layers.19.mixer.conv1d_b.weight', 'layers.19.mixer.conv1d_b.bias', 'layers.19.mixer.x_proj_b.weight', 'layers.19.mixer.dt_proj_b.weight', 'layers.19.mixer.dt_proj_b.bias', 'layers.19.mixer.out_proj.weight', 'layers.19.norm.weight', 'layers.20.mixer.A_log', 'layers.20.mixer.D', 'layers.20.mixer.A_b_log', 'layers.20.mixer.D_b', 'layers.20.mixer.in_proj.weight', 'layers.20.mixer.conv1d.weight', 'layers.20.mixer.conv1d.bias', 'layers.20.mixer.x_proj.weight', 'layers.20.mixer.dt_proj.weight', 'layers.20.mixer.dt_proj.bias', 'layers.20.mixer.conv1d_b.weight', 'layers.20.mixer.conv1d_b.bias', 'layers.20.mixer.x_proj_b.weight', 'layers.20.mixer.dt_proj_b.weight', 'layers.20.mixer.dt_proj_b.bias', 'layers.20.mixer.out_proj.weight', 'layers.20.norm.weight', 'layers.21.mixer.A_log', 'layers.21.mixer.D', 'layers.21.mixer.A_b_log', 'layers.21.mixer.D_b', 'layers.21.mixer.in_proj.weight', 'layers.21.mixer.conv1d.weight', 'layers.21.mixer.conv1d.bias', 'layers.21.mixer.x_proj.weight', 'layers.21.mixer.dt_proj.weight', 'layers.21.mixer.dt_proj.bias', 'layers.21.mixer.conv1d_b.weight', 'layers.21.mixer.conv1d_b.bias', 'layers.21.mixer.x_proj_b.weight', 'layers.21.mixer.dt_proj_b.weight', 'layers.21.mixer.dt_proj_b.bias', 'layers.21.mixer.out_proj.weight', 'layers.21.norm.weight', 'layers.22.mixer.A_log', 'layers.22.mixer.D', 'layers.22.mixer.A_b_log', 'layers.22.mixer.D_b', 'layers.22.mixer.in_proj.weight', 'layers.22.mixer.conv1d.weight', 'layers.22.mixer.conv1d.bias', 'layers.22.mixer.x_proj.weight', 'layers.22.mixer.dt_proj.weight', 'layers.22.mixer.dt_proj.bias', 'layers.22.mixer.conv1d_b.weight', 'layers.22.mixer.conv1d_b.bias', 'layers.22.mixer.x_proj_b.weight', 'layers.22.mixer.dt_proj_b.weight', 'layers.22.mixer.dt_proj_b.bias', 'layers.22.mixer.out_proj.weight', 'layers.22.norm.weight', 'layers.23.mixer.A_log', 'layers.23.mixer.D', 'layers.23.mixer.A_b_log', 'layers.23.mixer.D_b', 'layers.23.mixer.in_proj.weight', 'layers.23.mixer.conv1d.weight', 'layers.23.mixer.conv1d.bias', 'layers.23.mixer.x_proj.weight', 'layers.23.mixer.dt_proj.weight', 'layers.23.mixer.dt_proj.bias', 'layers.23.mixer.conv1d_b.weight', 'layers.23.mixer.conv1d_b.bias', 'layers.23.mixer.x_proj_b.weight', 'layers.23.mixer.dt_proj_b.weight', 'layers.23.mixer.dt_proj_b.bias', 'layers.23.mixer.out_proj.weight', 'layers.23.norm.weight', 'norm_f.weight']
number of params: 33044734
ratio_weight= 2.0 reconstruction_weight 0.02 pruning_weight 0.1
Start training for 100 epochs
Epoch: [0]  [   0/1251]  eta: 11:18:18  lr: 0.000001  loss: 7.8899 (7.8899)  time: 32.5329  data: 22.4480  max mem: 19423
Epoch: [0]  [  10/1251]  eta: 1:16:47  lr: 0.000001  loss: 7.8456 (7.8183)  time: 3.7124  data: 2.0412  max mem: 19733
Epoch: [0]  [  20/1251]  eta: 0:47:47  lr: 0.000001  loss: 7.8199 (7.8265)  time: 0.8190  data: 0.0005  max mem: 19733
Epoch: [0]  [  30/1251]  eta: 0:37:26  lr: 0.000001  loss: 7.8028 (7.8052)  time: 0.8094  data: 0.0004  max mem: 19733
Epoch: [0]  [  40/1251]  eta: 0:32:10  lr: 0.000001  loss: 7.7249 (7.7849)  time: 0.8230  data: 0.0004  max mem: 19733
Epoch: [0]  [  50/1251]  eta: 0:28:50  lr: 0.000001  loss: 7.6995 (7.7592)  time: 0.8223  data: 0.0004  max mem: 19733
Epoch: [0]  [  60/1251]  eta: 0:26:36  lr: 0.000001  loss: 7.6603 (7.7419)  time: 0.8188  data: 0.0005  max mem: 19733
Epoch: [0]  [  70/1251]  eta: 0:24:55  lr: 0.000001  loss: 7.6340 (7.7198)  time: 0.8208  data: 0.0005  max mem: 19733
Epoch: [0]  [  80/1251]  eta: 0:23:36  lr: 0.000001  loss: 7.5554 (7.6980)  time: 0.8102  data: 0.0005  max mem: 19733
Epoch: [0]  [  90/1251]  eta: 0:22:32  lr: 0.000001  loss: 7.5426 (7.6829)  time: 0.8067  data: 0.0005  max mem: 19733
loss info: cls_loss=7.0537, ratio_loss=0.8391, pruning_loss=0.2689, mse_loss=1.2839
Epoch: [0]  [ 100/1251]  eta: 0:21:40  lr: 0.000001  loss: 7.5091 (7.6659)  time: 0.8094  data: 0.0005  max mem: 19733
Epoch: [0]  [ 110/1251]  eta: 0:20:56  lr: 0.000001  loss: 7.4833 (7.6468)  time: 0.8106  data: 0.0005  max mem: 19733
Epoch: [0]  [ 120/1251]  eta: 0:20:18  lr: 0.000001  loss: 7.4510 (7.6257)  time: 0.8097  data: 0.0006  max mem: 19733
Epoch: [0]  [ 130/1251]  eta: 0:19:44  lr: 0.000001  loss: 7.3254 (7.6039)  time: 0.8107  data: 0.0005  max mem: 19733
Epoch: [0]  [ 140/1251]  eta: 0:19:14  lr: 0.000001  loss: 7.2854 (7.5775)  time: 0.8108  data: 0.0005  max mem: 19733
Epoch: [0]  [ 150/1251]  eta: 0:18:47  lr: 0.000001  loss: 7.2033 (7.5492)  time: 0.8102  data: 0.0005  max mem: 19733
Epoch: [0]  [ 160/1251]  eta: 0:18:23  lr: 0.000001  loss: 7.1162 (7.5201)  time: 0.8119  data: 0.0005  max mem: 19733
Epoch: [0]  [ 170/1251]  eta: 0:18:00  lr: 0.000001  loss: 7.0930 (7.4911)  time: 0.8147  data: 0.0005  max mem: 19733
Epoch: [0]  [ 180/1251]  eta: 0:17:39  lr: 0.000001  loss: 6.9990 (7.4607)  time: 0.8140  data: 0.0006  max mem: 19733
Epoch: [0]  [ 190/1251]  eta: 0:17:20  lr: 0.000001  loss: 6.9429 (7.4282)  time: 0.8215  data: 0.0006  max mem: 19733
loss info: cls_loss=6.5193, ratio_loss=0.8311, pruning_loss=0.2686, mse_loss=1.3300
Epoch: [0]  [ 200/1251]  eta: 0:17:03  lr: 0.000001  loss: 6.8172 (7.3940)  time: 0.8326  data: 0.0007  max mem: 19733
Epoch: [0]  [ 210/1251]  eta: 0:16:46  lr: 0.000001  loss: 6.8192 (7.3648)  time: 0.8301  data: 0.0007  max mem: 19733
Epoch: [0]  [ 220/1251]  eta: 0:16:29  lr: 0.000001  loss: 6.8463 (7.3357)  time: 0.8199  data: 0.0005  max mem: 19733
Epoch: [0]  [ 230/1251]  eta: 0:16:13  lr: 0.000001  loss: 6.8463 (7.3053)  time: 0.8178  data: 0.0005  max mem: 19733
Epoch: [0]  [ 240/1251]  eta: 0:15:58  lr: 0.000001  loss: 6.6372 (7.2747)  time: 0.8187  data: 0.0005  max mem: 19733
Epoch: [0]  [ 250/1251]  eta: 0:15:43  lr: 0.000001  loss: 6.6372 (7.2407)  time: 0.8142  data: 0.0005  max mem: 19733
Epoch: [0]  [ 260/1251]  eta: 0:15:29  lr: 0.000001  loss: 6.3298 (7.2067)  time: 0.8160  data: 0.0005  max mem: 19733
Epoch: [0]  [ 270/1251]  eta: 0:15:15  lr: 0.000001  loss: 6.3082 (7.1725)  time: 0.8185  data: 0.0005  max mem: 19733
Epoch: [0]  [ 280/1251]  eta: 0:15:02  lr: 0.000001  loss: 6.6220 (7.1520)  time: 0.8177  data: 0.0005  max mem: 19733
Epoch: [0]  [ 290/1251]  eta: 0:14:49  lr: 0.000001  loss: 6.6024 (7.1145)  time: 0.8180  data: 0.0006  max mem: 19733
loss info: cls_loss=5.8549, ratio_loss=0.8245, pruning_loss=0.2765, mse_loss=1.3230
Epoch: [0]  [ 300/1251]  eta: 0:14:36  lr: 0.000001  loss: 5.9762 (7.0805)  time: 0.8152  data: 0.0005  max mem: 19733
Epoch: [0]  [ 310/1251]  eta: 0:14:24  lr: 0.000001  loss: 6.0892 (7.0573)  time: 0.8140  data: 0.0004  max mem: 19733
Epoch: [0]  [ 320/1251]  eta: 0:14:12  lr: 0.000001  loss: 6.7500 (7.0438)  time: 0.8169  data: 0.0005  max mem: 19733
Epoch: [0]  [ 330/1251]  eta: 0:14:00  lr: 0.000001  loss: 6.6765 (7.0199)  time: 0.8254  data: 0.0005  max mem: 19733
Epoch: [0]  [ 340/1251]  eta: 0:13:48  lr: 0.000001  loss: 6.3786 (7.0007)  time: 0.8247  data: 0.0005  max mem: 19733
Epoch: [0]  [ 350/1251]  eta: 0:13:38  lr: 0.000001  loss: 6.3882 (6.9781)  time: 0.8283  data: 0.0005  max mem: 19733
Epoch: [0]  [ 360/1251]  eta: 0:13:26  lr: 0.000001  loss: 6.3882 (6.9534)  time: 0.8296  data: 0.0005  max mem: 19733
Epoch: [0]  [ 370/1251]  eta: 0:13:15  lr: 0.000001  loss: 6.0734 (6.9301)  time: 0.8198  data: 0.0005  max mem: 19733
Epoch: [0]  [ 380/1251]  eta: 0:13:04  lr: 0.000001  loss: 6.0521 (6.9051)  time: 0.8196  data: 0.0004  max mem: 19733
Epoch: [0]  [ 390/1251]  eta: 0:12:53  lr: 0.000001  loss: 5.8960 (6.8782)  time: 0.8186  data: 0.0005  max mem: 19733
loss info: cls_loss=5.6066, ratio_loss=0.8190, pruning_loss=0.2737, mse_loss=1.2795
Epoch: [0]  [ 400/1251]  eta: 0:12:43  lr: 0.000001  loss: 6.2077 (6.8632)  time: 0.8143  data: 0.0005  max mem: 19733
Epoch: [0]  [ 410/1251]  eta: 0:12:32  lr: 0.000001  loss: 6.3462 (6.8505)  time: 0.8103  data: 0.0005  max mem: 19733
Epoch: [0]  [ 420/1251]  eta: 0:12:21  lr: 0.000001  loss: 6.3478 (6.8295)  time: 0.8120  data: 0.0005  max mem: 19733
Epoch: [0]  [ 430/1251]  eta: 0:12:11  lr: 0.000001  loss: 6.3478 (6.8163)  time: 0.8144  data: 0.0005  max mem: 19733
Epoch: [0]  [ 440/1251]  eta: 0:12:01  lr: 0.000001  loss: 6.3096 (6.8018)  time: 0.8136  data: 0.0005  max mem: 19733
Epoch: [0]  [ 450/1251]  eta: 0:11:50  lr: 0.000001  loss: 6.0347 (6.7814)  time: 0.8135  data: 0.0005  max mem: 19733
Epoch: [0]  [ 460/1251]  eta: 0:11:40  lr: 0.000001  loss: 5.7703 (6.7583)  time: 0.8158  data: 0.0006  max mem: 19733
Epoch: [0]  [ 470/1251]  eta: 0:11:30  lr: 0.000001  loss: 5.4808 (6.7302)  time: 0.8146  data: 0.0005  max mem: 19733
Epoch: [0]  [ 480/1251]  eta: 0:11:20  lr: 0.000001  loss: 5.6995 (6.7116)  time: 0.8184  data: 0.0005  max mem: 19733
Epoch: [0]  [ 490/1251]  eta: 0:11:11  lr: 0.000001  loss: 5.9418 (6.7017)  time: 0.8224  data: 0.0004  max mem: 19733
loss info: cls_loss=5.3712, ratio_loss=0.8109, pruning_loss=0.2776, mse_loss=1.2515
Epoch: [0]  [ 500/1251]  eta: 0:11:01  lr: 0.000001  loss: 5.9418 (6.6843)  time: 0.8328  data: 0.0004  max mem: 19733
Epoch: [0]  [ 510/1251]  eta: 0:10:51  lr: 0.000001  loss: 5.9535 (6.6743)  time: 0.8324  data: 0.0004  max mem: 19733
Epoch: [0]  [ 520/1251]  eta: 0:10:42  lr: 0.000001  loss: 6.1692 (6.6598)  time: 0.8205  data: 0.0004  max mem: 19733
Epoch: [0]  [ 530/1251]  eta: 0:10:32  lr: 0.000001  loss: 6.1201 (6.6392)  time: 0.8187  data: 0.0005  max mem: 19733
Epoch: [0]  [ 540/1251]  eta: 0:10:23  lr: 0.000001  loss: 6.1201 (6.6267)  time: 0.8175  data: 0.0005  max mem: 19733
Epoch: [0]  [ 550/1251]  eta: 0:10:13  lr: 0.000001  loss: 5.9732 (6.6116)  time: 0.8185  data: 0.0005  max mem: 19733
Epoch: [0]  [ 560/1251]  eta: 0:10:04  lr: 0.000001  loss: 6.1370 (6.6030)  time: 0.8164  data: 0.0005  max mem: 19733
Epoch: [0]  [ 570/1251]  eta: 0:09:54  lr: 0.000001  loss: 6.2695 (6.5937)  time: 0.8133  data: 0.0005  max mem: 19733
Epoch: [0]  [ 580/1251]  eta: 0:09:45  lr: 0.000001  loss: 6.1652 (6.5813)  time: 0.8160  data: 0.0005  max mem: 19733
Epoch: [0]  [ 590/1251]  eta: 0:09:35  lr: 0.000001  loss: 6.1956 (6.5711)  time: 0.8169  data: 0.0005  max mem: 19733
loss info: cls_loss=5.3494, ratio_loss=0.8061, pruning_loss=0.2782, mse_loss=1.2696
Epoch: [0]  [ 600/1251]  eta: 0:09:26  lr: 0.000001  loss: 6.1331 (6.5567)  time: 0.8132  data: 0.0004  max mem: 19733
Epoch: [0]  [ 610/1251]  eta: 0:09:17  lr: 0.000001  loss: 5.8880 (6.5451)  time: 0.8151  data: 0.0004  max mem: 19733
Epoch: [0]  [ 620/1251]  eta: 0:09:08  lr: 0.000001  loss: 5.9017 (6.5357)  time: 0.8138  data: 0.0005  max mem: 19733
Epoch: [0]  [ 630/1251]  eta: 0:08:58  lr: 0.000001  loss: 6.1125 (6.5247)  time: 0.8183  data: 0.0005  max mem: 19733
Epoch: [0]  [ 640/1251]  eta: 0:08:50  lr: 0.000001  loss: 5.5702 (6.5092)  time: 0.8378  data: 0.0004  max mem: 19733
Epoch: [0]  [ 650/1251]  eta: 0:08:40  lr: 0.000001  loss: 5.6787 (6.4984)  time: 0.8308  data: 0.0005  max mem: 19733
Epoch: [0]  [ 660/1251]  eta: 0:08:31  lr: 0.000001  loss: 5.5738 (6.4798)  time: 0.8139  data: 0.0005  max mem: 19733
Epoch: [0]  [ 670/1251]  eta: 0:08:22  lr: 0.000001  loss: 5.9201 (6.4737)  time: 0.8165  data: 0.0005  max mem: 19733
Epoch: [0]  [ 680/1251]  eta: 0:08:13  lr: 0.000001  loss: 6.0179 (6.4633)  time: 0.8171  data: 0.0004  max mem: 19733
Epoch: [0]  [ 690/1251]  eta: 0:08:04  lr: 0.000001  loss: 5.4462 (6.4482)  time: 0.8150  data: 0.0004  max mem: 19733
loss info: cls_loss=5.1027, ratio_loss=0.8004, pruning_loss=0.2838, mse_loss=1.2387
Epoch: [0]  [ 700/1251]  eta: 0:07:55  lr: 0.000001  loss: 5.4462 (6.4359)  time: 0.8131  data: 0.0005  max mem: 19733
Epoch: [0]  [ 710/1251]  eta: 0:07:46  lr: 0.000001  loss: 6.1044 (6.4237)  time: 0.8142  data: 0.0004  max mem: 19733
Epoch: [0]  [ 720/1251]  eta: 0:07:37  lr: 0.000001  loss: 5.9153 (6.4099)  time: 0.8132  data: 0.0005  max mem: 19733
Epoch: [0]  [ 730/1251]  eta: 0:07:28  lr: 0.000001  loss: 5.6176 (6.4003)  time: 0.8118  data: 0.0006  max mem: 19733
Epoch: [0]  [ 740/1251]  eta: 0:07:19  lr: 0.000001  loss: 5.9446 (6.3967)  time: 0.8121  data: 0.0006  max mem: 19733
Epoch: [0]  [ 750/1251]  eta: 0:07:10  lr: 0.000001  loss: 5.9928 (6.3844)  time: 0.8114  data: 0.0005  max mem: 19733
Epoch: [0]  [ 760/1251]  eta: 0:07:01  lr: 0.000001  loss: 5.9928 (6.3763)  time: 0.8132  data: 0.0005  max mem: 19733
Epoch: [0]  [ 770/1251]  eta: 0:06:52  lr: 0.000001  loss: 6.0179 (6.3673)  time: 0.8160  data: 0.0005  max mem: 19733
Epoch: [0]  [ 780/1251]  eta: 0:06:44  lr: 0.000001  loss: 5.7930 (6.3590)  time: 0.8255  data: 0.0005  max mem: 19733
Epoch: [0]  [ 790/1251]  eta: 0:06:35  lr: 0.000001  loss: 5.5606 (6.3487)  time: 0.8564  data: 0.0004  max mem: 19733
loss info: cls_loss=5.0928, ratio_loss=0.7956, pruning_loss=0.2833, mse_loss=1.2744
Epoch: [0]  [ 800/1251]  eta: 0:06:26  lr: 0.000001  loss: 5.6276 (6.3408)  time: 0.8456  data: 0.0004  max mem: 19733
Epoch: [0]  [ 810/1251]  eta: 0:06:18  lr: 0.000001  loss: 5.2593 (6.3237)  time: 0.8119  data: 0.0005  max mem: 19733
Epoch: [0]  [ 820/1251]  eta: 0:06:09  lr: 0.000001  loss: 5.2889 (6.3177)  time: 0.8119  data: 0.0005  max mem: 19733
Epoch: [0]  [ 830/1251]  eta: 0:06:00  lr: 0.000001  loss: 5.8915 (6.3046)  time: 0.8139  data: 0.0005  max mem: 19733
Epoch: [0]  [ 840/1251]  eta: 0:05:51  lr: 0.000001  loss: 5.1041 (6.2942)  time: 0.8159  data: 0.0005  max mem: 19733
Epoch: [0]  [ 850/1251]  eta: 0:05:43  lr: 0.000001  loss: 5.4628 (6.2841)  time: 0.8163  data: 0.0005  max mem: 19733
Epoch: [0]  [ 860/1251]  eta: 0:05:34  lr: 0.000001  loss: 5.8377 (6.2758)  time: 0.8164  data: 0.0005  max mem: 19733
Epoch: [0]  [ 870/1251]  eta: 0:05:25  lr: 0.000001  loss: 5.5522 (6.2691)  time: 0.8154  data: 0.0005  max mem: 19733
Epoch: [0]  [ 880/1251]  eta: 0:05:16  lr: 0.000001  loss: 5.5120 (6.2571)  time: 0.8125  data: 0.0005  max mem: 19733
Epoch: [0]  [ 890/1251]  eta: 0:05:08  lr: 0.000001  loss: 5.2794 (6.2468)  time: 0.8115  data: 0.0005  max mem: 19733
loss info: cls_loss=4.8524, ratio_loss=0.7891, pruning_loss=0.2894, mse_loss=1.2446
Epoch: [0]  [ 900/1251]  eta: 0:04:59  lr: 0.000001  loss: 5.6527 (6.2400)  time: 0.8124  data: 0.0006  max mem: 19733
Epoch: [0]  [ 910/1251]  eta: 0:04:50  lr: 0.000001  loss: 5.9090 (6.2336)  time: 0.8146  data: 0.0005  max mem: 19733
Epoch: [0]  [ 920/1251]  eta: 0:04:42  lr: 0.000001  loss: 5.9090 (6.2280)  time: 0.8141  data: 0.0005  max mem: 19733
Epoch: [0]  [ 930/1251]  eta: 0:04:33  lr: 0.000001  loss: 5.8926 (6.2251)  time: 0.8314  data: 0.0005  max mem: 19733
Epoch: [0]  [ 940/1251]  eta: 0:04:25  lr: 0.000001  loss: 5.8112 (6.2163)  time: 0.8526  data: 0.0005  max mem: 19733
Epoch: [0]  [ 950/1251]  eta: 0:04:16  lr: 0.000001  loss: 4.9978 (6.2017)  time: 0.8325  data: 0.0005  max mem: 19733
Epoch: [0]  [ 960/1251]  eta: 0:04:07  lr: 0.000001  loss: 5.2900 (6.1959)  time: 0.8140  data: 0.0004  max mem: 19733
Epoch: [0]  [ 970/1251]  eta: 0:03:59  lr: 0.000001  loss: 5.9105 (6.1908)  time: 0.8176  data: 0.0005  max mem: 19733
Epoch: [0]  [ 980/1251]  eta: 0:03:50  lr: 0.000001  loss: 5.7860 (6.1843)  time: 0.8172  data: 0.0005  max mem: 19733
Epoch: [0]  [ 990/1251]  eta: 0:03:41  lr: 0.000001  loss: 5.9049 (6.1781)  time: 0.8171  data: 0.0005  max mem: 19733
loss info: cls_loss=4.9843, ratio_loss=0.7830, pruning_loss=0.2859, mse_loss=1.2732
Epoch: [0]  [1000/1251]  eta: 0:03:33  lr: 0.000001  loss: 5.7480 (6.1712)  time: 0.8165  data: 0.0005  max mem: 19733
Epoch: [0]  [1010/1251]  eta: 0:03:24  lr: 0.000001  loss: 5.7124 (6.1670)  time: 0.8156  data: 0.0004  max mem: 19733
Epoch: [0]  [1020/1251]  eta: 0:03:16  lr: 0.000001  loss: 5.6458 (6.1592)  time: 0.8170  data: 0.0004  max mem: 19733
Epoch: [0]  [1030/1251]  eta: 0:03:07  lr: 0.000001  loss: 5.3820 (6.1488)  time: 0.8174  data: 0.0004  max mem: 19733
Epoch: [0]  [1040/1251]  eta: 0:02:59  lr: 0.000001  loss: 5.3820 (6.1446)  time: 0.8158  data: 0.0005  max mem: 19733
Epoch: [0]  [1050/1251]  eta: 0:02:50  lr: 0.000001  loss: 5.8067 (6.1376)  time: 0.8174  data: 0.0005  max mem: 19733
Epoch: [0]  [1060/1251]  eta: 0:02:41  lr: 0.000001  loss: 5.6994 (6.1336)  time: 0.8181  data: 0.0005  max mem: 19733
Epoch: [0]  [1070/1251]  eta: 0:02:33  lr: 0.000001  loss: 5.6362 (6.1266)  time: 0.8219  data: 0.0004  max mem: 19733
Epoch: [0]  [1080/1251]  eta: 0:02:25  lr: 0.000001  loss: 5.3025 (6.1203)  time: 0.8423  data: 0.0004  max mem: 19733
Epoch: [0]  [1090/1251]  eta: 0:02:16  lr: 0.000001  loss: 5.6344 (6.1142)  time: 0.8463  data: 0.0004  max mem: 19733
loss info: cls_loss=4.8938, ratio_loss=0.7761, pruning_loss=0.2871, mse_loss=1.2493
Epoch: [0]  [1100/1251]  eta: 0:02:07  lr: 0.000001  loss: 5.6344 (6.1077)  time: 0.8272  data: 0.0004  max mem: 19733
Epoch: [0]  [1110/1251]  eta: 0:01:59  lr: 0.000001  loss: 5.7126 (6.1028)  time: 0.8167  data: 0.0004  max mem: 19733
Epoch: [0]  [1120/1251]  eta: 0:01:50  lr: 0.000001  loss: 5.8775 (6.0995)  time: 0.8146  data: 0.0004  max mem: 19733
Epoch: [0]  [1130/1251]  eta: 0:01:42  lr: 0.000001  loss: 5.7523 (6.0955)  time: 0.8154  data: 0.0004  max mem: 19733
Epoch: [0]  [1140/1251]  eta: 0:01:33  lr: 0.000001  loss: 5.5438 (6.0879)  time: 0.8176  data: 0.0005  max mem: 19733
Epoch: [0]  [1150/1251]  eta: 0:01:25  lr: 0.000001  loss: 5.4023 (6.0827)  time: 0.8176  data: 0.0005  max mem: 19733
Epoch: [0]  [1160/1251]  eta: 0:01:16  lr: 0.000001  loss: 5.6927 (6.0769)  time: 0.8165  data: 0.0004  max mem: 19733
Epoch: [0]  [1170/1251]  eta: 0:01:08  lr: 0.000001  loss: 5.6927 (6.0718)  time: 0.8166  data: 0.0004  max mem: 19733
Epoch: [0]  [1180/1251]  eta: 0:01:00  lr: 0.000001  loss: 5.8633 (6.0698)  time: 0.8147  data: 0.0004  max mem: 19733
Epoch: [0]  [1190/1251]  eta: 0:00:51  lr: 0.000001  loss: 5.8633 (6.0665)  time: 0.8114  data: 0.0007  max mem: 19733
loss info: cls_loss=5.0085, ratio_loss=0.7695, pruning_loss=0.2819, mse_loss=1.2310
Epoch: [0]  [1200/1251]  eta: 0:00:43  lr: 0.000001  loss: 5.7174 (6.0630)  time: 0.8039  data: 0.0005  max mem: 19733
Epoch: [0]  [1210/1251]  eta: 0:00:34  lr: 0.000001  loss: 5.6235 (6.0570)  time: 0.7979  data: 0.0001  max mem: 19733
Epoch: [0]  [1220/1251]  eta: 0:00:26  lr: 0.000001  loss: 5.3594 (6.0509)  time: 0.8059  data: 0.0002  max mem: 19733
Epoch: [0]  [1230/1251]  eta: 0:00:17  lr: 0.000001  loss: 5.3594 (6.0457)  time: 0.8336  data: 0.0002  max mem: 19733
Epoch: [0]  [1240/1251]  eta: 0:00:09  lr: 0.000001  loss: 5.4042 (6.0395)  time: 0.8266  data: 0.0002  max mem: 19733
Epoch: [0]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 5.2689 (6.0321)  time: 0.7984  data: 0.0002  max mem: 19733
Epoch: [0] Total time: 0:17:36 (0.8443 s / it)
Averaged stats: lr: 0.000001  loss: 5.2689 (6.0342)
Test:  [  0/261]  eta: 2:15:55  loss: 1.3821 (1.3821)  acc1: 73.4375 (73.4375)  acc5: 91.1458 (91.1458)  time: 31.2482  data: 25.6933  max mem: 19733
Test:  [ 10/261]  eta: 0:13:06  loss: 1.3821 (1.3646)  acc1: 77.6042 (75.4261)  acc5: 94.2708 (91.8087)  time: 3.1334  data: 2.3442  max mem: 19733
Test:  [ 20/261]  eta: 0:07:14  loss: 1.4835 (1.5177)  acc1: 68.2292 (70.1885)  acc5: 90.6250 (90.1290)  time: 0.3300  data: 0.0117  max mem: 19733
Test:  [ 30/261]  eta: 0:05:11  loss: 1.3980 (1.4238)  acc1: 73.4375 (73.0343)  acc5: 90.1042 (90.6418)  time: 0.3672  data: 0.0149  max mem: 19733
Test:  [ 40/261]  eta: 0:04:13  loss: 1.2337 (1.4234)  acc1: 77.0833 (72.9421)  acc5: 93.2292 (90.7012)  time: 0.4586  data: 0.0756  max mem: 19733
Test:  [ 50/261]  eta: 0:03:35  loss: 1.6543 (1.5018)  acc1: 65.1042 (70.2410)  acc5: 86.4583 (89.7876)  time: 0.5079  data: 0.0772  max mem: 19733
Test:  [ 60/261]  eta: 0:03:01  loss: 1.6938 (1.5230)  acc1: 61.9792 (69.3221)  acc5: 88.0208 (89.7370)  time: 0.4075  data: 0.0166  max mem: 19733
Test:  [ 70/261]  eta: 0:02:53  loss: 1.6846 (1.5330)  acc1: 64.0625 (68.6106)  acc5: 90.1042 (89.9575)  time: 0.6352  data: 0.3091  max mem: 19733
Test:  [ 80/261]  eta: 0:02:32  loss: 1.4791 (1.5149)  acc1: 69.2708 (69.1808)  acc5: 92.1875 (90.2263)  time: 0.6519  data: 0.3096  max mem: 19733
Test:  [ 90/261]  eta: 0:02:16  loss: 1.3970 (1.4889)  acc1: 73.9583 (69.6486)  acc5: 92.7083 (90.5907)  time: 0.4011  data: 0.0130  max mem: 19733
Test:  [100/261]  eta: 0:02:05  loss: 1.2550 (1.4776)  acc1: 74.4792 (69.8123)  acc5: 91.6667 (90.6095)  time: 0.5219  data: 0.2239  max mem: 19733
Test:  [110/261]  eta: 0:01:49  loss: 1.3626 (1.4947)  acc1: 71.8750 (69.5571)  acc5: 89.5833 (90.2074)  time: 0.3806  data: 0.2224  max mem: 19733
Test:  [120/261]  eta: 0:01:36  loss: 1.7999 (1.5284)  acc1: 61.9792 (68.7758)  acc5: 83.8542 (89.5833)  time: 0.1939  data: 0.0088  max mem: 19733
Test:  [130/261]  eta: 0:01:31  loss: 1.9566 (1.5694)  acc1: 58.8542 (67.9310)  acc5: 80.7292 (88.8597)  time: 0.5446  data: 0.3487  max mem: 19733
Test:  [140/261]  eta: 0:01:22  loss: 1.8309 (1.5884)  acc1: 57.8125 (67.3131)  acc5: 82.8125 (88.6192)  time: 0.6673  data: 0.3503  max mem: 19733
Test:  [150/261]  eta: 0:01:14  loss: 1.6950 (1.5881)  acc1: 61.9792 (67.4289)  acc5: 86.9792 (88.4899)  time: 0.4932  data: 0.1329  max mem: 19733
Test:  [160/261]  eta: 0:01:04  loss: 1.6345 (1.6103)  acc1: 67.7083 (67.0516)  acc5: 86.4583 (88.0338)  time: 0.3857  data: 0.1332  max mem: 19733
Test:  [170/261]  eta: 0:00:56  loss: 2.0057 (1.6390)  acc1: 56.2500 (66.3651)  acc5: 79.1667 (87.5518)  time: 0.2700  data: 0.0134  max mem: 19733
Test:  [180/261]  eta: 0:00:49  loss: 1.9394 (1.6557)  acc1: 55.2083 (65.9329)  acc5: 80.7292 (87.2640)  time: 0.3326  data: 0.0808  max mem: 19733
Test:  [190/261]  eta: 0:00:41  loss: 1.9394 (1.6711)  acc1: 57.2917 (65.6414)  acc5: 82.2917 (87.0092)  time: 0.2727  data: 0.0794  max mem: 19733
Test:  [200/261]  eta: 0:00:34  loss: 1.9856 (1.6827)  acc1: 59.3750 (65.3452)  acc5: 81.7708 (86.7822)  time: 0.1662  data: 0.0075  max mem: 19733
Test:  [210/261]  eta: 0:00:27  loss: 1.8632 (1.6900)  acc1: 60.9375 (65.2078)  acc5: 82.2917 (86.6262)  time: 0.1576  data: 0.0027  max mem: 19733
Test:  [220/261]  eta: 0:00:21  loss: 1.9492 (1.7144)  acc1: 58.8542 (64.6305)  acc5: 81.7708 (86.2486)  time: 0.1542  data: 0.0026  max mem: 19733
Test:  [230/261]  eta: 0:00:15  loss: 2.0514 (1.7244)  acc1: 52.0833 (64.3669)  acc5: 80.7292 (86.0773)  time: 0.1513  data: 0.0025  max mem: 19733
Test:  [240/261]  eta: 0:00:10  loss: 1.7800 (1.7243)  acc1: 59.3750 (64.2613)  acc5: 82.8125 (86.0974)  time: 0.1450  data: 0.0001  max mem: 19733
Test:  [250/261]  eta: 0:00:05  loss: 1.5475 (1.7111)  acc1: 66.1458 (64.5149)  acc5: 88.5417 (86.2778)  time: 0.1453  data: 0.0002  max mem: 19733
Test:  [260/261]  eta: 0:00:00  loss: 1.4002 (1.7048)  acc1: 67.7083 (64.6740)  acc5: 91.6667 (86.4280)  time: 0.1423  data: 0.0002  max mem: 19733
Test: Total time: 0:02:02 (0.4699 s / it)
* Acc@1 64.674 Acc@5 86.428 loss 1.705
Accuracy of the network on the 50000 test images: 64.7%
Max accuracy: 64.67%
Epoch: [1]  [   0/1251]  eta: 5:40:37  lr: 0.000001  loss: 5.4266 (5.4266)  time: 16.3369  data: 14.4125  max mem: 19734
Epoch: [1]  [  10/1251]  eta: 0:46:58  lr: 0.000001  loss: 5.6800 (5.5378)  time: 2.2711  data: 1.3107  max mem: 19734
Epoch: [1]  [  20/1251]  eta: 0:32:21  lr: 0.000001  loss: 5.6323 (5.5041)  time: 0.8391  data: 0.0005  max mem: 19734
Epoch: [1]  [  30/1251]  eta: 0:27:08  lr: 0.000001  loss: 5.8141 (5.5992)  time: 0.8180  data: 0.0005  max mem: 19734
Epoch: [1]  [  40/1251]  eta: 0:24:20  lr: 0.000001  loss: 5.7786 (5.5632)  time: 0.8171  data: 0.0005  max mem: 19734
loss info: cls_loss=4.8641, ratio_loss=0.7616, pruning_loss=0.2824, mse_loss=1.2262
Epoch: [1]  [  50/1251]  eta: 0:22:36  lr: 0.000001  loss: 5.6419 (5.5474)  time: 0.8121  data: 0.0005  max mem: 19734
Epoch: [1]  [  60/1251]  eta: 0:21:23  lr: 0.000001  loss: 5.4552 (5.4848)  time: 0.8138  data: 0.0005  max mem: 19734
Epoch: [1]  [  70/1251]  eta: 0:20:30  lr: 0.000001  loss: 4.9347 (5.3869)  time: 0.8205  data: 0.0004  max mem: 19734
Epoch: [1]  [  80/1251]  eta: 0:19:47  lr: 0.000001  loss: 5.2176 (5.4118)  time: 0.8214  data: 0.0005  max mem: 19734
Epoch: [1]  [  90/1251]  eta: 0:19:12  lr: 0.000001  loss: 5.8064 (5.4120)  time: 0.8187  data: 0.0005  max mem: 19734
Epoch: [1]  [ 100/1251]  eta: 0:18:43  lr: 0.000001  loss: 5.4535 (5.4253)  time: 0.8194  data: 0.0005  max mem: 19734
Epoch: [1]  [ 110/1251]  eta: 0:18:23  lr: 0.000001  loss: 5.6625 (5.4386)  time: 0.8484  data: 0.0005  max mem: 19734
Epoch: [1]  [ 120/1251]  eta: 0:18:01  lr: 0.000001  loss: 5.6872 (5.4496)  time: 0.8558  data: 0.0004  max mem: 19734
Epoch: [1]  [ 130/1251]  eta: 0:17:39  lr: 0.000001  loss: 5.6728 (5.4553)  time: 0.8246  data: 0.0005  max mem: 19734
Epoch: [1]  [ 140/1251]  eta: 0:17:20  lr: 0.000001  loss: 5.6304 (5.4384)  time: 0.8166  data: 0.0005  max mem: 19734
loss info: cls_loss=4.8236, ratio_loss=0.7529, pruning_loss=0.2815, mse_loss=1.2505
Epoch: [1]  [ 150/1251]  eta: 0:17:02  lr: 0.000001  loss: 5.4699 (5.4402)  time: 0.8200  data: 0.0006  max mem: 19734
Epoch: [1]  [ 160/1251]  eta: 0:16:45  lr: 0.000001  loss: 5.4834 (5.4458)  time: 0.8217  data: 0.0006  max mem: 19734
Epoch: [1]  [ 170/1251]  eta: 0:16:29  lr: 0.000001  loss: 5.4834 (5.4426)  time: 0.8167  data: 0.0004  max mem: 19734
Epoch: [1]  [ 180/1251]  eta: 0:16:14  lr: 0.000001  loss: 5.4588 (5.4331)  time: 0.8135  data: 0.0005  max mem: 19734
Epoch: [1]  [ 190/1251]  eta: 0:16:00  lr: 0.000001  loss: 5.3319 (5.4214)  time: 0.8149  data: 0.0005  max mem: 19734
Epoch: [1]  [ 200/1251]  eta: 0:15:46  lr: 0.000001  loss: 5.5151 (5.4160)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [1]  [ 210/1251]  eta: 0:15:34  lr: 0.000001  loss: 5.6868 (5.4215)  time: 0.8272  data: 0.0005  max mem: 19734
Epoch: [1]  [ 220/1251]  eta: 0:15:22  lr: 0.000001  loss: 5.5532 (5.4111)  time: 0.8309  data: 0.0005  max mem: 19734
Epoch: [1]  [ 230/1251]  eta: 0:15:09  lr: 0.000001  loss: 4.9068 (5.3864)  time: 0.8196  data: 0.0005  max mem: 19734
Epoch: [1]  [ 240/1251]  eta: 0:14:57  lr: 0.000001  loss: 4.8180 (5.3636)  time: 0.8152  data: 0.0006  max mem: 19734
loss info: cls_loss=4.6793, ratio_loss=0.7441, pruning_loss=0.2833, mse_loss=1.2198
Epoch: [1]  [ 250/1251]  eta: 0:14:46  lr: 0.000001  loss: 4.9647 (5.3520)  time: 0.8246  data: 0.0006  max mem: 19734
Epoch: [1]  [ 260/1251]  eta: 0:14:37  lr: 0.000001  loss: 5.3541 (5.3626)  time: 0.8526  data: 0.0005  max mem: 19734
Epoch: [1]  [ 270/1251]  eta: 0:14:26  lr: 0.000001  loss: 5.6189 (5.3668)  time: 0.8522  data: 0.0005  max mem: 19734
Epoch: [1]  [ 280/1251]  eta: 0:14:15  lr: 0.000001  loss: 5.5000 (5.3599)  time: 0.8257  data: 0.0005  max mem: 19734
Epoch: [1]  [ 290/1251]  eta: 0:14:04  lr: 0.000001  loss: 5.5748 (5.3601)  time: 0.8193  data: 0.0006  max mem: 19734
Epoch: [1]  [ 300/1251]  eta: 0:13:53  lr: 0.000001  loss: 5.2559 (5.3439)  time: 0.8205  data: 0.0005  max mem: 19734
Epoch: [1]  [ 310/1251]  eta: 0:13:43  lr: 0.000001  loss: 5.2559 (5.3367)  time: 0.8193  data: 0.0005  max mem: 19734
Epoch: [1]  [ 320/1251]  eta: 0:13:33  lr: 0.000001  loss: 5.0840 (5.3277)  time: 0.8209  data: 0.0005  max mem: 19734
Epoch: [1]  [ 330/1251]  eta: 0:13:22  lr: 0.000001  loss: 5.0840 (5.3204)  time: 0.8188  data: 0.0004  max mem: 19734
Epoch: [1]  [ 340/1251]  eta: 0:13:12  lr: 0.000001  loss: 5.4701 (5.3254)  time: 0.8184  data: 0.0005  max mem: 19734
loss info: cls_loss=4.7363, ratio_loss=0.7350, pruning_loss=0.2829, mse_loss=1.2416
Epoch: [1]  [ 350/1251]  eta: 0:13:02  lr: 0.000001  loss: 5.6054 (5.3331)  time: 0.8184  data: 0.0006  max mem: 19734
Epoch: [1]  [ 360/1251]  eta: 0:12:52  lr: 0.000001  loss: 5.6296 (5.3342)  time: 0.8232  data: 0.0006  max mem: 19734
Epoch: [1]  [ 370/1251]  eta: 0:12:43  lr: 0.000001  loss: 5.3095 (5.3282)  time: 0.8236  data: 0.0005  max mem: 19734
Epoch: [1]  [ 380/1251]  eta: 0:12:33  lr: 0.000001  loss: 5.3877 (5.3303)  time: 0.8174  data: 0.0005  max mem: 19734
Epoch: [1]  [ 390/1251]  eta: 0:12:23  lr: 0.000001  loss: 5.3951 (5.3275)  time: 0.8186  data: 0.0004  max mem: 19734
Epoch: [1]  [ 400/1251]  eta: 0:12:14  lr: 0.000001  loss: 5.0827 (5.3068)  time: 0.8387  data: 0.0006  max mem: 19734
Epoch: [1]  [ 410/1251]  eta: 0:12:06  lr: 0.000001  loss: 4.6162 (5.2953)  time: 0.8629  data: 0.0007  max mem: 19734
Epoch: [1]  [ 420/1251]  eta: 0:11:56  lr: 0.000001  loss: 4.8366 (5.2895)  time: 0.8435  data: 0.0006  max mem: 19734
Epoch: [1]  [ 430/1251]  eta: 0:11:47  lr: 0.000001  loss: 5.5047 (5.2888)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [1]  [ 440/1251]  eta: 0:11:37  lr: 0.000001  loss: 5.6346 (5.2909)  time: 0.8147  data: 0.0006  max mem: 19734
loss info: cls_loss=4.6251, ratio_loss=0.7272, pruning_loss=0.2822, mse_loss=1.2122
Epoch: [1]  [ 450/1251]  eta: 0:11:28  lr: 0.000001  loss: 5.6346 (5.2962)  time: 0.8155  data: 0.0006  max mem: 19734
Epoch: [1]  [ 460/1251]  eta: 0:11:19  lr: 0.000001  loss: 5.4274 (5.2906)  time: 0.8170  data: 0.0005  max mem: 19734
Epoch: [1]  [ 470/1251]  eta: 0:11:09  lr: 0.000001  loss: 5.0198 (5.2853)  time: 0.8203  data: 0.0006  max mem: 19734
Epoch: [1]  [ 480/1251]  eta: 0:11:00  lr: 0.000001  loss: 5.6375 (5.2907)  time: 0.8223  data: 0.0005  max mem: 19734
Epoch: [1]  [ 490/1251]  eta: 0:10:51  lr: 0.000001  loss: 5.6375 (5.2856)  time: 0.8217  data: 0.0005  max mem: 19734
Epoch: [1]  [ 500/1251]  eta: 0:10:42  lr: 0.000001  loss: 4.8014 (5.2782)  time: 0.8176  data: 0.0006  max mem: 19734
Epoch: [1]  [ 510/1251]  eta: 0:10:33  lr: 0.000001  loss: 4.9613 (5.2757)  time: 0.8252  data: 0.0006  max mem: 19734
Epoch: [1]  [ 520/1251]  eta: 0:10:24  lr: 0.000001  loss: 5.2175 (5.2742)  time: 0.8286  data: 0.0005  max mem: 19734
Epoch: [1]  [ 530/1251]  eta: 0:10:15  lr: 0.000001  loss: 5.5272 (5.2769)  time: 0.8203  data: 0.0005  max mem: 19734
Epoch: [1]  [ 540/1251]  eta: 0:10:06  lr: 0.000001  loss: 5.3457 (5.2725)  time: 0.8294  data: 0.0005  max mem: 19734
loss info: cls_loss=4.6629, ratio_loss=0.7155, pruning_loss=0.2809, mse_loss=1.2313
Epoch: [1]  [ 550/1251]  eta: 0:09:58  lr: 0.000001  loss: 5.3036 (5.2747)  time: 0.8461  data: 0.0005  max mem: 19734
Epoch: [1]  [ 560/1251]  eta: 0:09:49  lr: 0.000001  loss: 5.3172 (5.2713)  time: 0.8508  data: 0.0005  max mem: 19734
Epoch: [1]  [ 570/1251]  eta: 0:09:40  lr: 0.000001  loss: 5.0470 (5.2685)  time: 0.8326  data: 0.0004  max mem: 19734
Epoch: [1]  [ 580/1251]  eta: 0:09:31  lr: 0.000001  loss: 5.4526 (5.2725)  time: 0.8172  data: 0.0006  max mem: 19734
Epoch: [1]  [ 590/1251]  eta: 0:09:22  lr: 0.000001  loss: 5.4516 (5.2675)  time: 0.8172  data: 0.0007  max mem: 19734
Epoch: [1]  [ 600/1251]  eta: 0:09:14  lr: 0.000001  loss: 4.9873 (5.2612)  time: 0.8172  data: 0.0005  max mem: 19734
Epoch: [1]  [ 610/1251]  eta: 0:09:05  lr: 0.000001  loss: 4.9873 (5.2595)  time: 0.8163  data: 0.0008  max mem: 19734
Epoch: [1]  [ 620/1251]  eta: 0:08:56  lr: 0.000001  loss: 4.9660 (5.2522)  time: 0.8175  data: 0.0008  max mem: 19734
Epoch: [1]  [ 630/1251]  eta: 0:08:47  lr: 0.000001  loss: 5.4140 (5.2575)  time: 0.8202  data: 0.0004  max mem: 19734
Epoch: [1]  [ 640/1251]  eta: 0:08:38  lr: 0.000001  loss: 5.3789 (5.2489)  time: 0.8207  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5600, ratio_loss=0.7032, pruning_loss=0.2802, mse_loss=1.2341
Epoch: [1]  [ 650/1251]  eta: 0:08:29  lr: 0.000001  loss: 4.9805 (5.2466)  time: 0.8195  data: 0.0004  max mem: 19734
Epoch: [1]  [ 660/1251]  eta: 0:08:21  lr: 0.000001  loss: 5.2885 (5.2468)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [1]  [ 670/1251]  eta: 0:08:12  lr: 0.000001  loss: 4.9413 (5.2368)  time: 0.8206  data: 0.0005  max mem: 19734
Epoch: [1]  [ 680/1251]  eta: 0:08:03  lr: 0.000001  loss: 4.7149 (5.2312)  time: 0.8196  data: 0.0005  max mem: 19734
Epoch: [1]  [ 690/1251]  eta: 0:07:55  lr: 0.000001  loss: 4.8456 (5.2272)  time: 0.8349  data: 0.0005  max mem: 19734
Epoch: [1]  [ 700/1251]  eta: 0:07:46  lr: 0.000001  loss: 5.0798 (5.2222)  time: 0.8520  data: 0.0005  max mem: 19734
Epoch: [1]  [ 710/1251]  eta: 0:07:38  lr: 0.000001  loss: 5.1363 (5.2185)  time: 0.8421  data: 0.0005  max mem: 19734
Epoch: [1]  [ 720/1251]  eta: 0:07:29  lr: 0.000001  loss: 4.9808 (5.2111)  time: 0.8246  data: 0.0005  max mem: 19734
Epoch: [1]  [ 730/1251]  eta: 0:07:21  lr: 0.000001  loss: 4.6235 (5.2059)  time: 0.8200  data: 0.0005  max mem: 19734
Epoch: [1]  [ 740/1251]  eta: 0:07:12  lr: 0.000001  loss: 5.0134 (5.2031)  time: 0.8204  data: 0.0006  max mem: 19734
loss info: cls_loss=4.3848, ratio_loss=0.6922, pruning_loss=0.2893, mse_loss=1.2456
Epoch: [1]  [ 750/1251]  eta: 0:07:03  lr: 0.000001  loss: 5.1306 (5.1988)  time: 0.8165  data: 0.0006  max mem: 19734
Epoch: [1]  [ 760/1251]  eta: 0:06:55  lr: 0.000001  loss: 5.2414 (5.2033)  time: 0.8164  data: 0.0005  max mem: 19734
Epoch: [1]  [ 770/1251]  eta: 0:06:46  lr: 0.000001  loss: 5.4108 (5.2004)  time: 0.8218  data: 0.0005  max mem: 19734
Epoch: [1]  [ 780/1251]  eta: 0:06:37  lr: 0.000001  loss: 5.4210 (5.2017)  time: 0.8217  data: 0.0005  max mem: 19734
Epoch: [1]  [ 790/1251]  eta: 0:06:29  lr: 0.000001  loss: 5.4912 (5.2004)  time: 0.8188  data: 0.0005  max mem: 19734
Epoch: [1]  [ 800/1251]  eta: 0:06:20  lr: 0.000001  loss: 5.0335 (5.1971)  time: 0.8263  data: 0.0005  max mem: 19734
Epoch: [1]  [ 810/1251]  eta: 0:06:12  lr: 0.000001  loss: 4.8849 (5.1910)  time: 0.8235  data: 0.0006  max mem: 19734
Epoch: [1]  [ 820/1251]  eta: 0:06:03  lr: 0.000001  loss: 4.9898 (5.1927)  time: 0.8173  data: 0.0006  max mem: 19734
Epoch: [1]  [ 830/1251]  eta: 0:05:55  lr: 0.000001  loss: 5.0429 (5.1874)  time: 0.8198  data: 0.0005  max mem: 19734
Epoch: [1]  [ 840/1251]  eta: 0:05:46  lr: 0.000001  loss: 5.0669 (5.1896)  time: 0.8467  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5855, ratio_loss=0.6795, pruning_loss=0.2751, mse_loss=1.2008
Epoch: [1]  [ 850/1251]  eta: 0:05:38  lr: 0.000001  loss: 5.3094 (5.1864)  time: 0.8695  data: 0.0004  max mem: 19734
Epoch: [1]  [ 860/1251]  eta: 0:05:29  lr: 0.000001  loss: 5.1608 (5.1853)  time: 0.8443  data: 0.0005  max mem: 19734
Epoch: [1]  [ 870/1251]  eta: 0:05:21  lr: 0.000001  loss: 5.3905 (5.1833)  time: 0.8227  data: 0.0005  max mem: 19734
Epoch: [1]  [ 880/1251]  eta: 0:05:12  lr: 0.000001  loss: 5.2726 (5.1832)  time: 0.8174  data: 0.0004  max mem: 19734
Epoch: [1]  [ 890/1251]  eta: 0:05:04  lr: 0.000001  loss: 5.4111 (5.1843)  time: 0.8150  data: 0.0005  max mem: 19734
Epoch: [1]  [ 900/1251]  eta: 0:04:55  lr: 0.000001  loss: 5.2055 (5.1815)  time: 0.8208  data: 0.0005  max mem: 19734
Epoch: [1]  [ 910/1251]  eta: 0:04:47  lr: 0.000001  loss: 5.1574 (5.1806)  time: 0.8220  data: 0.0005  max mem: 19734
Epoch: [1]  [ 920/1251]  eta: 0:04:38  lr: 0.000001  loss: 5.2469 (5.1800)  time: 0.8188  data: 0.0005  max mem: 19734
Epoch: [1]  [ 930/1251]  eta: 0:04:30  lr: 0.000001  loss: 5.1226 (5.1783)  time: 0.8204  data: 0.0005  max mem: 19734
Epoch: [1]  [ 940/1251]  eta: 0:04:21  lr: 0.000001  loss: 5.3195 (5.1779)  time: 0.8231  data: 0.0005  max mem: 19734
loss info: cls_loss=4.5628, ratio_loss=0.6647, pruning_loss=0.2763, mse_loss=1.2218
Epoch: [1]  [ 950/1251]  eta: 0:04:13  lr: 0.000001  loss: 5.1182 (5.1735)  time: 0.8254  data: 0.0005  max mem: 19734
Epoch: [1]  [ 960/1251]  eta: 0:04:04  lr: 0.000001  loss: 4.8533 (5.1710)  time: 0.8210  data: 0.0005  max mem: 19734
Epoch: [1]  [ 970/1251]  eta: 0:03:56  lr: 0.000001  loss: 4.7633 (5.1658)  time: 0.8157  data: 0.0005  max mem: 19734
Epoch: [1]  [ 980/1251]  eta: 0:03:47  lr: 0.000001  loss: 4.5635 (5.1605)  time: 0.8293  data: 0.0004  max mem: 19734
Epoch: [1]  [ 990/1251]  eta: 0:03:39  lr: 0.000001  loss: 4.7692 (5.1575)  time: 0.8477  data: 0.0004  max mem: 19734
Epoch: [1]  [1000/1251]  eta: 0:03:31  lr: 0.000001  loss: 4.7692 (5.1539)  time: 0.8554  data: 0.0005  max mem: 19734
Epoch: [1]  [1010/1251]  eta: 0:03:22  lr: 0.000001  loss: 4.7134 (5.1502)  time: 0.8391  data: 0.0004  max mem: 19734
Epoch: [1]  [1020/1251]  eta: 0:03:14  lr: 0.000001  loss: 4.7973 (5.1488)  time: 0.8187  data: 0.0005  max mem: 19734
Epoch: [1]  [1030/1251]  eta: 0:03:05  lr: 0.000001  loss: 5.1563 (5.1485)  time: 0.8169  data: 0.0005  max mem: 19734
Epoch: [1]  [1040/1251]  eta: 0:02:57  lr: 0.000001  loss: 5.1563 (5.1459)  time: 0.8192  data: 0.0004  max mem: 19734
loss info: cls_loss=4.3613, ratio_loss=0.6487, pruning_loss=0.2802, mse_loss=1.2304
Epoch: [1]  [1050/1251]  eta: 0:02:48  lr: 0.000001  loss: 4.7396 (5.1403)  time: 0.8209  data: 0.0004  max mem: 19734
Epoch: [1]  [1060/1251]  eta: 0:02:40  lr: 0.000001  loss: 4.8970 (5.1380)  time: 0.8202  data: 0.0005  max mem: 19734
Epoch: [1]  [1070/1251]  eta: 0:02:31  lr: 0.000001  loss: 5.1400 (5.1381)  time: 0.8159  data: 0.0006  max mem: 19734
Epoch: [1]  [1080/1251]  eta: 0:02:23  lr: 0.000001  loss: 5.3376 (5.1388)  time: 0.8135  data: 0.0006  max mem: 19734
Epoch: [1]  [1090/1251]  eta: 0:02:15  lr: 0.000001  loss: 5.1256 (5.1358)  time: 0.8132  data: 0.0006  max mem: 19734
Epoch: [1]  [1100/1251]  eta: 0:02:06  lr: 0.000001  loss: 4.8826 (5.1336)  time: 0.8190  data: 0.0005  max mem: 19734
Epoch: [1]  [1110/1251]  eta: 0:01:58  lr: 0.000001  loss: 4.8567 (5.1299)  time: 0.8218  data: 0.0005  max mem: 19734
Epoch: [1]  [1120/1251]  eta: 0:01:49  lr: 0.000001  loss: 4.8175 (5.1267)  time: 0.8157  data: 0.0007  max mem: 19734
Epoch: [1]  [1130/1251]  eta: 0:01:41  lr: 0.000001  loss: 4.9753 (5.1253)  time: 0.8237  data: 0.0008  max mem: 19734
Epoch: [1]  [1140/1251]  eta: 0:01:33  lr: 0.000001  loss: 4.8954 (5.1206)  time: 0.8447  data: 0.0006  max mem: 19734
loss info: cls_loss=4.3989, ratio_loss=0.6341, pruning_loss=0.2778, mse_loss=1.2227
Epoch: [1]  [1150/1251]  eta: 0:01:24  lr: 0.000001  loss: 4.5851 (5.1194)  time: 0.8435  data: 0.0005  max mem: 19734
Epoch: [1]  [1160/1251]  eta: 0:01:16  lr: 0.000001  loss: 5.2425 (5.1186)  time: 0.8202  data: 0.0005  max mem: 19734
Epoch: [1]  [1170/1251]  eta: 0:01:07  lr: 0.000001  loss: 4.8094 (5.1137)  time: 0.8112  data: 0.0005  max mem: 19734
Epoch: [1]  [1180/1251]  eta: 0:00:59  lr: 0.000001  loss: 4.9844 (5.1122)  time: 0.8146  data: 0.0005  max mem: 19734
Epoch: [1]  [1190/1251]  eta: 0:00:51  lr: 0.000001  loss: 5.0896 (5.1109)  time: 0.8138  data: 0.0011  max mem: 19734
Epoch: [1]  [1200/1251]  eta: 0:00:42  lr: 0.000001  loss: 5.0972 (5.1111)  time: 0.8046  data: 0.0009  max mem: 19734
Epoch: [1]  [1210/1251]  eta: 0:00:34  lr: 0.000001  loss: 5.2515 (5.1083)  time: 0.7977  data: 0.0001  max mem: 19734
Epoch: [1]  [1220/1251]  eta: 0:00:25  lr: 0.000001  loss: 5.2329 (5.1055)  time: 0.7987  data: 0.0001  max mem: 19734
Epoch: [1]  [1230/1251]  eta: 0:00:17  lr: 0.000001  loss: 4.5744 (5.1020)  time: 0.8013  data: 0.0002  max mem: 19734
Epoch: [1]  [1240/1251]  eta: 0:00:09  lr: 0.000001  loss: 5.0823 (5.1028)  time: 0.8067  data: 0.0002  max mem: 19734
loss info: cls_loss=4.4134, ratio_loss=0.6173, pruning_loss=0.2744, mse_loss=1.2043
Epoch: [1]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 5.0697 (5.0980)  time: 0.8057  data: 0.0001  max mem: 19734
Epoch: [1] Total time: 0:17:27 (0.8371 s / it)
Averaged stats: lr: 0.000001  loss: 5.0697 (5.1031)
Test:  [  0/261]  eta: 2:15:34  loss: 1.4665 (1.4665)  acc1: 70.3125 (70.3125)  acc5: 90.1042 (90.1042)  time: 31.1664  data: 30.7017  max mem: 19734
Test:  [ 10/261]  eta: 0:13:35  loss: 1.3676 (1.3907)  acc1: 75.5208 (72.7746)  acc5: 91.1458 (90.4356)  time: 3.2509  data: 2.8149  max mem: 19734
Test:  [ 20/261]  eta: 0:07:18  loss: 1.5307 (1.5721)  acc1: 65.6250 (67.8323)  acc5: 88.5417 (88.0456)  time: 0.3525  data: 0.0212  max mem: 19734
Test:  [ 30/261]  eta: 0:05:11  loss: 1.5216 (1.4850)  acc1: 69.2708 (70.2789)  acc5: 87.5000 (88.7097)  time: 0.3049  data: 0.0159  max mem: 19734
Test:  [ 40/261]  eta: 0:04:26  loss: 1.2977 (1.4843)  acc1: 74.4792 (70.1474)  acc5: 90.6250 (88.7449)  time: 0.5638  data: 0.1826  max mem: 19734
Test:  [ 50/261]  eta: 0:03:45  loss: 1.7429 (1.5728)  acc1: 63.0208 (67.3815)  acc5: 85.4167 (88.0617)  time: 0.6354  data: 0.1821  max mem: 19734
Test:  [ 60/261]  eta: 0:03:12  loss: 1.7957 (1.6039)  acc1: 58.8542 (66.4020)  acc5: 85.4167 (87.9013)  time: 0.4412  data: 0.0139  max mem: 19734
Test:  [ 70/261]  eta: 0:02:48  loss: 1.7799 (1.6134)  acc1: 62.5000 (65.9404)  acc5: 89.0625 (88.1749)  time: 0.4108  data: 0.0101  max mem: 19734
Test:  [ 80/261]  eta: 0:02:26  loss: 1.5533 (1.5945)  acc1: 66.6667 (66.4288)  acc5: 89.5833 (88.4259)  time: 0.3603  data: 0.0257  max mem: 19734
Test:  [ 90/261]  eta: 0:02:08  loss: 1.4313 (1.5654)  acc1: 71.3542 (67.0215)  acc5: 89.5833 (88.8565)  time: 0.2683  data: 0.0277  max mem: 19734
Test:  [100/261]  eta: 0:01:56  loss: 1.3746 (1.5561)  acc1: 71.3542 (67.2545)  acc5: 90.6250 (88.8872)  time: 0.3673  data: 0.1100  max mem: 19734
Test:  [110/261]  eta: 0:01:43  loss: 1.4465 (1.5686)  acc1: 69.7917 (67.0984)  acc5: 87.5000 (88.5557)  time: 0.3895  data: 0.1111  max mem: 19734
Test:  [120/261]  eta: 0:01:31  loss: 1.8098 (1.5965)  acc1: 62.5000 (66.5031)  acc5: 81.7708 (87.9907)  time: 0.2656  data: 0.0116  max mem: 19734
Test:  [130/261]  eta: 0:01:31  loss: 1.9554 (1.6337)  acc1: 55.2083 (65.7284)  acc5: 80.7292 (87.3648)  time: 0.7788  data: 0.5306  max mem: 19734
Test:  [140/261]  eta: 0:01:21  loss: 1.9655 (1.6491)  acc1: 55.2083 (65.2741)  acc5: 82.8125 (87.1454)  time: 0.8648  data: 0.5307  max mem: 19734
Test:  [150/261]  eta: 0:01:11  loss: 1.6959 (1.6464)  acc1: 63.5417 (65.3801)  acc5: 84.8958 (87.0378)  time: 0.3107  data: 0.0130  max mem: 19734
Test:  [160/261]  eta: 0:01:03  loss: 1.6896 (1.6665)  acc1: 64.0625 (65.0168)  acc5: 83.8542 (86.6751)  time: 0.2652  data: 0.0139  max mem: 19734
Test:  [170/261]  eta: 0:00:57  loss: 1.9566 (1.6941)  acc1: 55.7292 (64.4158)  acc5: 79.6875 (86.2451)  time: 0.4856  data: 0.2251  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: 2.0141 (1.7119)  acc1: 54.6875 (64.0452)  acc5: 79.6875 (86.0238)  time: 0.4441  data: 0.2240  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: 1.9975 (1.7250)  acc1: 57.8125 (63.7789)  acc5: 81.2500 (85.7875)  time: 0.2052  data: 0.0108  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.9952 (1.7364)  acc1: 58.8542 (63.5391)  acc5: 81.2500 (85.5592)  time: 0.2348  data: 0.0716  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.9472 (1.7431)  acc1: 59.3750 (63.4750)  acc5: 81.7708 (85.3994)  time: 0.2196  data: 0.0684  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.9838 (1.7661)  acc1: 58.3333 (62.9101)  acc5: 81.7708 (85.0561)  time: 0.1536  data: 0.0028  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 2.1158 (1.7747)  acc1: 52.0833 (62.7119)  acc5: 78.6458 (84.8936)  time: 0.1474  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.8342 (1.7746)  acc1: 58.8542 (62.6297)  acc5: 82.8125 (84.8894)  time: 0.1454  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.5405 (1.7586)  acc1: 65.6250 (62.9378)  acc5: 87.5000 (85.1158)  time: 0.1452  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.3788 (1.7509)  acc1: 67.7083 (63.1180)  acc5: 90.6250 (85.2640)  time: 0.1410  data: 0.0001  max mem: 19734
Test: Total time: 0:02:03 (0.4726 s / it)
* Acc@1 63.118 Acc@5 85.264 loss 1.751
Accuracy of the network on the 50000 test images: 63.1%
Max accuracy: 64.67%
Epoch: [2]  [   0/1251]  eta: 6:26:48  lr: 0.000003  loss: 4.4279 (4.4279)  time: 18.5519  data: 14.8011  max mem: 19734
Epoch: [2]  [  10/1251]  eta: 0:53:03  lr: 0.000003  loss: 4.4279 (4.7165)  time: 2.5649  data: 1.3469  max mem: 19734
Epoch: [2]  [  20/1251]  eta: 0:35:57  lr: 0.000003  loss: 4.8861 (4.8253)  time: 0.9128  data: 0.0010  max mem: 19734
Epoch: [2]  [  30/1251]  eta: 0:29:43  lr: 0.000003  loss: 5.1868 (4.9010)  time: 0.8530  data: 0.0005  max mem: 19734
Epoch: [2]  [  40/1251]  eta: 0:26:20  lr: 0.000003  loss: 5.1851 (4.8788)  time: 0.8351  data: 0.0004  max mem: 19734
Epoch: [2]  [  50/1251]  eta: 0:24:11  lr: 0.000003  loss: 5.1851 (4.9160)  time: 0.8184  data: 0.0006  max mem: 19734
Epoch: [2]  [  60/1251]  eta: 0:22:42  lr: 0.000003  loss: 5.2104 (4.9535)  time: 0.8130  data: 0.0005  max mem: 19734
Epoch: [2]  [  70/1251]  eta: 0:21:35  lr: 0.000003  loss: 5.1589 (4.9467)  time: 0.8120  data: 0.0003  max mem: 19734
Epoch: [2]  [  80/1251]  eta: 0:20:43  lr: 0.000003  loss: 4.8527 (4.9330)  time: 0.8120  data: 0.0004  max mem: 19734
Epoch: [2]  [  90/1251]  eta: 0:20:01  lr: 0.000003  loss: 4.8320 (4.8955)  time: 0.8150  data: 0.0004  max mem: 19734
loss info: cls_loss=4.4387, ratio_loss=0.5811, pruning_loss=0.2679, mse_loss=1.2267
Epoch: [2]  [ 100/1251]  eta: 0:19:27  lr: 0.000003  loss: 4.8669 (4.8871)  time: 0.8222  data: 0.0004  max mem: 19734
Epoch: [2]  [ 110/1251]  eta: 0:18:56  lr: 0.000003  loss: 4.6082 (4.8338)  time: 0.8201  data: 0.0004  max mem: 19734
Epoch: [2]  [ 120/1251]  eta: 0:18:29  lr: 0.000003  loss: 4.6563 (4.8365)  time: 0.8115  data: 0.0003  max mem: 19734
Epoch: [2]  [ 130/1251]  eta: 0:18:05  lr: 0.000003  loss: 4.8290 (4.8247)  time: 0.8119  data: 0.0003  max mem: 19734
Epoch: [2]  [ 140/1251]  eta: 0:17:43  lr: 0.000003  loss: 4.8290 (4.8251)  time: 0.8140  data: 0.0004  max mem: 19734
Epoch: [2]  [ 150/1251]  eta: 0:17:23  lr: 0.000003  loss: 4.8579 (4.7979)  time: 0.8164  data: 0.0004  max mem: 19734
Epoch: [2]  [ 160/1251]  eta: 0:17:07  lr: 0.000003  loss: 4.8579 (4.7998)  time: 0.8318  data: 0.0004  max mem: 19734
Epoch: [2]  [ 170/1251]  eta: 0:16:50  lr: 0.000003  loss: 4.8365 (4.7895)  time: 0.8366  data: 0.0004  max mem: 19734
Epoch: [2]  [ 180/1251]  eta: 0:16:36  lr: 0.000003  loss: 4.8840 (4.7916)  time: 0.8421  data: 0.0004  max mem: 19734
Epoch: [2]  [ 190/1251]  eta: 0:16:20  lr: 0.000003  loss: 5.0309 (4.7878)  time: 0.8348  data: 0.0004  max mem: 19734
loss info: cls_loss=4.2981, ratio_loss=0.5180, pruning_loss=0.2632, mse_loss=1.2004
Epoch: [2]  [ 200/1251]  eta: 0:16:05  lr: 0.000003  loss: 4.7821 (4.7897)  time: 0.8146  data: 0.0004  max mem: 19734
Epoch: [2]  [ 210/1251]  eta: 0:15:51  lr: 0.000003  loss: 4.7996 (4.7901)  time: 0.8155  data: 0.0004  max mem: 19734
Epoch: [2]  [ 220/1251]  eta: 0:15:37  lr: 0.000003  loss: 4.7315 (4.7765)  time: 0.8136  data: 0.0004  max mem: 19734
Epoch: [2]  [ 230/1251]  eta: 0:15:24  lr: 0.000003  loss: 4.7315 (4.7796)  time: 0.8123  data: 0.0004  max mem: 19734
Epoch: [2]  [ 240/1251]  eta: 0:15:11  lr: 0.000003  loss: 4.5384 (4.7545)  time: 0.8135  data: 0.0005  max mem: 19734
Epoch: [2]  [ 250/1251]  eta: 0:14:59  lr: 0.000003  loss: 4.2884 (4.7408)  time: 0.8210  data: 0.0005  max mem: 19734
Epoch: [2]  [ 260/1251]  eta: 0:14:47  lr: 0.000003  loss: 4.5600 (4.7286)  time: 0.8224  data: 0.0005  max mem: 19734
Epoch: [2]  [ 270/1251]  eta: 0:14:35  lr: 0.000003  loss: 4.8490 (4.7322)  time: 0.8153  data: 0.0004  max mem: 19734
Epoch: [2]  [ 280/1251]  eta: 0:14:24  lr: 0.000003  loss: 4.7715 (4.7178)  time: 0.8164  data: 0.0004  max mem: 19734
Epoch: [2]  [ 290/1251]  eta: 0:14:12  lr: 0.000003  loss: 4.0633 (4.6909)  time: 0.8182  data: 0.0004  max mem: 19734
loss info: cls_loss=4.1272, ratio_loss=0.4443, pruning_loss=0.2642, mse_loss=1.1995
Epoch: [2]  [ 300/1251]  eta: 0:14:01  lr: 0.000003  loss: 4.0633 (4.6793)  time: 0.8152  data: 0.0004  max mem: 19734
Epoch: [2]  [ 310/1251]  eta: 0:13:51  lr: 0.000003  loss: 4.5628 (4.6700)  time: 0.8304  data: 0.0003  max mem: 19734
Epoch: [2]  [ 320/1251]  eta: 0:13:42  lr: 0.000003  loss: 4.4819 (4.6639)  time: 0.8539  data: 0.0004  max mem: 19734
Epoch: [2]  [ 330/1251]  eta: 0:13:31  lr: 0.000003  loss: 4.2656 (4.6517)  time: 0.8419  data: 0.0004  max mem: 19734
Epoch: [2]  [ 340/1251]  eta: 0:13:21  lr: 0.000003  loss: 4.0026 (4.6389)  time: 0.8217  data: 0.0004  max mem: 19734
Epoch: [2]  [ 350/1251]  eta: 0:13:10  lr: 0.000003  loss: 4.0026 (4.6243)  time: 0.8189  data: 0.0005  max mem: 19734
Epoch: [2]  [ 360/1251]  eta: 0:13:00  lr: 0.000003  loss: 3.9411 (4.5951)  time: 0.8162  data: 0.0006  max mem: 19734
Epoch: [2]  [ 370/1251]  eta: 0:12:50  lr: 0.000003  loss: 3.7586 (4.5833)  time: 0.8146  data: 0.0005  max mem: 19734
Epoch: [2]  [ 380/1251]  eta: 0:12:40  lr: 0.000003  loss: 4.3906 (4.5746)  time: 0.8163  data: 0.0005  max mem: 19734
Epoch: [2]  [ 390/1251]  eta: 0:12:30  lr: 0.000003  loss: 4.2586 (4.5596)  time: 0.8166  data: 0.0005  max mem: 19734
loss info: cls_loss=3.8882, ratio_loss=0.3634, pruning_loss=0.2638, mse_loss=1.2227
Epoch: [2]  [ 400/1251]  eta: 0:12:20  lr: 0.000003  loss: 4.2420 (4.5519)  time: 0.8219  data: 0.0004  max mem: 19734
Epoch: [2]  [ 410/1251]  eta: 0:12:10  lr: 0.000003  loss: 4.2482 (4.5393)  time: 0.8217  data: 0.0005  max mem: 19734
Epoch: [2]  [ 420/1251]  eta: 0:12:01  lr: 0.000003  loss: 4.2482 (4.5329)  time: 0.8168  data: 0.0005  max mem: 19734
Epoch: [2]  [ 430/1251]  eta: 0:11:51  lr: 0.000003  loss: 4.4309 (4.5294)  time: 0.8165  data: 0.0004  max mem: 19734
Epoch: [2]  [ 440/1251]  eta: 0:11:41  lr: 0.000003  loss: 4.5097 (4.5262)  time: 0.8158  data: 0.0004  max mem: 19734
Epoch: [2]  [ 450/1251]  eta: 0:11:32  lr: 0.000003  loss: 4.1595 (4.5110)  time: 0.8302  data: 0.0004  max mem: 19734
Epoch: [2]  [ 460/1251]  eta: 0:11:23  lr: 0.000003  loss: 3.9319 (4.4992)  time: 0.8336  data: 0.0004  max mem: 19734
Epoch: [2]  [ 470/1251]  eta: 0:11:14  lr: 0.000003  loss: 4.2112 (4.4952)  time: 0.8389  data: 0.0004  max mem: 19734
Epoch: [2]  [ 480/1251]  eta: 0:11:05  lr: 0.000003  loss: 4.2699 (4.4842)  time: 0.8420  data: 0.0005  max mem: 19734
Epoch: [2]  [ 490/1251]  eta: 0:10:56  lr: 0.000003  loss: 4.3435 (4.4831)  time: 0.8232  data: 0.0005  max mem: 19734
loss info: cls_loss=3.9190, ratio_loss=0.2792, pruning_loss=0.2559, mse_loss=1.1956
Epoch: [2]  [ 500/1251]  eta: 0:10:46  lr: 0.000003  loss: 4.3965 (4.4749)  time: 0.8168  data: 0.0005  max mem: 19734
Epoch: [2]  [ 510/1251]  eta: 0:10:37  lr: 0.000003  loss: 4.2042 (4.4693)  time: 0.8168  data: 0.0005  max mem: 19734
Epoch: [2]  [ 520/1251]  eta: 0:10:28  lr: 0.000003  loss: 4.1043 (4.4602)  time: 0.8165  data: 0.0005  max mem: 19734
Epoch: [2]  [ 530/1251]  eta: 0:10:19  lr: 0.000003  loss: 4.1010 (4.4488)  time: 0.8165  data: 0.0006  max mem: 19734
Epoch: [2]  [ 540/1251]  eta: 0:10:09  lr: 0.000003  loss: 4.1871 (4.4426)  time: 0.8166  data: 0.0005  max mem: 19734
Epoch: [2]  [ 550/1251]  eta: 0:10:01  lr: 0.000003  loss: 4.2601 (4.4377)  time: 0.8259  data: 0.0007  max mem: 19734
Epoch: [2]  [ 560/1251]  eta: 0:09:51  lr: 0.000003  loss: 4.0535 (4.4278)  time: 0.8246  data: 0.0006  max mem: 19734
Epoch: [2]  [ 570/1251]  eta: 0:09:42  lr: 0.000003  loss: 3.9871 (4.4199)  time: 0.8141  data: 0.0007  max mem: 19734
Epoch: [2]  [ 580/1251]  eta: 0:09:33  lr: 0.000003  loss: 3.9871 (4.4085)  time: 0.8124  data: 0.0007  max mem: 19734
Epoch: [2]  [ 590/1251]  eta: 0:09:24  lr: 0.000003  loss: 4.0095 (4.4012)  time: 0.8139  data: 0.0006  max mem: 19734
loss info: cls_loss=3.8246, ratio_loss=0.2030, pruning_loss=0.2557, mse_loss=1.2330
Epoch: [2]  [ 600/1251]  eta: 0:09:16  lr: 0.000003  loss: 4.2098 (4.3943)  time: 0.8315  data: 0.0006  max mem: 19734
Epoch: [2]  [ 610/1251]  eta: 0:09:07  lr: 0.000003  loss: 4.0890 (4.3865)  time: 0.8379  data: 0.0005  max mem: 19734
Epoch: [2]  [ 620/1251]  eta: 0:08:59  lr: 0.000003  loss: 3.9955 (4.3814)  time: 0.8588  data: 0.0005  max mem: 19734
Epoch: [2]  [ 630/1251]  eta: 0:08:50  lr: 0.000003  loss: 3.9968 (4.3759)  time: 0.8515  data: 0.0005  max mem: 19734
Epoch: [2]  [ 640/1251]  eta: 0:08:41  lr: 0.000003  loss: 3.6634 (4.3598)  time: 0.8157  data: 0.0005  max mem: 19734
Epoch: [2]  [ 650/1251]  eta: 0:08:32  lr: 0.000003  loss: 3.5685 (4.3522)  time: 0.8157  data: 0.0005  max mem: 19734
Epoch: [2]  [ 660/1251]  eta: 0:08:23  lr: 0.000003  loss: 4.0167 (4.3442)  time: 0.8176  data: 0.0006  max mem: 19734
Epoch: [2]  [ 670/1251]  eta: 0:08:14  lr: 0.000003  loss: 3.7643 (4.3369)  time: 0.8165  data: 0.0006  max mem: 19734
Epoch: [2]  [ 680/1251]  eta: 0:08:06  lr: 0.000003  loss: 3.9010 (4.3298)  time: 0.8156  data: 0.0006  max mem: 19734
Epoch: [2]  [ 690/1251]  eta: 0:07:57  lr: 0.000003  loss: 3.8863 (4.3208)  time: 0.8276  data: 0.0007  max mem: 19734
loss info: cls_loss=3.6877, ratio_loss=0.1407, pruning_loss=0.2520, mse_loss=1.2068
Epoch: [2]  [ 700/1251]  eta: 0:07:48  lr: 0.000003  loss: 3.9576 (4.3160)  time: 0.8292  data: 0.0007  max mem: 19734
Epoch: [2]  [ 710/1251]  eta: 0:07:39  lr: 0.000003  loss: 4.0855 (4.3117)  time: 0.8190  data: 0.0006  max mem: 19734
Epoch: [2]  [ 720/1251]  eta: 0:07:31  lr: 0.000003  loss: 3.9322 (4.3034)  time: 0.8159  data: 0.0005  max mem: 19734
Epoch: [2]  [ 730/1251]  eta: 0:07:22  lr: 0.000003  loss: 3.9612 (4.3008)  time: 0.8155  data: 0.0006  max mem: 19734
Epoch: [2]  [ 740/1251]  eta: 0:07:14  lr: 0.000003  loss: 4.2401 (4.2987)  time: 0.8398  data: 0.0006  max mem: 19734
Epoch: [2]  [ 750/1251]  eta: 0:07:05  lr: 0.000003  loss: 4.2401 (4.2926)  time: 0.8488  data: 0.0004  max mem: 19734
Epoch: [2]  [ 760/1251]  eta: 0:06:56  lr: 0.000003  loss: 3.7840 (4.2887)  time: 0.8438  data: 0.0004  max mem: 19734
Epoch: [2]  [ 770/1251]  eta: 0:06:48  lr: 0.000003  loss: 3.7464 (4.2796)  time: 0.8420  data: 0.0005  max mem: 19734
Epoch: [2]  [ 780/1251]  eta: 0:06:39  lr: 0.000003  loss: 3.5248 (4.2734)  time: 0.8227  data: 0.0005  max mem: 19734
Epoch: [2]  [ 790/1251]  eta: 0:06:31  lr: 0.000003  loss: 3.7189 (4.2678)  time: 0.8188  data: 0.0005  max mem: 19734
loss info: cls_loss=3.7921, ratio_loss=0.0941, pruning_loss=0.2431, mse_loss=1.1245
Epoch: [2]  [ 800/1251]  eta: 0:06:22  lr: 0.000003  loss: 4.0384 (4.2630)  time: 0.8181  data: 0.0006  max mem: 19734
Epoch: [2]  [ 810/1251]  eta: 0:06:13  lr: 0.000003  loss: 4.0363 (4.2582)  time: 0.8158  data: 0.0006  max mem: 19734
Epoch: [2]  [ 820/1251]  eta: 0:06:05  lr: 0.000003  loss: 4.0255 (4.2526)  time: 0.8189  data: 0.0005  max mem: 19734
Epoch: [2]  [ 830/1251]  eta: 0:05:56  lr: 0.000003  loss: 4.0189 (4.2438)  time: 0.8203  data: 0.0005  max mem: 19734
Epoch: [2]  [ 840/1251]  eta: 0:05:47  lr: 0.000003  loss: 3.4986 (4.2352)  time: 0.8276  data: 0.0005  max mem: 19734
Epoch: [2]  [ 850/1251]  eta: 0:05:39  lr: 0.000003  loss: 3.9248 (4.2313)  time: 0.8256  data: 0.0005  max mem: 19734
Epoch: [2]  [ 860/1251]  eta: 0:05:30  lr: 0.000003  loss: 3.9332 (4.2244)  time: 0.8165  data: 0.0006  max mem: 19734
Epoch: [2]  [ 870/1251]  eta: 0:05:22  lr: 0.000003  loss: 3.6431 (4.2175)  time: 0.8157  data: 0.0006  max mem: 19734
Epoch: [2]  [ 880/1251]  eta: 0:05:13  lr: 0.000003  loss: 3.8849 (4.2139)  time: 0.8149  data: 0.0006  max mem: 19734
Epoch: [2]  [ 890/1251]  eta: 0:05:05  lr: 0.000003  loss: 3.9943 (4.2102)  time: 0.8293  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6383, ratio_loss=0.0626, pruning_loss=0.2499, mse_loss=1.1605
Epoch: [2]  [ 900/1251]  eta: 0:04:56  lr: 0.000003  loss: 3.7693 (4.2036)  time: 0.8360  data: 0.0005  max mem: 19734
Epoch: [2]  [ 910/1251]  eta: 0:04:48  lr: 0.000003  loss: 3.7693 (4.1990)  time: 0.8409  data: 0.0005  max mem: 19734
Epoch: [2]  [ 920/1251]  eta: 0:04:39  lr: 0.000003  loss: 4.0378 (4.1958)  time: 0.8470  data: 0.0005  max mem: 19734
Epoch: [2]  [ 930/1251]  eta: 0:04:31  lr: 0.000003  loss: 3.8425 (4.1885)  time: 0.8294  data: 0.0006  max mem: 19734
Epoch: [2]  [ 940/1251]  eta: 0:04:22  lr: 0.000003  loss: 3.4692 (4.1813)  time: 0.8170  data: 0.0006  max mem: 19734
Epoch: [2]  [ 950/1251]  eta: 0:04:14  lr: 0.000003  loss: 3.4692 (4.1755)  time: 0.8155  data: 0.0005  max mem: 19734
Epoch: [2]  [ 960/1251]  eta: 0:04:05  lr: 0.000003  loss: 3.8696 (4.1709)  time: 0.8151  data: 0.0005  max mem: 19734
Epoch: [2]  [ 970/1251]  eta: 0:03:57  lr: 0.000003  loss: 3.4879 (4.1624)  time: 0.8168  data: 0.0005  max mem: 19734
Epoch: [2]  [ 980/1251]  eta: 0:03:48  lr: 0.000003  loss: 3.4968 (4.1562)  time: 0.8175  data: 0.0005  max mem: 19734
Epoch: [2]  [ 990/1251]  eta: 0:03:40  lr: 0.000003  loss: 3.6268 (4.1524)  time: 0.8202  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5489, ratio_loss=0.0414, pruning_loss=0.2478, mse_loss=1.1889
Epoch: [2]  [1000/1251]  eta: 0:03:31  lr: 0.000003  loss: 3.7175 (4.1464)  time: 0.8211  data: 0.0004  max mem: 19734
Epoch: [2]  [1010/1251]  eta: 0:03:23  lr: 0.000003  loss: 3.7175 (4.1427)  time: 0.8158  data: 0.0004  max mem: 19734
Epoch: [2]  [1020/1251]  eta: 0:03:14  lr: 0.000003  loss: 3.5588 (4.1359)  time: 0.8138  data: 0.0005  max mem: 19734
Epoch: [2]  [1030/1251]  eta: 0:03:06  lr: 0.000003  loss: 3.7222 (4.1323)  time: 0.8161  data: 0.0004  max mem: 19734
Epoch: [2]  [1040/1251]  eta: 0:02:57  lr: 0.000003  loss: 3.6318 (4.1259)  time: 0.8381  data: 0.0004  max mem: 19734
Epoch: [2]  [1050/1251]  eta: 0:02:49  lr: 0.000003  loss: 3.6450 (4.1215)  time: 0.8457  data: 0.0005  max mem: 19734
Epoch: [2]  [1060/1251]  eta: 0:02:40  lr: 0.000003  loss: 3.9129 (4.1196)  time: 0.8570  data: 0.0005  max mem: 19734
Epoch: [2]  [1070/1251]  eta: 0:02:32  lr: 0.000003  loss: 3.9163 (4.1158)  time: 0.8482  data: 0.0004  max mem: 19734
Epoch: [2]  [1080/1251]  eta: 0:02:23  lr: 0.000003  loss: 3.7256 (4.1124)  time: 0.8185  data: 0.0004  max mem: 19734
Epoch: [2]  [1090/1251]  eta: 0:02:15  lr: 0.000003  loss: 3.7874 (4.1091)  time: 0.8188  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6332, ratio_loss=0.0281, pruning_loss=0.2416, mse_loss=1.1217
Epoch: [2]  [1100/1251]  eta: 0:02:07  lr: 0.000003  loss: 3.7874 (4.1033)  time: 0.8169  data: 0.0005  max mem: 19734
Epoch: [2]  [1110/1251]  eta: 0:01:58  lr: 0.000003  loss: 3.8081 (4.1015)  time: 0.8178  data: 0.0004  max mem: 19734
Epoch: [2]  [1120/1251]  eta: 0:01:50  lr: 0.000003  loss: 3.8298 (4.0982)  time: 0.8184  data: 0.0005  max mem: 19734
Epoch: [2]  [1130/1251]  eta: 0:01:41  lr: 0.000003  loss: 3.7971 (4.0966)  time: 0.8176  data: 0.0005  max mem: 19734
Epoch: [2]  [1140/1251]  eta: 0:01:33  lr: 0.000003  loss: 3.8719 (4.0925)  time: 0.8225  data: 0.0005  max mem: 19734
Epoch: [2]  [1150/1251]  eta: 0:01:24  lr: 0.000003  loss: 3.8133 (4.0904)  time: 0.8230  data: 0.0005  max mem: 19734
Epoch: [2]  [1160/1251]  eta: 0:01:16  lr: 0.000003  loss: 3.8538 (4.0866)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [2]  [1170/1251]  eta: 0:01:08  lr: 0.000003  loss: 3.9760 (4.0845)  time: 0.8154  data: 0.0005  max mem: 19734
Epoch: [2]  [1180/1251]  eta: 0:00:59  lr: 0.000003  loss: 3.7745 (4.0811)  time: 0.8272  data: 0.0005  max mem: 19734
Epoch: [2]  [1190/1251]  eta: 0:00:51  lr: 0.000003  loss: 3.7147 (4.0775)  time: 0.8327  data: 0.0008  max mem: 19734
loss info: cls_loss=3.6578, ratio_loss=0.0196, pruning_loss=0.2425, mse_loss=1.1933
Epoch: [2]  [1200/1251]  eta: 0:00:42  lr: 0.000003  loss: 3.6814 (4.0734)  time: 0.8272  data: 0.0007  max mem: 19734
Epoch: [2]  [1210/1251]  eta: 0:00:34  lr: 0.000003  loss: 3.7256 (4.0694)  time: 0.8273  data: 0.0001  max mem: 19734
Epoch: [2]  [1220/1251]  eta: 0:00:26  lr: 0.000003  loss: 3.8282 (4.0673)  time: 0.8150  data: 0.0001  max mem: 19734
Epoch: [2]  [1230/1251]  eta: 0:00:17  lr: 0.000003  loss: 3.8728 (4.0650)  time: 0.8030  data: 0.0001  max mem: 19734
Epoch: [2]  [1240/1251]  eta: 0:00:09  lr: 0.000003  loss: 3.8211 (4.0623)  time: 0.8031  data: 0.0002  max mem: 19734
Epoch: [2]  [1250/1251]  eta: 0:00:00  lr: 0.000003  loss: 3.7757 (4.0578)  time: 0.8029  data: 0.0001  max mem: 19734
Epoch: [2] Total time: 0:17:30 (0.8394 s / it)
Averaged stats: lr: 0.000003  loss: 3.7757 (4.0378)
Test:  [  0/261]  eta: 2:06:44  loss: 1.2072 (1.2072)  acc1: 73.4375 (73.4375)  acc5: 91.6667 (91.6667)  time: 29.1350  data: 28.5235  max mem: 19734
Test:  [ 10/261]  eta: 0:12:13  loss: 1.2109 (1.3183)  acc1: 72.9167 (71.0227)  acc5: 91.6667 (89.7727)  time: 2.9208  data: 2.6097  max mem: 19734
Test:  [ 20/261]  eta: 0:06:39  loss: 1.4771 (1.5202)  acc1: 66.1458 (66.0962)  acc5: 84.8958 (86.7064)  time: 0.2855  data: 0.0147  max mem: 19734
Test:  [ 30/261]  eta: 0:04:37  loss: 1.4055 (1.4076)  acc1: 71.8750 (69.3884)  acc5: 86.4583 (87.4496)  time: 0.2526  data: 0.0148  max mem: 19734
Test:  [ 40/261]  eta: 0:03:58  loss: 1.1853 (1.3721)  acc1: 75.0000 (70.1855)  acc5: 89.5833 (87.9954)  time: 0.4723  data: 0.2201  max mem: 19734
Test:  [ 50/261]  eta: 0:03:24  loss: 1.5267 (1.4336)  acc1: 65.6250 (68.0964)  acc5: 88.5417 (87.6838)  time: 0.6130  data: 0.2170  max mem: 19734
Test:  [ 60/261]  eta: 0:02:57  loss: 1.5894 (1.4567)  acc1: 61.9792 (67.1107)  acc5: 85.9375 (87.5512)  time: 0.4752  data: 0.0128  max mem: 19734
Test:  [ 70/261]  eta: 0:02:37  loss: 1.4988 (1.4409)  acc1: 64.0625 (67.1362)  acc5: 89.5833 (88.0208)  time: 0.4481  data: 0.0116  max mem: 19734
Test:  [ 80/261]  eta: 0:02:18  loss: 1.4005 (1.4221)  acc1: 66.6667 (67.5283)  acc5: 90.6250 (88.4002)  time: 0.4154  data: 0.0079  max mem: 19734
Test:  [ 90/261]  eta: 0:02:01  loss: 1.2653 (1.3940)  acc1: 71.3542 (68.2406)  acc5: 89.5833 (88.6790)  time: 0.3114  data: 0.0098  max mem: 19734
Test:  [100/261]  eta: 0:01:47  loss: 1.1472 (1.3855)  acc1: 72.3958 (68.3684)  acc5: 90.1042 (88.8304)  time: 0.2723  data: 0.0107  max mem: 19734
Test:  [110/261]  eta: 0:01:36  loss: 1.2735 (1.3923)  acc1: 69.7917 (68.3089)  acc5: 89.5833 (88.6918)  time: 0.3220  data: 0.0791  max mem: 19734
Test:  [120/261]  eta: 0:01:25  loss: 1.5714 (1.4140)  acc1: 63.5417 (67.9106)  acc5: 86.4583 (88.4039)  time: 0.2997  data: 0.0840  max mem: 19734
Test:  [130/261]  eta: 0:01:17  loss: 1.6900 (1.4414)  acc1: 60.4167 (67.4658)  acc5: 84.3750 (87.9851)  time: 0.3094  data: 0.0854  max mem: 19734
Test:  [140/261]  eta: 0:01:10  loss: 1.6895 (1.4549)  acc1: 58.8542 (67.0952)  acc5: 84.8958 (87.8731)  time: 0.4562  data: 0.2544  max mem: 19734
Test:  [150/261]  eta: 0:01:07  loss: 1.4945 (1.4482)  acc1: 67.7083 (67.4531)  acc5: 86.9792 (87.9036)  time: 0.7123  data: 0.4659  max mem: 19734
Test:  [160/261]  eta: 0:00:59  loss: 1.3134 (1.4596)  acc1: 72.3958 (67.3460)  acc5: 86.9792 (87.6941)  time: 0.5917  data: 0.2967  max mem: 19734
Test:  [170/261]  eta: 0:00:51  loss: 1.6245 (1.4820)  acc1: 60.4167 (66.8494)  acc5: 82.2917 (87.4239)  time: 0.2747  data: 0.0431  max mem: 19734
Test:  [180/261]  eta: 0:00:46  loss: 1.7071 (1.4933)  acc1: 60.9375 (66.6436)  acc5: 83.3333 (87.3216)  time: 0.4675  data: 0.2803  max mem: 19734
Test:  [190/261]  eta: 0:00:39  loss: 1.6489 (1.4985)  acc1: 62.5000 (66.6067)  acc5: 86.4583 (87.2573)  time: 0.4382  data: 0.2502  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: 1.5498 (1.5053)  acc1: 66.1458 (66.4568)  acc5: 85.9375 (87.1165)  time: 0.1920  data: 0.0117  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: 1.6761 (1.5123)  acc1: 64.5833 (66.3729)  acc5: 83.8542 (87.0088)  time: 0.1937  data: 0.0288  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: 1.6781 (1.5295)  acc1: 61.4583 (65.9502)  acc5: 82.8125 (86.7836)  time: 0.1701  data: 0.0216  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.7226 (1.5358)  acc1: 60.4167 (65.8144)  acc5: 82.8125 (86.6748)  time: 0.1454  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: 1.6216 (1.5381)  acc1: 62.5000 (65.7374)  acc5: 84.8958 (86.6831)  time: 0.1454  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.2585 (1.5213)  acc1: 70.8333 (66.1375)  acc5: 89.0625 (86.9045)  time: 0.1456  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.1030 (1.5150)  acc1: 73.9583 (66.3020)  acc5: 91.6667 (87.0140)  time: 0.1412  data: 0.0001  max mem: 19734
Test: Total time: 0:01:57 (0.4488 s / it)
* Acc@1 66.302 Acc@5 87.014 loss 1.515
Accuracy of the network on the 50000 test images: 66.3%
Max accuracy: 66.30%
Epoch: [3]  [   0/1251]  eta: 5:14:35  lr: 0.000005  loss: 3.4728 (3.4728)  time: 15.0882  data: 14.2974  max mem: 19734
Epoch: [3]  [  10/1251]  eta: 0:46:06  lr: 0.000005  loss: 3.9668 (3.8493)  time: 2.2289  data: 1.3002  max mem: 19734
Epoch: [3]  [  20/1251]  eta: 0:31:57  lr: 0.000005  loss: 3.8444 (3.6883)  time: 0.8812  data: 0.0005  max mem: 19734
Epoch: [3]  [  30/1251]  eta: 0:26:49  lr: 0.000005  loss: 3.7205 (3.6570)  time: 0.8172  data: 0.0005  max mem: 19734
Epoch: [3]  [  40/1251]  eta: 0:24:08  lr: 0.000005  loss: 3.7205 (3.6656)  time: 0.8162  data: 0.0006  max mem: 19734
loss info: cls_loss=3.6319, ratio_loss=0.0153, pruning_loss=0.2360, mse_loss=1.1525
Epoch: [3]  [  50/1251]  eta: 0:22:28  lr: 0.000005  loss: 3.6128 (3.6713)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [3]  [  60/1251]  eta: 0:21:18  lr: 0.000005  loss: 3.4000 (3.6207)  time: 0.8212  data: 0.0004  max mem: 19734
Epoch: [3]  [  70/1251]  eta: 0:20:31  lr: 0.000005  loss: 3.6695 (3.6322)  time: 0.8383  data: 0.0005  max mem: 19734
Epoch: [3]  [  80/1251]  eta: 0:19:49  lr: 0.000005  loss: 3.8395 (3.6394)  time: 0.8402  data: 0.0005  max mem: 19734
Epoch: [3]  [  90/1251]  eta: 0:19:18  lr: 0.000005  loss: 3.7703 (3.6104)  time: 0.8401  data: 0.0006  max mem: 19734
Epoch: [3]  [ 100/1251]  eta: 0:18:52  lr: 0.000005  loss: 3.7691 (3.6326)  time: 0.8562  data: 0.0005  max mem: 19734
Epoch: [3]  [ 110/1251]  eta: 0:18:25  lr: 0.000005  loss: 3.7385 (3.6172)  time: 0.8370  data: 0.0005  max mem: 19734
Epoch: [3]  [ 120/1251]  eta: 0:18:03  lr: 0.000005  loss: 3.5652 (3.6125)  time: 0.8269  data: 0.0005  max mem: 19734
Epoch: [3]  [ 130/1251]  eta: 0:17:41  lr: 0.000005  loss: 3.5154 (3.5938)  time: 0.8268  data: 0.0005  max mem: 19734
Epoch: [3]  [ 140/1251]  eta: 0:17:22  lr: 0.000005  loss: 3.4272 (3.5910)  time: 0.8183  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4664, ratio_loss=0.0102, pruning_loss=0.2437, mse_loss=1.1683
Epoch: [3]  [ 150/1251]  eta: 0:17:04  lr: 0.000005  loss: 3.6659 (3.5756)  time: 0.8192  data: 0.0005  max mem: 19734
Epoch: [3]  [ 160/1251]  eta: 0:16:47  lr: 0.000005  loss: 3.5129 (3.5709)  time: 0.8158  data: 0.0005  max mem: 19734
Epoch: [3]  [ 170/1251]  eta: 0:16:30  lr: 0.000005  loss: 3.5129 (3.5705)  time: 0.8143  data: 0.0004  max mem: 19734
Epoch: [3]  [ 180/1251]  eta: 0:16:16  lr: 0.000005  loss: 3.7591 (3.5691)  time: 0.8177  data: 0.0005  max mem: 19734
Epoch: [3]  [ 190/1251]  eta: 0:16:01  lr: 0.000005  loss: 3.2229 (3.5579)  time: 0.8173  data: 0.0004  max mem: 19734
Epoch: [3]  [ 200/1251]  eta: 0:15:48  lr: 0.000005  loss: 3.2917 (3.5520)  time: 0.8181  data: 0.0004  max mem: 19734
Epoch: [3]  [ 210/1251]  eta: 0:15:35  lr: 0.000005  loss: 3.5960 (3.5596)  time: 0.8250  data: 0.0005  max mem: 19734
Epoch: [3]  [ 220/1251]  eta: 0:15:23  lr: 0.000005  loss: 3.7902 (3.5700)  time: 0.8349  data: 0.0005  max mem: 19734
Epoch: [3]  [ 230/1251]  eta: 0:15:12  lr: 0.000005  loss: 3.7460 (3.5733)  time: 0.8395  data: 0.0005  max mem: 19734
Epoch: [3]  [ 240/1251]  eta: 0:15:02  lr: 0.000005  loss: 3.7460 (3.5822)  time: 0.8517  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5144, ratio_loss=0.0075, pruning_loss=0.2372, mse_loss=1.1066
Epoch: [3]  [ 250/1251]  eta: 0:14:50  lr: 0.000005  loss: 3.7820 (3.5752)  time: 0.8425  data: 0.0005  max mem: 19734
Epoch: [3]  [ 260/1251]  eta: 0:14:38  lr: 0.000005  loss: 3.4836 (3.5675)  time: 0.8190  data: 0.0004  max mem: 19734
Epoch: [3]  [ 270/1251]  eta: 0:14:28  lr: 0.000005  loss: 3.5723 (3.5717)  time: 0.8269  data: 0.0004  max mem: 19734
Epoch: [3]  [ 280/1251]  eta: 0:14:16  lr: 0.000005  loss: 3.5947 (3.5683)  time: 0.8278  data: 0.0004  max mem: 19734
Epoch: [3]  [ 290/1251]  eta: 0:14:06  lr: 0.000005  loss: 3.7380 (3.5790)  time: 0.8209  data: 0.0004  max mem: 19734
Epoch: [3]  [ 300/1251]  eta: 0:13:55  lr: 0.000005  loss: 3.8172 (3.5861)  time: 0.8195  data: 0.0004  max mem: 19734
Epoch: [3]  [ 310/1251]  eta: 0:13:44  lr: 0.000005  loss: 3.7987 (3.5904)  time: 0.8200  data: 0.0005  max mem: 19734
Epoch: [3]  [ 320/1251]  eta: 0:13:34  lr: 0.000005  loss: 3.3890 (3.5750)  time: 0.8229  data: 0.0005  max mem: 19734
Epoch: [3]  [ 330/1251]  eta: 0:13:24  lr: 0.000005  loss: 3.1196 (3.5644)  time: 0.8231  data: 0.0005  max mem: 19734
Epoch: [3]  [ 340/1251]  eta: 0:13:14  lr: 0.000005  loss: 3.3434 (3.5658)  time: 0.8222  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4979, ratio_loss=0.0062, pruning_loss=0.2413, mse_loss=1.1408
Epoch: [3]  [ 350/1251]  eta: 0:13:04  lr: 0.000005  loss: 3.5356 (3.5616)  time: 0.8207  data: 0.0005  max mem: 19734
Epoch: [3]  [ 360/1251]  eta: 0:12:55  lr: 0.000005  loss: 3.6049 (3.5623)  time: 0.8374  data: 0.0005  max mem: 19734
Epoch: [3]  [ 370/1251]  eta: 0:12:45  lr: 0.000005  loss: 3.6049 (3.5614)  time: 0.8488  data: 0.0005  max mem: 19734
Epoch: [3]  [ 380/1251]  eta: 0:12:36  lr: 0.000005  loss: 3.3638 (3.5580)  time: 0.8401  data: 0.0005  max mem: 19734
Epoch: [3]  [ 390/1251]  eta: 0:12:26  lr: 0.000005  loss: 3.5727 (3.5606)  time: 0.8358  data: 0.0005  max mem: 19734
Epoch: [3]  [ 400/1251]  eta: 0:12:17  lr: 0.000005  loss: 3.6415 (3.5608)  time: 0.8256  data: 0.0005  max mem: 19734
Epoch: [3]  [ 410/1251]  eta: 0:12:07  lr: 0.000005  loss: 3.6415 (3.5611)  time: 0.8206  data: 0.0004  max mem: 19734
Epoch: [3]  [ 420/1251]  eta: 0:11:58  lr: 0.000005  loss: 3.7854 (3.5670)  time: 0.8266  data: 0.0004  max mem: 19734
Epoch: [3]  [ 430/1251]  eta: 0:11:48  lr: 0.000005  loss: 3.8186 (3.5714)  time: 0.8255  data: 0.0004  max mem: 19734
Epoch: [3]  [ 440/1251]  eta: 0:11:39  lr: 0.000005  loss: 3.4656 (3.5615)  time: 0.8186  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5107, ratio_loss=0.0050, pruning_loss=0.2365, mse_loss=1.1161
Epoch: [3]  [ 450/1251]  eta: 0:11:29  lr: 0.000005  loss: 3.4160 (3.5635)  time: 0.8176  data: 0.0005  max mem: 19734
Epoch: [3]  [ 460/1251]  eta: 0:11:20  lr: 0.000005  loss: 3.7743 (3.5671)  time: 0.8188  data: 0.0005  max mem: 19734
Epoch: [3]  [ 470/1251]  eta: 0:11:11  lr: 0.000005  loss: 3.7743 (3.5678)  time: 0.8183  data: 0.0005  max mem: 19734
Epoch: [3]  [ 480/1251]  eta: 0:11:01  lr: 0.000005  loss: 3.7078 (3.5622)  time: 0.8172  data: 0.0005  max mem: 19734
Epoch: [3]  [ 490/1251]  eta: 0:10:52  lr: 0.000005  loss: 3.6522 (3.5642)  time: 0.8211  data: 0.0005  max mem: 19734
Epoch: [3]  [ 500/1251]  eta: 0:10:43  lr: 0.000005  loss: 3.6522 (3.5645)  time: 0.8244  data: 0.0005  max mem: 19734
Epoch: [3]  [ 510/1251]  eta: 0:10:35  lr: 0.000005  loss: 3.4358 (3.5614)  time: 0.8387  data: 0.0005  max mem: 19734
Epoch: [3]  [ 520/1251]  eta: 0:10:26  lr: 0.000005  loss: 3.5412 (3.5612)  time: 0.8490  data: 0.0005  max mem: 19734
Epoch: [3]  [ 530/1251]  eta: 0:10:17  lr: 0.000005  loss: 3.6276 (3.5586)  time: 0.8466  data: 0.0005  max mem: 19734
Epoch: [3]  [ 540/1251]  eta: 0:10:08  lr: 0.000005  loss: 3.6355 (3.5594)  time: 0.8393  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4858, ratio_loss=0.0045, pruning_loss=0.2387, mse_loss=1.1896
Epoch: [3]  [ 550/1251]  eta: 0:09:59  lr: 0.000005  loss: 3.6355 (3.5543)  time: 0.8216  data: 0.0005  max mem: 19734
Epoch: [3]  [ 560/1251]  eta: 0:09:50  lr: 0.000005  loss: 3.6830 (3.5556)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [3]  [ 570/1251]  eta: 0:09:41  lr: 0.000005  loss: 3.7233 (3.5613)  time: 0.8242  data: 0.0005  max mem: 19734
Epoch: [3]  [ 580/1251]  eta: 0:09:32  lr: 0.000005  loss: 3.7232 (3.5566)  time: 0.8243  data: 0.0005  max mem: 19734
Epoch: [3]  [ 590/1251]  eta: 0:09:23  lr: 0.000005  loss: 3.4262 (3.5524)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [3]  [ 600/1251]  eta: 0:09:15  lr: 0.000005  loss: 3.8182 (3.5532)  time: 0.8188  data: 0.0005  max mem: 19734
Epoch: [3]  [ 610/1251]  eta: 0:09:06  lr: 0.000005  loss: 3.7734 (3.5515)  time: 0.8199  data: 0.0005  max mem: 19734
Epoch: [3]  [ 620/1251]  eta: 0:08:57  lr: 0.000005  loss: 3.7697 (3.5559)  time: 0.8198  data: 0.0004  max mem: 19734
Epoch: [3]  [ 630/1251]  eta: 0:08:48  lr: 0.000005  loss: 3.7838 (3.5537)  time: 0.8202  data: 0.0004  max mem: 19734
Epoch: [3]  [ 640/1251]  eta: 0:08:39  lr: 0.000005  loss: 3.7140 (3.5558)  time: 0.8179  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5103, ratio_loss=0.0042, pruning_loss=0.2374, mse_loss=1.1815
Epoch: [3]  [ 650/1251]  eta: 0:08:31  lr: 0.000005  loss: 3.8504 (3.5599)  time: 0.8274  data: 0.0005  max mem: 19734
Epoch: [3]  [ 660/1251]  eta: 0:08:22  lr: 0.000005  loss: 3.8309 (3.5594)  time: 0.8314  data: 0.0005  max mem: 19734
Epoch: [3]  [ 670/1251]  eta: 0:08:13  lr: 0.000005  loss: 3.7641 (3.5602)  time: 0.8327  data: 0.0006  max mem: 19734
Epoch: [3]  [ 680/1251]  eta: 0:08:05  lr: 0.000005  loss: 3.7641 (3.5594)  time: 0.8558  data: 0.0006  max mem: 19734
Epoch: [3]  [ 690/1251]  eta: 0:07:56  lr: 0.000005  loss: 3.6433 (3.5594)  time: 0.8451  data: 0.0005  max mem: 19734
Epoch: [3]  [ 700/1251]  eta: 0:07:47  lr: 0.000005  loss: 3.8166 (3.5581)  time: 0.8183  data: 0.0006  max mem: 19734
Epoch: [3]  [ 710/1251]  eta: 0:07:39  lr: 0.000005  loss: 3.8230 (3.5570)  time: 0.8205  data: 0.0005  max mem: 19734
Epoch: [3]  [ 720/1251]  eta: 0:07:30  lr: 0.000005  loss: 3.8230 (3.5630)  time: 0.8213  data: 0.0004  max mem: 19734
Epoch: [3]  [ 730/1251]  eta: 0:07:21  lr: 0.000005  loss: 3.7999 (3.5631)  time: 0.8171  data: 0.0005  max mem: 19734
Epoch: [3]  [ 740/1251]  eta: 0:07:13  lr: 0.000005  loss: 3.4458 (3.5584)  time: 0.8178  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5073, ratio_loss=0.0042, pruning_loss=0.2352, mse_loss=1.1520
Epoch: [3]  [ 750/1251]  eta: 0:07:04  lr: 0.000005  loss: 3.3192 (3.5578)  time: 0.8171  data: 0.0005  max mem: 19734
Epoch: [3]  [ 760/1251]  eta: 0:06:55  lr: 0.000005  loss: 3.7635 (3.5600)  time: 0.8184  data: 0.0005  max mem: 19734
Epoch: [3]  [ 770/1251]  eta: 0:06:47  lr: 0.000005  loss: 3.7635 (3.5595)  time: 0.8202  data: 0.0005  max mem: 19734
Epoch: [3]  [ 780/1251]  eta: 0:06:38  lr: 0.000005  loss: 3.6839 (3.5593)  time: 0.8200  data: 0.0005  max mem: 19734
Epoch: [3]  [ 790/1251]  eta: 0:06:29  lr: 0.000005  loss: 3.6814 (3.5585)  time: 0.8216  data: 0.0006  max mem: 19734
Epoch: [3]  [ 800/1251]  eta: 0:06:21  lr: 0.000005  loss: 3.5487 (3.5576)  time: 0.8355  data: 0.0007  max mem: 19734
Epoch: [3]  [ 810/1251]  eta: 0:06:12  lr: 0.000005  loss: 3.6573 (3.5596)  time: 0.8400  data: 0.0005  max mem: 19734
Epoch: [3]  [ 820/1251]  eta: 0:06:04  lr: 0.000005  loss: 3.7253 (3.5598)  time: 0.8386  data: 0.0005  max mem: 19734
Epoch: [3]  [ 830/1251]  eta: 0:05:56  lr: 0.000005  loss: 3.7253 (3.5601)  time: 0.8484  data: 0.0005  max mem: 19734
Epoch: [3]  [ 840/1251]  eta: 0:05:47  lr: 0.000005  loss: 3.8141 (3.5623)  time: 0.8351  data: 0.0007  max mem: 19734
loss info: cls_loss=3.5364, ratio_loss=0.0036, pruning_loss=0.2327, mse_loss=1.1319
Epoch: [3]  [ 850/1251]  eta: 0:05:38  lr: 0.000005  loss: 3.6675 (3.5629)  time: 0.8206  data: 0.0007  max mem: 19734
Epoch: [3]  [ 860/1251]  eta: 0:05:30  lr: 0.000005  loss: 3.5415 (3.5605)  time: 0.8267  data: 0.0004  max mem: 19734
Epoch: [3]  [ 870/1251]  eta: 0:05:21  lr: 0.000005  loss: 3.4527 (3.5616)  time: 0.8244  data: 0.0004  max mem: 19734
Epoch: [3]  [ 880/1251]  eta: 0:05:13  lr: 0.000005  loss: 3.6968 (3.5614)  time: 0.8185  data: 0.0007  max mem: 19734
Epoch: [3]  [ 890/1251]  eta: 0:05:04  lr: 0.000005  loss: 3.7432 (3.5629)  time: 0.8197  data: 0.0007  max mem: 19734
Epoch: [3]  [ 900/1251]  eta: 0:04:56  lr: 0.000005  loss: 3.7432 (3.5638)  time: 0.8222  data: 0.0005  max mem: 19734
Epoch: [3]  [ 910/1251]  eta: 0:04:47  lr: 0.000005  loss: 3.6346 (3.5644)  time: 0.8222  data: 0.0005  max mem: 19734
Epoch: [3]  [ 920/1251]  eta: 0:04:39  lr: 0.000005  loss: 3.6346 (3.5655)  time: 0.8224  data: 0.0005  max mem: 19734
Epoch: [3]  [ 930/1251]  eta: 0:04:30  lr: 0.000005  loss: 3.7122 (3.5663)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [3]  [ 940/1251]  eta: 0:04:22  lr: 0.000005  loss: 3.8279 (3.5685)  time: 0.8156  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5963, ratio_loss=0.0037, pruning_loss=0.2280, mse_loss=1.0760
Epoch: [3]  [ 950/1251]  eta: 0:04:13  lr: 0.000005  loss: 3.7607 (3.5707)  time: 0.8307  data: 0.0005  max mem: 19734
Epoch: [3]  [ 960/1251]  eta: 0:04:05  lr: 0.000005  loss: 3.5599 (3.5696)  time: 0.8419  data: 0.0004  max mem: 19734
Epoch: [3]  [ 970/1251]  eta: 0:03:56  lr: 0.000005  loss: 3.7405 (3.5710)  time: 0.8477  data: 0.0004  max mem: 19734
Epoch: [3]  [ 980/1251]  eta: 0:03:48  lr: 0.000005  loss: 3.7473 (3.5690)  time: 0.8420  data: 0.0005  max mem: 19734
Epoch: [3]  [ 990/1251]  eta: 0:03:39  lr: 0.000005  loss: 3.6609 (3.5696)  time: 0.8222  data: 0.0005  max mem: 19734
Epoch: [3]  [1000/1251]  eta: 0:03:31  lr: 0.000005  loss: 3.7517 (3.5700)  time: 0.8164  data: 0.0004  max mem: 19734
Epoch: [3]  [1010/1251]  eta: 0:03:22  lr: 0.000005  loss: 3.7631 (3.5701)  time: 0.8221  data: 0.0004  max mem: 19734
Epoch: [3]  [1020/1251]  eta: 0:03:14  lr: 0.000005  loss: 3.6607 (3.5670)  time: 0.8221  data: 0.0005  max mem: 19734
Epoch: [3]  [1030/1251]  eta: 0:03:06  lr: 0.000005  loss: 3.5077 (3.5666)  time: 0.8166  data: 0.0006  max mem: 19734
Epoch: [3]  [1040/1251]  eta: 0:02:57  lr: 0.000005  loss: 3.7001 (3.5669)  time: 0.8156  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4843, ratio_loss=0.0037, pruning_loss=0.2339, mse_loss=1.1446
Epoch: [3]  [1050/1251]  eta: 0:02:49  lr: 0.000005  loss: 3.7568 (3.5672)  time: 0.8144  data: 0.0005  max mem: 19734
Epoch: [3]  [1060/1251]  eta: 0:02:40  lr: 0.000005  loss: 3.4730 (3.5668)  time: 0.8136  data: 0.0005  max mem: 19734
Epoch: [3]  [1070/1251]  eta: 0:02:32  lr: 0.000005  loss: 3.4138 (3.5645)  time: 0.8159  data: 0.0004  max mem: 19734
Epoch: [3]  [1080/1251]  eta: 0:02:23  lr: 0.000005  loss: 3.6427 (3.5652)  time: 0.8168  data: 0.0005  max mem: 19734
Epoch: [3]  [1090/1251]  eta: 0:02:15  lr: 0.000005  loss: 3.6687 (3.5652)  time: 0.8236  data: 0.0005  max mem: 19734
Epoch: [3]  [1100/1251]  eta: 0:02:06  lr: 0.000005  loss: 3.5714 (3.5632)  time: 0.8391  data: 0.0004  max mem: 19734
Epoch: [3]  [1110/1251]  eta: 0:01:58  lr: 0.000005  loss: 3.3704 (3.5598)  time: 0.8375  data: 0.0006  max mem: 19734
Epoch: [3]  [1120/1251]  eta: 0:01:50  lr: 0.000005  loss: 3.3704 (3.5581)  time: 0.8485  data: 0.0007  max mem: 19734
Epoch: [3]  [1130/1251]  eta: 0:01:41  lr: 0.000005  loss: 3.5086 (3.5574)  time: 0.8423  data: 0.0005  max mem: 19734
Epoch: [3]  [1140/1251]  eta: 0:01:33  lr: 0.000005  loss: 3.5926 (3.5580)  time: 0.8164  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4098, ratio_loss=0.0033, pruning_loss=0.2351, mse_loss=1.1465
Epoch: [3]  [1150/1251]  eta: 0:01:24  lr: 0.000005  loss: 3.5701 (3.5569)  time: 0.8221  data: 0.0004  max mem: 19734
Epoch: [3]  [1160/1251]  eta: 0:01:16  lr: 0.000005  loss: 3.3883 (3.5565)  time: 0.8216  data: 0.0006  max mem: 19734
Epoch: [3]  [1170/1251]  eta: 0:01:08  lr: 0.000005  loss: 3.6762 (3.5570)  time: 0.8161  data: 0.0006  max mem: 19734
Epoch: [3]  [1180/1251]  eta: 0:00:59  lr: 0.000005  loss: 3.7226 (3.5572)  time: 0.8171  data: 0.0004  max mem: 19734
Epoch: [3]  [1190/1251]  eta: 0:00:51  lr: 0.000005  loss: 3.7226 (3.5583)  time: 0.8160  data: 0.0006  max mem: 19734
Epoch: [3]  [1200/1251]  eta: 0:00:42  lr: 0.000005  loss: 3.6436 (3.5589)  time: 0.8092  data: 0.0004  max mem: 19734
Epoch: [3]  [1210/1251]  eta: 0:00:34  lr: 0.000005  loss: 3.5530 (3.5564)  time: 0.8024  data: 0.0001  max mem: 19734
Epoch: [3]  [1220/1251]  eta: 0:00:25  lr: 0.000005  loss: 3.5639 (3.5576)  time: 0.8034  data: 0.0001  max mem: 19734
Epoch: [3]  [1230/1251]  eta: 0:00:17  lr: 0.000005  loss: 3.8798 (3.5607)  time: 0.8035  data: 0.0001  max mem: 19734
Epoch: [3]  [1240/1251]  eta: 0:00:09  lr: 0.000005  loss: 3.8570 (3.5600)  time: 0.8149  data: 0.0001  max mem: 19734
loss info: cls_loss=3.5704, ratio_loss=0.0034, pruning_loss=0.2303, mse_loss=1.0901
Epoch: [3]  [1250/1251]  eta: 0:00:00  lr: 0.000005  loss: 3.7435 (3.5619)  time: 0.8269  data: 0.0002  max mem: 19734
Epoch: [3] Total time: 0:17:29 (0.8387 s / it)
Averaged stats: lr: 0.000005  loss: 3.7435 (3.5674)
Test:  [  0/261]  eta: 1:56:44  loss: 0.9754 (0.9754)  acc1: 77.6042 (77.6042)  acc5: 94.2708 (94.2708)  time: 26.8365  data: 26.4817  max mem: 19734
Test:  [ 10/261]  eta: 0:11:45  loss: 0.9754 (1.0059)  acc1: 80.2083 (78.5038)  acc5: 94.2708 (93.1345)  time: 2.8121  data: 2.4985  max mem: 19734
Test:  [ 20/261]  eta: 0:06:48  loss: 1.2097 (1.2143)  acc1: 71.3542 (72.4950)  acc5: 91.6667 (90.7490)  time: 0.4379  data: 0.0544  max mem: 19734
Test:  [ 30/261]  eta: 0:04:44  loss: 1.1025 (1.1061)  acc1: 75.0000 (75.9073)  acc5: 91.6667 (91.5995)  time: 0.3654  data: 0.0127  max mem: 19734
Test:  [ 40/261]  eta: 0:04:05  loss: 0.8356 (1.0713)  acc1: 82.8125 (76.6387)  acc5: 94.2708 (92.1494)  time: 0.4982  data: 0.3004  max mem: 19734
Test:  [ 50/261]  eta: 0:03:22  loss: 1.2848 (1.1452)  acc1: 69.7917 (74.3975)  acc5: 90.6250 (91.6667)  time: 0.5322  data: 0.2976  max mem: 19734
Test:  [ 60/261]  eta: 0:02:50  loss: 1.3676 (1.1722)  acc1: 69.2708 (73.5314)  acc5: 90.6250 (91.6496)  time: 0.3155  data: 0.0106  max mem: 19734
Test:  [ 70/261]  eta: 0:02:28  loss: 1.2712 (1.1720)  acc1: 70.8333 (73.0781)  acc5: 92.1875 (91.9381)  time: 0.3205  data: 0.0118  max mem: 19734
Test:  [ 80/261]  eta: 0:02:14  loss: 1.1125 (1.1607)  acc1: 72.9167 (73.3732)  acc5: 93.7500 (92.1296)  time: 0.4183  data: 0.1245  max mem: 19734
Test:  [ 90/261]  eta: 0:01:58  loss: 1.0212 (1.1344)  acc1: 77.6042 (74.0327)  acc5: 93.7500 (92.4050)  time: 0.3939  data: 0.1246  max mem: 19734
Test:  [100/261]  eta: 0:01:44  loss: 0.9745 (1.1310)  acc1: 78.6458 (74.0873)  acc5: 93.2292 (92.4763)  time: 0.2668  data: 0.0130  max mem: 19734
Test:  [110/261]  eta: 0:01:40  loss: 1.0836 (1.1424)  acc1: 73.4375 (73.9255)  acc5: 92.1875 (92.2626)  time: 0.5269  data: 0.2852  max mem: 19734
Test:  [120/261]  eta: 0:01:28  loss: 1.4084 (1.1719)  acc1: 66.1458 (73.2395)  acc5: 88.0208 (91.8431)  time: 0.5103  data: 0.2828  max mem: 19734
Test:  [130/261]  eta: 0:01:20  loss: 1.5571 (1.2083)  acc1: 61.9792 (72.5390)  acc5: 85.4167 (91.3407)  time: 0.3396  data: 0.0111  max mem: 19734
Test:  [140/261]  eta: 0:01:15  loss: 1.5119 (1.2277)  acc1: 63.0208 (72.0228)  acc5: 87.5000 (91.1237)  time: 0.6211  data: 0.1921  max mem: 19734
Test:  [150/261]  eta: 0:01:07  loss: 1.3426 (1.2266)  acc1: 68.7500 (72.2234)  acc5: 88.5417 (91.0286)  time: 0.5862  data: 0.1931  max mem: 19734
Test:  [160/261]  eta: 0:01:00  loss: 1.1394 (1.2422)  acc1: 76.0417 (72.0562)  acc5: 90.1042 (90.7835)  time: 0.4222  data: 0.0154  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.4915 (1.2672)  acc1: 62.5000 (71.4669)  acc5: 86.4583 (90.5001)  time: 0.5817  data: 0.1807  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: 1.5862 (1.2815)  acc1: 61.4583 (71.1441)  acc5: 86.4583 (90.3430)  time: 0.5069  data: 0.1788  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: 1.5239 (1.2893)  acc1: 64.0625 (71.0624)  acc5: 87.5000 (90.2214)  time: 0.3083  data: 0.0118  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.4411 (1.2984)  acc1: 69.2708 (70.8618)  acc5: 87.5000 (90.0523)  time: 0.3246  data: 0.0438  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.5214 (1.3074)  acc1: 67.7083 (70.7198)  acc5: 86.4583 (89.9388)  time: 0.2382  data: 0.0369  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.5970 (1.3252)  acc1: 64.0625 (70.2536)  acc5: 85.9375 (89.7082)  time: 0.1490  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.5937 (1.3338)  acc1: 62.5000 (70.0352)  acc5: 85.9375 (89.5833)  time: 0.1454  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.5168 (1.3396)  acc1: 65.6250 (69.9105)  acc5: 87.5000 (89.5163)  time: 0.1451  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1946 (1.3281)  acc1: 71.8750 (70.2150)  acc5: 90.6250 (89.6643)  time: 0.1448  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0449 (1.3253)  acc1: 76.5625 (70.2860)  acc5: 92.7083 (89.7600)  time: 0.1404  data: 0.0001  max mem: 19734
Test: Total time: 0:02:02 (0.4683 s / it)
* Acc@1 70.286 Acc@5 89.760 loss 1.325
Accuracy of the network on the 50000 test images: 70.3%
Max accuracy: 70.29%
Epoch: [4]  [   0/1251]  eta: 6:16:17  lr: 0.000006  loss: 4.2000 (4.2000)  time: 18.0474  data: 17.0873  max mem: 19734
Epoch: [4]  [  10/1251]  eta: 0:51:35  lr: 0.000006  loss: 3.6677 (3.6547)  time: 2.4940  data: 1.5543  max mem: 19734
Epoch: [4]  [  20/1251]  eta: 0:34:50  lr: 0.000006  loss: 3.6677 (3.5632)  time: 0.8809  data: 0.0008  max mem: 19734
Epoch: [4]  [  30/1251]  eta: 0:28:45  lr: 0.000006  loss: 3.5936 (3.4612)  time: 0.8192  data: 0.0005  max mem: 19734
Epoch: [4]  [  40/1251]  eta: 0:25:37  lr: 0.000006  loss: 3.5963 (3.5085)  time: 0.8190  data: 0.0005  max mem: 19734
Epoch: [4]  [  50/1251]  eta: 0:23:38  lr: 0.000006  loss: 3.6387 (3.4989)  time: 0.8204  data: 0.0005  max mem: 19734
Epoch: [4]  [  60/1251]  eta: 0:22:16  lr: 0.000006  loss: 3.7183 (3.5276)  time: 0.8193  data: 0.0005  max mem: 19734
Epoch: [4]  [  70/1251]  eta: 0:21:15  lr: 0.000006  loss: 3.6819 (3.4963)  time: 0.8231  data: 0.0005  max mem: 19734
Epoch: [4]  [  80/1251]  eta: 0:20:27  lr: 0.000006  loss: 3.5329 (3.4841)  time: 0.8251  data: 0.0005  max mem: 19734
Epoch: [4]  [  90/1251]  eta: 0:19:48  lr: 0.000006  loss: 3.4579 (3.4687)  time: 0.8230  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4203, ratio_loss=0.0033, pruning_loss=0.2339, mse_loss=1.1186
Epoch: [4]  [ 100/1251]  eta: 0:19:15  lr: 0.000006  loss: 3.5292 (3.4875)  time: 0.8230  data: 0.0005  max mem: 19734
Epoch: [4]  [ 110/1251]  eta: 0:18:46  lr: 0.000006  loss: 3.6462 (3.4869)  time: 0.8234  data: 0.0007  max mem: 19734
Epoch: [4]  [ 120/1251]  eta: 0:18:23  lr: 0.000006  loss: 3.3209 (3.4825)  time: 0.8338  data: 0.0007  max mem: 19734
Epoch: [4]  [ 130/1251]  eta: 0:18:03  lr: 0.000006  loss: 3.5802 (3.4972)  time: 0.8498  data: 0.0005  max mem: 19734
Epoch: [4]  [ 140/1251]  eta: 0:17:43  lr: 0.000006  loss: 3.8252 (3.5154)  time: 0.8471  data: 0.0005  max mem: 19734
Epoch: [4]  [ 150/1251]  eta: 0:17:27  lr: 0.000006  loss: 3.6517 (3.5092)  time: 0.8499  data: 0.0005  max mem: 19734
Epoch: [4]  [ 160/1251]  eta: 0:17:10  lr: 0.000006  loss: 3.6115 (3.5268)  time: 0.8543  data: 0.0005  max mem: 19734
Epoch: [4]  [ 170/1251]  eta: 0:16:53  lr: 0.000006  loss: 3.6411 (3.5192)  time: 0.8356  data: 0.0005  max mem: 19734
Epoch: [4]  [ 180/1251]  eta: 0:16:37  lr: 0.000006  loss: 3.6646 (3.5266)  time: 0.8201  data: 0.0005  max mem: 19734
Epoch: [4]  [ 190/1251]  eta: 0:16:21  lr: 0.000006  loss: 3.5733 (3.5154)  time: 0.8173  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5272, ratio_loss=0.0035, pruning_loss=0.2288, mse_loss=1.1121
Epoch: [4]  [ 200/1251]  eta: 0:16:06  lr: 0.000006  loss: 3.6016 (3.5275)  time: 0.8186  data: 0.0005  max mem: 19734
Epoch: [4]  [ 210/1251]  eta: 0:15:52  lr: 0.000006  loss: 3.9197 (3.5369)  time: 0.8182  data: 0.0005  max mem: 19734
Epoch: [4]  [ 220/1251]  eta: 0:15:38  lr: 0.000006  loss: 3.8153 (3.5418)  time: 0.8182  data: 0.0005  max mem: 19734
Epoch: [4]  [ 230/1251]  eta: 0:15:25  lr: 0.000006  loss: 3.6870 (3.5345)  time: 0.8211  data: 0.0006  max mem: 19734
Epoch: [4]  [ 240/1251]  eta: 0:15:12  lr: 0.000006  loss: 3.8183 (3.5512)  time: 0.8173  data: 0.0005  max mem: 19734
Epoch: [4]  [ 250/1251]  eta: 0:15:00  lr: 0.000006  loss: 3.8990 (3.5544)  time: 0.8183  data: 0.0005  max mem: 19734
Epoch: [4]  [ 260/1251]  eta: 0:14:48  lr: 0.000006  loss: 3.7326 (3.5558)  time: 0.8191  data: 0.0005  max mem: 19734
Epoch: [4]  [ 270/1251]  eta: 0:14:37  lr: 0.000006  loss: 3.7326 (3.5573)  time: 0.8308  data: 0.0005  max mem: 19734
Epoch: [4]  [ 280/1251]  eta: 0:14:27  lr: 0.000006  loss: 3.8592 (3.5623)  time: 0.8484  data: 0.0005  max mem: 19734
Epoch: [4]  [ 290/1251]  eta: 0:14:15  lr: 0.000006  loss: 3.7703 (3.5592)  time: 0.8335  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6023, ratio_loss=0.0032, pruning_loss=0.2261, mse_loss=1.0542
Epoch: [4]  [ 300/1251]  eta: 0:14:06  lr: 0.000006  loss: 3.6796 (3.5669)  time: 0.8495  data: 0.0005  max mem: 19734
Epoch: [4]  [ 310/1251]  eta: 0:13:55  lr: 0.000006  loss: 3.6658 (3.5587)  time: 0.8499  data: 0.0005  max mem: 19734
Epoch: [4]  [ 320/1251]  eta: 0:13:44  lr: 0.000006  loss: 3.3800 (3.5537)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [4]  [ 330/1251]  eta: 0:13:33  lr: 0.000006  loss: 3.3800 (3.5462)  time: 0.8161  data: 0.0005  max mem: 19734
Epoch: [4]  [ 340/1251]  eta: 0:13:23  lr: 0.000006  loss: 3.7238 (3.5508)  time: 0.8182  data: 0.0005  max mem: 19734
Epoch: [4]  [ 350/1251]  eta: 0:13:12  lr: 0.000006  loss: 3.8599 (3.5547)  time: 0.8169  data: 0.0005  max mem: 19734
Epoch: [4]  [ 360/1251]  eta: 0:13:02  lr: 0.000006  loss: 3.6813 (3.5526)  time: 0.8174  data: 0.0005  max mem: 19734
Epoch: [4]  [ 370/1251]  eta: 0:12:52  lr: 0.000006  loss: 3.4233 (3.5511)  time: 0.8204  data: 0.0004  max mem: 19734
Epoch: [4]  [ 380/1251]  eta: 0:12:41  lr: 0.000006  loss: 3.4365 (3.5473)  time: 0.8178  data: 0.0004  max mem: 19734
Epoch: [4]  [ 390/1251]  eta: 0:12:31  lr: 0.000006  loss: 3.5919 (3.5454)  time: 0.8155  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4620, ratio_loss=0.0032, pruning_loss=0.2331, mse_loss=1.1121
Epoch: [4]  [ 400/1251]  eta: 0:12:22  lr: 0.000006  loss: 3.7644 (3.5493)  time: 0.8173  data: 0.0005  max mem: 19734
Epoch: [4]  [ 410/1251]  eta: 0:12:12  lr: 0.000006  loss: 3.7190 (3.5514)  time: 0.8280  data: 0.0005  max mem: 19734
Epoch: [4]  [ 420/1251]  eta: 0:12:02  lr: 0.000006  loss: 3.5749 (3.5468)  time: 0.8263  data: 0.0005  max mem: 19734
Epoch: [4]  [ 430/1251]  eta: 0:11:53  lr: 0.000006  loss: 3.6589 (3.5470)  time: 0.8284  data: 0.0004  max mem: 19734
Epoch: [4]  [ 440/1251]  eta: 0:11:43  lr: 0.000006  loss: 3.4675 (3.5452)  time: 0.8296  data: 0.0004  max mem: 19734
Epoch: [4]  [ 450/1251]  eta: 0:11:35  lr: 0.000006  loss: 3.4675 (3.5457)  time: 0.8482  data: 0.0004  max mem: 19734
Epoch: [4]  [ 460/1251]  eta: 0:11:25  lr: 0.000006  loss: 3.7837 (3.5496)  time: 0.8478  data: 0.0004  max mem: 19734
Epoch: [4]  [ 470/1251]  eta: 0:11:16  lr: 0.000006  loss: 3.8424 (3.5524)  time: 0.8168  data: 0.0004  max mem: 19734
Epoch: [4]  [ 480/1251]  eta: 0:11:06  lr: 0.000006  loss: 3.4186 (3.5459)  time: 0.8188  data: 0.0004  max mem: 19734
Epoch: [4]  [ 490/1251]  eta: 0:10:57  lr: 0.000006  loss: 3.5420 (3.5494)  time: 0.8197  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4920, ratio_loss=0.0033, pruning_loss=0.2303, mse_loss=1.1253
Epoch: [4]  [ 500/1251]  eta: 0:10:48  lr: 0.000006  loss: 3.6644 (3.5498)  time: 0.8167  data: 0.0005  max mem: 19734
Epoch: [4]  [ 510/1251]  eta: 0:10:38  lr: 0.000006  loss: 3.3223 (3.5458)  time: 0.8154  data: 0.0005  max mem: 19734
Epoch: [4]  [ 520/1251]  eta: 0:10:29  lr: 0.000006  loss: 3.6261 (3.5479)  time: 0.8189  data: 0.0006  max mem: 19734
Epoch: [4]  [ 530/1251]  eta: 0:10:20  lr: 0.000006  loss: 3.4534 (3.5406)  time: 0.8176  data: 0.0005  max mem: 19734
Epoch: [4]  [ 540/1251]  eta: 0:10:11  lr: 0.000006  loss: 3.4105 (3.5393)  time: 0.8147  data: 0.0004  max mem: 19734
Epoch: [4]  [ 550/1251]  eta: 0:10:02  lr: 0.000006  loss: 3.7164 (3.5369)  time: 0.8193  data: 0.0005  max mem: 19734
Epoch: [4]  [ 560/1251]  eta: 0:09:53  lr: 0.000006  loss: 3.6193 (3.5380)  time: 0.8252  data: 0.0005  max mem: 19734
Epoch: [4]  [ 570/1251]  eta: 0:09:44  lr: 0.000006  loss: 3.4529 (3.5347)  time: 0.8315  data: 0.0004  max mem: 19734
Epoch: [4]  [ 580/1251]  eta: 0:09:35  lr: 0.000006  loss: 3.6014 (3.5358)  time: 0.8286  data: 0.0004  max mem: 19734
Epoch: [4]  [ 590/1251]  eta: 0:09:26  lr: 0.000006  loss: 3.7820 (3.5400)  time: 0.8320  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4574, ratio_loss=0.0031, pruning_loss=0.2285, mse_loss=1.0468
Epoch: [4]  [ 600/1251]  eta: 0:09:17  lr: 0.000006  loss: 3.7820 (3.5369)  time: 0.8417  data: 0.0004  max mem: 19734
Epoch: [4]  [ 610/1251]  eta: 0:09:08  lr: 0.000006  loss: 3.6134 (3.5363)  time: 0.8289  data: 0.0003  max mem: 19734
Epoch: [4]  [ 620/1251]  eta: 0:08:59  lr: 0.000006  loss: 3.6995 (3.5366)  time: 0.8160  data: 0.0004  max mem: 19734
Epoch: [4]  [ 630/1251]  eta: 0:08:50  lr: 0.000006  loss: 3.8350 (3.5421)  time: 0.8157  data: 0.0004  max mem: 19734
Epoch: [4]  [ 640/1251]  eta: 0:08:42  lr: 0.000006  loss: 3.8350 (3.5415)  time: 0.8160  data: 0.0004  max mem: 19734
Epoch: [4]  [ 650/1251]  eta: 0:08:33  lr: 0.000006  loss: 3.6217 (3.5411)  time: 0.8152  data: 0.0004  max mem: 19734
Epoch: [4]  [ 660/1251]  eta: 0:08:24  lr: 0.000006  loss: 3.3529 (3.5388)  time: 0.8159  data: 0.0004  max mem: 19734
Epoch: [4]  [ 670/1251]  eta: 0:08:15  lr: 0.000006  loss: 3.3529 (3.5374)  time: 0.8169  data: 0.0004  max mem: 19734
Epoch: [4]  [ 680/1251]  eta: 0:08:06  lr: 0.000006  loss: 3.5145 (3.5392)  time: 0.8180  data: 0.0004  max mem: 19734
Epoch: [4]  [ 690/1251]  eta: 0:07:57  lr: 0.000006  loss: 3.6587 (3.5393)  time: 0.8193  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4904, ratio_loss=0.0032, pruning_loss=0.2274, mse_loss=1.0659
Epoch: [4]  [ 700/1251]  eta: 0:07:49  lr: 0.000006  loss: 3.7691 (3.5411)  time: 0.8253  data: 0.0004  max mem: 19734
Epoch: [4]  [ 710/1251]  eta: 0:07:40  lr: 0.000006  loss: 3.7691 (3.5388)  time: 0.8240  data: 0.0004  max mem: 19734
Epoch: [4]  [ 720/1251]  eta: 0:07:31  lr: 0.000006  loss: 3.3090 (3.5350)  time: 0.8301  data: 0.0004  max mem: 19734
Epoch: [4]  [ 730/1251]  eta: 0:07:23  lr: 0.000006  loss: 3.3579 (3.5338)  time: 0.8300  data: 0.0004  max mem: 19734
Epoch: [4]  [ 740/1251]  eta: 0:07:14  lr: 0.000006  loss: 3.4698 (3.5326)  time: 0.8355  data: 0.0004  max mem: 19734
Epoch: [4]  [ 750/1251]  eta: 0:07:05  lr: 0.000006  loss: 3.5866 (3.5342)  time: 0.8452  data: 0.0004  max mem: 19734
Epoch: [4]  [ 760/1251]  eta: 0:06:57  lr: 0.000006  loss: 3.7994 (3.5366)  time: 0.8262  data: 0.0004  max mem: 19734
Epoch: [4]  [ 770/1251]  eta: 0:06:48  lr: 0.000006  loss: 3.5791 (3.5358)  time: 0.8170  data: 0.0004  max mem: 19734
Epoch: [4]  [ 780/1251]  eta: 0:06:39  lr: 0.000006  loss: 3.5791 (3.5353)  time: 0.8165  data: 0.0004  max mem: 19734
Epoch: [4]  [ 790/1251]  eta: 0:06:31  lr: 0.000006  loss: 3.4672 (3.5321)  time: 0.8157  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4318, ratio_loss=0.0032, pruning_loss=0.2263, mse_loss=1.0832
Epoch: [4]  [ 800/1251]  eta: 0:06:22  lr: 0.000006  loss: 3.4672 (3.5323)  time: 0.8156  data: 0.0005  max mem: 19734
Epoch: [4]  [ 810/1251]  eta: 0:06:13  lr: 0.000006  loss: 3.5979 (3.5319)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [4]  [ 820/1251]  eta: 0:06:05  lr: 0.000006  loss: 3.6248 (3.5338)  time: 0.8143  data: 0.0005  max mem: 19734
Epoch: [4]  [ 830/1251]  eta: 0:05:56  lr: 0.000006  loss: 3.6971 (3.5352)  time: 0.8162  data: 0.0004  max mem: 19734
Epoch: [4]  [ 840/1251]  eta: 0:05:47  lr: 0.000006  loss: 3.6966 (3.5352)  time: 0.8198  data: 0.0005  max mem: 19734
Epoch: [4]  [ 850/1251]  eta: 0:05:39  lr: 0.000006  loss: 3.5460 (3.5336)  time: 0.8262  data: 0.0005  max mem: 19734
Epoch: [4]  [ 860/1251]  eta: 0:05:30  lr: 0.000006  loss: 3.7870 (3.5379)  time: 0.8324  data: 0.0005  max mem: 19734
Epoch: [4]  [ 870/1251]  eta: 0:05:22  lr: 0.000006  loss: 3.8856 (3.5412)  time: 0.8357  data: 0.0005  max mem: 19734
Epoch: [4]  [ 880/1251]  eta: 0:05:13  lr: 0.000006  loss: 3.7988 (3.5417)  time: 0.8282  data: 0.0005  max mem: 19734
Epoch: [4]  [ 890/1251]  eta: 0:05:05  lr: 0.000006  loss: 3.5813 (3.5401)  time: 0.8450  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5162, ratio_loss=0.0032, pruning_loss=0.2236, mse_loss=1.0639
Epoch: [4]  [ 900/1251]  eta: 0:04:56  lr: 0.000006  loss: 3.2476 (3.5374)  time: 0.8443  data: 0.0005  max mem: 19734
Epoch: [4]  [ 910/1251]  eta: 0:04:48  lr: 0.000006  loss: 3.5802 (3.5395)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [4]  [ 920/1251]  eta: 0:04:39  lr: 0.000006  loss: 3.8355 (3.5404)  time: 0.8185  data: 0.0005  max mem: 19734
Epoch: [4]  [ 930/1251]  eta: 0:04:31  lr: 0.000006  loss: 3.8338 (3.5412)  time: 0.8203  data: 0.0005  max mem: 19734
Epoch: [4]  [ 940/1251]  eta: 0:04:22  lr: 0.000006  loss: 3.7323 (3.5423)  time: 0.8184  data: 0.0005  max mem: 19734
Epoch: [4]  [ 950/1251]  eta: 0:04:14  lr: 0.000006  loss: 3.7055 (3.5415)  time: 0.8153  data: 0.0004  max mem: 19734
Epoch: [4]  [ 960/1251]  eta: 0:04:05  lr: 0.000006  loss: 3.6721 (3.5401)  time: 0.8165  data: 0.0004  max mem: 19734
Epoch: [4]  [ 970/1251]  eta: 0:03:57  lr: 0.000006  loss: 3.7345 (3.5410)  time: 0.8183  data: 0.0005  max mem: 19734
Epoch: [4]  [ 980/1251]  eta: 0:03:48  lr: 0.000006  loss: 3.7345 (3.5417)  time: 0.8188  data: 0.0005  max mem: 19734
Epoch: [4]  [ 990/1251]  eta: 0:03:40  lr: 0.000006  loss: 3.5022 (3.5411)  time: 0.8208  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5473, ratio_loss=0.0031, pruning_loss=0.2233, mse_loss=1.0843
Epoch: [4]  [1000/1251]  eta: 0:03:31  lr: 0.000006  loss: 3.6340 (3.5416)  time: 0.8316  data: 0.0005  max mem: 19734
Epoch: [4]  [1010/1251]  eta: 0:03:23  lr: 0.000006  loss: 3.7119 (3.5436)  time: 0.8474  data: 0.0005  max mem: 19734
Epoch: [4]  [1020/1251]  eta: 0:03:14  lr: 0.000006  loss: 3.7980 (3.5428)  time: 0.8328  data: 0.0005  max mem: 19734
Epoch: [4]  [1030/1251]  eta: 0:03:06  lr: 0.000006  loss: 3.2298 (3.5398)  time: 0.8213  data: 0.0005  max mem: 19734
Epoch: [4]  [1040/1251]  eta: 0:02:57  lr: 0.000006  loss: 3.4403 (3.5393)  time: 0.8398  data: 0.0005  max mem: 19734
Epoch: [4]  [1050/1251]  eta: 0:02:49  lr: 0.000006  loss: 3.5060 (3.5389)  time: 0.8336  data: 0.0006  max mem: 19734
Epoch: [4]  [1060/1251]  eta: 0:02:40  lr: 0.000006  loss: 3.7053 (3.5397)  time: 0.8178  data: 0.0006  max mem: 19734
Epoch: [4]  [1070/1251]  eta: 0:02:32  lr: 0.000006  loss: 3.7194 (3.5411)  time: 0.8173  data: 0.0004  max mem: 19734
Epoch: [4]  [1080/1251]  eta: 0:02:24  lr: 0.000006  loss: 3.6399 (3.5411)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [4]  [1090/1251]  eta: 0:02:15  lr: 0.000006  loss: 3.6378 (3.5413)  time: 0.8164  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4920, ratio_loss=0.0032, pruning_loss=0.2233, mse_loss=1.0697
Epoch: [4]  [1100/1251]  eta: 0:02:07  lr: 0.000006  loss: 3.5534 (3.5414)  time: 0.8175  data: 0.0004  max mem: 19734
Epoch: [4]  [1110/1251]  eta: 0:01:58  lr: 0.000006  loss: 3.2865 (3.5383)  time: 0.8223  data: 0.0004  max mem: 19734
Epoch: [4]  [1120/1251]  eta: 0:01:50  lr: 0.000006  loss: 3.3512 (3.5379)  time: 0.8234  data: 0.0004  max mem: 19734
Epoch: [4]  [1130/1251]  eta: 0:01:41  lr: 0.000006  loss: 3.4889 (3.5374)  time: 0.8229  data: 0.0005  max mem: 19734
Epoch: [4]  [1140/1251]  eta: 0:01:33  lr: 0.000006  loss: 3.2893 (3.5341)  time: 0.8305  data: 0.0005  max mem: 19734
Epoch: [4]  [1150/1251]  eta: 0:01:24  lr: 0.000006  loss: 3.3139 (3.5340)  time: 0.8284  data: 0.0005  max mem: 19734
Epoch: [4]  [1160/1251]  eta: 0:01:16  lr: 0.000006  loss: 3.5783 (3.5327)  time: 0.8308  data: 0.0004  max mem: 19734
Epoch: [4]  [1170/1251]  eta: 0:01:08  lr: 0.000006  loss: 3.6080 (3.5345)  time: 0.8314  data: 0.0004  max mem: 19734
Epoch: [4]  [1180/1251]  eta: 0:00:59  lr: 0.000006  loss: 3.6443 (3.5340)  time: 0.8359  data: 0.0004  max mem: 19734
Epoch: [4]  [1190/1251]  eta: 0:00:51  lr: 0.000006  loss: 3.6047 (3.5338)  time: 0.8423  data: 0.0008  max mem: 19734
loss info: cls_loss=3.4164, ratio_loss=0.0032, pruning_loss=0.2272, mse_loss=1.0699
Epoch: [4]  [1200/1251]  eta: 0:00:42  lr: 0.000006  loss: 3.6294 (3.5348)  time: 0.8171  data: 0.0007  max mem: 19734
Epoch: [4]  [1210/1251]  eta: 0:00:34  lr: 0.000006  loss: 3.3084 (3.5307)  time: 0.8019  data: 0.0002  max mem: 19734
Epoch: [4]  [1220/1251]  eta: 0:00:26  lr: 0.000006  loss: 3.3445 (3.5312)  time: 0.8019  data: 0.0002  max mem: 19734
Epoch: [4]  [1230/1251]  eta: 0:00:17  lr: 0.000006  loss: 3.5103 (3.5299)  time: 0.8016  data: 0.0002  max mem: 19734
Epoch: [4]  [1240/1251]  eta: 0:00:09  lr: 0.000006  loss: 3.7884 (3.5312)  time: 0.8039  data: 0.0002  max mem: 19734
Epoch: [4]  [1250/1251]  eta: 0:00:00  lr: 0.000006  loss: 3.7884 (3.5335)  time: 0.8047  data: 0.0002  max mem: 19734
Epoch: [4] Total time: 0:17:30 (0.8399 s / it)
Averaged stats: lr: 0.000006  loss: 3.7884 (3.5307)
Test:  [  0/261]  eta: 2:36:24  loss: 0.7619 (0.7619)  acc1: 84.3750 (84.3750)  acc5: 96.3542 (96.3542)  time: 35.9548  data: 35.5430  max mem: 19734
Test:  [ 10/261]  eta: 0:15:02  loss: 0.7619 (0.7989)  acc1: 86.4583 (83.0966)  acc5: 96.3542 (95.8333)  time: 3.5940  data: 3.2440  max mem: 19734
Test:  [ 20/261]  eta: 0:08:13  loss: 1.0313 (0.9641)  acc1: 78.1250 (78.0754)  acc5: 93.7500 (94.1964)  time: 0.3536  data: 0.0126  max mem: 19734
Test:  [ 30/261]  eta: 0:05:50  loss: 0.8467 (0.8736)  acc1: 81.2500 (81.0652)  acc5: 93.7500 (94.7581)  time: 0.3764  data: 0.0125  max mem: 19734
Test:  [ 40/261]  eta: 0:04:32  loss: 0.6145 (0.8416)  acc1: 87.5000 (82.0376)  acc5: 96.3542 (95.0203)  time: 0.3726  data: 0.0108  max mem: 19734
Test:  [ 50/261]  eta: 0:03:37  loss: 0.9852 (0.9135)  acc1: 76.0417 (79.9428)  acc5: 94.2708 (94.4751)  time: 0.2808  data: 0.0081  max mem: 19734
Test:  [ 60/261]  eta: 0:02:59  loss: 1.1074 (0.9313)  acc1: 72.9167 (79.0727)  acc5: 94.2708 (94.5782)  time: 0.1962  data: 0.0091  max mem: 19734
Test:  [ 70/261]  eta: 0:02:44  loss: 1.0241 (0.9348)  acc1: 73.9583 (78.5798)  acc5: 95.8333 (94.8063)  time: 0.4298  data: 0.2417  max mem: 19734
Test:  [ 80/261]  eta: 0:02:23  loss: 0.9091 (0.9313)  acc1: 79.1667 (78.7873)  acc5: 96.3542 (94.8817)  time: 0.4957  data: 0.2474  max mem: 19734
Test:  [ 90/261]  eta: 0:02:07  loss: 0.8479 (0.9138)  acc1: 82.8125 (79.2067)  acc5: 95.8333 (95.0321)  time: 0.3283  data: 0.0173  max mem: 19734
Test:  [100/261]  eta: 0:02:04  loss: 0.8444 (0.9139)  acc1: 82.8125 (79.3007)  acc5: 95.3125 (95.1062)  time: 0.6812  data: 0.3043  max mem: 19734
Test:  [110/261]  eta: 0:01:50  loss: 0.8582 (0.9334)  acc1: 75.0000 (78.9133)  acc5: 94.2708 (94.8574)  time: 0.6787  data: 0.3116  max mem: 19734
Test:  [120/261]  eta: 0:01:38  loss: 1.2500 (0.9693)  acc1: 70.3125 (78.0733)  acc5: 89.0625 (94.3053)  time: 0.3193  data: 0.0260  max mem: 19734
Test:  [130/261]  eta: 0:01:27  loss: 1.4439 (1.0124)  acc1: 66.6667 (77.1748)  acc5: 85.4167 (93.7182)  time: 0.3128  data: 0.0449  max mem: 19734
Test:  [140/261]  eta: 0:01:18  loss: 1.3861 (1.0374)  acc1: 68.2292 (76.5182)  acc5: 88.5417 (93.4471)  time: 0.3442  data: 0.0362  max mem: 19734
Test:  [150/261]  eta: 0:01:08  loss: 1.2009 (1.0393)  acc1: 72.3958 (76.6108)  acc5: 90.6250 (93.3499)  time: 0.3067  data: 0.0097  max mem: 19734
Test:  [160/261]  eta: 0:01:00  loss: 0.9974 (1.0593)  acc1: 78.1250 (76.3263)  acc5: 91.6667 (93.0674)  time: 0.2765  data: 0.0111  max mem: 19734
Test:  [170/261]  eta: 0:00:56  loss: 1.3791 (1.0886)  acc1: 65.6250 (75.5696)  acc5: 86.9792 (92.7205)  time: 0.6060  data: 0.2700  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: 1.4784 (1.1054)  acc1: 64.5833 (75.1583)  acc5: 88.0208 (92.5443)  time: 0.5924  data: 0.2736  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: 1.4432 (1.1184)  acc1: 67.1875 (74.9536)  acc5: 90.1042 (92.3920)  time: 0.2771  data: 0.0157  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.3542 (1.1328)  acc1: 69.2708 (74.6269)  acc5: 89.5833 (92.1720)  time: 0.3409  data: 0.1166  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.4458 (1.1444)  acc1: 68.2292 (74.3607)  acc5: 88.5417 (92.0271)  time: 0.2738  data: 0.1093  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4540 (1.1638)  acc1: 65.6250 (73.8570)  acc5: 88.0208 (91.8199)  time: 0.1494  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4884 (1.1746)  acc1: 66.1458 (73.6089)  acc5: 88.5417 (91.6937)  time: 0.1454  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.4383 (1.1828)  acc1: 66.6667 (73.3921)  acc5: 90.1042 (91.6364)  time: 0.1453  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0261 (1.1745)  acc1: 76.5625 (73.6160)  acc5: 93.2292 (91.7517)  time: 0.1454  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0022 (1.1738)  acc1: 77.0833 (73.6380)  acc5: 94.7917 (91.8080)  time: 0.1412  data: 0.0001  max mem: 19734
Test: Total time: 0:02:04 (0.4770 s / it)
* Acc@1 73.638 Acc@5 91.808 loss 1.174
Accuracy of the network on the 50000 test images: 73.6%
Max accuracy: 73.64%
Epoch: [5]  [   0/1251]  eta: 5:49:41  lr: 0.000008  loss: 3.6474 (3.6474)  time: 16.7718  data: 15.8099  max mem: 19734
Epoch: [5]  [  10/1251]  eta: 0:49:11  lr: 0.000008  loss: 3.7919 (3.7717)  time: 2.3781  data: 1.4385  max mem: 19734
Epoch: [5]  [  20/1251]  eta: 0:33:44  lr: 0.000008  loss: 3.7842 (3.6808)  time: 0.8886  data: 0.0009  max mem: 19734
Epoch: [5]  [  30/1251]  eta: 0:28:08  lr: 0.000008  loss: 3.6778 (3.6235)  time: 0.8350  data: 0.0004  max mem: 19734
Epoch: [5]  [  40/1251]  eta: 0:25:07  lr: 0.000008  loss: 3.6778 (3.6768)  time: 0.8246  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5387, ratio_loss=0.0032, pruning_loss=0.2219, mse_loss=1.0599
Epoch: [5]  [  50/1251]  eta: 0:23:18  lr: 0.000008  loss: 3.7902 (3.6925)  time: 0.8259  data: 0.0004  max mem: 19734
Epoch: [5]  [  60/1251]  eta: 0:21:58  lr: 0.000008  loss: 3.7902 (3.6852)  time: 0.8248  data: 0.0004  max mem: 19734
Epoch: [5]  [  70/1251]  eta: 0:21:06  lr: 0.000008  loss: 3.8332 (3.6828)  time: 0.8373  data: 0.0004  max mem: 19734
Epoch: [5]  [  80/1251]  eta: 0:20:19  lr: 0.000008  loss: 3.7271 (3.6626)  time: 0.8398  data: 0.0004  max mem: 19734
Epoch: [5]  [  90/1251]  eta: 0:19:40  lr: 0.000008  loss: 3.5683 (3.6556)  time: 0.8215  data: 0.0005  max mem: 19734
Epoch: [5]  [ 100/1251]  eta: 0:19:08  lr: 0.000008  loss: 3.5357 (3.6156)  time: 0.8213  data: 0.0005  max mem: 19734
Epoch: [5]  [ 110/1251]  eta: 0:18:39  lr: 0.000008  loss: 3.5986 (3.6146)  time: 0.8185  data: 0.0004  max mem: 19734
Epoch: [5]  [ 120/1251]  eta: 0:18:14  lr: 0.000008  loss: 3.7708 (3.6195)  time: 0.8182  data: 0.0004  max mem: 19734
Epoch: [5]  [ 130/1251]  eta: 0:17:51  lr: 0.000008  loss: 3.7402 (3.6136)  time: 0.8174  data: 0.0005  max mem: 19734
Epoch: [5]  [ 140/1251]  eta: 0:17:31  lr: 0.000008  loss: 3.5000 (3.6059)  time: 0.8151  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5299, ratio_loss=0.0031, pruning_loss=0.2196, mse_loss=1.0245
Epoch: [5]  [ 150/1251]  eta: 0:17:12  lr: 0.000008  loss: 3.6726 (3.6042)  time: 0.8188  data: 0.0005  max mem: 19734
Epoch: [5]  [ 160/1251]  eta: 0:16:55  lr: 0.000008  loss: 3.6866 (3.6005)  time: 0.8184  data: 0.0006  max mem: 19734
Epoch: [5]  [ 170/1251]  eta: 0:16:39  lr: 0.000008  loss: 3.7302 (3.6050)  time: 0.8205  data: 0.0005  max mem: 19734
Epoch: [5]  [ 180/1251]  eta: 0:16:24  lr: 0.000008  loss: 3.4734 (3.5900)  time: 0.8275  data: 0.0005  max mem: 19734
Epoch: [5]  [ 190/1251]  eta: 0:16:10  lr: 0.000008  loss: 3.3541 (3.5819)  time: 0.8294  data: 0.0005  max mem: 19734
Epoch: [5]  [ 200/1251]  eta: 0:15:55  lr: 0.000008  loss: 3.6174 (3.5694)  time: 0.8242  data: 0.0005  max mem: 19734
Epoch: [5]  [ 210/1251]  eta: 0:15:42  lr: 0.000008  loss: 3.5528 (3.5546)  time: 0.8201  data: 0.0005  max mem: 19734
Epoch: [5]  [ 220/1251]  eta: 0:15:31  lr: 0.000008  loss: 3.2181 (3.5437)  time: 0.8397  data: 0.0004  max mem: 19734
Epoch: [5]  [ 230/1251]  eta: 0:15:18  lr: 0.000008  loss: 3.4163 (3.5384)  time: 0.8357  data: 0.0004  max mem: 19734
Epoch: [5]  [ 240/1251]  eta: 0:15:05  lr: 0.000008  loss: 3.5981 (3.5371)  time: 0.8144  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3557, ratio_loss=0.0035, pruning_loss=0.2251, mse_loss=1.0654
Epoch: [5]  [ 250/1251]  eta: 0:14:53  lr: 0.000008  loss: 3.6943 (3.5299)  time: 0.8133  data: 0.0004  max mem: 19734
Epoch: [5]  [ 260/1251]  eta: 0:14:42  lr: 0.000008  loss: 3.6943 (3.5362)  time: 0.8195  data: 0.0004  max mem: 19734
Epoch: [5]  [ 270/1251]  eta: 0:14:30  lr: 0.000008  loss: 3.6336 (3.5331)  time: 0.8247  data: 0.0005  max mem: 19734
Epoch: [5]  [ 280/1251]  eta: 0:14:19  lr: 0.000008  loss: 3.6470 (3.5340)  time: 0.8206  data: 0.0005  max mem: 19734
Epoch: [5]  [ 290/1251]  eta: 0:14:08  lr: 0.000008  loss: 3.6779 (3.5384)  time: 0.8181  data: 0.0005  max mem: 19734
Epoch: [5]  [ 300/1251]  eta: 0:13:57  lr: 0.000008  loss: 3.6527 (3.5376)  time: 0.8174  data: 0.0005  max mem: 19734
Epoch: [5]  [ 310/1251]  eta: 0:13:46  lr: 0.000008  loss: 3.5590 (3.5374)  time: 0.8170  data: 0.0005  max mem: 19734
Epoch: [5]  [ 320/1251]  eta: 0:13:36  lr: 0.000008  loss: 3.6193 (3.5367)  time: 0.8237  data: 0.0005  max mem: 19734
Epoch: [5]  [ 330/1251]  eta: 0:13:26  lr: 0.000008  loss: 3.6277 (3.5334)  time: 0.8327  data: 0.0005  max mem: 19734
Epoch: [5]  [ 340/1251]  eta: 0:13:16  lr: 0.000008  loss: 3.6386 (3.5370)  time: 0.8393  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5339, ratio_loss=0.0032, pruning_loss=0.2174, mse_loss=1.0158
Epoch: [5]  [ 350/1251]  eta: 0:13:06  lr: 0.000008  loss: 3.5917 (3.5293)  time: 0.8322  data: 0.0004  max mem: 19734
Epoch: [5]  [ 360/1251]  eta: 0:12:57  lr: 0.000008  loss: 3.5045 (3.5285)  time: 0.8379  data: 0.0004  max mem: 19734
Epoch: [5]  [ 370/1251]  eta: 0:12:47  lr: 0.000008  loss: 3.6491 (3.5367)  time: 0.8394  data: 0.0004  max mem: 19734
Epoch: [5]  [ 380/1251]  eta: 0:12:37  lr: 0.000008  loss: 3.7112 (3.5332)  time: 0.8194  data: 0.0004  max mem: 19734
Epoch: [5]  [ 390/1251]  eta: 0:12:27  lr: 0.000008  loss: 3.5952 (3.5335)  time: 0.8173  data: 0.0005  max mem: 19734
Epoch: [5]  [ 400/1251]  eta: 0:12:18  lr: 0.000008  loss: 3.4240 (3.5273)  time: 0.8217  data: 0.0008  max mem: 19734
Epoch: [5]  [ 410/1251]  eta: 0:12:08  lr: 0.000008  loss: 3.4240 (3.5262)  time: 0.8214  data: 0.0008  max mem: 19734
Epoch: [5]  [ 420/1251]  eta: 0:11:58  lr: 0.000008  loss: 3.4909 (3.5202)  time: 0.8182  data: 0.0005  max mem: 19734
Epoch: [5]  [ 430/1251]  eta: 0:11:49  lr: 0.000008  loss: 3.2418 (3.5135)  time: 0.8166  data: 0.0005  max mem: 19734
Epoch: [5]  [ 440/1251]  eta: 0:11:39  lr: 0.000008  loss: 3.4465 (3.5109)  time: 0.8166  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3909, ratio_loss=0.0033, pruning_loss=0.2233, mse_loss=1.0927
Epoch: [5]  [ 450/1251]  eta: 0:11:30  lr: 0.000008  loss: 3.7284 (3.5167)  time: 0.8190  data: 0.0005  max mem: 19734
Epoch: [5]  [ 460/1251]  eta: 0:11:21  lr: 0.000008  loss: 3.6522 (3.5193)  time: 0.8200  data: 0.0006  max mem: 19734
Epoch: [5]  [ 470/1251]  eta: 0:11:12  lr: 0.000008  loss: 3.6569 (3.5239)  time: 0.8287  data: 0.0005  max mem: 19734
Epoch: [5]  [ 480/1251]  eta: 0:11:03  lr: 0.000008  loss: 3.7147 (3.5244)  time: 0.8428  data: 0.0006  max mem: 19734
Epoch: [5]  [ 490/1251]  eta: 0:10:54  lr: 0.000008  loss: 3.3850 (3.5185)  time: 0.8356  data: 0.0005  max mem: 19734
Epoch: [5]  [ 500/1251]  eta: 0:10:44  lr: 0.000008  loss: 3.3850 (3.5176)  time: 0.8190  data: 0.0004  max mem: 19734
Epoch: [5]  [ 510/1251]  eta: 0:10:36  lr: 0.000008  loss: 3.4438 (3.5137)  time: 0.8317  data: 0.0004  max mem: 19734
Epoch: [5]  [ 520/1251]  eta: 0:10:27  lr: 0.000008  loss: 3.2954 (3.5153)  time: 0.8350  data: 0.0004  max mem: 19734
Epoch: [5]  [ 530/1251]  eta: 0:10:17  lr: 0.000008  loss: 3.7958 (3.5163)  time: 0.8184  data: 0.0004  max mem: 19734
Epoch: [5]  [ 540/1251]  eta: 0:10:08  lr: 0.000008  loss: 3.7206 (3.5213)  time: 0.8169  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4796, ratio_loss=0.0034, pruning_loss=0.2215, mse_loss=1.0571
Epoch: [5]  [ 550/1251]  eta: 0:09:59  lr: 0.000008  loss: 3.6660 (3.5149)  time: 0.8201  data: 0.0004  max mem: 19734
Epoch: [5]  [ 560/1251]  eta: 0:09:50  lr: 0.000008  loss: 3.6032 (3.5150)  time: 0.8192  data: 0.0004  max mem: 19734
Epoch: [5]  [ 570/1251]  eta: 0:09:41  lr: 0.000008  loss: 3.5374 (3.5155)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [5]  [ 580/1251]  eta: 0:09:32  lr: 0.000008  loss: 3.4792 (3.5129)  time: 0.8161  data: 0.0005  max mem: 19734
Epoch: [5]  [ 590/1251]  eta: 0:09:23  lr: 0.000008  loss: 3.5564 (3.5149)  time: 0.8173  data: 0.0004  max mem: 19734
Epoch: [5]  [ 600/1251]  eta: 0:09:14  lr: 0.000008  loss: 3.3500 (3.5084)  time: 0.8171  data: 0.0004  max mem: 19734
Epoch: [5]  [ 610/1251]  eta: 0:09:06  lr: 0.000008  loss: 3.1685 (3.5079)  time: 0.8177  data: 0.0006  max mem: 19734
Epoch: [5]  [ 620/1251]  eta: 0:08:57  lr: 0.000008  loss: 3.5024 (3.5111)  time: 0.8322  data: 0.0006  max mem: 19734
Epoch: [5]  [ 630/1251]  eta: 0:08:48  lr: 0.000008  loss: 3.7515 (3.5099)  time: 0.8463  data: 0.0005  max mem: 19734
Epoch: [5]  [ 640/1251]  eta: 0:08:40  lr: 0.000008  loss: 3.6853 (3.5098)  time: 0.8329  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4192, ratio_loss=0.0034, pruning_loss=0.2235, mse_loss=1.0034
Epoch: [5]  [ 650/1251]  eta: 0:08:31  lr: 0.000008  loss: 3.4894 (3.5044)  time: 0.8153  data: 0.0005  max mem: 19734
Epoch: [5]  [ 660/1251]  eta: 0:08:22  lr: 0.000008  loss: 3.3083 (3.5049)  time: 0.8416  data: 0.0005  max mem: 19734
Epoch: [5]  [ 670/1251]  eta: 0:08:14  lr: 0.000008  loss: 3.6385 (3.5033)  time: 0.8437  data: 0.0005  max mem: 19734
Epoch: [5]  [ 680/1251]  eta: 0:08:05  lr: 0.000008  loss: 3.5500 (3.5067)  time: 0.8178  data: 0.0006  max mem: 19734
Epoch: [5]  [ 690/1251]  eta: 0:07:56  lr: 0.000008  loss: 3.4054 (3.5022)  time: 0.8163  data: 0.0006  max mem: 19734
Epoch: [5]  [ 700/1251]  eta: 0:07:47  lr: 0.000008  loss: 3.2532 (3.4986)  time: 0.8146  data: 0.0005  max mem: 19734
Epoch: [5]  [ 710/1251]  eta: 0:07:39  lr: 0.000008  loss: 3.5352 (3.4994)  time: 0.8169  data: 0.0005  max mem: 19734
Epoch: [5]  [ 720/1251]  eta: 0:07:30  lr: 0.000008  loss: 3.5959 (3.4979)  time: 0.8183  data: 0.0005  max mem: 19734
Epoch: [5]  [ 730/1251]  eta: 0:07:21  lr: 0.000008  loss: 3.3931 (3.4974)  time: 0.8198  data: 0.0005  max mem: 19734
Epoch: [5]  [ 740/1251]  eta: 0:07:12  lr: 0.000008  loss: 3.4602 (3.4953)  time: 0.8198  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3476, ratio_loss=0.0034, pruning_loss=0.2242, mse_loss=1.0264
Epoch: [5]  [ 750/1251]  eta: 0:07:04  lr: 0.000008  loss: 3.0638 (3.4896)  time: 0.8163  data: 0.0005  max mem: 19734
Epoch: [5]  [ 760/1251]  eta: 0:06:55  lr: 0.000008  loss: 3.3594 (3.4910)  time: 0.8154  data: 0.0005  max mem: 19734
Epoch: [5]  [ 770/1251]  eta: 0:06:47  lr: 0.000008  loss: 3.6984 (3.4907)  time: 0.8388  data: 0.0005  max mem: 19734
Epoch: [5]  [ 780/1251]  eta: 0:06:38  lr: 0.000008  loss: 3.6700 (3.4921)  time: 0.8523  data: 0.0005  max mem: 19734
Epoch: [5]  [ 790/1251]  eta: 0:06:30  lr: 0.000008  loss: 3.5512 (3.4884)  time: 0.8295  data: 0.0005  max mem: 19734
Epoch: [5]  [ 800/1251]  eta: 0:06:21  lr: 0.000008  loss: 3.5478 (3.4873)  time: 0.8333  data: 0.0005  max mem: 19734
Epoch: [5]  [ 810/1251]  eta: 0:06:13  lr: 0.000008  loss: 3.7678 (3.4906)  time: 0.8419  data: 0.0005  max mem: 19734
Epoch: [5]  [ 820/1251]  eta: 0:06:04  lr: 0.000008  loss: 3.6331 (3.4892)  time: 0.8239  data: 0.0005  max mem: 19734
Epoch: [5]  [ 830/1251]  eta: 0:05:55  lr: 0.000008  loss: 3.1873 (3.4865)  time: 0.8151  data: 0.0005  max mem: 19734
Epoch: [5]  [ 840/1251]  eta: 0:05:47  lr: 0.000008  loss: 3.4656 (3.4879)  time: 0.8148  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4167, ratio_loss=0.0035, pruning_loss=0.2259, mse_loss=1.0758
Epoch: [5]  [ 850/1251]  eta: 0:05:38  lr: 0.000008  loss: 3.5805 (3.4888)  time: 0.8137  data: 0.0005  max mem: 19734
Epoch: [5]  [ 860/1251]  eta: 0:05:30  lr: 0.000008  loss: 3.4695 (3.4875)  time: 0.8144  data: 0.0006  max mem: 19734
Epoch: [5]  [ 870/1251]  eta: 0:05:21  lr: 0.000008  loss: 3.4695 (3.4883)  time: 0.8140  data: 0.0005  max mem: 19734
Epoch: [5]  [ 880/1251]  eta: 0:05:12  lr: 0.000008  loss: 3.5957 (3.4890)  time: 0.8127  data: 0.0004  max mem: 19734
Epoch: [5]  [ 890/1251]  eta: 0:05:04  lr: 0.000008  loss: 3.5957 (3.4897)  time: 0.8125  data: 0.0007  max mem: 19734
Epoch: [5]  [ 900/1251]  eta: 0:04:55  lr: 0.000008  loss: 3.9055 (3.4947)  time: 0.8143  data: 0.0007  max mem: 19734
Epoch: [5]  [ 910/1251]  eta: 0:04:47  lr: 0.000008  loss: 3.9055 (3.4961)  time: 0.8219  data: 0.0005  max mem: 19734
Epoch: [5]  [ 920/1251]  eta: 0:04:38  lr: 0.000008  loss: 3.8315 (3.4994)  time: 0.8359  data: 0.0005  max mem: 19734
Epoch: [5]  [ 930/1251]  eta: 0:04:30  lr: 0.000008  loss: 3.6990 (3.4955)  time: 0.8374  data: 0.0006  max mem: 19734
Epoch: [5]  [ 940/1251]  eta: 0:04:21  lr: 0.000008  loss: 3.4219 (3.4951)  time: 0.8226  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5009, ratio_loss=0.0034, pruning_loss=0.2208, mse_loss=1.0076
Epoch: [5]  [ 950/1251]  eta: 0:04:13  lr: 0.000008  loss: 3.4382 (3.4952)  time: 0.8299  data: 0.0005  max mem: 19734
Epoch: [5]  [ 960/1251]  eta: 0:04:05  lr: 0.000008  loss: 3.4997 (3.4941)  time: 0.8395  data: 0.0004  max mem: 19734
Epoch: [5]  [ 970/1251]  eta: 0:03:56  lr: 0.000008  loss: 3.5618 (3.4954)  time: 0.8249  data: 0.0004  max mem: 19734
Epoch: [5]  [ 980/1251]  eta: 0:03:48  lr: 0.000008  loss: 3.6198 (3.4957)  time: 0.8149  data: 0.0004  max mem: 19734
Epoch: [5]  [ 990/1251]  eta: 0:03:39  lr: 0.000008  loss: 3.6677 (3.4971)  time: 0.8152  data: 0.0005  max mem: 19734
Epoch: [5]  [1000/1251]  eta: 0:03:31  lr: 0.000008  loss: 3.6677 (3.4966)  time: 0.8173  data: 0.0005  max mem: 19734
Epoch: [5]  [1010/1251]  eta: 0:03:22  lr: 0.000008  loss: 3.7656 (3.4996)  time: 0.8152  data: 0.0005  max mem: 19734
Epoch: [5]  [1020/1251]  eta: 0:03:14  lr: 0.000008  loss: 3.7656 (3.4999)  time: 0.8132  data: 0.0005  max mem: 19734
Epoch: [5]  [1030/1251]  eta: 0:03:05  lr: 0.000008  loss: 3.1427 (3.4969)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [5]  [1040/1251]  eta: 0:02:57  lr: 0.000008  loss: 3.3750 (3.4970)  time: 0.8145  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4605, ratio_loss=0.0036, pruning_loss=0.2182, mse_loss=0.9977
Epoch: [5]  [1050/1251]  eta: 0:02:48  lr: 0.000008  loss: 3.5154 (3.4981)  time: 0.8122  data: 0.0006  max mem: 19734
Epoch: [5]  [1060/1251]  eta: 0:02:40  lr: 0.000008  loss: 3.5154 (3.4983)  time: 0.8244  data: 0.0004  max mem: 19734
Epoch: [5]  [1070/1251]  eta: 0:02:32  lr: 0.000008  loss: 3.4362 (3.4977)  time: 0.8345  data: 0.0004  max mem: 19734
Epoch: [5]  [1080/1251]  eta: 0:02:23  lr: 0.000008  loss: 3.3530 (3.4959)  time: 0.8292  data: 0.0004  max mem: 19734
Epoch: [5]  [1090/1251]  eta: 0:02:15  lr: 0.000008  loss: 3.5990 (3.4988)  time: 0.8243  data: 0.0005  max mem: 19734
Epoch: [5]  [1100/1251]  eta: 0:02:06  lr: 0.000008  loss: 3.6657 (3.4991)  time: 0.8365  data: 0.0006  max mem: 19734
Epoch: [5]  [1110/1251]  eta: 0:01:58  lr: 0.000008  loss: 3.6752 (3.4992)  time: 0.8316  data: 0.0006  max mem: 19734
Epoch: [5]  [1120/1251]  eta: 0:01:49  lr: 0.000008  loss: 3.6752 (3.4982)  time: 0.8142  data: 0.0006  max mem: 19734
Epoch: [5]  [1130/1251]  eta: 0:01:41  lr: 0.000008  loss: 3.4884 (3.4981)  time: 0.8159  data: 0.0007  max mem: 19734
Epoch: [5]  [1140/1251]  eta: 0:01:33  lr: 0.000008  loss: 3.5994 (3.4987)  time: 0.8141  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4537, ratio_loss=0.0034, pruning_loss=0.2206, mse_loss=0.9855
Epoch: [5]  [1150/1251]  eta: 0:01:24  lr: 0.000008  loss: 3.4410 (3.4967)  time: 0.8140  data: 0.0005  max mem: 19734
Epoch: [5]  [1160/1251]  eta: 0:01:16  lr: 0.000008  loss: 3.4791 (3.4965)  time: 0.8180  data: 0.0005  max mem: 19734
Epoch: [5]  [1170/1251]  eta: 0:01:07  lr: 0.000008  loss: 3.6442 (3.4982)  time: 0.8154  data: 0.0005  max mem: 19734
Epoch: [5]  [1180/1251]  eta: 0:00:59  lr: 0.000008  loss: 3.4977 (3.4967)  time: 0.8142  data: 0.0005  max mem: 19734
Epoch: [5]  [1190/1251]  eta: 0:00:51  lr: 0.000008  loss: 3.4028 (3.4943)  time: 0.8154  data: 0.0008  max mem: 19734
Epoch: [5]  [1200/1251]  eta: 0:00:42  lr: 0.000008  loss: 3.4899 (3.4945)  time: 0.8076  data: 0.0006  max mem: 19734
Epoch: [5]  [1210/1251]  eta: 0:00:34  lr: 0.000008  loss: 3.6915 (3.4955)  time: 0.8168  data: 0.0002  max mem: 19734
Epoch: [5]  [1220/1251]  eta: 0:00:25  lr: 0.000008  loss: 3.8291 (3.4966)  time: 0.8311  data: 0.0002  max mem: 19734
Epoch: [5]  [1230/1251]  eta: 0:00:17  lr: 0.000008  loss: 3.4620 (3.4964)  time: 0.8152  data: 0.0001  max mem: 19734
Epoch: [5]  [1240/1251]  eta: 0:00:09  lr: 0.000008  loss: 3.3861 (3.4951)  time: 0.8196  data: 0.0001  max mem: 19734
loss info: cls_loss=3.4262, ratio_loss=0.0033, pruning_loss=0.2196, mse_loss=1.0174
Epoch: [5]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 3.5567 (3.4947)  time: 0.8274  data: 0.0001  max mem: 19734
Epoch: [5] Total time: 0:17:27 (0.8377 s / it)
Averaged stats: lr: 0.000008  loss: 3.5567 (3.5016)
Test:  [  0/261]  eta: 2:10:28  loss: 0.7389 (0.7389)  acc1: 81.7708 (81.7708)  acc5: 96.3542 (96.3542)  time: 29.9930  data: 29.7482  max mem: 19734
Test:  [ 10/261]  eta: 0:12:13  loss: 0.7389 (0.7651)  acc1: 85.9375 (83.8542)  acc5: 96.8750 (96.1648)  time: 2.9214  data: 2.7103  max mem: 19734
Test:  [ 20/261]  eta: 0:06:42  loss: 0.9573 (0.9262)  acc1: 79.6875 (79.0923)  acc5: 94.2708 (94.7669)  time: 0.2552  data: 0.0097  max mem: 19734
Test:  [ 30/261]  eta: 0:04:37  loss: 0.8035 (0.8388)  acc1: 83.8542 (81.8044)  acc5: 94.7917 (95.2957)  time: 0.2551  data: 0.0120  max mem: 19734
Test:  [ 40/261]  eta: 0:03:53  loss: 0.5917 (0.8080)  acc1: 87.5000 (82.8887)  acc5: 96.3542 (95.5031)  time: 0.4144  data: 0.2095  max mem: 19734
Test:  [ 50/261]  eta: 0:03:28  loss: 0.9592 (0.8760)  acc1: 77.6042 (80.9130)  acc5: 93.7500 (94.9142)  time: 0.6570  data: 0.4015  max mem: 19734
Test:  [ 60/261]  eta: 0:03:01  loss: 1.0383 (0.8941)  acc1: 76.0417 (80.2852)  acc5: 93.7500 (94.9197)  time: 0.5880  data: 0.2043  max mem: 19734
Test:  [ 70/261]  eta: 0:02:45  loss: 1.0017 (0.8964)  acc1: 77.0833 (79.7975)  acc5: 95.3125 (95.1658)  time: 0.5515  data: 0.1749  max mem: 19734
Test:  [ 80/261]  eta: 0:02:28  loss: 0.8461 (0.8959)  acc1: 78.6458 (79.9704)  acc5: 96.8750 (95.2611)  time: 0.5728  data: 0.2347  max mem: 19734
Test:  [ 90/261]  eta: 0:02:12  loss: 0.8425 (0.8809)  acc1: 83.8542 (80.4373)  acc5: 95.8333 (95.3640)  time: 0.4591  data: 0.0767  max mem: 19734
Test:  [100/261]  eta: 0:01:58  loss: 0.8501 (0.8851)  acc1: 83.8542 (80.4765)  acc5: 95.3125 (95.4002)  time: 0.3823  data: 0.0837  max mem: 19734
Test:  [110/261]  eta: 0:01:44  loss: 0.9038 (0.9110)  acc1: 77.6042 (79.9925)  acc5: 94.2708 (95.0638)  time: 0.3133  data: 0.0813  max mem: 19734
Test:  [120/261]  eta: 0:01:34  loss: 1.2839 (0.9520)  acc1: 71.8750 (79.0806)  acc5: 89.5833 (94.4861)  time: 0.3385  data: 0.0153  max mem: 19734
Test:  [130/261]  eta: 0:01:24  loss: 1.5420 (0.9998)  acc1: 67.7083 (78.0972)  acc5: 86.9792 (93.8892)  time: 0.3774  data: 0.0646  max mem: 19734
Test:  [140/261]  eta: 0:01:16  loss: 1.3564 (1.0268)  acc1: 69.7917 (77.4638)  acc5: 89.0625 (93.6355)  time: 0.3820  data: 0.1740  max mem: 19734
Test:  [150/261]  eta: 0:01:07  loss: 1.2538 (1.0334)  acc1: 72.9167 (77.4559)  acc5: 90.6250 (93.4775)  time: 0.3783  data: 0.1245  max mem: 19734
Test:  [160/261]  eta: 0:01:00  loss: 1.1670 (1.0555)  acc1: 75.0000 (77.0833)  acc5: 90.6250 (93.1483)  time: 0.4112  data: 0.0125  max mem: 19734
Test:  [170/261]  eta: 0:00:56  loss: 1.3623 (1.0885)  acc1: 64.5833 (76.3006)  acc5: 86.9792 (92.7936)  time: 0.7094  data: 0.2830  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: 1.5256 (1.1054)  acc1: 64.5833 (75.9064)  acc5: 89.0625 (92.6249)  time: 0.5963  data: 0.2896  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: 1.4693 (1.1188)  acc1: 66.6667 (75.6435)  acc5: 90.1042 (92.4766)  time: 0.2290  data: 0.0139  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.3926 (1.1329)  acc1: 70.3125 (75.3161)  acc5: 89.5833 (92.3067)  time: 0.1893  data: 0.0170  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.4313 (1.1479)  acc1: 68.7500 (75.0444)  acc5: 89.5833 (92.1110)  time: 0.1654  data: 0.0145  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4978 (1.1671)  acc1: 65.6250 (74.5452)  acc5: 88.0208 (91.9306)  time: 0.1453  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.5105 (1.1778)  acc1: 65.1042 (74.3213)  acc5: 88.0208 (91.7704)  time: 0.1455  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.4268 (1.1875)  acc1: 66.1458 (74.0556)  acc5: 90.1042 (91.7034)  time: 0.1455  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0161 (1.1781)  acc1: 76.5625 (74.2758)  acc5: 93.7500 (91.8285)  time: 0.1453  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9337 (1.1767)  acc1: 76.5625 (74.2780)  acc5: 95.3125 (91.9020)  time: 0.1411  data: 0.0001  max mem: 19734
Test: Total time: 0:02:01 (0.4657 s / it)
* Acc@1 74.278 Acc@5 91.902 loss 1.177
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.28%
Epoch: [6]  [   0/1251]  eta: 3:06:28  lr: 0.000010  loss: 2.7970 (2.7970)  time: 8.9439  data: 8.0156  max mem: 19734
Epoch: [6]  [  10/1251]  eta: 0:34:53  lr: 0.000010  loss: 3.3561 (3.3613)  time: 1.6867  data: 0.8209  max mem: 19734
Epoch: [6]  [  20/1251]  eta: 0:26:04  lr: 0.000010  loss: 3.4585 (3.3790)  time: 0.8875  data: 0.0510  max mem: 19734
Epoch: [6]  [  30/1251]  eta: 0:22:53  lr: 0.000010  loss: 3.5390 (3.3856)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [6]  [  40/1251]  eta: 0:21:17  lr: 0.000010  loss: 3.5390 (3.4372)  time: 0.8283  data: 0.0005  max mem: 19734
Epoch: [6]  [  50/1251]  eta: 0:20:12  lr: 0.000010  loss: 3.6354 (3.4004)  time: 0.8303  data: 0.0005  max mem: 19734
Epoch: [6]  [  60/1251]  eta: 0:19:24  lr: 0.000010  loss: 3.0717 (3.3443)  time: 0.8191  data: 0.0004  max mem: 19734
Epoch: [6]  [  70/1251]  eta: 0:18:47  lr: 0.000010  loss: 3.4044 (3.3695)  time: 0.8145  data: 0.0005  max mem: 19734
Epoch: [6]  [  80/1251]  eta: 0:18:17  lr: 0.000010  loss: 3.5948 (3.4035)  time: 0.8141  data: 0.0006  max mem: 19734
Epoch: [6]  [  90/1251]  eta: 0:17:56  lr: 0.000010  loss: 3.5948 (3.4035)  time: 0.8296  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3666, ratio_loss=0.0035, pruning_loss=0.2202, mse_loss=1.0088
Epoch: [6]  [ 100/1251]  eta: 0:17:39  lr: 0.000010  loss: 3.6669 (3.4197)  time: 0.8511  data: 0.0006  max mem: 19734
Epoch: [6]  [ 110/1251]  eta: 0:17:20  lr: 0.000010  loss: 3.6631 (3.4060)  time: 0.8427  data: 0.0005  max mem: 19734
Epoch: [6]  [ 120/1251]  eta: 0:17:04  lr: 0.000010  loss: 3.5478 (3.4145)  time: 0.8307  data: 0.0004  max mem: 19734
Epoch: [6]  [ 130/1251]  eta: 0:16:48  lr: 0.000010  loss: 3.6428 (3.4261)  time: 0.8323  data: 0.0004  max mem: 19734
Epoch: [6]  [ 140/1251]  eta: 0:16:33  lr: 0.000010  loss: 3.4942 (3.4301)  time: 0.8243  data: 0.0004  max mem: 19734
Epoch: [6]  [ 150/1251]  eta: 0:16:18  lr: 0.000010  loss: 3.4942 (3.4456)  time: 0.8172  data: 0.0004  max mem: 19734
Epoch: [6]  [ 160/1251]  eta: 0:16:04  lr: 0.000010  loss: 3.7830 (3.4591)  time: 0.8169  data: 0.0004  max mem: 19734
Epoch: [6]  [ 170/1251]  eta: 0:15:51  lr: 0.000010  loss: 3.6180 (3.4499)  time: 0.8175  data: 0.0004  max mem: 19734
Epoch: [6]  [ 180/1251]  eta: 0:15:39  lr: 0.000010  loss: 3.4829 (3.4537)  time: 0.8173  data: 0.0004  max mem: 19734
Epoch: [6]  [ 190/1251]  eta: 0:15:27  lr: 0.000010  loss: 3.5805 (3.4635)  time: 0.8239  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4869, ratio_loss=0.0036, pruning_loss=0.2162, mse_loss=1.0032
Epoch: [6]  [ 200/1251]  eta: 0:15:15  lr: 0.000010  loss: 3.7554 (3.4788)  time: 0.8222  data: 0.0005  max mem: 19734
Epoch: [6]  [ 210/1251]  eta: 0:15:04  lr: 0.000010  loss: 3.5641 (3.4758)  time: 0.8158  data: 0.0007  max mem: 19734
Epoch: [6]  [ 220/1251]  eta: 0:14:53  lr: 0.000010  loss: 3.2200 (3.4526)  time: 0.8179  data: 0.0007  max mem: 19734
Epoch: [6]  [ 230/1251]  eta: 0:14:42  lr: 0.000010  loss: 3.3352 (3.4552)  time: 0.8185  data: 0.0005  max mem: 19734
Epoch: [6]  [ 240/1251]  eta: 0:14:32  lr: 0.000010  loss: 3.4881 (3.4570)  time: 0.8260  data: 0.0005  max mem: 19734
Epoch: [6]  [ 250/1251]  eta: 0:14:23  lr: 0.000010  loss: 3.4881 (3.4539)  time: 0.8403  data: 0.0004  max mem: 19734
Epoch: [6]  [ 260/1251]  eta: 0:14:13  lr: 0.000010  loss: 3.4250 (3.4557)  time: 0.8372  data: 0.0005  max mem: 19734
Epoch: [6]  [ 270/1251]  eta: 0:14:04  lr: 0.000010  loss: 3.6081 (3.4576)  time: 0.8323  data: 0.0005  max mem: 19734
Epoch: [6]  [ 280/1251]  eta: 0:13:54  lr: 0.000010  loss: 3.2938 (3.4551)  time: 0.8354  data: 0.0006  max mem: 19734
Epoch: [6]  [ 290/1251]  eta: 0:13:44  lr: 0.000010  loss: 3.4622 (3.4579)  time: 0.8242  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3728, ratio_loss=0.0036, pruning_loss=0.2213, mse_loss=0.9793
Epoch: [6]  [ 300/1251]  eta: 0:13:34  lr: 0.000010  loss: 3.4819 (3.4592)  time: 0.8149  data: 0.0004  max mem: 19734
Epoch: [6]  [ 310/1251]  eta: 0:13:24  lr: 0.000010  loss: 3.6277 (3.4607)  time: 0.8149  data: 0.0005  max mem: 19734
Epoch: [6]  [ 320/1251]  eta: 0:13:15  lr: 0.000010  loss: 3.5713 (3.4553)  time: 0.8163  data: 0.0004  max mem: 19734
Epoch: [6]  [ 330/1251]  eta: 0:13:05  lr: 0.000010  loss: 3.5713 (3.4644)  time: 0.8166  data: 0.0004  max mem: 19734
Epoch: [6]  [ 340/1251]  eta: 0:12:56  lr: 0.000010  loss: 3.7427 (3.4648)  time: 0.8279  data: 0.0005  max mem: 19734
Epoch: [6]  [ 350/1251]  eta: 0:12:47  lr: 0.000010  loss: 3.5979 (3.4597)  time: 0.8282  data: 0.0005  max mem: 19734
Epoch: [6]  [ 360/1251]  eta: 0:12:37  lr: 0.000010  loss: 3.7330 (3.4662)  time: 0.8165  data: 0.0005  max mem: 19734
Epoch: [6]  [ 370/1251]  eta: 0:12:28  lr: 0.000010  loss: 3.8038 (3.4709)  time: 0.8177  data: 0.0005  max mem: 19734
Epoch: [6]  [ 380/1251]  eta: 0:12:19  lr: 0.000010  loss: 3.6770 (3.4673)  time: 0.8158  data: 0.0005  max mem: 19734
Epoch: [6]  [ 390/1251]  eta: 0:12:10  lr: 0.000010  loss: 3.6770 (3.4706)  time: 0.8308  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4846, ratio_loss=0.0036, pruning_loss=0.2162, mse_loss=0.9603
Epoch: [6]  [ 400/1251]  eta: 0:12:02  lr: 0.000010  loss: 3.7875 (3.4775)  time: 0.8477  data: 0.0005  max mem: 19734
Epoch: [6]  [ 410/1251]  eta: 0:11:53  lr: 0.000010  loss: 3.3931 (3.4730)  time: 0.8316  data: 0.0006  max mem: 19734
Epoch: [6]  [ 420/1251]  eta: 0:11:44  lr: 0.000010  loss: 3.3869 (3.4766)  time: 0.8351  data: 0.0006  max mem: 19734
Epoch: [6]  [ 430/1251]  eta: 0:11:35  lr: 0.000010  loss: 3.5702 (3.4753)  time: 0.8345  data: 0.0005  max mem: 19734
Epoch: [6]  [ 440/1251]  eta: 0:11:26  lr: 0.000010  loss: 3.5866 (3.4784)  time: 0.8139  data: 0.0004  max mem: 19734
Epoch: [6]  [ 450/1251]  eta: 0:11:17  lr: 0.000010  loss: 3.5593 (3.4730)  time: 0.8118  data: 0.0005  max mem: 19734
Epoch: [6]  [ 460/1251]  eta: 0:11:08  lr: 0.000010  loss: 3.4496 (3.4705)  time: 0.8132  data: 0.0005  max mem: 19734
Epoch: [6]  [ 470/1251]  eta: 0:10:59  lr: 0.000010  loss: 3.5416 (3.4696)  time: 0.8159  data: 0.0005  max mem: 19734
Epoch: [6]  [ 480/1251]  eta: 0:10:50  lr: 0.000010  loss: 3.3439 (3.4677)  time: 0.8257  data: 0.0005  max mem: 19734
Epoch: [6]  [ 490/1251]  eta: 0:10:41  lr: 0.000010  loss: 3.3258 (3.4660)  time: 0.8232  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3920, ratio_loss=0.0037, pruning_loss=0.2183, mse_loss=1.0062
Epoch: [6]  [ 500/1251]  eta: 0:10:33  lr: 0.000010  loss: 3.5071 (3.4665)  time: 0.8129  data: 0.0004  max mem: 19734
Epoch: [6]  [ 510/1251]  eta: 0:10:24  lr: 0.000010  loss: 3.3796 (3.4632)  time: 0.8156  data: 0.0004  max mem: 19734
Epoch: [6]  [ 520/1251]  eta: 0:10:15  lr: 0.000010  loss: 3.2085 (3.4606)  time: 0.8154  data: 0.0004  max mem: 19734
Epoch: [6]  [ 530/1251]  eta: 0:10:06  lr: 0.000010  loss: 3.5175 (3.4638)  time: 0.8144  data: 0.0004  max mem: 19734
Epoch: [6]  [ 540/1251]  eta: 0:09:58  lr: 0.000010  loss: 3.7065 (3.4634)  time: 0.8358  data: 0.0004  max mem: 19734
Epoch: [6]  [ 550/1251]  eta: 0:09:49  lr: 0.000010  loss: 3.3298 (3.4566)  time: 0.8421  data: 0.0004  max mem: 19734
Epoch: [6]  [ 560/1251]  eta: 0:09:41  lr: 0.000010  loss: 3.5914 (3.4564)  time: 0.8224  data: 0.0004  max mem: 19734
Epoch: [6]  [ 570/1251]  eta: 0:09:32  lr: 0.000010  loss: 3.7354 (3.4606)  time: 0.8347  data: 0.0004  max mem: 19734
Epoch: [6]  [ 580/1251]  eta: 0:09:24  lr: 0.000010  loss: 3.7354 (3.4624)  time: 0.8322  data: 0.0004  max mem: 19734
Epoch: [6]  [ 590/1251]  eta: 0:09:15  lr: 0.000010  loss: 3.7659 (3.4684)  time: 0.8124  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4303, ratio_loss=0.0038, pruning_loss=0.2155, mse_loss=0.9663
Epoch: [6]  [ 600/1251]  eta: 0:09:06  lr: 0.000010  loss: 3.6352 (3.4648)  time: 0.8142  data: 0.0007  max mem: 19734
Epoch: [6]  [ 610/1251]  eta: 0:08:58  lr: 0.000010  loss: 3.2524 (3.4607)  time: 0.8148  data: 0.0007  max mem: 19734
Epoch: [6]  [ 620/1251]  eta: 0:08:49  lr: 0.000010  loss: 3.5368 (3.4642)  time: 0.8159  data: 0.0004  max mem: 19734
Epoch: [6]  [ 630/1251]  eta: 0:08:40  lr: 0.000010  loss: 3.5296 (3.4611)  time: 0.8220  data: 0.0004  max mem: 19734
Epoch: [6]  [ 640/1251]  eta: 0:08:32  lr: 0.000010  loss: 3.4638 (3.4627)  time: 0.8190  data: 0.0003  max mem: 19734
Epoch: [6]  [ 650/1251]  eta: 0:08:23  lr: 0.000010  loss: 3.6063 (3.4614)  time: 0.8124  data: 0.0004  max mem: 19734
Epoch: [6]  [ 660/1251]  eta: 0:08:15  lr: 0.000010  loss: 3.3222 (3.4592)  time: 0.8164  data: 0.0005  max mem: 19734
Epoch: [6]  [ 670/1251]  eta: 0:08:06  lr: 0.000010  loss: 3.4923 (3.4609)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [6]  [ 680/1251]  eta: 0:07:58  lr: 0.000010  loss: 3.5793 (3.4639)  time: 0.8371  data: 0.0005  max mem: 19734
Epoch: [6]  [ 690/1251]  eta: 0:07:49  lr: 0.000010  loss: 3.5265 (3.4619)  time: 0.8422  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3953, ratio_loss=0.0039, pruning_loss=0.2164, mse_loss=0.9702
Epoch: [6]  [ 700/1251]  eta: 0:07:41  lr: 0.000010  loss: 3.5268 (3.4656)  time: 0.8291  data: 0.0005  max mem: 19734
Epoch: [6]  [ 710/1251]  eta: 0:07:32  lr: 0.000010  loss: 3.7557 (3.4650)  time: 0.8298  data: 0.0005  max mem: 19734
Epoch: [6]  [ 720/1251]  eta: 0:07:24  lr: 0.000010  loss: 3.1788 (3.4624)  time: 0.8329  data: 0.0005  max mem: 19734
Epoch: [6]  [ 730/1251]  eta: 0:07:16  lr: 0.000010  loss: 3.0247 (3.4603)  time: 0.8279  data: 0.0004  max mem: 19734
Epoch: [6]  [ 740/1251]  eta: 0:07:07  lr: 0.000010  loss: 3.6842 (3.4613)  time: 0.8168  data: 0.0004  max mem: 19734
Epoch: [6]  [ 750/1251]  eta: 0:06:59  lr: 0.000010  loss: 3.6514 (3.4615)  time: 0.8169  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 760/1251]  eta: 0:06:50  lr: 0.000010  loss: 2.1645 (3.4213)  time: 0.8074  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 770/1251]  eta: 0:06:41  lr: 0.000010  loss: 0.0000 (3.3769)  time: 0.7896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 780/1251]  eta: 0:06:33  lr: 0.000010  loss: 0.0000 (3.3337)  time: 0.7851  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 790/1251]  eta: 0:06:24  lr: 0.000010  loss: 0.0000 (3.2915)  time: 0.7862  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 800/1251]  eta: 0:06:15  lr: 0.000010  loss: 0.0000 (3.2504)  time: 0.7865  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 810/1251]  eta: 0:06:07  lr: 0.000010  loss: 0.0000 (3.2104)  time: 0.7851  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 820/1251]  eta: 0:05:58  lr: 0.000010  loss: 0.0000 (3.1713)  time: 0.7792  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 830/1251]  eta: 0:05:50  lr: 0.000010  loss: 0.0000 (3.1331)  time: 0.8056  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 840/1251]  eta: 0:05:41  lr: 0.000010  loss: 0.0000 (3.0958)  time: 0.8183  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 850/1251]  eta: 0:05:33  lr: 0.000010  loss: 0.0000 (3.0595)  time: 0.7922  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 860/1251]  eta: 0:05:24  lr: 0.000010  loss: 0.0000 (3.0239)  time: 0.7984  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 870/1251]  eta: 0:05:16  lr: 0.000010  loss: 0.0000 (2.9892)  time: 0.8026  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 880/1251]  eta: 0:05:07  lr: 0.000010  loss: 0.0000 (2.9553)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 890/1251]  eta: 0:04:59  lr: 0.000010  loss: 0.0000 (2.9221)  time: 0.7813  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 900/1251]  eta: 0:04:50  lr: 0.000010  loss: 0.0000 (2.8897)  time: 0.7817  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 910/1251]  eta: 0:04:42  lr: 0.000010  loss: 0.0000 (2.8580)  time: 0.7869  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 920/1251]  eta: 0:04:34  lr: 0.000010  loss: 0.0000 (2.8269)  time: 0.7871  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 930/1251]  eta: 0:04:25  lr: 0.000010  loss: 0.0000 (2.7966)  time: 0.7879  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 940/1251]  eta: 0:04:17  lr: 0.000010  loss: 0.0000 (2.7668)  time: 0.7871  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 950/1251]  eta: 0:04:08  lr: 0.000010  loss: 0.0000 (2.7378)  time: 0.7859  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 960/1251]  eta: 0:04:00  lr: 0.000010  loss: 0.0000 (2.7093)  time: 0.7848  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 970/1251]  eta: 0:03:52  lr: 0.000010  loss: 0.0000 (2.6814)  time: 0.7803  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 980/1251]  eta: 0:03:43  lr: 0.000010  loss: 0.0000 (2.6540)  time: 0.8075  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 990/1251]  eta: 0:03:35  lr: 0.000010  loss: 0.0000 (2.6272)  time: 0.8242  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1000/1251]  eta: 0:03:27  lr: 0.000010  loss: 0.0000 (2.6010)  time: 0.8135  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1010/1251]  eta: 0:03:18  lr: 0.000010  loss: 0.0000 (2.5753)  time: 0.8032  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1020/1251]  eta: 0:03:10  lr: 0.000010  loss: 0.0000 (2.5500)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1030/1251]  eta: 0:03:02  lr: 0.000010  loss: 0.0000 (2.5253)  time: 0.7861  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1040/1251]  eta: 0:02:53  lr: 0.000010  loss: 0.0000 (2.5011)  time: 0.7826  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1050/1251]  eta: 0:02:45  lr: 0.000010  loss: 0.0000 (2.4773)  time: 0.7791  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1060/1251]  eta: 0:02:37  lr: 0.000010  loss: 0.0000 (2.4539)  time: 0.7808  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1070/1251]  eta: 0:02:28  lr: 0.000010  loss: 0.0000 (2.4310)  time: 0.7853  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1080/1251]  eta: 0:02:20  lr: 0.000010  loss: 0.0000 (2.4085)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1090/1251]  eta: 0:02:12  lr: 0.000010  loss: 0.0000 (2.3864)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1100/1251]  eta: 0:02:04  lr: 0.000010  loss: 0.0000 (2.3648)  time: 0.7862  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1110/1251]  eta: 0:01:55  lr: 0.000010  loss: 0.0000 (2.3435)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1120/1251]  eta: 0:01:47  lr: 0.000010  loss: 0.0000 (2.3226)  time: 0.7939  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1130/1251]  eta: 0:01:39  lr: 0.000010  loss: 0.0000 (2.3020)  time: 0.8023  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1140/1251]  eta: 0:01:31  lr: 0.000010  loss: 0.0000 (2.2819)  time: 0.7976  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1150/1251]  eta: 0:01:22  lr: 0.000010  loss: 0.0000 (2.2620)  time: 0.8055  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1160/1251]  eta: 0:01:14  lr: 0.000010  loss: 0.0000 (2.2426)  time: 0.8051  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1170/1251]  eta: 0:01:06  lr: 0.000010  loss: 0.0000 (2.2234)  time: 0.7899  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1180/1251]  eta: 0:00:58  lr: 0.000010  loss: 0.0000 (2.2046)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1190/1251]  eta: 0:00:50  lr: 0.000010  loss: 0.0000 (2.1861)  time: 0.7830  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (2.1679)  time: 0.7700  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (2.1500)  time: 0.7696  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (2.1324)  time: 0.7724  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1230/1251]  eta: 0:00:17  lr: 0.000010  loss: 0.0000 (2.1150)  time: 0.7740  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (2.0980)  time: 0.7706  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (2.0812)  time: 0.7694  data: 0.0001  max mem: 19734
Epoch: [6] Total time: 0:17:03 (0.8184 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (2.0988)
Test:  [  0/261]  eta: 2:15:53  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 31.2396  data: 30.9750  max mem: 19734
Test:  [ 10/261]  eta: 0:13:41  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.2717  data: 3.0138  max mem: 19734
Test:  [ 20/261]  eta: 0:07:19  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3550  data: 0.1130  max mem: 19734
Test:  [ 30/261]  eta: 0:05:05  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2493  data: 0.0136  max mem: 19734
Test:  [ 40/261]  eta: 0:04:06  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3712  data: 0.0831  max mem: 19734
Test:  [ 50/261]  eta: 0:03:19  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.3606  data: 0.0791  max mem: 19734
Test:  [ 60/261]  eta: 0:02:52  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3361  data: 0.0082  max mem: 19734
Test:  [ 70/261]  eta: 0:02:43  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.6254  data: 0.2483  max mem: 19734
Test:  [ 80/261]  eta: 0:02:22  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.5682  data: 0.2506  max mem: 19734
Test:  [ 90/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2972  data: 0.0111  max mem: 19734
Test:  [100/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.7301  data: 0.4236  max mem: 19734
Test:  [110/261]  eta: 0:01:51  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.7820  data: 0.4219  max mem: 19734
Test:  [120/261]  eta: 0:01:39  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3421  data: 0.0106  max mem: 19734
Test:  [130/261]  eta: 0:01:28  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3042  data: 0.0129  max mem: 19734
Test:  [140/261]  eta: 0:01:18  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3149  data: 0.0693  max mem: 19734
Test:  [150/261]  eta: 0:01:10  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4018  data: 0.0742  max mem: 19734
Test:  [160/261]  eta: 0:01:03  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.4671  data: 0.0187  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.4249  data: 0.0155  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.3153  data: 0.0138  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1923  data: 0.0079  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1469  data: 0.0043  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1396  data: 0.0020  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1361  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1359  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1314  data: 0.0001  max mem: 19734
Test: Total time: 0:01:58 (0.4539 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [7]  [   0/1251]  eta: 6:20:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 18.2455  data: 15.9240  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  10/1251]  eta: 0:54:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.6203  data: 1.4484  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  20/1251]  eta: 0:36:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.9310  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  30/1251]  eta: 0:29:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  40/1251]  eta: 0:26:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8001  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  50/1251]  eta: 0:23:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8155  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  60/1251]  eta: 0:22:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  70/1251]  eta: 0:21:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  80/1251]  eta: 0:20:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  90/1251]  eta: 0:19:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8030  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 100/1251]  eta: 0:19:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7978  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 110/1251]  eta: 0:18:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 120/1251]  eta: 0:18:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 130/1251]  eta: 0:17:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 140/1251]  eta: 0:17:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 150/1251]  eta: 0:17:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 160/1251]  eta: 0:16:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8189  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 170/1251]  eta: 0:16:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8235  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 180/1251]  eta: 0:16:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 190/1251]  eta: 0:15:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8046  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 200/1251]  eta: 0:15:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8018  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 210/1251]  eta: 0:15:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 220/1251]  eta: 0:15:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 230/1251]  eta: 0:15:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 240/1251]  eta: 0:14:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 250/1251]  eta: 0:14:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 260/1251]  eta: 0:14:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7821  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 270/1251]  eta: 0:14:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 280/1251]  eta: 0:14:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 290/1251]  eta: 0:13:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 300/1251]  eta: 0:13:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 310/1251]  eta: 0:13:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8126  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 320/1251]  eta: 0:13:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8108  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 330/1251]  eta: 0:13:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 340/1251]  eta: 0:12:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8081  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 350/1251]  eta: 0:12:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 360/1251]  eta: 0:12:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 370/1251]  eta: 0:12:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 380/1251]  eta: 0:12:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 390/1251]  eta: 0:12:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 400/1251]  eta: 0:11:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 410/1251]  eta: 0:11:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7794  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 420/1251]  eta: 0:11:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 430/1251]  eta: 0:11:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 440/1251]  eta: 0:11:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 450/1251]  eta: 0:11:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8052  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 460/1251]  eta: 0:11:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8182  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 470/1251]  eta: 0:10:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8014  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 480/1251]  eta: 0:10:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7957  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 490/1251]  eta: 0:10:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 500/1251]  eta: 0:10:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 510/1251]  eta: 0:10:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 520/1251]  eta: 0:10:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7780  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 530/1251]  eta: 0:10:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 540/1251]  eta: 0:09:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7946  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 550/1251]  eta: 0:09:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 560/1251]  eta: 0:09:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 570/1251]  eta: 0:09:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 580/1251]  eta: 0:09:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7883  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 590/1251]  eta: 0:09:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 600/1251]  eta: 0:08:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8049  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 610/1251]  eta: 0:08:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8074  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 620/1251]  eta: 0:08:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8075  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 630/1251]  eta: 0:08:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 640/1251]  eta: 0:08:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 650/1251]  eta: 0:08:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 660/1251]  eta: 0:08:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 670/1251]  eta: 0:07:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 680/1251]  eta: 0:07:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 690/1251]  eta: 0:07:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 700/1251]  eta: 0:07:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7820  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 710/1251]  eta: 0:07:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7794  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 720/1251]  eta: 0:07:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 730/1251]  eta: 0:07:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 740/1251]  eta: 0:06:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8010  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 750/1251]  eta: 0:06:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8037  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 760/1251]  eta: 0:06:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 770/1251]  eta: 0:06:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8044  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 780/1251]  eta: 0:06:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 790/1251]  eta: 0:06:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 800/1251]  eta: 0:06:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 810/1251]  eta: 0:06:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7898  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 820/1251]  eta: 0:05:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 830/1251]  eta: 0:05:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 840/1251]  eta: 0:05:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 850/1251]  eta: 0:05:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 860/1251]  eta: 0:05:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 870/1251]  eta: 0:05:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 880/1251]  eta: 0:05:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 890/1251]  eta: 0:04:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8113  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 900/1251]  eta: 0:04:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8105  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 910/1251]  eta: 0:04:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7974  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 920/1251]  eta: 0:04:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8012  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 930/1251]  eta: 0:04:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 940/1251]  eta: 0:04:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 950/1251]  eta: 0:04:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 960/1251]  eta: 0:03:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 970/1251]  eta: 0:03:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 980/1251]  eta: 0:03:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1000/1251]  eta: 0:03:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1010/1251]  eta: 0:03:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7824  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1020/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7824  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1030/1251]  eta: 0:02:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8068  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1050/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7942  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1060/1251]  eta: 0:02:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8041  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1070/1251]  eta: 0:02:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8065  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1080/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1090/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1130/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1140/1251]  eta: 0:01:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1150/1251]  eta: 0:01:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7805  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7989  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7765  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1230/1251]  eta: 0:00:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7713  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7629  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7661  data: 0.0001  max mem: 19734
Epoch: [7] Total time: 0:16:51 (0.8088 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:22:04  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 32.6605  data: 32.3677  max mem: 19734
Test:  [ 10/261]  eta: 0:13:32  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.2374  data: 2.9492  max mem: 19734
Test:  [ 20/261]  eta: 0:07:37  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3593  data: 0.0102  max mem: 19734
Test:  [ 30/261]  eta: 0:05:18  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3556  data: 0.0158  max mem: 19734
Test:  [ 40/261]  eta: 0:04:17  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3997  data: 0.1594  max mem: 19734
Test:  [ 50/261]  eta: 0:03:24  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.3320  data: 0.1529  max mem: 19734
Test:  [ 60/261]  eta: 0:02:50  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1922  data: 0.0066  max mem: 19734
Test:  [ 70/261]  eta: 0:02:38  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.4835  data: 0.2799  max mem: 19734
Test:  [ 80/261]  eta: 0:02:26  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.7043  data: 0.4282  max mem: 19734
Test:  [ 90/261]  eta: 0:02:07  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4493  data: 0.1584  max mem: 19734
Test:  [100/261]  eta: 0:01:51  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.2194  data: 0.0224  max mem: 19734
Test:  [110/261]  eta: 0:01:41  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3175  data: 0.0340  max mem: 19734
Test:  [120/261]  eta: 0:01:30  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3643  data: 0.0265  max mem: 19734
Test:  [130/261]  eta: 0:01:26  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.6164  data: 0.2473  max mem: 19734
Test:  [140/261]  eta: 0:01:17  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.6207  data: 0.2540  max mem: 19734
Test:  [150/261]  eta: 0:01:08  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.3426  data: 0.0320  max mem: 19734
Test:  [160/261]  eta: 0:01:00  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3535  data: 0.0624  max mem: 19734
Test:  [170/261]  eta: 0:00:52  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2790  data: 0.0699  max mem: 19734
Test:  [180/261]  eta: 0:00:45  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2335  data: 0.0291  max mem: 19734
Test:  [190/261]  eta: 0:00:38  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.2633  data: 0.0654  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.2200  data: 0.0691  max mem: 19734
Test:  [210/261]  eta: 0:00:25  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1484  data: 0.0088  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1359  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:14  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1315  data: 0.0001  max mem: 19734
Test: Total time: 0:01:54 (0.4380 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [8]  [   0/1251]  eta: 5:04:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 14.6032  data: 11.4576  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  10/1251]  eta: 0:51:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.4859  data: 1.0428  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  20/1251]  eta: 0:34:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 1.0346  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  30/1251]  eta: 0:28:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  40/1251]  eta: 0:25:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  50/1251]  eta: 0:23:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  60/1251]  eta: 0:21:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  70/1251]  eta: 0:20:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8073  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  80/1251]  eta: 0:20:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8101  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  90/1251]  eta: 0:19:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8201  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 100/1251]  eta: 0:18:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8036  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 110/1251]  eta: 0:18:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 120/1251]  eta: 0:17:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7946  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 130/1251]  eta: 0:17:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 140/1251]  eta: 0:17:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 150/1251]  eta: 0:16:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 160/1251]  eta: 0:16:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 170/1251]  eta: 0:16:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 180/1251]  eta: 0:16:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 190/1251]  eta: 0:15:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7816  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 200/1251]  eta: 0:15:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 210/1251]  eta: 0:15:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8018  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 220/1251]  eta: 0:15:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8057  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 230/1251]  eta: 0:14:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8058  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 240/1251]  eta: 0:14:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8045  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 250/1251]  eta: 0:14:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 260/1251]  eta: 0:14:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 270/1251]  eta: 0:14:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 280/1251]  eta: 0:13:55  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 290/1251]  eta: 0:13:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 300/1251]  eta: 0:13:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 310/1251]  eta: 0:13:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 320/1251]  eta: 0:13:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 330/1251]  eta: 0:13:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 340/1251]  eta: 0:12:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 350/1251]  eta: 0:12:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 360/1251]  eta: 0:12:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8038  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 370/1251]  eta: 0:12:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8076  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 380/1251]  eta: 0:12:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8175  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 390/1251]  eta: 0:12:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8182  data: 0.0007  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 400/1251]  eta: 0:11:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8027  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 410/1251]  eta: 0:11:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 420/1251]  eta: 0:11:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 430/1251]  eta: 0:11:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 440/1251]  eta: 0:11:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 450/1251]  eta: 0:11:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 460/1251]  eta: 0:11:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 470/1251]  eta: 0:10:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 480/1251]  eta: 0:10:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 490/1251]  eta: 0:10:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0006  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 500/1251]  eta: 0:10:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 510/1251]  eta: 0:10:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8039  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 520/1251]  eta: 0:10:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8063  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 530/1251]  eta: 0:09:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8044  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 540/1251]  eta: 0:09:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 550/1251]  eta: 0:09:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 560/1251]  eta: 0:09:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7942  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 570/1251]  eta: 0:09:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 580/1251]  eta: 0:09:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 590/1251]  eta: 0:09:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 600/1251]  eta: 0:08:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 610/1251]  eta: 0:08:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 620/1251]  eta: 0:08:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 630/1251]  eta: 0:08:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 640/1251]  eta: 0:08:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7820  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 650/1251]  eta: 0:08:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8003  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 660/1251]  eta: 0:08:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8131  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 670/1251]  eta: 0:07:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8164  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 680/1251]  eta: 0:07:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8045  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 690/1251]  eta: 0:07:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 700/1251]  eta: 0:07:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 710/1251]  eta: 0:07:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 720/1251]  eta: 0:07:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 730/1251]  eta: 0:07:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 740/1251]  eta: 0:06:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 750/1251]  eta: 0:06:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 760/1251]  eta: 0:06:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7787  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 770/1251]  eta: 0:06:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7814  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 780/1251]  eta: 0:06:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 790/1251]  eta: 0:06:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0006  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 800/1251]  eta: 0:06:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 810/1251]  eta: 0:06:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8006  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 820/1251]  eta: 0:05:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8168  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 830/1251]  eta: 0:05:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8024  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 840/1251]  eta: 0:05:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 850/1251]  eta: 0:05:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 860/1251]  eta: 0:05:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 870/1251]  eta: 0:05:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 880/1251]  eta: 0:05:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 890/1251]  eta: 0:04:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 900/1251]  eta: 0:04:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 910/1251]  eta: 0:04:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7836  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 920/1251]  eta: 0:04:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 930/1251]  eta: 0:04:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 940/1251]  eta: 0:04:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 950/1251]  eta: 0:04:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8126  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 960/1251]  eta: 0:03:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8051  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 970/1251]  eta: 0:03:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8050  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 980/1251]  eta: 0:03:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7974  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1000/1251]  eta: 0:03:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1010/1251]  eta: 0:03:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1020/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1030/1251]  eta: 0:02:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1050/1251]  eta: 0:02:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1060/1251]  eta: 0:02:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1070/1251]  eta: 0:02:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1080/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1090/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8025  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8169  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8055  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1130/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1140/1251]  eta: 0:01:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1150/1251]  eta: 0:01:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7807  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7807  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0011  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7750  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7659  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7680  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1230/1251]  eta: 0:00:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0001  max mem: 19734
Epoch: [8] Total time: 0:16:50 (0.8081 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:08:14  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 29.4801  data: 29.3081  max mem: 19734
Test:  [ 10/261]  eta: 0:12:00  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.8703  data: 2.6754  max mem: 19734
Test:  [ 20/261]  eta: 0:06:59  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3530  data: 0.0843  max mem: 19734
Test:  [ 30/261]  eta: 0:05:14  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.5345  data: 0.0882  max mem: 19734
Test:  [ 40/261]  eta: 0:04:12  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5129  data: 0.1132  max mem: 19734
Test:  [ 50/261]  eta: 0:03:25  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.3731  data: 0.1292  max mem: 19734
Test:  [ 60/261]  eta: 0:02:54  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3141  data: 0.0337  max mem: 19734
Test:  [ 70/261]  eta: 0:02:33  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3676  data: 0.0113  max mem: 19734
Test:  [ 80/261]  eta: 0:02:25  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.5922  data: 0.2837  max mem: 19734
Test:  [ 90/261]  eta: 0:02:09  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.5986  data: 0.2847  max mem: 19734
Test:  [100/261]  eta: 0:01:55  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3849  data: 0.0149  max mem: 19734
Test:  [110/261]  eta: 0:01:41  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.2920  data: 0.0265  max mem: 19734
Test:  [120/261]  eta: 0:01:29  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2279  data: 0.0174  max mem: 19734
Test:  [130/261]  eta: 0:01:19  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2191  data: 0.0035  max mem: 19734
Test:  [140/261]  eta: 0:01:12  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3555  data: 0.1161  max mem: 19734
Test:  [150/261]  eta: 0:01:11  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.8916  data: 0.6721  max mem: 19734
Test:  [160/261]  eta: 0:01:03  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.8739  data: 0.5638  max mem: 19734
Test:  [170/261]  eta: 0:00:56  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.4798  data: 0.0118  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.4519  data: 0.0332  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.2835  data: 0.0296  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1482  data: 0.0031  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1383  data: 0.0004  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1358  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1313  data: 0.0001  max mem: 19734
Test: Total time: 0:02:02 (0.4676 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [9]  [   0/1251]  eta: 6:04:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 17.5057  data: 13.2455  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  10/1251]  eta: 0:51:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.4830  data: 1.2048  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  20/1251]  eta: 0:34:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8925  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  30/1251]  eta: 0:28:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [  40/1251]  eta: 0:25:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  50/1251]  eta: 0:23:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  60/1251]  eta: 0:21:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  70/1251]  eta: 0:20:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  80/1251]  eta: 0:19:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7818  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  90/1251]  eta: 0:19:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 100/1251]  eta: 0:18:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 110/1251]  eta: 0:18:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7997  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 120/1251]  eta: 0:17:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8053  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 130/1251]  eta: 0:17:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8105  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 140/1251]  eta: 0:17:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8082  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 150/1251]  eta: 0:16:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8014  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 160/1251]  eta: 0:16:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8088  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 170/1251]  eta: 0:16:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8044  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 180/1251]  eta: 0:16:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 190/1251]  eta: 0:15:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7793  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 200/1251]  eta: 0:15:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7786  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 210/1251]  eta: 0:15:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 220/1251]  eta: 0:15:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 230/1251]  eta: 0:14:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7808  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 240/1251]  eta: 0:14:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 250/1251]  eta: 0:14:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 260/1251]  eta: 0:14:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 270/1251]  eta: 0:14:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 280/1251]  eta: 0:13:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8206  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 290/1251]  eta: 0:13:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8155  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 300/1251]  eta: 0:13:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 310/1251]  eta: 0:13:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8048  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 320/1251]  eta: 0:13:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 330/1251]  eta: 0:13:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 340/1251]  eta: 0:12:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 350/1251]  eta: 0:12:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 360/1251]  eta: 0:12:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 370/1251]  eta: 0:12:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 380/1251]  eta: 0:12:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 390/1251]  eta: 0:12:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 400/1251]  eta: 0:11:55  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 410/1251]  eta: 0:11:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 420/1251]  eta: 0:11:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 430/1251]  eta: 0:11:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8061  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 440/1251]  eta: 0:11:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8016  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 450/1251]  eta: 0:11:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 460/1251]  eta: 0:11:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7972  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 470/1251]  eta: 0:10:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7963  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 480/1251]  eta: 0:10:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 490/1251]  eta: 0:10:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 500/1251]  eta: 0:10:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 510/1251]  eta: 0:10:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 520/1251]  eta: 0:10:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 530/1251]  eta: 0:09:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7806  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 540/1251]  eta: 0:09:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 550/1251]  eta: 0:09:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 560/1251]  eta: 0:09:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8066  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 570/1251]  eta: 0:09:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8050  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 580/1251]  eta: 0:09:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8078  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 590/1251]  eta: 0:09:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8031  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 600/1251]  eta: 0:08:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 610/1251]  eta: 0:08:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 620/1251]  eta: 0:08:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7921  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 630/1251]  eta: 0:08:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 640/1251]  eta: 0:08:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 650/1251]  eta: 0:08:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7821  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 660/1251]  eta: 0:08:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 670/1251]  eta: 0:07:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7921  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 680/1251]  eta: 0:07:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 690/1251]  eta: 0:07:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 700/1251]  eta: 0:07:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8066  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 710/1251]  eta: 0:07:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 720/1251]  eta: 0:07:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 730/1251]  eta: 0:07:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8095  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 740/1251]  eta: 0:06:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 750/1251]  eta: 0:06:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8074  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 760/1251]  eta: 0:06:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 770/1251]  eta: 0:06:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 780/1251]  eta: 0:06:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 790/1251]  eta: 0:06:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 800/1251]  eta: 0:06:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 810/1251]  eta: 0:06:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 820/1251]  eta: 0:05:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 830/1251]  eta: 0:05:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 840/1251]  eta: 0:05:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 850/1251]  eta: 0:05:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8063  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 860/1251]  eta: 0:05:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8038  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 870/1251]  eta: 0:05:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 880/1251]  eta: 0:05:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 890/1251]  eta: 0:04:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 900/1251]  eta: 0:04:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8087  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 910/1251]  eta: 0:04:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 920/1251]  eta: 0:04:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 930/1251]  eta: 0:04:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 940/1251]  eta: 0:04:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 950/1251]  eta: 0:04:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 960/1251]  eta: 0:03:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 970/1251]  eta: 0:03:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 980/1251]  eta: 0:03:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1000/1251]  eta: 0:03:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7989  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1010/1251]  eta: 0:03:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8150  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1020/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8040  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1030/1251]  eta: 0:02:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1050/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1060/1251]  eta: 0:02:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1070/1251]  eta: 0:02:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1080/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1090/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7816  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1130/1251]  eta: 0:01:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [1140/1251]  eta: 0:01:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1150/1251]  eta: 0:01:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8050  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8003  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7700  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7673  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1230/1251]  eta: 0:00:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7708  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7678  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7668  data: 0.0001  max mem: 19734
Epoch: [9] Total time: 0:16:51 (0.8083 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:37:20  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 36.1697  data: 35.9669  max mem: 19734
Test:  [ 10/261]  eta: 0:14:35  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.4880  data: 3.2796  max mem: 19734
Test:  [ 20/261]  eta: 0:07:55  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2614  data: 0.0131  max mem: 19734
Test:  [ 30/261]  eta: 0:05:34  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3284  data: 0.0154  max mem: 19734
Test:  [ 40/261]  eta: 0:04:45  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5753  data: 0.2556  max mem: 19734
Test:  [ 50/261]  eta: 0:03:59  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6491  data: 0.2513  max mem: 19734
Test:  [ 60/261]  eta: 0:03:20  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3958  data: 0.0134  max mem: 19734
Test:  [ 70/261]  eta: 0:02:58  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.4300  data: 0.1498  max mem: 19734
Test:  [ 80/261]  eta: 0:02:36  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4695  data: 0.1445  max mem: 19734
Test:  [ 90/261]  eta: 0:02:18  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3716  data: 0.0080  max mem: 19734
Test:  [100/261]  eta: 0:02:02  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3225  data: 0.0064  max mem: 19734
Test:  [110/261]  eta: 0:01:58  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.6697  data: 0.3872  max mem: 19734
Test:  [120/261]  eta: 0:01:44  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.6649  data: 0.3903  max mem: 19734
Test:  [130/261]  eta: 0:01:32  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2684  data: 0.0102  max mem: 19734
Test:  [140/261]  eta: 0:01:23  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3574  data: 0.1233  max mem: 19734
Test:  [150/261]  eta: 0:01:12  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.2922  data: 0.1215  max mem: 19734
Test:  [160/261]  eta: 0:01:02  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1437  data: 0.0035  max mem: 19734
Test:  [170/261]  eta: 0:00:54  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1485  data: 0.0022  max mem: 19734
Test:  [180/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1576  data: 0.0018  max mem: 19734
Test:  [190/261]  eta: 0:00:38  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1477  data: 0.0009  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1352  data: 0.0005  max mem: 19734
Test:  [210/261]  eta: 0:00:25  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1356  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:14  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1358  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1317  data: 0.0001  max mem: 19734
Test: Total time: 0:01:54 (0.4371 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [10]  [   0/1251]  eta: 5:33:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 16.0036  data: 15.0024  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  10/1251]  eta: 0:50:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.4221  data: 1.3655  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  20/1251]  eta: 0:33:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.9273  data: 0.0012  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  30/1251]  eta: 0:28:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8031  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  40/1251]  eta: 0:24:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8089  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  50/1251]  eta: 0:23:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8054  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  60/1251]  eta: 0:21:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8127  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  70/1251]  eta: 0:20:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  80/1251]  eta: 0:19:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  90/1251]  eta: 0:19:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 100/1251]  eta: 0:18:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 110/1251]  eta: 0:18:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 120/1251]  eta: 0:17:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 130/1251]  eta: 0:17:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 140/1251]  eta: 0:17:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 150/1251]  eta: 0:16:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 160/1251]  eta: 0:16:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 170/1251]  eta: 0:16:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7972  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 180/1251]  eta: 0:15:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8137  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 190/1251]  eta: 0:15:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8166  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 200/1251]  eta: 0:15:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8145  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 210/1251]  eta: 0:15:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8117  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 220/1251]  eta: 0:15:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8100  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 230/1251]  eta: 0:14:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 240/1251]  eta: 0:14:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 250/1251]  eta: 0:14:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 260/1251]  eta: 0:14:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 270/1251]  eta: 0:14:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 280/1251]  eta: 0:13:55  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 290/1251]  eta: 0:13:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 300/1251]  eta: 0:13:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 310/1251]  eta: 0:13:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 320/1251]  eta: 0:13:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7978  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 330/1251]  eta: 0:13:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8048  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 340/1251]  eta: 0:12:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8127  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 350/1251]  eta: 0:12:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8245  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 360/1251]  eta: 0:12:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8140  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 370/1251]  eta: 0:12:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 380/1251]  eta: 0:12:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 390/1251]  eta: 0:12:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 400/1251]  eta: 0:11:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 410/1251]  eta: 0:11:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 420/1251]  eta: 0:11:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 430/1251]  eta: 0:11:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 440/1251]  eta: 0:11:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 450/1251]  eta: 0:11:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 460/1251]  eta: 0:11:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 470/1251]  eta: 0:10:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8089  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 480/1251]  eta: 0:10:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8208  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 490/1251]  eta: 0:10:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8048  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 500/1251]  eta: 0:10:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 510/1251]  eta: 0:10:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8128  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 520/1251]  eta: 0:10:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 530/1251]  eta: 0:09:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 540/1251]  eta: 0:09:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 550/1251]  eta: 0:09:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 560/1251]  eta: 0:09:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 570/1251]  eta: 0:09:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 580/1251]  eta: 0:09:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 590/1251]  eta: 0:09:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 600/1251]  eta: 0:08:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7836  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 610/1251]  eta: 0:08:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 620/1251]  eta: 0:08:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8154  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 630/1251]  eta: 0:08:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8118  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 640/1251]  eta: 0:08:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 650/1251]  eta: 0:08:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 660/1251]  eta: 0:08:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8074  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 670/1251]  eta: 0:07:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 680/1251]  eta: 0:07:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 690/1251]  eta: 0:07:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 700/1251]  eta: 0:07:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 710/1251]  eta: 0:07:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 720/1251]  eta: 0:07:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 730/1251]  eta: 0:07:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 740/1251]  eta: 0:06:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 750/1251]  eta: 0:06:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 760/1251]  eta: 0:06:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7974  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 770/1251]  eta: 0:06:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8087  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 780/1251]  eta: 0:06:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8156  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 790/1251]  eta: 0:06:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8116  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 800/1251]  eta: 0:06:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 810/1251]  eta: 0:06:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8048  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 820/1251]  eta: 0:05:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7972  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 830/1251]  eta: 0:05:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7818  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 840/1251]  eta: 0:05:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7796  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 850/1251]  eta: 0:05:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 860/1251]  eta: 0:05:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 870/1251]  eta: 0:05:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7825  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 880/1251]  eta: 0:05:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 890/1251]  eta: 0:04:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 900/1251]  eta: 0:04:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 910/1251]  eta: 0:04:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7972  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 920/1251]  eta: 0:04:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8130  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 930/1251]  eta: 0:04:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8077  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 940/1251]  eta: 0:04:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 950/1251]  eta: 0:04:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8088  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 960/1251]  eta: 0:03:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 970/1251]  eta: 0:03:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 980/1251]  eta: 0:03:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7821  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1000/1251]  eta: 0:03:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1010/1251]  eta: 0:03:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1020/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1030/1251]  eta: 0:02:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1050/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1060/1251]  eta: 0:02:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8071  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1070/1251]  eta: 0:02:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8118  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1080/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8110  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1090/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7971  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1130/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1140/1251]  eta: 0:01:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1150/1251]  eta: 0:01:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7791  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7811  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1230/1251]  eta: 0:00:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7763  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7731  data: 0.0002  max mem: 19734
Epoch: [10] Total time: 0:16:52 (0.8096 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 0:42:48  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 9.8421  data: 9.4690  max mem: 19734
Test:  [ 10/261]  eta: 0:10:35  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.5337  data: 2.2417  max mem: 19734
Test:  [ 20/261]  eta: 0:05:56  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 1.0612  data: 0.7692  max mem: 19734
Test:  [ 30/261]  eta: 0:04:25  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3900  data: 0.0188  max mem: 19734
Test:  [ 40/261]  eta: 0:03:49  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5733  data: 0.2039  max mem: 19734
Test:  [ 50/261]  eta: 0:03:04  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4451  data: 0.2015  max mem: 19734
Test:  [ 60/261]  eta: 0:02:35  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2384  data: 0.0167  max mem: 19734
Test:  [ 70/261]  eta: 0:02:17  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3198  data: 0.0317  max mem: 19734
Test:  [ 80/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4366  data: 0.1255  max mem: 19734
Test:  [ 90/261]  eta: 0:01:48  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3395  data: 0.1127  max mem: 19734
Test:  [100/261]  eta: 0:01:43  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4302  data: 0.2512  max mem: 19734
Test:  [110/261]  eta: 0:01:31  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4793  data: 0.2478  max mem: 19734
Test:  [120/261]  eta: 0:01:21  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2646  data: 0.0102  max mem: 19734
Test:  [130/261]  eta: 0:01:19  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.6240  data: 0.3874  max mem: 19734
Test:  [140/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.9082  data: 0.6116  max mem: 19734
Test:  [150/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.6088  data: 0.2385  max mem: 19734
Test:  [160/261]  eta: 0:00:59  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3426  data: 0.0186  max mem: 19734
Test:  [170/261]  eta: 0:00:51  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2786  data: 0.0232  max mem: 19734
Test:  [180/261]  eta: 0:00:44  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2639  data: 0.0216  max mem: 19734
Test:  [190/261]  eta: 0:00:38  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.2631  data: 0.0137  max mem: 19734
Test:  [200/261]  eta: 0:00:31  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.2598  data: 0.0537  max mem: 19734
Test:  [210/261]  eta: 0:00:25  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1972  data: 0.0476  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:14  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1315  data: 0.0001  max mem: 19734
Test: Total time: 0:01:53 (0.4357 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [11]  [   0/1251]  eta: 5:26:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 15.6720  data: 8.6287  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  10/1251]  eta: 0:52:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.5243  data: 0.8239  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  20/1251]  eta: 0:35:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 1.0078  data: 0.0220  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  30/1251]  eta: 0:28:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  40/1251]  eta: 0:25:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  50/1251]  eta: 0:23:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  60/1251]  eta: 0:21:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  70/1251]  eta: 0:20:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  80/1251]  eta: 0:20:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  90/1251]  eta: 0:19:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8095  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 100/1251]  eta: 0:19:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8439  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 110/1251]  eta: 0:18:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8275  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 120/1251]  eta: 0:18:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 130/1251]  eta: 0:17:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 140/1251]  eta: 0:17:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 150/1251]  eta: 0:16:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 160/1251]  eta: 0:16:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 170/1251]  eta: 0:16:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 180/1251]  eta: 0:16:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 190/1251]  eta: 0:15:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 200/1251]  eta: 0:15:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 210/1251]  eta: 0:15:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7898  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 220/1251]  eta: 0:15:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 230/1251]  eta: 0:14:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8013  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 240/1251]  eta: 0:14:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8088  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 250/1251]  eta: 0:14:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8112  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 260/1251]  eta: 0:14:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8056  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 270/1251]  eta: 0:14:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 280/1251]  eta: 0:13:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 290/1251]  eta: 0:13:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 300/1251]  eta: 0:13:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 310/1251]  eta: 0:13:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 320/1251]  eta: 0:13:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 330/1251]  eta: 0:13:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 340/1251]  eta: 0:12:55  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 350/1251]  eta: 0:12:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 360/1251]  eta: 0:12:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 370/1251]  eta: 0:12:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 380/1251]  eta: 0:12:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7984  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 390/1251]  eta: 0:12:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8131  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 400/1251]  eta: 0:11:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8166  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 410/1251]  eta: 0:11:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8089  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 420/1251]  eta: 0:11:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 430/1251]  eta: 0:11:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 440/1251]  eta: 0:11:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 450/1251]  eta: 0:11:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 460/1251]  eta: 0:11:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 470/1251]  eta: 0:10:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 480/1251]  eta: 0:10:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 490/1251]  eta: 0:10:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 500/1251]  eta: 0:10:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 510/1251]  eta: 0:10:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 520/1251]  eta: 0:10:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 530/1251]  eta: 0:09:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 540/1251]  eta: 0:09:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8113  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 550/1251]  eta: 0:09:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8255  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 560/1251]  eta: 0:09:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8053  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 570/1251]  eta: 0:09:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7883  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 580/1251]  eta: 0:09:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 590/1251]  eta: 0:09:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 600/1251]  eta: 0:08:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 610/1251]  eta: 0:08:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 620/1251]  eta: 0:08:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 630/1251]  eta: 0:08:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 640/1251]  eta: 0:08:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 650/1251]  eta: 0:08:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 660/1251]  eta: 0:08:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 670/1251]  eta: 0:07:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 680/1251]  eta: 0:07:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8089  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 690/1251]  eta: 0:07:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8030  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 700/1251]  eta: 0:07:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8107  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 710/1251]  eta: 0:07:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8086  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 720/1251]  eta: 0:07:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 730/1251]  eta: 0:07:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 740/1251]  eta: 0:06:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 750/1251]  eta: 0:06:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 760/1251]  eta: 0:06:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 770/1251]  eta: 0:06:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 780/1251]  eta: 0:06:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 790/1251]  eta: 0:06:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 800/1251]  eta: 0:06:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 810/1251]  eta: 0:06:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 820/1251]  eta: 0:05:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8177  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 830/1251]  eta: 0:05:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8139  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 840/1251]  eta: 0:05:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8036  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 850/1251]  eta: 0:05:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8039  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 860/1251]  eta: 0:05:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 870/1251]  eta: 0:05:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 880/1251]  eta: 0:05:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 890/1251]  eta: 0:04:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 900/1251]  eta: 0:04:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 910/1251]  eta: 0:04:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7794  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 920/1251]  eta: 0:04:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 930/1251]  eta: 0:04:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 940/1251]  eta: 0:04:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 950/1251]  eta: 0:04:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 960/1251]  eta: 0:03:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 970/1251]  eta: 0:03:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8144  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 980/1251]  eta: 0:03:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8158  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8092  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1000/1251]  eta: 0:03:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8017  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1010/1251]  eta: 0:03:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7966  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1020/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1030/1251]  eta: 0:02:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1050/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1060/1251]  eta: 0:02:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1070/1251]  eta: 0:02:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1080/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1090/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8076  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8208  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1130/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8142  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1140/1251]  eta: 0:01:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8119  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1150/1251]  eta: 0:01:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8013  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7784  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7679  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7664  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1230/1251]  eta: 0:00:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7723  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7724  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7747  data: 0.0002  max mem: 19734
Epoch: [11] Total time: 0:16:52 (0.8097 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:56:28  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 40.5694  data: 40.2140  max mem: 19734
Test:  [ 10/261]  eta: 0:16:49  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 4.0213  data: 3.6685  max mem: 19734
Test:  [ 20/261]  eta: 0:08:58  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3193  data: 0.0136  max mem: 19734
Test:  [ 30/261]  eta: 0:06:10  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2726  data: 0.0151  max mem: 19734
Test:  [ 40/261]  eta: 0:05:00  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4396  data: 0.1931  max mem: 19734
Test:  [ 50/261]  eta: 0:03:57  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.3905  data: 0.1920  max mem: 19734
Test:  [ 60/261]  eta: 0:03:15  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1827  data: 0.0113  max mem: 19734
Test:  [ 70/261]  eta: 0:02:49  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.2840  data: 0.0608  max mem: 19734
Test:  [ 80/261]  eta: 0:02:28  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3582  data: 0.1156  max mem: 19734
Test:  [ 90/261]  eta: 0:02:08  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2564  data: 0.0617  max mem: 19734
Test:  [100/261]  eta: 0:02:07  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.6749  data: 0.4736  max mem: 19734
Test:  [110/261]  eta: 0:01:55  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.8145  data: 0.5736  max mem: 19734
Test:  [120/261]  eta: 0:01:42  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4074  data: 0.1094  max mem: 19734
Test:  [130/261]  eta: 0:01:31  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3450  data: 0.0135  max mem: 19734
Test:  [140/261]  eta: 0:01:20  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.2808  data: 0.0121  max mem: 19734
Test:  [150/261]  eta: 0:01:10  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.2407  data: 0.0414  max mem: 19734
Test:  [160/261]  eta: 0:01:02  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2572  data: 0.0794  max mem: 19734
Test:  [170/261]  eta: 0:00:54  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3200  data: 0.1576  max mem: 19734
Test:  [180/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2755  data: 0.1345  max mem: 19734
Test:  [190/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1563  data: 0.0199  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1356  data: 0.0003  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1362  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1358  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:14  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1350  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1349  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1351  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1316  data: 0.0001  max mem: 19734
Test: Total time: 0:01:55 (0.4425 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [12]  [   0/1251]  eta: 6:57:55  lr: 0.000010  loss: 0.0000 (0.0000)  time: 20.0443  data: 11.2578  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  10/1251]  eta: 0:55:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.6877  data: 1.1062  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  20/1251]  eta: 0:36:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8781  data: 0.0458  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  30/1251]  eta: 0:29:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  40/1251]  eta: 0:26:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  50/1251]  eta: 0:24:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  60/1251]  eta: 0:22:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  70/1251]  eta: 0:21:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  80/1251]  eta: 0:20:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  90/1251]  eta: 0:19:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 100/1251]  eta: 0:19:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 110/1251]  eta: 0:18:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 120/1251]  eta: 0:18:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 130/1251]  eta: 0:17:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 140/1251]  eta: 0:17:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 150/1251]  eta: 0:17:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8299  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 160/1251]  eta: 0:16:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8398  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 170/1251]  eta: 0:16:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8147  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 180/1251]  eta: 0:16:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8139  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 190/1251]  eta: 0:16:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 200/1251]  eta: 0:15:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 210/1251]  eta: 0:15:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7945  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 220/1251]  eta: 0:15:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 230/1251]  eta: 0:15:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 240/1251]  eta: 0:14:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 250/1251]  eta: 0:14:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7921  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 260/1251]  eta: 0:14:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 270/1251]  eta: 0:14:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 280/1251]  eta: 0:14:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7974  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 290/1251]  eta: 0:13:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 300/1251]  eta: 0:13:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8084  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 310/1251]  eta: 0:13:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8163  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 320/1251]  eta: 0:13:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 330/1251]  eta: 0:13:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 340/1251]  eta: 0:13:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 350/1251]  eta: 0:12:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7824  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 360/1251]  eta: 0:12:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 370/1251]  eta: 0:12:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 380/1251]  eta: 0:12:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7795  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 390/1251]  eta: 0:12:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 400/1251]  eta: 0:12:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 410/1251]  eta: 0:11:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 420/1251]  eta: 0:11:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 430/1251]  eta: 0:11:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 440/1251]  eta: 0:11:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8091  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 450/1251]  eta: 0:11:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8275  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 460/1251]  eta: 0:11:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8192  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 470/1251]  eta: 0:10:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 480/1251]  eta: 0:10:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 490/1251]  eta: 0:10:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 500/1251]  eta: 0:10:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 510/1251]  eta: 0:10:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 520/1251]  eta: 0:10:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 530/1251]  eta: 0:10:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7800  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 540/1251]  eta: 0:09:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7824  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 550/1251]  eta: 0:09:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 560/1251]  eta: 0:09:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 570/1251]  eta: 0:09:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 580/1251]  eta: 0:09:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 590/1251]  eta: 0:09:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8176  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 600/1251]  eta: 0:09:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8310  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 610/1251]  eta: 0:08:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8102  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 620/1251]  eta: 0:08:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 630/1251]  eta: 0:08:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 640/1251]  eta: 0:08:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 650/1251]  eta: 0:08:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 660/1251]  eta: 0:08:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 670/1251]  eta: 0:07:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 680/1251]  eta: 0:07:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 690/1251]  eta: 0:07:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 700/1251]  eta: 0:07:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 710/1251]  eta: 0:07:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 720/1251]  eta: 0:07:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 730/1251]  eta: 0:07:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8017  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 740/1251]  eta: 0:07:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8171  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 750/1251]  eta: 0:06:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8247  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 760/1251]  eta: 0:06:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8001  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 770/1251]  eta: 0:06:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7968  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 780/1251]  eta: 0:06:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 790/1251]  eta: 0:06:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 800/1251]  eta: 0:06:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 810/1251]  eta: 0:06:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 820/1251]  eta: 0:05:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 830/1251]  eta: 0:05:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7803  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 840/1251]  eta: 0:05:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 850/1251]  eta: 0:05:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 860/1251]  eta: 0:05:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 870/1251]  eta: 0:05:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 880/1251]  eta: 0:05:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 890/1251]  eta: 0:04:55  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8206  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 900/1251]  eta: 0:04:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8121  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 910/1251]  eta: 0:04:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 920/1251]  eta: 0:04:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7942  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 930/1251]  eta: 0:04:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 940/1251]  eta: 0:04:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 950/1251]  eta: 0:04:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 960/1251]  eta: 0:03:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 970/1251]  eta: 0:03:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 980/1251]  eta: 0:03:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1000/1251]  eta: 0:03:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1010/1251]  eta: 0:03:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1020/1251]  eta: 0:03:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1030/1251]  eta: 0:03:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8114  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8238  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1050/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8034  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1060/1251]  eta: 0:02:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7997  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1070/1251]  eta: 0:02:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1080/1251]  eta: 0:02:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1090/1251]  eta: 0:02:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7805  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1130/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1140/1251]  eta: 0:01:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1150/1251]  eta: 0:01:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8145  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8294  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7688  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7714  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1230/1251]  eta: 0:00:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7719  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7686  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7656  data: 0.0002  max mem: 19734
Epoch: [12] Total time: 0:16:54 (0.8109 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:13:35  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 16.9174  data: 16.7088  max mem: 19734
Test:  [ 10/261]  eta: 0:08:54  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.1283  data: 1.8872  max mem: 19734
Test:  [ 20/261]  eta: 0:05:26  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.5745  data: 0.2096  max mem: 19734
Test:  [ 30/261]  eta: 0:04:18  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.5615  data: 0.0127  max mem: 19734
Test:  [ 40/261]  eta: 0:03:47  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6898  data: 0.2379  max mem: 19734
Test:  [ 50/261]  eta: 0:03:12  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5988  data: 0.2403  max mem: 19734
Test:  [ 60/261]  eta: 0:02:46  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.4128  data: 0.0187  max mem: 19734
Test:  [ 70/261]  eta: 0:02:29  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.4518  data: 0.0844  max mem: 19734
Test:  [ 80/261]  eta: 0:02:08  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3493  data: 0.0773  max mem: 19734
Test:  [ 90/261]  eta: 0:01:53  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2275  data: 0.0077  max mem: 19734
Test:  [100/261]  eta: 0:01:42  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3569  data: 0.0760  max mem: 19734
Test:  [110/261]  eta: 0:01:41  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.7126  data: 0.3551  max mem: 19734
Test:  [120/261]  eta: 0:01:31  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.7013  data: 0.2903  max mem: 19734
Test:  [130/261]  eta: 0:01:21  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3722  data: 0.0136  max mem: 19734
Test:  [140/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4621  data: 0.2009  max mem: 19734
Test:  [150/261]  eta: 0:01:06  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4086  data: 0.2008  max mem: 19734
Test:  [160/261]  eta: 0:00:58  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2416  data: 0.0140  max mem: 19734
Test:  [170/261]  eta: 0:00:51  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3352  data: 0.0804  max mem: 19734
Test:  [180/261]  eta: 0:00:44  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.3781  data: 0.0787  max mem: 19734
Test:  [190/261]  eta: 0:00:38  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.3627  data: 0.0142  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.3001  data: 0.0122  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.2581  data: 0.0716  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.2152  data: 0.0678  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1360  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1319  data: 0.0001  max mem: 19734
Test: Total time: 0:01:56 (0.4449 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [13]  [   0/1251]  eta: 6:20:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 18.2539  data: 12.8900  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  10/1251]  eta: 0:51:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.5084  data: 1.1756  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  20/1251]  eta: 0:34:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8675  data: 0.0024  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  30/1251]  eta: 0:28:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7978  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  40/1251]  eta: 0:25:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  50/1251]  eta: 0:23:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  60/1251]  eta: 0:22:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8097  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  70/1251]  eta: 0:21:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8322  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  80/1251]  eta: 0:20:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8194  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  90/1251]  eta: 0:19:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8036  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 100/1251]  eta: 0:19:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8012  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 110/1251]  eta: 0:18:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 120/1251]  eta: 0:18:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 130/1251]  eta: 0:17:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 140/1251]  eta: 0:17:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 150/1251]  eta: 0:16:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 160/1251]  eta: 0:16:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 170/1251]  eta: 0:16:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 180/1251]  eta: 0:16:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 190/1251]  eta: 0:15:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 200/1251]  eta: 0:15:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 210/1251]  eta: 0:15:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8297  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 220/1251]  eta: 0:15:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8369  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 230/1251]  eta: 0:15:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8017  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 240/1251]  eta: 0:14:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 250/1251]  eta: 0:14:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7972  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 260/1251]  eta: 0:14:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 270/1251]  eta: 0:14:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 280/1251]  eta: 0:14:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 290/1251]  eta: 0:13:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 300/1251]  eta: 0:13:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 310/1251]  eta: 0:13:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 320/1251]  eta: 0:13:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 330/1251]  eta: 0:13:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 340/1251]  eta: 0:12:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 350/1251]  eta: 0:12:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8099  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 360/1251]  eta: 0:12:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8299  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 370/1251]  eta: 0:12:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8098  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 380/1251]  eta: 0:12:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 390/1251]  eta: 0:12:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 400/1251]  eta: 0:11:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 410/1251]  eta: 0:11:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 420/1251]  eta: 0:11:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 430/1251]  eta: 0:11:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 440/1251]  eta: 0:11:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 450/1251]  eta: 0:11:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 460/1251]  eta: 0:11:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7816  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 470/1251]  eta: 0:10:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 480/1251]  eta: 0:10:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8019  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 490/1251]  eta: 0:10:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8074  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 500/1251]  eta: 0:10:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8138  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 510/1251]  eta: 0:10:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8197  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 520/1251]  eta: 0:10:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8071  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 530/1251]  eta: 0:10:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7989  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 540/1251]  eta: 0:09:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 550/1251]  eta: 0:09:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 560/1251]  eta: 0:09:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 570/1251]  eta: 0:09:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7786  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 580/1251]  eta: 0:09:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 590/1251]  eta: 0:09:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 600/1251]  eta: 0:08:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 610/1251]  eta: 0:08:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7824  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 620/1251]  eta: 0:08:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 630/1251]  eta: 0:08:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 640/1251]  eta: 0:08:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8019  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 650/1251]  eta: 0:08:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8109  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 660/1251]  eta: 0:08:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8124  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 670/1251]  eta: 0:07:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 680/1251]  eta: 0:07:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 690/1251]  eta: 0:07:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 700/1251]  eta: 0:07:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 710/1251]  eta: 0:07:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 720/1251]  eta: 0:07:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 730/1251]  eta: 0:07:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 740/1251]  eta: 0:06:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 750/1251]  eta: 0:06:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 760/1251]  eta: 0:06:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 770/1251]  eta: 0:06:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 780/1251]  eta: 0:06:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7990  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 790/1251]  eta: 0:06:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7968  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 800/1251]  eta: 0:06:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8107  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 810/1251]  eta: 0:06:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8134  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 820/1251]  eta: 0:05:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8029  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 830/1251]  eta: 0:05:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 840/1251]  eta: 0:05:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7808  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 850/1251]  eta: 0:05:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 860/1251]  eta: 0:05:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 870/1251]  eta: 0:05:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 880/1251]  eta: 0:05:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 890/1251]  eta: 0:04:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 900/1251]  eta: 0:04:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 910/1251]  eta: 0:04:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 920/1251]  eta: 0:04:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 930/1251]  eta: 0:04:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8130  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 940/1251]  eta: 0:04:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8169  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 950/1251]  eta: 0:04:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8063  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 960/1251]  eta: 0:03:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 970/1251]  eta: 0:03:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 980/1251]  eta: 0:03:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1000/1251]  eta: 0:03:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1010/1251]  eta: 0:03:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1020/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7800  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1030/1251]  eta: 0:02:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7793  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1050/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1060/1251]  eta: 0:02:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1070/1251]  eta: 0:02:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1080/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1090/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8209  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8148  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1130/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1140/1251]  eta: 0:01:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1150/1251]  eta: 0:01:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7795  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7754  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7674  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7788  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1230/1251]  eta: 0:00:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7784  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7780  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0002  max mem: 19734
Epoch: [13] Total time: 0:16:53 (0.8098 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:08:30  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 15.7500  data: 15.5867  max mem: 19734
Test:  [ 10/261]  eta: 0:09:44  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.3274  data: 2.0692  max mem: 19734
Test:  [ 20/261]  eta: 0:05:47  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.7269  data: 0.3678  max mem: 19734
Test:  [ 30/261]  eta: 0:04:16  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4436  data: 0.0206  max mem: 19734
Test:  [ 40/261]  eta: 0:03:50  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6256  data: 0.3165  max mem: 19734
Test:  [ 50/261]  eta: 0:03:08  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5544  data: 0.3128  max mem: 19734
Test:  [ 60/261]  eta: 0:02:45  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3714  data: 0.0135  max mem: 19734
Test:  [ 70/261]  eta: 0:02:39  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.6909  data: 0.2205  max mem: 19734
Test:  [ 80/261]  eta: 0:02:21  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.6621  data: 0.2193  max mem: 19734
Test:  [ 90/261]  eta: 0:02:06  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3996  data: 0.0135  max mem: 19734
Test:  [100/261]  eta: 0:01:54  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4053  data: 0.0149  max mem: 19734
Test:  [110/261]  eta: 0:01:43  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4270  data: 0.0658  max mem: 19734
Test:  [120/261]  eta: 0:01:32  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4005  data: 0.0653  max mem: 19734
Test:  [130/261]  eta: 0:01:23  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3737  data: 0.0180  max mem: 19734
Test:  [140/261]  eta: 0:01:21  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.7636  data: 0.3192  max mem: 19734
Test:  [150/261]  eta: 0:01:12  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.7684  data: 0.3129  max mem: 19734
Test:  [160/261]  eta: 0:01:04  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3688  data: 0.0124  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3082  data: 0.0109  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2921  data: 0.0077  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.2810  data: 0.0069  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.2221  data: 0.0069  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1923  data: 0.0163  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1651  data: 0.0137  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1397  data: 0.0003  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1356  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1321  data: 0.0001  max mem: 19734
Test: Total time: 0:02:01 (0.4653 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [14]  [   0/1251]  eta: 6:25:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 18.4961  data: 17.5582  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [  10/1251]  eta: 0:53:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.5745  data: 1.5968  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [  20/1251]  eta: 0:35:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8898  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [  30/1251]  eta: 0:29:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [  40/1251]  eta: 0:25:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [  50/1251]  eta: 0:23:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [  60/1251]  eta: 0:22:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [  70/1251]  eta: 0:21:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7968  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [  80/1251]  eta: 0:20:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [  90/1251]  eta: 0:19:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 100/1251]  eta: 0:18:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8041  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 110/1251]  eta: 0:18:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8095  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 120/1251]  eta: 0:18:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8185  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 130/1251]  eta: 0:17:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8194  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 140/1251]  eta: 0:17:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 150/1251]  eta: 0:17:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8006  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 160/1251]  eta: 0:16:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 170/1251]  eta: 0:16:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 180/1251]  eta: 0:16:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 190/1251]  eta: 0:15:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 200/1251]  eta: 0:15:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 210/1251]  eta: 0:15:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 220/1251]  eta: 0:15:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 230/1251]  eta: 0:14:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 240/1251]  eta: 0:14:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 250/1251]  eta: 0:14:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 260/1251]  eta: 0:14:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8082  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 270/1251]  eta: 0:14:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8061  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 280/1251]  eta: 0:14:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 290/1251]  eta: 0:13:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 300/1251]  eta: 0:13:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 310/1251]  eta: 0:13:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 320/1251]  eta: 0:13:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 330/1251]  eta: 0:13:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 340/1251]  eta: 0:12:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 350/1251]  eta: 0:12:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 360/1251]  eta: 0:12:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 370/1251]  eta: 0:12:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 380/1251]  eta: 0:12:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7786  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 390/1251]  eta: 0:12:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 400/1251]  eta: 0:11:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8154  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 410/1251]  eta: 0:11:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8208  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 420/1251]  eta: 0:11:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8188  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 430/1251]  eta: 0:11:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 440/1251]  eta: 0:11:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7968  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 450/1251]  eta: 0:11:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 460/1251]  eta: 0:11:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7800  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 470/1251]  eta: 0:10:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 480/1251]  eta: 0:10:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 490/1251]  eta: 0:10:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 500/1251]  eta: 0:10:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 510/1251]  eta: 0:10:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 520/1251]  eta: 0:10:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 530/1251]  eta: 0:09:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 540/1251]  eta: 0:09:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 550/1251]  eta: 0:09:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8063  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 560/1251]  eta: 0:09:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8132  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 570/1251]  eta: 0:09:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8164  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 580/1251]  eta: 0:09:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 590/1251]  eta: 0:09:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 600/1251]  eta: 0:08:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 610/1251]  eta: 0:08:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7807  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 620/1251]  eta: 0:08:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 630/1251]  eta: 0:08:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 640/1251]  eta: 0:08:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 650/1251]  eta: 0:08:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7793  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 660/1251]  eta: 0:08:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 670/1251]  eta: 0:07:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 680/1251]  eta: 0:07:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 690/1251]  eta: 0:07:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8070  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 700/1251]  eta: 0:07:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8193  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 710/1251]  eta: 0:07:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8130  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 720/1251]  eta: 0:07:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7999  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 730/1251]  eta: 0:07:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 740/1251]  eta: 0:06:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 750/1251]  eta: 0:06:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 760/1251]  eta: 0:06:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 770/1251]  eta: 0:06:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 780/1251]  eta: 0:06:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 790/1251]  eta: 0:06:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 800/1251]  eta: 0:06:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7811  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 810/1251]  eta: 0:06:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 820/1251]  eta: 0:05:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 830/1251]  eta: 0:05:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 840/1251]  eta: 0:05:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 850/1251]  eta: 0:05:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8068  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 860/1251]  eta: 0:05:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8108  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 870/1251]  eta: 0:05:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7962  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 880/1251]  eta: 0:05:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 890/1251]  eta: 0:04:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 900/1251]  eta: 0:04:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 910/1251]  eta: 0:04:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 920/1251]  eta: 0:04:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 930/1251]  eta: 0:04:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 940/1251]  eta: 0:04:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 950/1251]  eta: 0:04:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 960/1251]  eta: 0:03:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 970/1251]  eta: 0:03:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 980/1251]  eta: 0:03:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8064  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8115  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1000/1251]  eta: 0:03:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8064  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1010/1251]  eta: 0:03:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8029  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1020/1251]  eta: 0:03:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1030/1251]  eta: 0:02:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1050/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1060/1251]  eta: 0:02:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1070/1251]  eta: 0:02:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1080/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1090/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1130/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8088  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1140/1251]  eta: 0:01:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8170  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1150/1251]  eta: 0:01:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8105  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8020  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7804  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7696  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7661  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1230/1251]  eta: 0:00:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7714  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7723  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [14]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7668  data: 0.0002  max mem: 19734
Epoch: [14] Total time: 0:16:52 (0.8097 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:17:13  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 31.5477  data: 31.2554  max mem: 19734
Test:  [ 10/261]  eta: 0:13:22  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.1986  data: 2.8480  max mem: 19734
Test:  [ 20/261]  eta: 0:07:16  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3228  data: 0.0099  max mem: 19734
Test:  [ 30/261]  eta: 0:05:09  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3154  data: 0.0118  max mem: 19734
Test:  [ 40/261]  eta: 0:04:32  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6315  data: 0.1940  max mem: 19734
Test:  [ 50/261]  eta: 0:03:43  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6298  data: 0.1961  max mem: 19734
Test:  [ 60/261]  eta: 0:03:13  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.4036  data: 0.0174  max mem: 19734
Test:  [ 70/261]  eta: 0:02:52  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5004  data: 0.0821  max mem: 19734
Test:  [ 80/261]  eta: 0:02:29  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4093  data: 0.0804  max mem: 19734
Test:  [ 90/261]  eta: 0:02:09  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2445  data: 0.0167  max mem: 19734
Test:  [100/261]  eta: 0:01:55  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.2888  data: 0.0154  max mem: 19734
Test:  [110/261]  eta: 0:01:41  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.2652  data: 0.0102  max mem: 19734
Test:  [120/261]  eta: 0:01:28  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.1733  data: 0.0072  max mem: 19734
Test:  [130/261]  eta: 0:01:19  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2796  data: 0.0966  max mem: 19734
Test:  [140/261]  eta: 0:01:10  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.2979  data: 0.1135  max mem: 19734
Test:  [150/261]  eta: 0:01:09  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.7334  data: 0.4975  max mem: 19734
Test:  [160/261]  eta: 0:01:00  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.7455  data: 0.4826  max mem: 19734
Test:  [170/261]  eta: 0:00:52  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2501  data: 0.0356  max mem: 19734
Test:  [180/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2968  data: 0.1248  max mem: 19734
Test:  [190/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.2702  data: 0.0981  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1968  data: 0.0048  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1701  data: 0.0016  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1438  data: 0.0003  max mem: 19734
Test:  [230/261]  eta: 0:00:14  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1372  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1362  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1361  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1318  data: 0.0001  max mem: 19734
Test: Total time: 0:01:55 (0.4422 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [15]  [   0/1251]  eta: 5:29:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 15.8246  data: 11.5848  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [  10/1251]  eta: 0:52:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.5426  data: 1.0578  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [  20/1251]  eta: 0:35:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 1.0027  data: 0.0032  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [  30/1251]  eta: 0:28:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8100  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [  40/1251]  eta: 0:25:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8208  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [  50/1251]  eta: 0:23:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8039  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [  60/1251]  eta: 0:22:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [  70/1251]  eta: 0:21:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [  80/1251]  eta: 0:20:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [  90/1251]  eta: 0:19:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 100/1251]  eta: 0:18:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 110/1251]  eta: 0:18:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 120/1251]  eta: 0:17:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 130/1251]  eta: 0:17:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 140/1251]  eta: 0:17:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 150/1251]  eta: 0:16:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8062  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 160/1251]  eta: 0:16:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8075  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 170/1251]  eta: 0:16:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8213  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 180/1251]  eta: 0:16:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8103  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 190/1251]  eta: 0:15:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8099  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 200/1251]  eta: 0:15:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8084  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 210/1251]  eta: 0:15:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 220/1251]  eta: 0:15:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 230/1251]  eta: 0:15:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 240/1251]  eta: 0:14:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 250/1251]  eta: 0:14:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 260/1251]  eta: 0:14:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 270/1251]  eta: 0:14:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 280/1251]  eta: 0:14:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 290/1251]  eta: 0:13:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 300/1251]  eta: 0:13:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8113  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 310/1251]  eta: 0:13:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8018  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 320/1251]  eta: 0:13:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8049  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 330/1251]  eta: 0:13:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8229  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 340/1251]  eta: 0:12:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8104  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 350/1251]  eta: 0:12:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7968  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 360/1251]  eta: 0:12:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 370/1251]  eta: 0:12:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 380/1251]  eta: 0:12:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 390/1251]  eta: 0:12:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 400/1251]  eta: 0:12:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 410/1251]  eta: 0:11:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 420/1251]  eta: 0:11:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 430/1251]  eta: 0:11:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 440/1251]  eta: 0:11:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 450/1251]  eta: 0:11:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 460/1251]  eta: 0:11:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8071  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 470/1251]  eta: 0:10:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8026  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 480/1251]  eta: 0:10:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8034  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 490/1251]  eta: 0:10:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7963  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 500/1251]  eta: 0:10:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 510/1251]  eta: 0:10:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 520/1251]  eta: 0:10:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 530/1251]  eta: 0:10:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 540/1251]  eta: 0:09:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 550/1251]  eta: 0:09:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 560/1251]  eta: 0:09:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 570/1251]  eta: 0:09:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 580/1251]  eta: 0:09:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 590/1251]  eta: 0:09:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 600/1251]  eta: 0:08:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8087  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 610/1251]  eta: 0:08:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8112  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 620/1251]  eta: 0:08:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 630/1251]  eta: 0:08:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7999  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 640/1251]  eta: 0:08:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 650/1251]  eta: 0:08:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 660/1251]  eta: 0:08:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 670/1251]  eta: 0:07:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 680/1251]  eta: 0:07:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 690/1251]  eta: 0:07:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 700/1251]  eta: 0:07:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 710/1251]  eta: 0:07:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 720/1251]  eta: 0:07:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 730/1251]  eta: 0:07:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 740/1251]  eta: 0:06:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 750/1251]  eta: 0:06:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8126  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 760/1251]  eta: 0:06:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8079  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 770/1251]  eta: 0:06:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8076  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 780/1251]  eta: 0:06:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8020  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 790/1251]  eta: 0:06:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 800/1251]  eta: 0:06:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 810/1251]  eta: 0:06:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 820/1251]  eta: 0:05:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 830/1251]  eta: 0:05:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 840/1251]  eta: 0:05:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 850/1251]  eta: 0:05:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 860/1251]  eta: 0:05:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 870/1251]  eta: 0:05:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 880/1251]  eta: 0:05:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 890/1251]  eta: 0:04:55  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8108  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 900/1251]  eta: 0:04:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8144  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 910/1251]  eta: 0:04:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 920/1251]  eta: 0:04:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8036  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 930/1251]  eta: 0:04:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8092  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 940/1251]  eta: 0:04:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 950/1251]  eta: 0:04:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 960/1251]  eta: 0:03:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7808  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 970/1251]  eta: 0:03:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 980/1251]  eta: 0:03:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1000/1251]  eta: 0:03:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1010/1251]  eta: 0:03:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1020/1251]  eta: 0:03:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1030/1251]  eta: 0:02:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1050/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8144  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1060/1251]  eta: 0:02:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8133  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1070/1251]  eta: 0:02:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1080/1251]  eta: 0:02:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1090/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1130/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1140/1251]  eta: 0:01:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1150/1251]  eta: 0:01:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8012  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8024  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7816  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1230/1251]  eta: 0:00:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7768  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7710  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [15]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7716  data: 0.0002  max mem: 19734
Epoch: [15] Total time: 0:16:53 (0.8103 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:33:16  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 21.4409  data: 21.0493  max mem: 19734
Test:  [ 10/261]  eta: 0:14:07  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.3753  data: 3.0769  max mem: 19734
Test:  [ 20/261]  eta: 0:07:59  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 1.0159  data: 0.6440  max mem: 19734
Test:  [ 30/261]  eta: 0:05:39  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4190  data: 0.0112  max mem: 19734
Test:  [ 40/261]  eta: 0:05:00  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.7037  data: 0.3772  max mem: 19734
Test:  [ 50/261]  eta: 0:04:03  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6705  data: 0.3747  max mem: 19734
Test:  [ 60/261]  eta: 0:03:25  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3280  data: 0.0108  max mem: 19734
Test:  [ 70/261]  eta: 0:03:07  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5345  data: 0.1944  max mem: 19734
Test:  [ 80/261]  eta: 0:02:42  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.5121  data: 0.1929  max mem: 19734
Test:  [ 90/261]  eta: 0:02:26  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4134  data: 0.0127  max mem: 19734
Test:  [100/261]  eta: 0:02:18  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.7167  data: 0.2605  max mem: 19734
Test:  [110/261]  eta: 0:02:03  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.6416  data: 0.2624  max mem: 19734
Test:  [120/261]  eta: 0:01:50  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3951  data: 0.0146  max mem: 19734
Test:  [130/261]  eta: 0:01:37  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3582  data: 0.0073  max mem: 19734
Test:  [140/261]  eta: 0:01:26  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.2992  data: 0.0062  max mem: 19734
Test:  [150/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.2627  data: 0.0100  max mem: 19734
Test:  [160/261]  eta: 0:01:05  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2145  data: 0.0097  max mem: 19734
Test:  [170/261]  eta: 0:00:56  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1814  data: 0.0126  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1486  data: 0.0094  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1375  data: 0.0019  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1379  data: 0.0021  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1365  data: 0.0006  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1358  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1358  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1358  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1319  data: 0.0001  max mem: 19734
Test: Total time: 0:01:59 (0.4565 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [16]  [   0/1251]  eta: 6:24:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 18.4500  data: 17.1184  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [  10/1251]  eta: 0:52:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.5377  data: 1.6055  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [  20/1251]  eta: 0:34:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8640  data: 0.0276  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [  30/1251]  eta: 0:28:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [  40/1251]  eta: 0:25:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8009  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [  50/1251]  eta: 0:23:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [  60/1251]  eta: 0:21:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [  70/1251]  eta: 0:20:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8073  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [  80/1251]  eta: 0:20:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8233  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [  90/1251]  eta: 0:19:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8162  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 100/1251]  eta: 0:18:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8016  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 110/1251]  eta: 0:18:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 120/1251]  eta: 0:18:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 130/1251]  eta: 0:17:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 140/1251]  eta: 0:17:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 150/1251]  eta: 0:16:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 160/1251]  eta: 0:16:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 170/1251]  eta: 0:16:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 180/1251]  eta: 0:16:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 190/1251]  eta: 0:15:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 200/1251]  eta: 0:15:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 210/1251]  eta: 0:15:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 220/1251]  eta: 0:15:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8020  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 230/1251]  eta: 0:14:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8034  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 240/1251]  eta: 0:14:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7989  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 250/1251]  eta: 0:14:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 260/1251]  eta: 0:14:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7985  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 270/1251]  eta: 0:14:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 280/1251]  eta: 0:13:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 290/1251]  eta: 0:13:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 300/1251]  eta: 0:13:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 310/1251]  eta: 0:13:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 320/1251]  eta: 0:13:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7811  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 330/1251]  eta: 0:13:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 340/1251]  eta: 0:12:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 350/1251]  eta: 0:12:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 360/1251]  eta: 0:12:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 370/1251]  eta: 0:12:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 380/1251]  eta: 0:12:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8163  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 390/1251]  eta: 0:12:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8123  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 400/1251]  eta: 0:11:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 410/1251]  eta: 0:11:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 420/1251]  eta: 0:11:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 430/1251]  eta: 0:11:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7813  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 440/1251]  eta: 0:11:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 450/1251]  eta: 0:11:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7883  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 460/1251]  eta: 0:11:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 470/1251]  eta: 0:10:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 480/1251]  eta: 0:10:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 490/1251]  eta: 0:10:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 500/1251]  eta: 0:10:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 510/1251]  eta: 0:10:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8005  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 520/1251]  eta: 0:10:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8185  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 530/1251]  eta: 0:09:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8197  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 540/1251]  eta: 0:09:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8019  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 550/1251]  eta: 0:09:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 560/1251]  eta: 0:09:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 570/1251]  eta: 0:09:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 580/1251]  eta: 0:09:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7796  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 590/1251]  eta: 0:09:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 600/1251]  eta: 0:08:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 610/1251]  eta: 0:08:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7883  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 620/1251]  eta: 0:08:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 630/1251]  eta: 0:08:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 640/1251]  eta: 0:08:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 650/1251]  eta: 0:08:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7945  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 660/1251]  eta: 0:08:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 670/1251]  eta: 0:07:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8143  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 680/1251]  eta: 0:07:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8197  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 690/1251]  eta: 0:07:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7990  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 700/1251]  eta: 0:07:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 710/1251]  eta: 0:07:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7962  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 720/1251]  eta: 0:07:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 730/1251]  eta: 0:07:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 740/1251]  eta: 0:06:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 750/1251]  eta: 0:06:50  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 760/1251]  eta: 0:06:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 770/1251]  eta: 0:06:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 780/1251]  eta: 0:06:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 790/1251]  eta: 0:06:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 800/1251]  eta: 0:06:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 810/1251]  eta: 0:06:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8239  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 820/1251]  eta: 0:05:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8297  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 830/1251]  eta: 0:05:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 840/1251]  eta: 0:05:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 850/1251]  eta: 0:05:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 860/1251]  eta: 0:05:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 870/1251]  eta: 0:05:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 880/1251]  eta: 0:05:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7818  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 890/1251]  eta: 0:04:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7824  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 900/1251]  eta: 0:04:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 910/1251]  eta: 0:04:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 920/1251]  eta: 0:04:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 930/1251]  eta: 0:04:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 940/1251]  eta: 0:04:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8001  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 950/1251]  eta: 0:04:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 960/1251]  eta: 0:03:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8073  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 970/1251]  eta: 0:03:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8271  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 980/1251]  eta: 0:03:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8026  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [ 990/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1000/1251]  eta: 0:03:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1010/1251]  eta: 0:03:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1020/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7883  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1030/1251]  eta: 0:02:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1040/1251]  eta: 0:02:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1050/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1060/1251]  eta: 0:02:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1070/1251]  eta: 0:02:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1080/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1090/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1100/1251]  eta: 0:02:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1110/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8061  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1120/1251]  eta: 0:01:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8210  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1130/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1140/1251]  eta: 0:01:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1150/1251]  eta: 0:01:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1160/1251]  eta: 0:01:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1170/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1180/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7816  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1190/1251]  eta: 0:00:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7778  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1200/1251]  eta: 0:00:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7765  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1210/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7706  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1220/1251]  eta: 0:00:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7647  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1230/1251]  eta: 0:00:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7721  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1240/1251]  eta: 0:00:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [16]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0002  max mem: 19734
Epoch: [16] Total time: 0:16:51 (0.8089 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:51:07  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 25.5465  data: 25.3886  max mem: 19734
Test:  [ 10/261]  eta: 0:10:37  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.5390  data: 2.3160  max mem: 19734
Test:  [ 20/261]  eta: 0:05:48  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2407  data: 0.0088  max mem: 19734
Test:  [ 30/261]  eta: 0:03:58  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2046  data: 0.0074  max mem: 19734
Test:  [ 40/261]  eta: 0:03:44  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5637  data: 0.3343  max mem: 19734
Test:  [ 50/261]  eta: 0:03:06  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6531  data: 0.3378  max mem: 19734
Test:  [ 60/261]  eta: 0:02:38  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3263  data: 0.0135  max mem: 19734
Test:  [ 70/261]  eta: 0:02:28  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5094  data: 0.2077  max mem: 19734
Test:  [ 80/261]  eta: 0:02:09  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4870  data: 0.2112  max mem: 19734
Test:  [ 90/261]  eta: 0:01:55  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3121  data: 0.0158  max mem: 19734
Test:  [100/261]  eta: 0:01:47  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4737  data: 0.1069  max mem: 19734
Test:  [110/261]  eta: 0:01:45  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.7884  data: 0.3429  max mem: 19734
Test:  [120/261]  eta: 0:01:36  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.7675  data: 0.2490  max mem: 19734
Test:  [130/261]  eta: 0:01:27  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.5151  data: 0.0148  max mem: 19734
Test:  [140/261]  eta: 0:01:22  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.6668  data: 0.2117  max mem: 19734
Test:  [150/261]  eta: 0:01:13  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.6273  data: 0.2110  max mem: 19734
Test:  [160/261]  eta: 0:01:05  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.4183  data: 0.0106  max mem: 19734
Test:  [170/261]  eta: 0:00:57  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3493  data: 0.0115  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2369  data: 0.0132  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1910  data: 0.0087  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1737  data: 0.0050  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1597  data: 0.0026  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1424  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1359  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1319  data: 0.0001  max mem: 19734
Test: Total time: 0:02:01 (0.4639 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [17]  [   0/1251]  eta: 4:33:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 13.1329  data: 7.0439  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [  10/1251]  eta: 0:52:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.5347  data: 0.7156  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [  20/1251]  eta: 0:35:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 1.1360  data: 0.0417  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [  30/1251]  eta: 0:28:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [  40/1251]  eta: 0:25:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [  50/1251]  eta: 0:23:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [  60/1251]  eta: 0:21:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [  70/1251]  eta: 0:20:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [  80/1251]  eta: 0:20:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [  90/1251]  eta: 0:19:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 100/1251]  eta: 0:18:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 110/1251]  eta: 0:18:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 120/1251]  eta: 0:17:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 130/1251]  eta: 0:17:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8097  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 140/1251]  eta: 0:17:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8080  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 150/1251]  eta: 0:16:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 160/1251]  eta: 0:16:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8054  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 170/1251]  eta: 0:16:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 180/1251]  eta: 0:16:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 190/1251]  eta: 0:15:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 200/1251]  eta: 0:15:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 210/1251]  eta: 0:15:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 220/1251]  eta: 0:15:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 230/1251]  eta: 0:14:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 240/1251]  eta: 0:14:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 250/1251]  eta: 0:14:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 260/1251]  eta: 0:14:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 270/1251]  eta: 0:14:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8117  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 280/1251]  eta: 0:13:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8245  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 290/1251]  eta: 0:13:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8054  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 300/1251]  eta: 0:13:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8040  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 310/1251]  eta: 0:13:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8056  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 320/1251]  eta: 0:13:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 330/1251]  eta: 0:13:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 340/1251]  eta: 0:12:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 350/1251]  eta: 0:12:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 360/1251]  eta: 0:12:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 370/1251]  eta: 0:12:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 380/1251]  eta: 0:12:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 390/1251]  eta: 0:12:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 400/1251]  eta: 0:11:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 410/1251]  eta: 0:11:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 420/1251]  eta: 0:11:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8033  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 430/1251]  eta: 0:11:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8094  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 440/1251]  eta: 0:11:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8070  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 450/1251]  eta: 0:11:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8036  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 460/1251]  eta: 0:11:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8018  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 470/1251]  eta: 0:10:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 480/1251]  eta: 0:10:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 490/1251]  eta: 0:10:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 500/1251]  eta: 0:10:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 510/1251]  eta: 0:10:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7806  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 520/1251]  eta: 0:10:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 530/1251]  eta: 0:09:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 540/1251]  eta: 0:09:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 550/1251]  eta: 0:09:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 560/1251]  eta: 0:09:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7957  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 570/1251]  eta: 0:09:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8056  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 580/1251]  eta: 0:09:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8109  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 590/1251]  eta: 0:09:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 600/1251]  eta: 0:08:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8016  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 610/1251]  eta: 0:08:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 620/1251]  eta: 0:08:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 630/1251]  eta: 0:08:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 640/1251]  eta: 0:08:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 650/1251]  eta: 0:08:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 660/1251]  eta: 0:08:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7818  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 670/1251]  eta: 0:07:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 680/1251]  eta: 0:07:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7990  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 690/1251]  eta: 0:07:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 700/1251]  eta: 0:07:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 710/1251]  eta: 0:07:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7971  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 720/1251]  eta: 0:07:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8059  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 730/1251]  eta: 0:07:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 740/1251]  eta: 0:06:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8035  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 750/1251]  eta: 0:06:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8069  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 760/1251]  eta: 0:06:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7963  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 770/1251]  eta: 0:06:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 780/1251]  eta: 0:06:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 790/1251]  eta: 0:06:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 800/1251]  eta: 0:06:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 810/1251]  eta: 0:06:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 820/1251]  eta: 0:05:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 830/1251]  eta: 0:05:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 840/1251]  eta: 0:05:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 850/1251]  eta: 0:05:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 860/1251]  eta: 0:05:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8142  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 870/1251]  eta: 0:05:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8215  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 880/1251]  eta: 0:05:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8037  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 890/1251]  eta: 0:04:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 900/1251]  eta: 0:04:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 910/1251]  eta: 0:04:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 920/1251]  eta: 0:04:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 930/1251]  eta: 0:04:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 940/1251]  eta: 0:04:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 950/1251]  eta: 0:04:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 960/1251]  eta: 0:03:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7825  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [ 990/1251]  eta: 0:03:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7985  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1000/1251]  eta: 0:03:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8005  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1010/1251]  eta: 0:03:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1020/1251]  eta: 0:03:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8067  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8002  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7984  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1060/1251]  eta: 0:02:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1070/1251]  eta: 0:02:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1080/1251]  eta: 0:02:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7806  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1130/1251]  eta: 0:01:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1140/1251]  eta: 0:01:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1150/1251]  eta: 0:01:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8056  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8031  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8049  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8088  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7962  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7750  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7707  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1230/1251]  eta: 0:00:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7648  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7695  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [17]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7715  data: 0.0002  max mem: 19734
Epoch: [17] Total time: 0:16:52 (0.8097 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:45:27  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 38.0365  data: 37.8461  max mem: 19734
Test:  [ 10/261]  eta: 0:15:14  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.6421  data: 3.4510  max mem: 19734
Test:  [ 20/261]  eta: 0:08:00  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.1921  data: 0.0121  max mem: 19734
Test:  [ 30/261]  eta: 0:05:34  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2439  data: 0.0139  max mem: 19734
Test:  [ 40/261]  eta: 0:05:13  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.8181  data: 0.4430  max mem: 19734
Test:  [ 50/261]  eta: 0:04:24  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.9547  data: 0.4389  max mem: 19734
Test:  [ 60/261]  eta: 0:03:49  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.5766  data: 0.0101  max mem: 19734
Test:  [ 70/261]  eta: 0:03:20  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5168  data: 0.0139  max mem: 19734
Test:  [ 80/261]  eta: 0:02:51  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3411  data: 0.0176  max mem: 19734
Test:  [ 90/261]  eta: 0:02:38  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4961  data: 0.0215  max mem: 19734
Test:  [100/261]  eta: 0:02:21  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.5960  data: 0.0161  max mem: 19734
Test:  [110/261]  eta: 0:02:04  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3783  data: 0.0125  max mem: 19734
Test:  [120/261]  eta: 0:01:52  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4062  data: 0.1247  max mem: 19734
Test:  [130/261]  eta: 0:01:38  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3287  data: 0.1224  max mem: 19734
Test:  [140/261]  eta: 0:01:25  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.1608  data: 0.0077  max mem: 19734
Test:  [150/261]  eta: 0:01:14  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.1391  data: 0.0026  max mem: 19734
Test:  [160/261]  eta: 0:01:04  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1367  data: 0.0011  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1365  data: 0.0011  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1359  data: 0.0004  max mem: 19734
Test:  [190/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1369  data: 0.0015  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1365  data: 0.0015  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1352  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1359  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1315  data: 0.0001  max mem: 19734
Test: Total time: 0:01:56 (0.4456 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [18]  [   0/1251]  eta: 6:30:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 18.7527  data: 14.3084  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [  10/1251]  eta: 0:53:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.5972  data: 1.3040  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [  20/1251]  eta: 0:35:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8813  data: 0.0021  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [  30/1251]  eta: 0:29:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [  40/1251]  eta: 0:25:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8047  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [  50/1251]  eta: 0:23:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8069  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [  60/1251]  eta: 0:22:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8236  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [  70/1251]  eta: 0:21:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8187  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [  80/1251]  eta: 0:20:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0006  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [  90/1251]  eta: 0:19:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 100/1251]  eta: 0:19:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 110/1251]  eta: 0:18:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 120/1251]  eta: 0:18:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 130/1251]  eta: 0:17:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 140/1251]  eta: 0:17:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 150/1251]  eta: 0:17:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 160/1251]  eta: 0:16:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 170/1251]  eta: 0:16:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 180/1251]  eta: 0:16:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0007  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 190/1251]  eta: 0:15:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8097  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 200/1251]  eta: 0:15:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8053  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 210/1251]  eta: 0:15:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8109  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 220/1251]  eta: 0:15:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8146  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 230/1251]  eta: 0:15:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 240/1251]  eta: 0:14:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 250/1251]  eta: 0:14:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 260/1251]  eta: 0:14:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 270/1251]  eta: 0:14:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 280/1251]  eta: 0:14:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 290/1251]  eta: 0:13:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 300/1251]  eta: 0:13:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 310/1251]  eta: 0:13:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 320/1251]  eta: 0:13:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 330/1251]  eta: 0:13:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8071  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 340/1251]  eta: 0:13:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8101  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 350/1251]  eta: 0:12:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8186  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 360/1251]  eta: 0:12:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8161  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 370/1251]  eta: 0:12:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7966  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 380/1251]  eta: 0:12:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 390/1251]  eta: 0:12:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 400/1251]  eta: 0:12:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 410/1251]  eta: 0:11:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 420/1251]  eta: 0:11:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7946  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 430/1251]  eta: 0:11:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 440/1251]  eta: 0:11:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 450/1251]  eta: 0:11:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 460/1251]  eta: 0:11:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 470/1251]  eta: 0:10:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 480/1251]  eta: 0:10:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8130  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 490/1251]  eta: 0:10:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8105  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 500/1251]  eta: 0:10:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8150  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 510/1251]  eta: 0:10:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8187  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 520/1251]  eta: 0:10:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7987  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 530/1251]  eta: 0:10:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 540/1251]  eta: 0:09:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 550/1251]  eta: 0:09:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 560/1251]  eta: 0:09:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 570/1251]  eta: 0:09:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 580/1251]  eta: 0:09:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 590/1251]  eta: 0:09:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 600/1251]  eta: 0:09:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 610/1251]  eta: 0:08:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 620/1251]  eta: 0:08:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7974  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 630/1251]  eta: 0:08:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 640/1251]  eta: 0:08:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 650/1251]  eta: 0:08:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8168  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 660/1251]  eta: 0:08:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8071  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 670/1251]  eta: 0:08:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 680/1251]  eta: 0:07:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 690/1251]  eta: 0:07:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 700/1251]  eta: 0:07:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 710/1251]  eta: 0:07:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 720/1251]  eta: 0:07:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 730/1251]  eta: 0:07:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 740/1251]  eta: 0:07:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 750/1251]  eta: 0:06:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7968  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 760/1251]  eta: 0:06:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 770/1251]  eta: 0:06:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 780/1251]  eta: 0:06:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7979  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 790/1251]  eta: 0:06:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8066  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 800/1251]  eta: 0:06:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8134  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 810/1251]  eta: 0:06:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 820/1251]  eta: 0:05:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 830/1251]  eta: 0:05:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 840/1251]  eta: 0:05:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 850/1251]  eta: 0:05:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7791  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 860/1251]  eta: 0:05:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 870/1251]  eta: 0:05:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 880/1251]  eta: 0:05:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 890/1251]  eta: 0:04:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 900/1251]  eta: 0:04:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 910/1251]  eta: 0:04:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 920/1251]  eta: 0:04:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 930/1251]  eta: 0:04:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8108  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 940/1251]  eta: 0:04:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8137  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 950/1251]  eta: 0:04:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8025  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 960/1251]  eta: 0:03:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 970/1251]  eta: 0:03:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 980/1251]  eta: 0:03:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7883  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [ 990/1251]  eta: 0:03:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1000/1251]  eta: 0:03:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7806  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1010/1251]  eta: 0:03:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1020/1251]  eta: 0:03:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1030/1251]  eta: 0:03:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1060/1251]  eta: 0:02:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7962  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1070/1251]  eta: 0:02:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7968  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1080/1251]  eta: 0:02:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8167  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1090/1251]  eta: 0:02:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8157  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1130/1251]  eta: 0:01:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1140/1251]  eta: 0:01:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1150/1251]  eta: 0:01:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7792  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7945  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7946  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7799  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7792  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7836  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1230/1251]  eta: 0:00:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [18]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7782  data: 0.0002  max mem: 19734
Epoch: [18] Total time: 0:16:54 (0.8112 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:07:05  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 29.2158  data: 28.6532  max mem: 19734
Test:  [ 10/261]  eta: 0:13:08  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.1416  data: 2.8710  max mem: 19734
Test:  [ 20/261]  eta: 0:07:12  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.4223  data: 0.1529  max mem: 19734
Test:  [ 30/261]  eta: 0:05:10  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3535  data: 0.0150  max mem: 19734
Test:  [ 40/261]  eta: 0:04:34  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6615  data: 0.3506  max mem: 19734
Test:  [ 50/261]  eta: 0:03:53  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.7407  data: 0.3525  max mem: 19734
Test:  [ 60/261]  eta: 0:03:23  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.5427  data: 0.0207  max mem: 19734
Test:  [ 70/261]  eta: 0:02:59  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5062  data: 0.0175  max mem: 19734
Test:  [ 80/261]  eta: 0:02:43  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.5669  data: 0.1713  max mem: 19734
Test:  [ 90/261]  eta: 0:02:21  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4294  data: 0.1706  max mem: 19734
Test:  [100/261]  eta: 0:02:04  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.2586  data: 0.0138  max mem: 19734
Test:  [110/261]  eta: 0:01:51  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3272  data: 0.0345  max mem: 19734
Test:  [120/261]  eta: 0:01:42  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4840  data: 0.0333  max mem: 19734
Test:  [130/261]  eta: 0:01:32  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.5583  data: 0.0145  max mem: 19734
Test:  [140/261]  eta: 0:01:29  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.7909  data: 0.3202  max mem: 19734
Test:  [150/261]  eta: 0:01:18  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.7053  data: 0.3689  max mem: 19734
Test:  [160/261]  eta: 0:01:08  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2708  data: 0.0838  max mem: 19734
Test:  [170/261]  eta: 0:00:58  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1799  data: 0.0293  max mem: 19734
Test:  [180/261]  eta: 0:00:50  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1390  data: 0.0029  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1375  data: 0.0006  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1363  data: 0.0004  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1355  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1352  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1316  data: 0.0001  max mem: 19734
Test: Total time: 0:02:02 (0.4710 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [19]  [   0/1251]  eta: 5:36:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 16.1302  data: 12.7879  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [  10/1251]  eta: 0:52:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.5188  data: 1.2180  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [  20/1251]  eta: 0:34:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.9717  data: 0.0308  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [  30/1251]  eta: 0:28:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [  40/1251]  eta: 0:25:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7921  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [  50/1251]  eta: 0:23:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [  60/1251]  eta: 0:21:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [  70/1251]  eta: 0:20:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [  80/1251]  eta: 0:19:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [  90/1251]  eta: 0:19:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 100/1251]  eta: 0:18:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8135  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 110/1251]  eta: 0:18:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8226  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 120/1251]  eta: 0:17:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8042  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 130/1251]  eta: 0:17:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 140/1251]  eta: 0:17:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 150/1251]  eta: 0:16:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 160/1251]  eta: 0:16:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 170/1251]  eta: 0:16:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 180/1251]  eta: 0:16:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 190/1251]  eta: 0:15:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 200/1251]  eta: 0:15:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 210/1251]  eta: 0:15:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 220/1251]  eta: 0:15:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 230/1251]  eta: 0:14:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7984  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 240/1251]  eta: 0:14:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7988  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 250/1251]  eta: 0:14:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 260/1251]  eta: 0:14:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8115  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 270/1251]  eta: 0:14:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8088  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 280/1251]  eta: 0:13:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 290/1251]  eta: 0:13:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 300/1251]  eta: 0:13:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 310/1251]  eta: 0:13:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 320/1251]  eta: 0:13:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 330/1251]  eta: 0:13:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 340/1251]  eta: 0:12:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 350/1251]  eta: 0:12:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 360/1251]  eta: 0:12:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 370/1251]  eta: 0:12:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 380/1251]  eta: 0:12:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 390/1251]  eta: 0:12:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 400/1251]  eta: 0:11:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 410/1251]  eta: 0:11:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8162  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 420/1251]  eta: 0:11:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8112  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 430/1251]  eta: 0:11:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 440/1251]  eta: 0:11:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 450/1251]  eta: 0:11:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 460/1251]  eta: 0:10:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 470/1251]  eta: 0:10:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 480/1251]  eta: 0:10:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 490/1251]  eta: 0:10:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 500/1251]  eta: 0:10:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7824  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 510/1251]  eta: 0:10:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7803  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 520/1251]  eta: 0:10:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 530/1251]  eta: 0:09:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8045  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 540/1251]  eta: 0:09:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8002  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 550/1251]  eta: 0:09:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8100  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 560/1251]  eta: 0:09:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8144  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 570/1251]  eta: 0:09:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 580/1251]  eta: 0:09:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7821  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 590/1251]  eta: 0:09:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 600/1251]  eta: 0:08:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 610/1251]  eta: 0:08:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 620/1251]  eta: 0:08:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 630/1251]  eta: 0:08:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 640/1251]  eta: 0:08:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 650/1251]  eta: 0:08:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 660/1251]  eta: 0:08:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 670/1251]  eta: 0:07:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 680/1251]  eta: 0:07:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8027  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 690/1251]  eta: 0:07:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 700/1251]  eta: 0:07:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8112  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 710/1251]  eta: 0:07:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8136  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 720/1251]  eta: 0:07:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 730/1251]  eta: 0:07:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 740/1251]  eta: 0:06:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 750/1251]  eta: 0:06:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 760/1251]  eta: 0:06:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 770/1251]  eta: 0:06:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 780/1251]  eta: 0:06:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 790/1251]  eta: 0:06:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 800/1251]  eta: 0:06:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 810/1251]  eta: 0:05:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 820/1251]  eta: 0:05:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 830/1251]  eta: 0:05:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8014  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 840/1251]  eta: 0:05:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8033  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 850/1251]  eta: 0:05:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8086  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 860/1251]  eta: 0:05:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 870/1251]  eta: 0:05:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 880/1251]  eta: 0:05:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 890/1251]  eta: 0:04:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7813  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 900/1251]  eta: 0:04:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 910/1251]  eta: 0:04:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7968  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 920/1251]  eta: 0:04:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 930/1251]  eta: 0:04:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 940/1251]  eta: 0:04:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 950/1251]  eta: 0:04:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 960/1251]  eta: 0:03:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8125  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [ 990/1251]  eta: 0:03:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8013  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1000/1251]  eta: 0:03:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8009  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1010/1251]  eta: 0:03:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7988  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1020/1251]  eta: 0:03:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1050/1251]  eta: 0:02:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1060/1251]  eta: 0:02:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1070/1251]  eta: 0:02:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1080/1251]  eta: 0:02:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7795  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8041  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8006  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [1130/1251]  eta: 0:01:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8001  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1140/1251]  eta: 0:01:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8270  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1150/1251]  eta: 0:01:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8211  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7798  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7771  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7694  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [19]  [1230/1251]  eta: 0:00:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7628  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7681  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [19]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7711  data: 0.0001  max mem: 19734
Epoch: [19] Total time: 0:16:50 (0.8078 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:23:30  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 32.9896  data: 32.5371  max mem: 19734
Test:  [ 10/261]  eta: 0:13:16  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.1722  data: 2.9746  max mem: 19734
Test:  [ 20/261]  eta: 0:07:28  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3054  data: 0.0188  max mem: 19734
Test:  [ 30/261]  eta: 0:05:10  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3415  data: 0.0143  max mem: 19734
Test:  [ 40/261]  eta: 0:03:56  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.2362  data: 0.0110  max mem: 19734
Test:  [ 50/261]  eta: 0:03:09  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.2028  data: 0.0082  max mem: 19734
Test:  [ 60/261]  eta: 0:02:37  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1962  data: 0.0038  max mem: 19734
Test:  [ 70/261]  eta: 0:02:15  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.2210  data: 0.0398  max mem: 19734
Test:  [ 80/261]  eta: 0:02:10  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.5260  data: 0.3088  max mem: 19734
Test:  [ 90/261]  eta: 0:01:58  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.6438  data: 0.3382  max mem: 19734
Test:  [100/261]  eta: 0:01:48  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4758  data: 0.1717  max mem: 19734
Test:  [110/261]  eta: 0:01:37  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4340  data: 0.1106  max mem: 19734
Test:  [120/261]  eta: 0:01:27  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3676  data: 0.0105  max mem: 19734
Test:  [130/261]  eta: 0:01:19  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3840  data: 0.0591  max mem: 19734
Test:  [140/261]  eta: 0:01:10  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3569  data: 0.0569  max mem: 19734
Test:  [150/261]  eta: 0:01:03  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.3547  data: 0.0084  max mem: 19734
Test:  [160/261]  eta: 0:00:56  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3916  data: 0.0514  max mem: 19734
Test:  [170/261]  eta: 0:00:50  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.4044  data: 0.0844  max mem: 19734
Test:  [180/261]  eta: 0:00:43  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.4053  data: 0.0432  max mem: 19734
Test:  [190/261]  eta: 0:00:38  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.4555  data: 0.1783  max mem: 19734
Test:  [200/261]  eta: 0:00:31  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.3614  data: 0.1768  max mem: 19734
Test:  [210/261]  eta: 0:00:25  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1541  data: 0.0026  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1369  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:14  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1361  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1318  data: 0.0001  max mem: 19734
Test: Total time: 0:01:53 (0.4358 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [20]  [   0/1251]  eta: 5:09:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 14.8337  data: 13.7767  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [  10/1251]  eta: 0:52:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.5228  data: 1.3268  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [  20/1251]  eta: 0:34:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 1.0463  data: 0.0412  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [  30/1251]  eta: 0:28:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8164  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [  40/1251]  eta: 0:25:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8188  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [  50/1251]  eta: 0:23:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [  60/1251]  eta: 0:22:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [  70/1251]  eta: 0:21:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [  80/1251]  eta: 0:20:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8024  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [  90/1251]  eta: 0:19:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 100/1251]  eta: 0:18:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 110/1251]  eta: 0:18:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 120/1251]  eta: 0:17:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 130/1251]  eta: 0:17:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 140/1251]  eta: 0:17:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 150/1251]  eta: 0:16:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 160/1251]  eta: 0:16:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7968  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 170/1251]  eta: 0:16:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 180/1251]  eta: 0:16:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8167  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 190/1251]  eta: 0:15:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8054  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 200/1251]  eta: 0:15:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 210/1251]  eta: 0:15:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 220/1251]  eta: 0:15:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8020  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 230/1251]  eta: 0:14:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 240/1251]  eta: 0:14:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 250/1251]  eta: 0:14:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7787  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 260/1251]  eta: 0:14:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7825  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 270/1251]  eta: 0:14:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 280/1251]  eta: 0:13:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 290/1251]  eta: 0:13:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7945  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 300/1251]  eta: 0:13:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8033  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 310/1251]  eta: 0:13:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 320/1251]  eta: 0:13:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 330/1251]  eta: 0:13:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8084  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 340/1251]  eta: 0:12:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7988  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 350/1251]  eta: 0:12:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 360/1251]  eta: 0:12:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 370/1251]  eta: 0:12:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 380/1251]  eta: 0:12:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 390/1251]  eta: 0:12:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 400/1251]  eta: 0:11:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7801  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 410/1251]  eta: 0:11:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 420/1251]  eta: 0:11:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 430/1251]  eta: 0:11:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 440/1251]  eta: 0:11:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 450/1251]  eta: 0:11:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 460/1251]  eta: 0:11:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8014  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 470/1251]  eta: 0:10:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8134  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 480/1251]  eta: 0:10:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8013  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 490/1251]  eta: 0:10:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 500/1251]  eta: 0:10:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 510/1251]  eta: 0:10:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 520/1251]  eta: 0:10:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 530/1251]  eta: 0:09:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 540/1251]  eta: 0:09:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 550/1251]  eta: 0:09:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 560/1251]  eta: 0:09:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 570/1251]  eta: 0:09:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 580/1251]  eta: 0:09:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 590/1251]  eta: 0:09:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 600/1251]  eta: 0:08:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7972  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 610/1251]  eta: 0:08:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8099  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 620/1251]  eta: 0:08:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8229  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 630/1251]  eta: 0:08:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 640/1251]  eta: 0:08:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 650/1251]  eta: 0:08:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 660/1251]  eta: 0:08:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 670/1251]  eta: 0:07:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7966  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 680/1251]  eta: 0:07:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 690/1251]  eta: 0:07:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 700/1251]  eta: 0:07:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7784  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 710/1251]  eta: 0:07:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7805  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 720/1251]  eta: 0:07:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 730/1251]  eta: 0:07:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 740/1251]  eta: 0:06:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7978  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 750/1251]  eta: 0:06:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8034  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 760/1251]  eta: 0:06:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8133  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 770/1251]  eta: 0:06:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8031  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 780/1251]  eta: 0:06:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 790/1251]  eta: 0:06:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 800/1251]  eta: 0:06:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 810/1251]  eta: 0:06:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 820/1251]  eta: 0:05:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 830/1251]  eta: 0:05:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 840/1251]  eta: 0:05:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 850/1251]  eta: 0:05:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 860/1251]  eta: 0:05:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7799  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 870/1251]  eta: 0:05:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 880/1251]  eta: 0:05:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 890/1251]  eta: 0:04:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 900/1251]  eta: 0:04:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 910/1251]  eta: 0:04:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8098  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 920/1251]  eta: 0:04:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 930/1251]  eta: 0:04:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 940/1251]  eta: 0:04:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 950/1251]  eta: 0:04:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 960/1251]  eta: 0:03:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [ 990/1251]  eta: 0:03:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1000/1251]  eta: 0:03:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1010/1251]  eta: 0:03:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1020/1251]  eta: 0:03:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8061  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1060/1251]  eta: 0:02:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8111  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1070/1251]  eta: 0:02:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1080/1251]  eta: 0:02:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7778  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7798  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1130/1251]  eta: 0:01:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1140/1251]  eta: 0:01:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1150/1251]  eta: 0:01:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7813  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7780  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8035  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7792  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1230/1251]  eta: 0:00:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7656  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7652  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [20]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7697  data: 0.0002  max mem: 19734
Epoch: [20] Total time: 0:16:50 (0.8078 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:32:06  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 34.9661  data: 34.7987  max mem: 19734
Test:  [ 10/261]  eta: 0:14:01  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.3521  data: 3.1735  max mem: 19734
Test:  [ 20/261]  eta: 0:07:25  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.1928  data: 0.0099  max mem: 19734
Test:  [ 30/261]  eta: 0:05:15  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2762  data: 0.0086  max mem: 19734
Test:  [ 40/261]  eta: 0:04:28  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5454  data: 0.2741  max mem: 19734
Test:  [ 50/261]  eta: 0:03:34  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4770  data: 0.2753  max mem: 19734
Test:  [ 60/261]  eta: 0:02:58  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2287  data: 0.0117  max mem: 19734
Test:  [ 70/261]  eta: 0:03:03  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.8053  data: 0.5998  max mem: 19734
Test:  [ 80/261]  eta: 0:02:40  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.8752  data: 0.5985  max mem: 19734
Test:  [ 90/261]  eta: 0:02:22  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3990  data: 0.0154  max mem: 19734
Test:  [100/261]  eta: 0:02:04  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3288  data: 0.0208  max mem: 19734
Test:  [110/261]  eta: 0:01:55  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4478  data: 0.1472  max mem: 19734
Test:  [120/261]  eta: 0:01:42  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4922  data: 0.1420  max mem: 19734
Test:  [130/261]  eta: 0:01:32  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3765  data: 0.0125  max mem: 19734
Test:  [140/261]  eta: 0:01:22  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4202  data: 0.0176  max mem: 19734
Test:  [150/261]  eta: 0:01:12  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.3218  data: 0.0137  max mem: 19734
Test:  [160/261]  eta: 0:01:03  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2066  data: 0.0082  max mem: 19734
Test:  [170/261]  eta: 0:00:56  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3488  data: 0.1496  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.4768  data: 0.2816  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.2945  data: 0.1408  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1435  data: 0.0044  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1360  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1314  data: 0.0001  max mem: 19734
Test: Total time: 0:02:01 (0.4654 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [21]  [   0/1251]  eta: 6:20:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 18.2469  data: 11.6931  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [  10/1251]  eta: 0:52:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.5534  data: 1.0650  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [  20/1251]  eta: 0:35:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8867  data: 0.0014  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [  30/1251]  eta: 0:28:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [  40/1251]  eta: 0:25:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [  50/1251]  eta: 0:23:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [  60/1251]  eta: 0:22:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [  70/1251]  eta: 0:20:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7998  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [  80/1251]  eta: 0:20:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8170  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [  90/1251]  eta: 0:19:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8185  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 100/1251]  eta: 0:18:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7972  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 110/1251]  eta: 0:18:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 120/1251]  eta: 0:18:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 130/1251]  eta: 0:17:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 140/1251]  eta: 0:17:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 150/1251]  eta: 0:16:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 160/1251]  eta: 0:16:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 170/1251]  eta: 0:16:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 180/1251]  eta: 0:16:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 190/1251]  eta: 0:15:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 200/1251]  eta: 0:15:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 210/1251]  eta: 0:15:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 220/1251]  eta: 0:15:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 230/1251]  eta: 0:14:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8091  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 240/1251]  eta: 0:14:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8112  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 250/1251]  eta: 0:14:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 260/1251]  eta: 0:14:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7825  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 270/1251]  eta: 0:14:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 280/1251]  eta: 0:13:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 290/1251]  eta: 0:13:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 300/1251]  eta: 0:13:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 310/1251]  eta: 0:13:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 320/1251]  eta: 0:13:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 330/1251]  eta: 0:13:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7799  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 340/1251]  eta: 0:12:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 350/1251]  eta: 0:12:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7984  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 360/1251]  eta: 0:12:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 370/1251]  eta: 0:12:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8024  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 380/1251]  eta: 0:12:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8176  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 390/1251]  eta: 0:12:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8087  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 400/1251]  eta: 0:11:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 410/1251]  eta: 0:11:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 420/1251]  eta: 0:11:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 430/1251]  eta: 0:11:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 440/1251]  eta: 0:11:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7794  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 450/1251]  eta: 0:11:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 460/1251]  eta: 0:11:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 470/1251]  eta: 0:10:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 480/1251]  eta: 0:10:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 490/1251]  eta: 0:10:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 500/1251]  eta: 0:10:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8076  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 510/1251]  eta: 0:10:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 520/1251]  eta: 0:10:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8085  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 530/1251]  eta: 0:09:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8220  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 540/1251]  eta: 0:09:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8041  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 550/1251]  eta: 0:09:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 560/1251]  eta: 0:09:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 570/1251]  eta: 0:09:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 580/1251]  eta: 0:09:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 590/1251]  eta: 0:09:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7815  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 600/1251]  eta: 0:08:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 610/1251]  eta: 0:08:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 620/1251]  eta: 0:08:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 630/1251]  eta: 0:08:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7781  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 640/1251]  eta: 0:08:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 650/1251]  eta: 0:08:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 660/1251]  eta: 0:08:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 670/1251]  eta: 0:07:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8113  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 680/1251]  eta: 0:07:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8109  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 690/1251]  eta: 0:07:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7898  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 700/1251]  eta: 0:07:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 710/1251]  eta: 0:07:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 720/1251]  eta: 0:07:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 730/1251]  eta: 0:07:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 740/1251]  eta: 0:06:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 750/1251]  eta: 0:06:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 760/1251]  eta: 0:06:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7883  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 770/1251]  eta: 0:06:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 780/1251]  eta: 0:06:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7772  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 790/1251]  eta: 0:06:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 800/1251]  eta: 0:06:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8087  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 810/1251]  eta: 0:06:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8103  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 820/1251]  eta: 0:05:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8211  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 830/1251]  eta: 0:05:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8048  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 840/1251]  eta: 0:05:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 850/1251]  eta: 0:05:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7799  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 860/1251]  eta: 0:05:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 870/1251]  eta: 0:05:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 880/1251]  eta: 0:05:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 890/1251]  eta: 0:04:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 900/1251]  eta: 0:04:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7825  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 910/1251]  eta: 0:04:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 920/1251]  eta: 0:04:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 930/1251]  eta: 0:04:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 940/1251]  eta: 0:04:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 950/1251]  eta: 0:04:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 960/1251]  eta: 0:03:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8070  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8004  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [ 990/1251]  eta: 0:03:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1000/1251]  eta: 0:03:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1010/1251]  eta: 0:03:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1020/1251]  eta: 0:03:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7803  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1060/1251]  eta: 0:02:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1070/1251]  eta: 0:02:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1080/1251]  eta: 0:02:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8141  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8197  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8047  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1130/1251]  eta: 0:01:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1140/1251]  eta: 0:01:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1150/1251]  eta: 0:01:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7699  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7682  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7687  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1230/1251]  eta: 0:00:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7705  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7807  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [21]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0002  max mem: 19734
Epoch: [21] Total time: 0:16:51 (0.8084 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:04:45  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 14.8852  data: 14.0031  max mem: 19734
Test:  [ 10/261]  eta: 0:14:01  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.3545  data: 2.9284  max mem: 19734
Test:  [ 20/261]  eta: 0:07:45  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 1.2830  data: 0.9200  max mem: 19734
Test:  [ 30/261]  eta: 0:05:34  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4016  data: 0.0204  max mem: 19734
Test:  [ 40/261]  eta: 0:04:32  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5031  data: 0.1034  max mem: 19734
Test:  [ 50/261]  eta: 0:03:51  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5478  data: 0.1064  max mem: 19734
Test:  [ 60/261]  eta: 0:03:15  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.4301  data: 0.0222  max mem: 19734
Test:  [ 70/261]  eta: 0:02:44  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.2610  data: 0.0140  max mem: 19734
Test:  [ 80/261]  eta: 0:02:20  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.1924  data: 0.0105  max mem: 19734
Test:  [ 90/261]  eta: 0:02:04  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2489  data: 0.0063  max mem: 19734
Test:  [100/261]  eta: 0:01:52  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3789  data: 0.0071  max mem: 19734
Test:  [110/261]  eta: 0:01:52  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.8438  data: 0.4022  max mem: 19734
Test:  [120/261]  eta: 0:01:47  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 1.0903  data: 0.6244  max mem: 19734
Test:  [130/261]  eta: 0:01:37  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.7075  data: 0.2336  max mem: 19734
Test:  [140/261]  eta: 0:01:27  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4760  data: 0.0131  max mem: 19734
Test:  [150/261]  eta: 0:01:17  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4002  data: 0.0642  max mem: 19734
Test:  [160/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2876  data: 0.0642  max mem: 19734
Test:  [170/261]  eta: 0:00:58  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2185  data: 0.0115  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1579  data: 0.0054  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1377  data: 0.0013  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1371  data: 0.0004  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1367  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1357  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1315  data: 0.0001  max mem: 19734
Test: Total time: 0:02:01 (0.4669 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [22]  [   0/1251]  eta: 7:12:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 20.7667  data: 19.9964  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [  10/1251]  eta: 0:55:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.6783  data: 1.8182  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [  20/1251]  eta: 0:36:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8337  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [  30/1251]  eta: 0:29:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [  40/1251]  eta: 0:26:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [  50/1251]  eta: 0:23:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [  60/1251]  eta: 0:22:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [  70/1251]  eta: 0:21:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [  80/1251]  eta: 0:20:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7957  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [  90/1251]  eta: 0:19:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 100/1251]  eta: 0:19:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 110/1251]  eta: 0:18:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8013  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 120/1251]  eta: 0:18:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8229  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 130/1251]  eta: 0:17:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8129  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 140/1251]  eta: 0:17:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 150/1251]  eta: 0:17:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8151  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 160/1251]  eta: 0:16:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8148  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 170/1251]  eta: 0:16:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 180/1251]  eta: 0:16:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 190/1251]  eta: 0:16:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 200/1251]  eta: 0:15:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 210/1251]  eta: 0:15:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 220/1251]  eta: 0:15:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 230/1251]  eta: 0:15:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 240/1251]  eta: 0:14:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 250/1251]  eta: 0:14:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 260/1251]  eta: 0:14:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 270/1251]  eta: 0:14:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8124  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 280/1251]  eta: 0:14:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8071  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 290/1251]  eta: 0:13:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 300/1251]  eta: 0:13:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 310/1251]  eta: 0:13:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8067  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 320/1251]  eta: 0:13:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7942  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 330/1251]  eta: 0:13:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 340/1251]  eta: 0:13:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 350/1251]  eta: 0:12:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 360/1251]  eta: 0:12:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 370/1251]  eta: 0:12:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 380/1251]  eta: 0:12:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 390/1251]  eta: 0:12:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 400/1251]  eta: 0:12:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 410/1251]  eta: 0:11:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 420/1251]  eta: 0:11:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8063  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 430/1251]  eta: 0:11:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 440/1251]  eta: 0:11:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 450/1251]  eta: 0:11:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8059  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 460/1251]  eta: 0:11:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7998  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 470/1251]  eta: 0:10:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 480/1251]  eta: 0:10:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 490/1251]  eta: 0:10:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 500/1251]  eta: 0:10:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 510/1251]  eta: 0:10:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 520/1251]  eta: 0:10:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7783  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 530/1251]  eta: 0:10:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 540/1251]  eta: 0:09:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 550/1251]  eta: 0:09:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 560/1251]  eta: 0:09:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8108  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 570/1251]  eta: 0:09:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8076  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 580/1251]  eta: 0:09:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 590/1251]  eta: 0:09:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7997  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 600/1251]  eta: 0:08:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8027  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 610/1251]  eta: 0:08:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7974  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 620/1251]  eta: 0:08:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7921  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 630/1251]  eta: 0:08:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7801  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 640/1251]  eta: 0:08:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 650/1251]  eta: 0:08:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 660/1251]  eta: 0:08:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 670/1251]  eta: 0:07:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 680/1251]  eta: 0:07:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 690/1251]  eta: 0:07:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8034  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 700/1251]  eta: 0:07:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8024  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 710/1251]  eta: 0:07:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 720/1251]  eta: 0:07:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 730/1251]  eta: 0:07:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 740/1251]  eta: 0:07:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8020  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 750/1251]  eta: 0:06:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7999  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 760/1251]  eta: 0:06:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 770/1251]  eta: 0:06:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 780/1251]  eta: 0:06:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7786  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 790/1251]  eta: 0:06:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 800/1251]  eta: 0:06:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 810/1251]  eta: 0:06:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 820/1251]  eta: 0:05:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 830/1251]  eta: 0:05:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 840/1251]  eta: 0:05:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7990  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 850/1251]  eta: 0:05:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8033  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 860/1251]  eta: 0:05:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 870/1251]  eta: 0:05:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 880/1251]  eta: 0:05:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 890/1251]  eta: 0:04:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 900/1251]  eta: 0:04:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 910/1251]  eta: 0:04:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 920/1251]  eta: 0:04:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 930/1251]  eta: 0:04:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 940/1251]  eta: 0:04:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 950/1251]  eta: 0:04:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 960/1251]  eta: 0:03:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7793  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [ 990/1251]  eta: 0:03:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1000/1251]  eta: 0:03:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8186  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1010/1251]  eta: 0:03:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8165  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1020/1251]  eta: 0:03:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8003  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1060/1251]  eta: 0:02:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7963  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1070/1251]  eta: 0:02:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1080/1251]  eta: 0:02:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1130/1251]  eta: 0:01:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8018  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1140/1251]  eta: 0:01:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8117  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1150/1251]  eta: 0:01:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8050  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7708  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7746  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7742  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1230/1251]  eta: 0:00:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7658  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7687  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [22]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7753  data: 0.0001  max mem: 19734
Epoch: [22] Total time: 0:16:53 (0.8102 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:15:17  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 31.1003  data: 30.5460  max mem: 19734
Test:  [ 10/261]  eta: 0:15:06  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.6133  data: 3.3566  max mem: 19734
Test:  [ 20/261]  eta: 0:08:35  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.6909  data: 0.3265  max mem: 19734
Test:  [ 30/261]  eta: 0:05:58  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4157  data: 0.0163  max mem: 19734
Test:  [ 40/261]  eta: 0:04:42  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3785  data: 0.0260  max mem: 19734
Test:  [ 50/261]  eta: 0:03:51  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.3977  data: 0.0242  max mem: 19734
Test:  [ 60/261]  eta: 0:03:16  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3638  data: 0.0124  max mem: 19734
Test:  [ 70/261]  eta: 0:02:59  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5384  data: 0.1230  max mem: 19734
Test:  [ 80/261]  eta: 0:02:38  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.5523  data: 0.1245  max mem: 19734
Test:  [ 90/261]  eta: 0:02:20  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4005  data: 0.0136  max mem: 19734
Test:  [100/261]  eta: 0:02:13  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.6439  data: 0.2404  max mem: 19734
Test:  [110/261]  eta: 0:01:58  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.6222  data: 0.2458  max mem: 19734
Test:  [120/261]  eta: 0:01:44  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3209  data: 0.0188  max mem: 19734
Test:  [130/261]  eta: 0:01:34  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3497  data: 0.0170  max mem: 19734
Test:  [140/261]  eta: 0:01:24  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4162  data: 0.1264  max mem: 19734
Test:  [150/261]  eta: 0:01:14  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.3822  data: 0.1305  max mem: 19734
Test:  [160/261]  eta: 0:01:06  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3472  data: 0.0195  max mem: 19734
Test:  [170/261]  eta: 0:00:57  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3541  data: 0.0778  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.3003  data: 0.0781  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1953  data: 0.0089  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1518  data: 0.0126  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1453  data: 0.0093  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1314  data: 0.0001  max mem: 19734
Test: Total time: 0:02:02 (0.4703 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [23]  [   0/1251]  eta: 6:53:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 19.8544  data: 5.1985  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [  10/1251]  eta: 0:55:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.6932  data: 0.6400  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [  20/1251]  eta: 0:36:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8860  data: 0.0923  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [  30/1251]  eta: 0:30:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8090  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [  40/1251]  eta: 0:26:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8167  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [  50/1251]  eta: 0:24:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7978  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [  60/1251]  eta: 0:22:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8046  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [  70/1251]  eta: 0:21:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8210  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [  80/1251]  eta: 0:20:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8029  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [  90/1251]  eta: 0:19:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 100/1251]  eta: 0:19:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 110/1251]  eta: 0:18:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 120/1251]  eta: 0:18:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 130/1251]  eta: 0:17:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 140/1251]  eta: 0:17:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 150/1251]  eta: 0:17:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 160/1251]  eta: 0:16:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 170/1251]  eta: 0:16:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 180/1251]  eta: 0:16:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8130  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 190/1251]  eta: 0:16:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8065  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 200/1251]  eta: 0:15:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 210/1251]  eta: 0:15:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8056  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 220/1251]  eta: 0:15:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8122  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 230/1251]  eta: 0:15:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 240/1251]  eta: 0:14:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 250/1251]  eta: 0:14:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7820  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 260/1251]  eta: 0:14:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 270/1251]  eta: 0:14:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 280/1251]  eta: 0:14:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 290/1251]  eta: 0:13:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 300/1251]  eta: 0:13:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 310/1251]  eta: 0:13:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7946  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 320/1251]  eta: 0:13:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 330/1251]  eta: 0:13:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8077  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 340/1251]  eta: 0:13:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8082  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 350/1251]  eta: 0:12:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 360/1251]  eta: 0:12:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 370/1251]  eta: 0:12:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7966  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 380/1251]  eta: 0:12:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 390/1251]  eta: 0:12:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 400/1251]  eta: 0:12:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 410/1251]  eta: 0:11:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 420/1251]  eta: 0:11:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 430/1251]  eta: 0:11:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 440/1251]  eta: 0:11:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 450/1251]  eta: 0:11:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 460/1251]  eta: 0:11:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8153  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 470/1251]  eta: 0:10:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8141  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 480/1251]  eta: 0:10:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8051  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 490/1251]  eta: 0:10:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8061  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 500/1251]  eta: 0:10:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 510/1251]  eta: 0:10:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 520/1251]  eta: 0:10:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 530/1251]  eta: 0:10:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 540/1251]  eta: 0:09:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 550/1251]  eta: 0:09:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 560/1251]  eta: 0:09:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 570/1251]  eta: 0:09:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 580/1251]  eta: 0:09:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 590/1251]  eta: 0:09:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 600/1251]  eta: 0:09:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 610/1251]  eta: 0:08:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8089  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 620/1251]  eta: 0:08:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8144  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 630/1251]  eta: 0:08:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 640/1251]  eta: 0:08:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 650/1251]  eta: 0:08:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8035  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 660/1251]  eta: 0:08:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 670/1251]  eta: 0:08:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 680/1251]  eta: 0:07:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 690/1251]  eta: 0:07:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 700/1251]  eta: 0:07:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 710/1251]  eta: 0:07:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 720/1251]  eta: 0:07:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 730/1251]  eta: 0:07:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 740/1251]  eta: 0:07:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 750/1251]  eta: 0:06:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8074  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 760/1251]  eta: 0:06:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8054  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 770/1251]  eta: 0:06:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 780/1251]  eta: 0:06:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8091  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 790/1251]  eta: 0:06:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8080  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 800/1251]  eta: 0:06:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8072  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 810/1251]  eta: 0:06:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 820/1251]  eta: 0:05:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 830/1251]  eta: 0:05:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 840/1251]  eta: 0:05:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 850/1251]  eta: 0:05:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 860/1251]  eta: 0:05:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 870/1251]  eta: 0:05:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 880/1251]  eta: 0:05:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 890/1251]  eta: 0:04:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 900/1251]  eta: 0:04:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 910/1251]  eta: 0:04:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8100  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 920/1251]  eta: 0:04:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 930/1251]  eta: 0:04:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 940/1251]  eta: 0:04:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8153  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 950/1251]  eta: 0:04:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8172  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 960/1251]  eta: 0:03:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 970/1251]  eta: 0:03:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 980/1251]  eta: 0:03:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [ 990/1251]  eta: 0:03:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1000/1251]  eta: 0:03:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1010/1251]  eta: 0:03:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1020/1251]  eta: 0:03:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1030/1251]  eta: 0:03:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1040/1251]  eta: 0:02:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1060/1251]  eta: 0:02:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8024  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1070/1251]  eta: 0:02:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1080/1251]  eta: 0:02:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1090/1251]  eta: 0:02:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8106  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1100/1251]  eta: 0:02:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8067  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1130/1251]  eta: 0:01:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1140/1251]  eta: 0:01:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1150/1251]  eta: 0:01:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1160/1251]  eta: 0:01:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7811  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1230/1251]  eta: 0:00:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7770  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [23]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0001  max mem: 19734
Epoch: [23] Total time: 0:16:56 (0.8124 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:04:30  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 28.6237  data: 27.8944  max mem: 19734
Test:  [ 10/261]  eta: 0:13:18  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.1813  data: 2.7624  max mem: 19734
Test:  [ 20/261]  eta: 0:07:44  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.5947  data: 0.1339  max mem: 19734
Test:  [ 30/261]  eta: 0:05:27  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4484  data: 0.0203  max mem: 19734
Test:  [ 40/261]  eta: 0:04:46  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6351  data: 0.1902  max mem: 19734
Test:  [ 50/261]  eta: 0:03:52  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6067  data: 0.1883  max mem: 19734
Test:  [ 60/261]  eta: 0:03:14  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2885  data: 0.0135  max mem: 19734
Test:  [ 70/261]  eta: 0:02:48  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3179  data: 0.0074  max mem: 19734
Test:  [ 80/261]  eta: 0:02:26  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3334  data: 0.0075  max mem: 19734
Test:  [ 90/261]  eta: 0:02:09  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3204  data: 0.0267  max mem: 19734
Test:  [100/261]  eta: 0:01:58  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4244  data: 0.1834  max mem: 19734
Test:  [110/261]  eta: 0:01:44  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3913  data: 0.1655  max mem: 19734
Test:  [120/261]  eta: 0:01:32  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2558  data: 0.0385  max mem: 19734
Test:  [130/261]  eta: 0:01:23  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3438  data: 0.0862  max mem: 19734
Test:  [140/261]  eta: 0:01:13  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3292  data: 0.0552  max mem: 19734
Test:  [150/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4335  data: 0.1345  max mem: 19734
Test:  [160/261]  eta: 0:01:00  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.5263  data: 0.1354  max mem: 19734
Test:  [170/261]  eta: 0:00:53  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.4402  data: 0.0806  max mem: 19734
Test:  [180/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.3364  data: 0.1075  max mem: 19734
Test:  [190/261]  eta: 0:00:38  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1744  data: 0.0332  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1400  data: 0.0026  max mem: 19734
Test:  [210/261]  eta: 0:00:25  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1361  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1355  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:14  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1313  data: 0.0001  max mem: 19734
Test: Total time: 0:01:54 (0.4374 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [24]  [   0/1251]  eta: 6:07:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 17.6097  data: 14.6114  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [  10/1251]  eta: 0:51:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.4740  data: 1.3309  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [  20/1251]  eta: 0:34:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8722  data: 0.0017  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [  30/1251]  eta: 0:28:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [  40/1251]  eta: 0:25:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [  50/1251]  eta: 0:23:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [  60/1251]  eta: 0:21:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [  70/1251]  eta: 0:20:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [  80/1251]  eta: 0:19:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8148  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [  90/1251]  eta: 0:19:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8230  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 100/1251]  eta: 0:18:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 110/1251]  eta: 0:18:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8057  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 120/1251]  eta: 0:17:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8147  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 130/1251]  eta: 0:17:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 140/1251]  eta: 0:17:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 150/1251]  eta: 0:16:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 160/1251]  eta: 0:16:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 170/1251]  eta: 0:16:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 180/1251]  eta: 0:16:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 190/1251]  eta: 0:15:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 200/1251]  eta: 0:15:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 210/1251]  eta: 0:15:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 220/1251]  eta: 0:15:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 230/1251]  eta: 0:14:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 240/1251]  eta: 0:14:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 250/1251]  eta: 0:14:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 260/1251]  eta: 0:14:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 270/1251]  eta: 0:14:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8163  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 280/1251]  eta: 0:13:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7978  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 290/1251]  eta: 0:13:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 300/1251]  eta: 0:13:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 310/1251]  eta: 0:13:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 320/1251]  eta: 0:13:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 330/1251]  eta: 0:13:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 340/1251]  eta: 0:12:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 350/1251]  eta: 0:12:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 360/1251]  eta: 0:12:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 370/1251]  eta: 0:12:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 380/1251]  eta: 0:12:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8100  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 390/1251]  eta: 0:12:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8048  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 400/1251]  eta: 0:11:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8059  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 410/1251]  eta: 0:11:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8103  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 420/1251]  eta: 0:11:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 430/1251]  eta: 0:11:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 440/1251]  eta: 0:11:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 450/1251]  eta: 0:11:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 460/1251]  eta: 0:11:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 470/1251]  eta: 0:10:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 480/1251]  eta: 0:10:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 490/1251]  eta: 0:10:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 500/1251]  eta: 0:10:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 510/1251]  eta: 0:10:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 520/1251]  eta: 0:10:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7942  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 530/1251]  eta: 0:09:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8020  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 540/1251]  eta: 0:09:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8058  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 550/1251]  eta: 0:09:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8020  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 560/1251]  eta: 0:09:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8120  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 570/1251]  eta: 0:09:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 580/1251]  eta: 0:09:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 590/1251]  eta: 0:09:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7804  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 600/1251]  eta: 0:08:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7825  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 610/1251]  eta: 0:08:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 620/1251]  eta: 0:08:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 630/1251]  eta: 0:08:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 640/1251]  eta: 0:08:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 650/1251]  eta: 0:08:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 660/1251]  eta: 0:08:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7957  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 670/1251]  eta: 0:07:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8009  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 680/1251]  eta: 0:07:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8092  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 690/1251]  eta: 0:07:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8088  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 700/1251]  eta: 0:07:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 710/1251]  eta: 0:07:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 720/1251]  eta: 0:07:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7984  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 730/1251]  eta: 0:07:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 740/1251]  eta: 0:06:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 750/1251]  eta: 0:06:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 760/1251]  eta: 0:06:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 770/1251]  eta: 0:06:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 780/1251]  eta: 0:06:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7791  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 790/1251]  eta: 0:06:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 800/1251]  eta: 0:06:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8049  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 810/1251]  eta: 0:06:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8045  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 820/1251]  eta: 0:05:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7987  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 830/1251]  eta: 0:05:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 840/1251]  eta: 0:05:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 850/1251]  eta: 0:05:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8073  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 860/1251]  eta: 0:05:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 870/1251]  eta: 0:05:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 880/1251]  eta: 0:05:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 890/1251]  eta: 0:04:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7821  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 900/1251]  eta: 0:04:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7824  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 910/1251]  eta: 0:04:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 920/1251]  eta: 0:04:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 930/1251]  eta: 0:04:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 940/1251]  eta: 0:04:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 950/1251]  eta: 0:04:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8030  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 960/1251]  eta: 0:03:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7946  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [ 990/1251]  eta: 0:03:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8068  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1000/1251]  eta: 0:03:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8084  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1010/1251]  eta: 0:03:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1020/1251]  eta: 0:03:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1060/1251]  eta: 0:02:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1070/1251]  eta: 0:02:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1080/1251]  eta: 0:02:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7808  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7962  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8131  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7987  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1130/1251]  eta: 0:01:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1140/1251]  eta: 0:01:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8081  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1150/1251]  eta: 0:01:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7942  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7801  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7692  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7689  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7723  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1230/1251]  eta: 0:00:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7662  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7797  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [24]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0002  max mem: 19734
Epoch: [24] Total time: 0:16:50 (0.8079 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:30:58  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 20.9126  data: 20.7539  max mem: 19734
Test:  [ 10/261]  eta: 0:10:47  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.5802  data: 2.3728  max mem: 19734
Test:  [ 20/261]  eta: 0:05:58  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.5144  data: 0.2740  max mem: 19734
Test:  [ 30/261]  eta: 0:04:10  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2637  data: 0.0123  max mem: 19734
Test:  [ 40/261]  eta: 0:04:24  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.8893  data: 0.6186  max mem: 19734
Test:  [ 50/261]  eta: 0:03:34  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.9113  data: 0.6218  max mem: 19734
Test:  [ 60/261]  eta: 0:03:03  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3329  data: 0.0168  max mem: 19734
Test:  [ 70/261]  eta: 0:02:38  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3451  data: 0.0125  max mem: 19734
Test:  [ 80/261]  eta: 0:02:15  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.2502  data: 0.0106  max mem: 19734
Test:  [ 90/261]  eta: 0:01:58  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2140  data: 0.0093  max mem: 19734
Test:  [100/261]  eta: 0:01:55  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.5825  data: 0.2810  max mem: 19734
Test:  [110/261]  eta: 0:01:42  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.6209  data: 0.2807  max mem: 19734
Test:  [120/261]  eta: 0:01:32  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3669  data: 0.0137  max mem: 19734
Test:  [130/261]  eta: 0:01:27  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.5908  data: 0.2305  max mem: 19734
Test:  [140/261]  eta: 0:01:19  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.6653  data: 0.2293  max mem: 19734
Test:  [150/261]  eta: 0:01:11  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4981  data: 0.0162  max mem: 19734
Test:  [160/261]  eta: 0:01:03  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3796  data: 0.0150  max mem: 19734
Test:  [170/261]  eta: 0:00:56  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.4114  data: 0.1587  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.3356  data: 0.1545  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1626  data: 0.0077  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1444  data: 0.0047  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1375  data: 0.0012  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1351  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1312  data: 0.0001  max mem: 19734
Test: Total time: 0:01:58 (0.4535 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [25]  [   0/1251]  eta: 4:41:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 13.5041  data: 8.5837  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [  10/1251]  eta: 0:50:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.4260  data: 1.1509  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [  20/1251]  eta: 0:33:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 1.0627  data: 0.2041  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [  30/1251]  eta: 0:28:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8064  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [  40/1251]  eta: 0:24:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [  50/1251]  eta: 0:23:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [  60/1251]  eta: 0:21:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [  70/1251]  eta: 0:20:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [  80/1251]  eta: 0:19:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [  90/1251]  eta: 0:19:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 100/1251]  eta: 0:18:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 110/1251]  eta: 0:18:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7800  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 120/1251]  eta: 0:17:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 130/1251]  eta: 0:17:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8063  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 140/1251]  eta: 0:17:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8133  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 150/1251]  eta: 0:16:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8120  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 160/1251]  eta: 0:16:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 170/1251]  eta: 0:16:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8005  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 180/1251]  eta: 0:15:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8037  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 190/1251]  eta: 0:15:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 200/1251]  eta: 0:15:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 210/1251]  eta: 0:15:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 220/1251]  eta: 0:15:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 230/1251]  eta: 0:14:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 240/1251]  eta: 0:14:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 250/1251]  eta: 0:14:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 260/1251]  eta: 0:14:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7801  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 270/1251]  eta: 0:14:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8047  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 280/1251]  eta: 0:13:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8083  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 290/1251]  eta: 0:13:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8039  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 300/1251]  eta: 0:13:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8078  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 310/1251]  eta: 0:13:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 320/1251]  eta: 0:13:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8119  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 330/1251]  eta: 0:13:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8045  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 340/1251]  eta: 0:12:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 350/1251]  eta: 0:12:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 360/1251]  eta: 0:12:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 370/1251]  eta: 0:12:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7814  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 380/1251]  eta: 0:12:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7813  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 390/1251]  eta: 0:12:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 400/1251]  eta: 0:11:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 410/1251]  eta: 0:11:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 420/1251]  eta: 0:11:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 430/1251]  eta: 0:11:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8086  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 440/1251]  eta: 0:11:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8047  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 450/1251]  eta: 0:11:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 460/1251]  eta: 0:11:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8030  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 470/1251]  eta: 0:10:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8126  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 480/1251]  eta: 0:10:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 490/1251]  eta: 0:10:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 500/1251]  eta: 0:10:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 510/1251]  eta: 0:10:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 520/1251]  eta: 0:10:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 530/1251]  eta: 0:09:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 540/1251]  eta: 0:09:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 550/1251]  eta: 0:09:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 560/1251]  eta: 0:09:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 570/1251]  eta: 0:09:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 580/1251]  eta: 0:09:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8084  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 590/1251]  eta: 0:09:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8141  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 600/1251]  eta: 0:08:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7998  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 610/1251]  eta: 0:08:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8125  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 620/1251]  eta: 0:08:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8073  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 630/1251]  eta: 0:08:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 640/1251]  eta: 0:08:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 650/1251]  eta: 0:08:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 660/1251]  eta: 0:08:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 670/1251]  eta: 0:07:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 680/1251]  eta: 0:07:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 690/1251]  eta: 0:07:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 700/1251]  eta: 0:07:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 710/1251]  eta: 0:07:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7997  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 720/1251]  eta: 0:07:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8063  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 730/1251]  eta: 0:07:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8050  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 740/1251]  eta: 0:06:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8059  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 750/1251]  eta: 0:06:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7957  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 760/1251]  eta: 0:06:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8111  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 770/1251]  eta: 0:06:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8093  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 780/1251]  eta: 0:06:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 790/1251]  eta: 0:06:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 800/1251]  eta: 0:06:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 810/1251]  eta: 0:06:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 820/1251]  eta: 0:05:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 830/1251]  eta: 0:05:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7811  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 840/1251]  eta: 0:05:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 850/1251]  eta: 0:05:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8076  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 860/1251]  eta: 0:05:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8041  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 870/1251]  eta: 0:05:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 880/1251]  eta: 0:05:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8027  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 890/1251]  eta: 0:04:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 900/1251]  eta: 0:04:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 910/1251]  eta: 0:04:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8034  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 920/1251]  eta: 0:04:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7985  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 930/1251]  eta: 0:04:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 940/1251]  eta: 0:04:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 950/1251]  eta: 0:04:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 960/1251]  eta: 0:03:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [ 990/1251]  eta: 0:03:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1000/1251]  eta: 0:03:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8108  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1010/1251]  eta: 0:03:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8075  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1020/1251]  eta: 0:03:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8061  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8068  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8076  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1060/1251]  eta: 0:02:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7971  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1070/1251]  eta: 0:02:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1080/1251]  eta: 0:02:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7825  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1130/1251]  eta: 0:01:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1140/1251]  eta: 0:01:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1150/1251]  eta: 0:01:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8049  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8068  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7705  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7693  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1230/1251]  eta: 0:00:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7675  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7642  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [25]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7676  data: 0.0002  max mem: 19734
Epoch: [25] Total time: 0:16:51 (0.8086 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 0:59:55  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 13.7766  data: 13.6349  max mem: 19734
Test:  [ 10/261]  eta: 0:09:01  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.1566  data: 1.9818  max mem: 19734
Test:  [ 20/261]  eta: 0:04:50  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.5777  data: 0.4098  max mem: 19734
Test:  [ 30/261]  eta: 0:03:22  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1736  data: 0.0042  max mem: 19734
Test:  [ 40/261]  eta: 0:02:41  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.2297  data: 0.0598  max mem: 19734
Test:  [ 50/261]  eta: 0:02:11  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.2248  data: 0.0598  max mem: 19734
Test:  [ 60/261]  eta: 0:01:50  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1734  data: 0.0051  max mem: 19734
Test:  [ 70/261]  eta: 0:02:18  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.9806  data: 0.7594  max mem: 19734
Test:  [ 80/261]  eta: 0:02:00  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 1.0288  data: 0.7641  max mem: 19734
Test:  [ 90/261]  eta: 0:01:47  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2858  data: 0.0172  max mem: 19734
Test:  [100/261]  eta: 0:01:37  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3562  data: 0.0656  max mem: 19734
Test:  [110/261]  eta: 0:01:26  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3459  data: 0.0631  max mem: 19734
Test:  [120/261]  eta: 0:01:16  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2264  data: 0.0125  max mem: 19734
Test:  [130/261]  eta: 0:01:10  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3246  data: 0.1712  max mem: 19734
Test:  [140/261]  eta: 0:01:01  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3147  data: 0.1701  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.1762  data: 0.0062  max mem: 19734
Test:  [160/261]  eta: 0:00:52  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.5596  data: 0.3086  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.5990  data: 0.3128  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2527  data: 0.0262  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1961  data: 0.0236  max mem: 19734
Test:  [200/261]  eta: 0:00:28  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.2056  data: 0.0053  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1954  data: 0.0030  max mem: 19734
Test:  [220/261]  eta: 0:00:18  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.3790  data: 0.2305  max mem: 19734
Test:  [230/261]  eta: 0:00:13  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.3951  data: 0.2337  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.4496  data: 0.2558  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.5255  data: 0.3475  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.2308  data: 0.0965  max mem: 19734
Test: Total time: 0:01:53 (0.4357 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [26]  [   0/1251]  eta: 6:09:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 17.7097  data: 16.6738  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [  10/1251]  eta: 0:51:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.4857  data: 1.5172  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [  20/1251]  eta: 0:34:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8782  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [  30/1251]  eta: 0:28:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [  40/1251]  eta: 0:25:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [  50/1251]  eta: 0:23:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8166  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [  60/1251]  eta: 0:21:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8166  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [  70/1251]  eta: 0:20:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [  80/1251]  eta: 0:20:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [  90/1251]  eta: 0:19:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8036  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 100/1251]  eta: 0:18:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 110/1251]  eta: 0:18:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 120/1251]  eta: 0:17:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 130/1251]  eta: 0:17:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 140/1251]  eta: 0:17:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 150/1251]  eta: 0:16:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 160/1251]  eta: 0:16:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 170/1251]  eta: 0:16:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 180/1251]  eta: 0:16:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8082  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 190/1251]  eta: 0:15:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 200/1251]  eta: 0:15:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8037  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 210/1251]  eta: 0:15:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8086  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 220/1251]  eta: 0:15:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8127  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 230/1251]  eta: 0:14:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8114  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 240/1251]  eta: 0:14:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 250/1251]  eta: 0:14:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 260/1251]  eta: 0:14:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 270/1251]  eta: 0:14:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 280/1251]  eta: 0:13:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 290/1251]  eta: 0:13:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 300/1251]  eta: 0:13:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 310/1251]  eta: 0:13:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7836  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 320/1251]  eta: 0:13:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 330/1251]  eta: 0:13:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8085  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 340/1251]  eta: 0:12:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8102  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 350/1251]  eta: 0:12:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8122  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 360/1251]  eta: 0:12:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 370/1251]  eta: 0:12:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 380/1251]  eta: 0:12:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 390/1251]  eta: 0:12:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7946  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 400/1251]  eta: 0:11:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 410/1251]  eta: 0:11:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 420/1251]  eta: 0:11:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 430/1251]  eta: 0:11:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 440/1251]  eta: 0:11:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 450/1251]  eta: 0:11:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7811  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 460/1251]  eta: 0:11:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 470/1251]  eta: 0:10:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8122  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 480/1251]  eta: 0:10:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8112  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 490/1251]  eta: 0:10:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 500/1251]  eta: 0:10:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8092  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 510/1251]  eta: 0:10:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 520/1251]  eta: 0:10:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 530/1251]  eta: 0:09:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7942  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 540/1251]  eta: 0:09:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 550/1251]  eta: 0:09:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 560/1251]  eta: 0:09:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 570/1251]  eta: 0:09:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 580/1251]  eta: 0:09:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 590/1251]  eta: 0:09:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 600/1251]  eta: 0:08:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 610/1251]  eta: 0:08:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 620/1251]  eta: 0:08:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8057  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 630/1251]  eta: 0:08:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8127  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 640/1251]  eta: 0:08:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 650/1251]  eta: 0:08:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 660/1251]  eta: 0:08:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8062  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 670/1251]  eta: 0:07:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8102  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 680/1251]  eta: 0:07:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 690/1251]  eta: 0:07:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 700/1251]  eta: 0:07:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 710/1251]  eta: 0:07:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 720/1251]  eta: 0:07:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7801  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 730/1251]  eta: 0:07:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 740/1251]  eta: 0:06:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 750/1251]  eta: 0:06:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 760/1251]  eta: 0:06:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 770/1251]  eta: 0:06:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8056  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 780/1251]  eta: 0:06:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8030  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 790/1251]  eta: 0:06:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8049  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 800/1251]  eta: 0:06:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8040  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 810/1251]  eta: 0:06:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8056  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 820/1251]  eta: 0:05:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8075  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 830/1251]  eta: 0:05:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 840/1251]  eta: 0:05:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 850/1251]  eta: 0:05:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 860/1251]  eta: 0:05:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 870/1251]  eta: 0:05:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 880/1251]  eta: 0:05:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 890/1251]  eta: 0:04:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 900/1251]  eta: 0:04:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 910/1251]  eta: 0:04:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 920/1251]  eta: 0:04:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8066  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 930/1251]  eta: 0:04:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8054  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 940/1251]  eta: 0:04:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 950/1251]  eta: 0:04:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8073  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 960/1251]  eta: 0:03:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8103  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [ 990/1251]  eta: 0:03:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1000/1251]  eta: 0:03:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1010/1251]  eta: 0:03:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1020/1251]  eta: 0:03:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7820  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1060/1251]  eta: 0:02:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8003  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1070/1251]  eta: 0:02:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8058  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1080/1251]  eta: 0:02:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8063  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8047  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8057  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1130/1251]  eta: 0:01:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1140/1251]  eta: 0:01:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1150/1251]  eta: 0:01:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1230/1251]  eta: 0:00:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7824  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [26]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7778  data: 0.0002  max mem: 19734
Epoch: [26] Total time: 0:16:53 (0.8099 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:51:30  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 25.6341  data: 25.4847  max mem: 19734
Test:  [ 10/261]  eta: 0:11:25  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.7315  data: 2.3988  max mem: 19734
Test:  [ 20/261]  eta: 0:06:23  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3900  data: 0.0981  max mem: 19734
Test:  [ 30/261]  eta: 0:04:36  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3496  data: 0.0588  max mem: 19734
Test:  [ 40/261]  eta: 0:03:30  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.2850  data: 0.0077  max mem: 19734
Test:  [ 50/261]  eta: 0:03:18  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5499  data: 0.2991  max mem: 19734
Test:  [ 60/261]  eta: 0:02:56  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.7145  data: 0.4050  max mem: 19734
Test:  [ 70/261]  eta: 0:02:33  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.4598  data: 0.1138  max mem: 19734
Test:  [ 80/261]  eta: 0:02:19  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4572  data: 0.1821  max mem: 19734
Test:  [ 90/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4784  data: 0.1836  max mem: 19734
Test:  [100/261]  eta: 0:01:51  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3790  data: 0.0123  max mem: 19734
Test:  [110/261]  eta: 0:01:39  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3222  data: 0.0085  max mem: 19734
Test:  [120/261]  eta: 0:01:29  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3484  data: 0.0229  max mem: 19734
Test:  [130/261]  eta: 0:01:21  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.4150  data: 0.1409  max mem: 19734
Test:  [140/261]  eta: 0:01:13  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3990  data: 0.1280  max mem: 19734
Test:  [150/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.5178  data: 0.1379  max mem: 19734
Test:  [160/261]  eta: 0:00:59  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.4563  data: 0.1388  max mem: 19734
Test:  [170/261]  eta: 0:00:51  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2439  data: 0.0162  max mem: 19734
Test:  [180/261]  eta: 0:00:45  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.3750  data: 0.0986  max mem: 19734
Test:  [190/261]  eta: 0:00:38  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.3757  data: 0.0929  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.2567  data: 0.0089  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.3497  data: 0.1314  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.2922  data: 0.1261  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1381  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1350  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1351  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1316  data: 0.0001  max mem: 19734
Test: Total time: 0:01:58 (0.4533 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [27]  [   0/1251]  eta: 6:04:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 17.4922  data: 11.6747  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [  10/1251]  eta: 0:52:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.5428  data: 1.2961  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [  20/1251]  eta: 0:35:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.9233  data: 0.1294  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [  30/1251]  eta: 0:28:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [  40/1251]  eta: 0:25:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [  50/1251]  eta: 0:23:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [  60/1251]  eta: 0:21:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [  70/1251]  eta: 0:20:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [  80/1251]  eta: 0:20:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [  90/1251]  eta: 0:19:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8017  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 100/1251]  eta: 0:18:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8136  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 110/1251]  eta: 0:18:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8047  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 120/1251]  eta: 0:17:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 130/1251]  eta: 0:17:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7999  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 140/1251]  eta: 0:17:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8072  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 150/1251]  eta: 0:16:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 160/1251]  eta: 0:16:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 170/1251]  eta: 0:16:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 180/1251]  eta: 0:16:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 190/1251]  eta: 0:15:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 200/1251]  eta: 0:15:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 210/1251]  eta: 0:15:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 220/1251]  eta: 0:15:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 230/1251]  eta: 0:14:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 240/1251]  eta: 0:14:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8105  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 250/1251]  eta: 0:14:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8191  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 260/1251]  eta: 0:14:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8109  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 270/1251]  eta: 0:14:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 280/1251]  eta: 0:14:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 290/1251]  eta: 0:13:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8163  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 300/1251]  eta: 0:13:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8039  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 310/1251]  eta: 0:13:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 320/1251]  eta: 0:13:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 330/1251]  eta: 0:13:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7988  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 340/1251]  eta: 0:12:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 350/1251]  eta: 0:12:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 360/1251]  eta: 0:12:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 370/1251]  eta: 0:12:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 380/1251]  eta: 0:12:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8017  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 390/1251]  eta: 0:12:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8033  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 400/1251]  eta: 0:11:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 410/1251]  eta: 0:11:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8027  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 420/1251]  eta: 0:11:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8088  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 430/1251]  eta: 0:11:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8017  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 440/1251]  eta: 0:11:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 450/1251]  eta: 0:11:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 460/1251]  eta: 0:11:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 470/1251]  eta: 0:10:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 480/1251]  eta: 0:10:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 490/1251]  eta: 0:10:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 500/1251]  eta: 0:10:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 510/1251]  eta: 0:10:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 520/1251]  eta: 0:10:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 530/1251]  eta: 0:10:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8089  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 540/1251]  eta: 0:09:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8083  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 550/1251]  eta: 0:09:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 560/1251]  eta: 0:09:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8101  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 570/1251]  eta: 0:09:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8036  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 580/1251]  eta: 0:09:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 590/1251]  eta: 0:09:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7999  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 600/1251]  eta: 0:08:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 610/1251]  eta: 0:08:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 620/1251]  eta: 0:08:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 630/1251]  eta: 0:08:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 640/1251]  eta: 0:08:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 650/1251]  eta: 0:08:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7805  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 660/1251]  eta: 0:08:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 670/1251]  eta: 0:07:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7963  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 680/1251]  eta: 0:07:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8006  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 690/1251]  eta: 0:07:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 700/1251]  eta: 0:07:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8037  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 710/1251]  eta: 0:07:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8099  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 720/1251]  eta: 0:07:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8026  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 730/1251]  eta: 0:07:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 740/1251]  eta: 0:06:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 750/1251]  eta: 0:06:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 760/1251]  eta: 0:06:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 770/1251]  eta: 0:06:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 780/1251]  eta: 0:06:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 790/1251]  eta: 0:06:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 800/1251]  eta: 0:06:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7814  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 810/1251]  eta: 0:06:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 820/1251]  eta: 0:05:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8009  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 830/1251]  eta: 0:05:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 840/1251]  eta: 0:05:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 850/1251]  eta: 0:05:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8006  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 860/1251]  eta: 0:05:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8038  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 870/1251]  eta: 0:05:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8030  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 880/1251]  eta: 0:05:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 890/1251]  eta: 0:04:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 900/1251]  eta: 0:04:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 910/1251]  eta: 0:04:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7819  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 920/1251]  eta: 0:04:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7818  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 930/1251]  eta: 0:04:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 940/1251]  eta: 0:04:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 950/1251]  eta: 0:04:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 960/1251]  eta: 0:03:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8029  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8121  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [ 990/1251]  eta: 0:03:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8042  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1000/1251]  eta: 0:03:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8041  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1010/1251]  eta: 0:03:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1020/1251]  eta: 0:03:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1060/1251]  eta: 0:02:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1070/1251]  eta: 0:02:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1080/1251]  eta: 0:02:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7806  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1130/1251]  eta: 0:01:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1140/1251]  eta: 0:01:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8051  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1150/1251]  eta: 0:01:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8051  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8151  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.8048  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7764  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7666  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7633  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1230/1251]  eta: 0:00:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7689  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7702  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [27]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.7665  data: 0.0001  max mem: 19734
Epoch: [27] Total time: 0:16:52 (0.8095 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:38:00  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 36.3225  data: 36.1356  max mem: 19734
Test:  [ 10/261]  eta: 0:14:45  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.5292  data: 3.2968  max mem: 19734
Test:  [ 20/261]  eta: 0:08:04  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2931  data: 0.0159  max mem: 19734
Test:  [ 30/261]  eta: 0:05:53  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4339  data: 0.0166  max mem: 19734
Test:  [ 40/261]  eta: 0:04:41  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5050  data: 0.0475  max mem: 19734
Test:  [ 50/261]  eta: 0:03:50  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4144  data: 0.0456  max mem: 19734
Test:  [ 60/261]  eta: 0:03:10  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2827  data: 0.0126  max mem: 19734
Test:  [ 70/261]  eta: 0:02:39  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.1838  data: 0.0093  max mem: 19734
Test:  [ 80/261]  eta: 0:02:25  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3593  data: 0.1832  max mem: 19734
Test:  [ 90/261]  eta: 0:02:08  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4355  data: 0.1851  max mem: 19734
Test:  [100/261]  eta: 0:01:54  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3365  data: 0.0217  max mem: 19734
Test:  [110/261]  eta: 0:01:46  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.5088  data: 0.2394  max mem: 19734
Test:  [120/261]  eta: 0:01:34  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4413  data: 0.2261  max mem: 19734
Test:  [130/261]  eta: 0:01:22  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2254  data: 0.0106  max mem: 19734
Test:  [140/261]  eta: 0:01:14  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3044  data: 0.0148  max mem: 19734
Test:  [150/261]  eta: 0:01:11  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.7185  data: 0.3237  max mem: 19734
Test:  [160/261]  eta: 0:01:05  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.8726  data: 0.5138  max mem: 19734
Test:  [170/261]  eta: 0:00:56  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.4395  data: 0.2043  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1598  data: 0.0069  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1381  data: 0.0017  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1365  data: 0.0004  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1359  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1314  data: 0.0001  max mem: 19734
Test: Total time: 0:01:58 (0.4536 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [28]  [   0/1251]  eta: 5:49:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 16.7820  data: 8.7369  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [  10/1251]  eta: 0:51:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.4940  data: 0.8320  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [  20/1251]  eta: 0:34:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.9396  data: 0.0210  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [  30/1251]  eta: 0:28:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8059  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [  40/1251]  eta: 0:25:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [  50/1251]  eta: 0:23:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [  60/1251]  eta: 0:21:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [  70/1251]  eta: 0:20:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0006  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [  80/1251]  eta: 0:20:04  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [  90/1251]  eta: 0:19:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 100/1251]  eta: 0:18:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 110/1251]  eta: 0:18:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 120/1251]  eta: 0:17:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 130/1251]  eta: 0:17:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7814  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 140/1251]  eta: 0:17:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8083  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 150/1251]  eta: 0:16:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8091  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 160/1251]  eta: 0:16:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7814  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 170/1251]  eta: 0:16:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7945  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 180/1251]  eta: 0:16:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8086  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 190/1251]  eta: 0:15:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8056  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 200/1251]  eta: 0:15:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 210/1251]  eta: 0:15:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 220/1251]  eta: 0:15:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 230/1251]  eta: 0:14:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 240/1251]  eta: 0:14:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 250/1251]  eta: 0:14:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 260/1251]  eta: 0:14:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 270/1251]  eta: 0:14:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 280/1251]  eta: 0:13:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 290/1251]  eta: 0:13:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8128  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 300/1251]  eta: 0:13:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8039  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 310/1251]  eta: 0:13:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 320/1251]  eta: 0:13:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7974  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 330/1251]  eta: 0:13:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8095  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 340/1251]  eta: 0:12:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 350/1251]  eta: 0:12:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 360/1251]  eta: 0:12:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 370/1251]  eta: 0:12:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 380/1251]  eta: 0:12:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 390/1251]  eta: 0:12:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7813  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 400/1251]  eta: 0:11:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 410/1251]  eta: 0:11:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 420/1251]  eta: 0:11:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 430/1251]  eta: 0:11:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 440/1251]  eta: 0:11:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8102  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 450/1251]  eta: 0:11:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 460/1251]  eta: 0:11:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 470/1251]  eta: 0:10:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8018  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 480/1251]  eta: 0:10:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8086  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 490/1251]  eta: 0:10:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8064  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 500/1251]  eta: 0:10:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 510/1251]  eta: 0:10:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 520/1251]  eta: 0:10:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 530/1251]  eta: 0:09:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 540/1251]  eta: 0:09:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 550/1251]  eta: 0:09:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 560/1251]  eta: 0:09:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 570/1251]  eta: 0:09:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 580/1251]  eta: 0:09:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 590/1251]  eta: 0:09:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8096  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 600/1251]  eta: 0:08:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 610/1251]  eta: 0:08:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 620/1251]  eta: 0:08:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8033  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 630/1251]  eta: 0:08:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8013  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 640/1251]  eta: 0:08:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 650/1251]  eta: 0:08:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 660/1251]  eta: 0:08:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 670/1251]  eta: 0:07:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 680/1251]  eta: 0:07:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 690/1251]  eta: 0:07:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 700/1251]  eta: 0:07:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 710/1251]  eta: 0:07:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 720/1251]  eta: 0:07:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 730/1251]  eta: 0:07:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 740/1251]  eta: 0:06:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8036  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 750/1251]  eta: 0:06:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 760/1251]  eta: 0:06:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 770/1251]  eta: 0:06:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 780/1251]  eta: 0:06:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7997  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 790/1251]  eta: 0:06:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 800/1251]  eta: 0:06:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7966  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 810/1251]  eta: 0:06:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 820/1251]  eta: 0:05:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 830/1251]  eta: 0:05:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 840/1251]  eta: 0:05:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 850/1251]  eta: 0:05:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 860/1251]  eta: 0:05:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 870/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 880/1251]  eta: 0:05:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8041  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 890/1251]  eta: 0:04:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 900/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 910/1251]  eta: 0:04:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7987  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 920/1251]  eta: 0:04:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 930/1251]  eta: 0:04:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8036  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 940/1251]  eta: 0:04:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 950/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 960/1251]  eta: 0:03:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 970/1251]  eta: 0:03:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 980/1251]  eta: 0:03:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [ 990/1251]  eta: 0:03:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1000/1251]  eta: 0:03:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1010/1251]  eta: 0:03:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1020/1251]  eta: 0:03:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1030/1251]  eta: 0:02:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1040/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1050/1251]  eta: 0:02:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1060/1251]  eta: 0:02:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8047  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1070/1251]  eta: 0:02:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1080/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1090/1251]  eta: 0:02:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1100/1251]  eta: 0:02:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7821  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1110/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1120/1251]  eta: 0:01:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1130/1251]  eta: 0:01:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1140/1251]  eta: 0:01:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1150/1251]  eta: 0:01:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7814  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1160/1251]  eta: 0:01:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1170/1251]  eta: 0:01:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8038  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1180/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8004  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1190/1251]  eta: 0:00:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1200/1251]  eta: 0:00:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1210/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7797  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1220/1251]  eta: 0:00:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7648  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1230/1251]  eta: 0:00:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7732  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1240/1251]  eta: 0:00:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7786  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [28]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7696  data: 0.0001  max mem: 19734
Epoch: [28] Total time: 0:16:51 (0.8088 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:11:47  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 30.2951  data: 29.5097  max mem: 19734
Test:  [ 10/261]  eta: 0:13:09  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.1454  data: 2.7063  max mem: 19734
Test:  [ 20/261]  eta: 0:07:35  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.4678  data: 0.0198  max mem: 19734
Test:  [ 30/261]  eta: 0:05:47  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.6046  data: 0.0207  max mem: 19734
Test:  [ 40/261]  eta: 0:04:50  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.7151  data: 0.1269  max mem: 19734
Test:  [ 50/261]  eta: 0:03:55  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5080  data: 0.1202  max mem: 19734
Test:  [ 60/261]  eta: 0:03:21  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3633  data: 0.0112  max mem: 19734
Test:  [ 70/261]  eta: 0:02:57  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.4577  data: 0.0126  max mem: 19734
Test:  [ 80/261]  eta: 0:02:36  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4504  data: 0.0158  max mem: 19734
Test:  [ 90/261]  eta: 0:02:17  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3574  data: 0.0175  max mem: 19734
Test:  [100/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4249  data: 0.0492  max mem: 19734
Test:  [110/261]  eta: 0:01:50  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4105  data: 0.0584  max mem: 19734
Test:  [120/261]  eta: 0:01:36  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2067  data: 0.0228  max mem: 19734
Test:  [130/261]  eta: 0:01:28  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3392  data: 0.1963  max mem: 19734
Test:  [140/261]  eta: 0:01:17  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4016  data: 0.1961  max mem: 19734
Test:  [150/261]  eta: 0:01:09  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.3505  data: 0.0875  max mem: 19734
Test:  [160/261]  eta: 0:01:01  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3660  data: 0.0902  max mem: 19734
Test:  [170/261]  eta: 0:00:53  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3064  data: 0.0604  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.4618  data: 0.2707  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.3781  data: 0.2174  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1377  data: 0.0014  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1356  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1361  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1359  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1312  data: 0.0001  max mem: 19734
Test: Total time: 0:01:58 (0.4541 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [29]  [   0/1251]  eta: 5:31:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 15.9194  data: 6.4601  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [  10/1251]  eta: 0:52:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.5398  data: 0.7039  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [  20/1251]  eta: 0:34:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.9936  data: 0.0645  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [  30/1251]  eta: 0:28:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [  40/1251]  eta: 0:25:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [  50/1251]  eta: 0:23:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7997  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [  60/1251]  eta: 0:22:04  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [  70/1251]  eta: 0:21:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8054  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [  80/1251]  eta: 0:20:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [  90/1251]  eta: 0:19:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8126  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 100/1251]  eta: 0:18:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 110/1251]  eta: 0:18:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 120/1251]  eta: 0:18:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 130/1251]  eta: 0:17:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 140/1251]  eta: 0:17:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 150/1251]  eta: 0:16:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 160/1251]  eta: 0:16:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 170/1251]  eta: 0:16:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 180/1251]  eta: 0:16:04  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 190/1251]  eta: 0:15:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8074  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 200/1251]  eta: 0:15:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 210/1251]  eta: 0:15:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 220/1251]  eta: 0:15:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8057  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 230/1251]  eta: 0:14:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 240/1251]  eta: 0:14:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8033  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 250/1251]  eta: 0:14:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8056  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 260/1251]  eta: 0:14:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 270/1251]  eta: 0:14:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 280/1251]  eta: 0:13:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 290/1251]  eta: 0:13:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 300/1251]  eta: 0:13:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 310/1251]  eta: 0:13:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 320/1251]  eta: 0:13:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7803  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 330/1251]  eta: 0:13:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 340/1251]  eta: 0:12:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8068  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 350/1251]  eta: 0:12:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 360/1251]  eta: 0:12:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 370/1251]  eta: 0:12:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 380/1251]  eta: 0:12:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8049  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 390/1251]  eta: 0:12:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 400/1251]  eta: 0:11:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 410/1251]  eta: 0:11:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 420/1251]  eta: 0:11:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 430/1251]  eta: 0:11:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 440/1251]  eta: 0:11:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 450/1251]  eta: 0:11:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 460/1251]  eta: 0:11:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 470/1251]  eta: 0:10:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 480/1251]  eta: 0:10:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 490/1251]  eta: 0:10:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8001  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 500/1251]  eta: 0:10:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8029  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 510/1251]  eta: 0:10:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 520/1251]  eta: 0:10:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 530/1251]  eta: 0:09:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8144  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 540/1251]  eta: 0:09:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8012  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 550/1251]  eta: 0:09:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 560/1251]  eta: 0:09:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7987  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 570/1251]  eta: 0:09:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 580/1251]  eta: 0:09:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 590/1251]  eta: 0:09:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 600/1251]  eta: 0:08:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 610/1251]  eta: 0:08:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 620/1251]  eta: 0:08:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 630/1251]  eta: 0:08:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 640/1251]  eta: 0:08:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7988  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 650/1251]  eta: 0:08:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 660/1251]  eta: 0:08:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 670/1251]  eta: 0:07:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8100  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 680/1251]  eta: 0:07:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8103  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 690/1251]  eta: 0:07:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 700/1251]  eta: 0:07:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 710/1251]  eta: 0:07:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 720/1251]  eta: 0:07:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 730/1251]  eta: 0:07:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7836  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 740/1251]  eta: 0:06:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 750/1251]  eta: 0:06:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 760/1251]  eta: 0:06:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 770/1251]  eta: 0:06:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 780/1251]  eta: 0:06:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8053  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 790/1251]  eta: 0:06:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 800/1251]  eta: 0:06:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 810/1251]  eta: 0:06:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 820/1251]  eta: 0:05:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8133  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 830/1251]  eta: 0:05:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8120  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 840/1251]  eta: 0:05:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 850/1251]  eta: 0:05:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7883  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 860/1251]  eta: 0:05:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 870/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 880/1251]  eta: 0:05:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 890/1251]  eta: 0:04:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 900/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 910/1251]  eta: 0:04:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 920/1251]  eta: 0:04:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 930/1251]  eta: 0:04:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 940/1251]  eta: 0:04:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8002  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 950/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8074  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 960/1251]  eta: 0:03:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 970/1251]  eta: 0:03:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8064  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 980/1251]  eta: 0:03:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8045  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [ 990/1251]  eta: 0:03:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1000/1251]  eta: 0:03:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1010/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [1020/1251]  eta: 0:03:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1030/1251]  eta: 0:02:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1040/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7806  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1050/1251]  eta: 0:02:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1060/1251]  eta: 0:02:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1070/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7942  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1080/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1090/1251]  eta: 0:02:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8016  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1100/1251]  eta: 0:02:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8092  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1110/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8196  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [1120/1251]  eta: 0:01:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8125  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1130/1251]  eta: 0:01:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1140/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1150/1251]  eta: 0:01:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1160/1251]  eta: 0:01:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1170/1251]  eta: 0:01:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1180/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1190/1251]  eta: 0:00:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7815  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1200/1251]  eta: 0:00:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7788  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1210/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7735  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [29]  [1220/1251]  eta: 0:00:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7790  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1230/1251]  eta: 0:00:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7779  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1240/1251]  eta: 0:00:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7758  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [29]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0002  max mem: 19734
Epoch: [29] Total time: 0:16:53 (0.8098 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:46:03  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 38.1743  data: 37.7449  max mem: 19734
Test:  [ 10/261]  eta: 0:15:53  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.7973  data: 3.4418  max mem: 19734
Test:  [ 20/261]  eta: 0:08:35  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3380  data: 0.0129  max mem: 19734
Test:  [ 30/261]  eta: 0:06:11  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4032  data: 0.0159  max mem: 19734
Test:  [ 40/261]  eta: 0:04:52  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4631  data: 0.0152  max mem: 19734
Test:  [ 50/261]  eta: 0:04:01  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4273  data: 0.0120  max mem: 19734
Test:  [ 60/261]  eta: 0:03:21  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3521  data: 0.0162  max mem: 19734
Test:  [ 70/261]  eta: 0:02:55  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3512  data: 0.0158  max mem: 19734
Test:  [ 80/261]  eta: 0:02:35  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4181  data: 0.0095  max mem: 19734
Test:  [ 90/261]  eta: 0:02:19  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4503  data: 0.0235  max mem: 19734
Test:  [100/261]  eta: 0:02:07  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.5254  data: 0.0992  max mem: 19734
Test:  [110/261]  eta: 0:01:52  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4218  data: 0.0875  max mem: 19734
Test:  [120/261]  eta: 0:01:43  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4539  data: 0.1358  max mem: 19734
Test:  [130/261]  eta: 0:01:34  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.5729  data: 0.2164  max mem: 19734
Test:  [140/261]  eta: 0:01:25  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.5074  data: 0.1272  max mem: 19734
Test:  [150/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4414  data: 0.1355  max mem: 19734
Test:  [160/261]  eta: 0:01:05  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2593  data: 0.0968  max mem: 19734
Test:  [170/261]  eta: 0:00:56  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1374  data: 0.0008  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1369  data: 0.0008  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1374  data: 0.0015  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1363  data: 0.0011  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1355  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1359  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1318  data: 0.0001  max mem: 19734
Test: Total time: 0:01:58 (0.4532 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [30]  [   0/1251]  eta: 6:40:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 19.1917  data: 5.9556  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [  10/1251]  eta: 0:54:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.6201  data: 0.6408  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [  20/1251]  eta: 0:35:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8753  data: 0.0549  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [  30/1251]  eta: 0:29:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [  40/1251]  eta: 0:25:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [  50/1251]  eta: 0:23:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [  60/1251]  eta: 0:22:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [  70/1251]  eta: 0:21:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [  80/1251]  eta: 0:20:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7971  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [  90/1251]  eta: 0:19:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 100/1251]  eta: 0:19:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7957  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 110/1251]  eta: 0:18:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7978  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 120/1251]  eta: 0:18:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 130/1251]  eta: 0:17:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7999  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 140/1251]  eta: 0:17:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 150/1251]  eta: 0:17:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8065  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 160/1251]  eta: 0:16:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8053  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 170/1251]  eta: 0:16:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 180/1251]  eta: 0:16:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 190/1251]  eta: 0:15:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 200/1251]  eta: 0:15:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 210/1251]  eta: 0:15:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 220/1251]  eta: 0:15:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 230/1251]  eta: 0:14:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 240/1251]  eta: 0:14:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 250/1251]  eta: 0:14:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 260/1251]  eta: 0:14:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 270/1251]  eta: 0:14:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8040  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 280/1251]  eta: 0:14:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 290/1251]  eta: 0:13:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8009  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 300/1251]  eta: 0:13:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8227  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 310/1251]  eta: 0:13:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8059  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 320/1251]  eta: 0:13:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 330/1251]  eta: 0:13:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7974  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 340/1251]  eta: 0:12:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 350/1251]  eta: 0:12:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 360/1251]  eta: 0:12:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 370/1251]  eta: 0:12:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 380/1251]  eta: 0:12:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 390/1251]  eta: 0:12:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 400/1251]  eta: 0:11:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 410/1251]  eta: 0:11:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8038  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 420/1251]  eta: 0:11:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8063  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 430/1251]  eta: 0:11:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 440/1251]  eta: 0:11:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8061  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 450/1251]  eta: 0:11:12  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8113  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 460/1251]  eta: 0:11:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8009  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 470/1251]  eta: 0:10:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7999  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 480/1251]  eta: 0:10:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 490/1251]  eta: 0:10:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 500/1251]  eta: 0:10:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 510/1251]  eta: 0:10:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 520/1251]  eta: 0:10:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 530/1251]  eta: 0:10:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 540/1251]  eta: 0:09:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 550/1251]  eta: 0:09:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8002  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 560/1251]  eta: 0:09:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 570/1251]  eta: 0:09:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7971  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 580/1251]  eta: 0:09:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7945  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 590/1251]  eta: 0:09:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8104  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 600/1251]  eta: 0:08:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8105  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 610/1251]  eta: 0:08:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7971  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 620/1251]  eta: 0:08:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 630/1251]  eta: 0:08:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 640/1251]  eta: 0:08:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 650/1251]  eta: 0:08:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 660/1251]  eta: 0:08:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 670/1251]  eta: 0:07:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 680/1251]  eta: 0:07:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7945  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 690/1251]  eta: 0:07:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 700/1251]  eta: 0:07:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 710/1251]  eta: 0:07:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8060  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 720/1251]  eta: 0:07:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7942  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 730/1251]  eta: 0:07:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 740/1251]  eta: 0:07:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8097  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 750/1251]  eta: 0:06:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8097  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 760/1251]  eta: 0:06:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 770/1251]  eta: 0:06:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 780/1251]  eta: 0:06:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 790/1251]  eta: 0:06:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7921  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 800/1251]  eta: 0:06:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 810/1251]  eta: 0:06:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 820/1251]  eta: 0:05:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 830/1251]  eta: 0:05:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8006  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 840/1251]  eta: 0:05:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 850/1251]  eta: 0:05:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 860/1251]  eta: 0:05:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8052  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 870/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 880/1251]  eta: 0:05:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 890/1251]  eta: 0:04:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 900/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 910/1251]  eta: 0:04:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 920/1251]  eta: 0:04:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 930/1251]  eta: 0:04:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 940/1251]  eta: 0:04:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7898  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 950/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7842  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 960/1251]  eta: 0:03:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7800  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 970/1251]  eta: 0:03:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 980/1251]  eta: 0:03:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7989  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [ 990/1251]  eta: 0:03:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1000/1251]  eta: 0:03:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1010/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1020/1251]  eta: 0:03:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1030/1251]  eta: 0:02:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8019  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1040/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8027  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1050/1251]  eta: 0:02:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1060/1251]  eta: 0:02:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1070/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7816  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1080/1251]  eta: 0:02:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1090/1251]  eta: 0:02:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1100/1251]  eta: 0:02:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1110/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7795  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1120/1251]  eta: 0:01:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1130/1251]  eta: 0:01:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8052  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1140/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8106  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1150/1251]  eta: 0:01:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1160/1251]  eta: 0:01:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1170/1251]  eta: 0:01:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1180/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8029  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1190/1251]  eta: 0:00:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1200/1251]  eta: 0:00:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7813  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1210/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7738  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1220/1251]  eta: 0:00:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7632  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1230/1251]  eta: 0:00:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7658  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1240/1251]  eta: 0:00:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7711  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [30]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7691  data: 0.0002  max mem: 19734
Epoch: [30] Total time: 0:16:53 (0.8099 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:51:35  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 25.6546  data: 24.7573  max mem: 19734
Test:  [ 10/261]  eta: 0:12:03  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.8836  data: 2.5866  max mem: 19734
Test:  [ 20/261]  eta: 0:06:40  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.4636  data: 0.1922  max mem: 19734
Test:  [ 30/261]  eta: 0:05:01  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4392  data: 0.1236  max mem: 19734
Test:  [ 40/261]  eta: 0:03:49  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3817  data: 0.1244  max mem: 19734
Test:  [ 50/261]  eta: 0:03:07  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.2455  data: 0.0140  max mem: 19734
Test:  [ 60/261]  eta: 0:02:40  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3048  data: 0.0133  max mem: 19734
Test:  [ 70/261]  eta: 0:02:15  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.2424  data: 0.0144  max mem: 19734
Test:  [ 80/261]  eta: 0:02:03  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3379  data: 0.0101  max mem: 19734
Test:  [ 90/261]  eta: 0:01:59  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.6552  data: 0.2300  max mem: 19734
Test:  [100/261]  eta: 0:01:47  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.6093  data: 0.2328  max mem: 19734
Test:  [110/261]  eta: 0:01:38  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4349  data: 0.0867  max mem: 19734
Test:  [120/261]  eta: 0:01:27  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3639  data: 0.1237  max mem: 19734
Test:  [130/261]  eta: 0:01:18  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3053  data: 0.0509  max mem: 19734
Test:  [140/261]  eta: 0:01:12  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4775  data: 0.1171  max mem: 19734
Test:  [150/261]  eta: 0:01:06  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.5856  data: 0.1139  max mem: 19734
Test:  [160/261]  eta: 0:00:58  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.4750  data: 0.0864  max mem: 19734
Test:  [170/261]  eta: 0:00:51  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3204  data: 0.0878  max mem: 19734
Test:  [180/261]  eta: 0:00:44  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.3258  data: 0.0131  max mem: 19734
Test:  [190/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.4610  data: 0.0131  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.5129  data: 0.0081  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.4059  data: 0.0353  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.2362  data: 0.0512  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1585  data: 0.0179  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1362  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1350  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1320  data: 0.0001  max mem: 19734
Test: Total time: 0:02:00 (0.4636 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [31]  [   0/1251]  eta: 5:38:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 16.2445  data: 13.7396  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [  10/1251]  eta: 0:53:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.5767  data: 1.2542  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [  20/1251]  eta: 0:35:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 1.0088  data: 0.0031  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [  30/1251]  eta: 0:29:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8236  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [  40/1251]  eta: 0:25:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8152  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [  50/1251]  eta: 0:23:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [  60/1251]  eta: 0:22:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8089  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [  70/1251]  eta: 0:21:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8137  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [  80/1251]  eta: 0:20:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [  90/1251]  eta: 0:19:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 100/1251]  eta: 0:19:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 110/1251]  eta: 0:18:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 120/1251]  eta: 0:18:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 130/1251]  eta: 0:17:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7898  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 140/1251]  eta: 0:17:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 150/1251]  eta: 0:17:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 160/1251]  eta: 0:16:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 170/1251]  eta: 0:16:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7957  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 180/1251]  eta: 0:16:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8003  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 190/1251]  eta: 0:15:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7966  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 200/1251]  eta: 0:15:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 210/1251]  eta: 0:15:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 220/1251]  eta: 0:15:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8044  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 230/1251]  eta: 0:15:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 240/1251]  eta: 0:14:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 250/1251]  eta: 0:14:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 260/1251]  eta: 0:14:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 270/1251]  eta: 0:14:12  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 280/1251]  eta: 0:14:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 290/1251]  eta: 0:13:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 300/1251]  eta: 0:13:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7962  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 310/1251]  eta: 0:13:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8010  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 320/1251]  eta: 0:13:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8133  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 330/1251]  eta: 0:13:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8110  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 340/1251]  eta: 0:12:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 350/1251]  eta: 0:12:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8058  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 360/1251]  eta: 0:12:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8100  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 370/1251]  eta: 0:12:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7979  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 380/1251]  eta: 0:12:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 390/1251]  eta: 0:12:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 400/1251]  eta: 0:12:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 410/1251]  eta: 0:11:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 420/1251]  eta: 0:11:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 430/1251]  eta: 0:11:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 440/1251]  eta: 0:11:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 450/1251]  eta: 0:11:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 460/1251]  eta: 0:11:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 470/1251]  eta: 0:10:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8088  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 480/1251]  eta: 0:10:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8081  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 490/1251]  eta: 0:10:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 500/1251]  eta: 0:10:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8071  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 510/1251]  eta: 0:10:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8031  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 520/1251]  eta: 0:10:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 530/1251]  eta: 0:10:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 540/1251]  eta: 0:09:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 550/1251]  eta: 0:09:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 560/1251]  eta: 0:09:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 570/1251]  eta: 0:09:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 580/1251]  eta: 0:09:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7861  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 590/1251]  eta: 0:09:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 600/1251]  eta: 0:08:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 610/1251]  eta: 0:08:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8038  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 620/1251]  eta: 0:08:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 630/1251]  eta: 0:08:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 640/1251]  eta: 0:08:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 650/1251]  eta: 0:08:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8026  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 660/1251]  eta: 0:08:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 670/1251]  eta: 0:07:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 680/1251]  eta: 0:07:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8004  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 690/1251]  eta: 0:07:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 700/1251]  eta: 0:07:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 710/1251]  eta: 0:07:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 720/1251]  eta: 0:07:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7957  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 730/1251]  eta: 0:07:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 740/1251]  eta: 0:07:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 750/1251]  eta: 0:06:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 760/1251]  eta: 0:06:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8082  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 770/1251]  eta: 0:06:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 780/1251]  eta: 0:06:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 790/1251]  eta: 0:06:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8148  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 800/1251]  eta: 0:06:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8123  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 810/1251]  eta: 0:06:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 820/1251]  eta: 0:05:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7945  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 830/1251]  eta: 0:05:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 840/1251]  eta: 0:05:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7806  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 850/1251]  eta: 0:05:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7797  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 860/1251]  eta: 0:05:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 870/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7982  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 880/1251]  eta: 0:05:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 890/1251]  eta: 0:04:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 900/1251]  eta: 0:04:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8046  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 910/1251]  eta: 0:04:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8055  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 920/1251]  eta: 0:04:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 930/1251]  eta: 0:04:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 940/1251]  eta: 0:04:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8125  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 950/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8087  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 960/1251]  eta: 0:03:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7816  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 970/1251]  eta: 0:03:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 980/1251]  eta: 0:03:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7946  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [ 990/1251]  eta: 0:03:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1000/1251]  eta: 0:03:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1010/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1020/1251]  eta: 0:03:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1030/1251]  eta: 0:03:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1040/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1050/1251]  eta: 0:02:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8104  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1060/1251]  eta: 0:02:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8094  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1070/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1080/1251]  eta: 0:02:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1090/1251]  eta: 0:02:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8107  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1100/1251]  eta: 0:02:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8050  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1110/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1120/1251]  eta: 0:01:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1130/1251]  eta: 0:01:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1140/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1150/1251]  eta: 0:01:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1160/1251]  eta: 0:01:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1170/1251]  eta: 0:01:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1180/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1190/1251]  eta: 0:00:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1200/1251]  eta: 0:00:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1210/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1220/1251]  eta: 0:00:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7676  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1230/1251]  eta: 0:00:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7777  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1240/1251]  eta: 0:00:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [31]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7772  data: 0.0001  max mem: 19734
Epoch: [31] Total time: 0:16:54 (0.8111 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:03:49  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 28.4669  data: 28.1739  max mem: 19734
Test:  [ 10/261]  eta: 0:11:46  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.8164  data: 2.5865  max mem: 19734
Test:  [ 20/261]  eta: 0:06:16  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2190  data: 0.0158  max mem: 19734
Test:  [ 30/261]  eta: 0:04:35  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2993  data: 0.0084  max mem: 19734
Test:  [ 40/261]  eta: 0:04:11  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6926  data: 0.3569  max mem: 19734
Test:  [ 50/261]  eta: 0:03:22  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5999  data: 0.3558  max mem: 19734
Test:  [ 60/261]  eta: 0:02:47  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2097  data: 0.0092  max mem: 19734
Test:  [ 70/261]  eta: 0:02:41  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5491  data: 0.2972  max mem: 19734
Test:  [ 80/261]  eta: 0:02:31  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.8547  data: 0.5315  max mem: 19734
Test:  [ 90/261]  eta: 0:02:13  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.5501  data: 0.2452  max mem: 19734
Test:  [100/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.5441  data: 0.2636  max mem: 19734
Test:  [110/261]  eta: 0:01:54  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.6558  data: 0.3940  max mem: 19734
Test:  [120/261]  eta: 0:01:41  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3958  data: 0.1453  max mem: 19734
Test:  [130/261]  eta: 0:01:29  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2721  data: 0.0154  max mem: 19734
Test:  [140/261]  eta: 0:01:18  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.2247  data: 0.0099  max mem: 19734
Test:  [150/261]  eta: 0:01:16  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.7044  data: 0.4463  max mem: 19734
Test:  [160/261]  eta: 0:01:06  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.7316  data: 0.4528  max mem: 19734
Test:  [170/261]  eta: 0:00:57  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2670  data: 0.0181  max mem: 19734
Test:  [180/261]  eta: 0:00:51  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.4289  data: 0.2058  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.3458  data: 0.1968  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1364  data: 0.0004  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1359  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1353  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1317  data: 0.0001  max mem: 19734
Test: Total time: 0:02:05 (0.4807 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [32]  [   0/1251]  eta: 5:41:12  lr: 0.000008  loss: 0.0000 (0.0000)  time: 16.3646  data: 7.2238  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [  10/1251]  eta: 0:51:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.5087  data: 0.7236  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [  20/1251]  eta: 0:34:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.9559  data: 0.0370  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [  30/1251]  eta: 0:28:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7985  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [  40/1251]  eta: 0:25:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7982  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [  50/1251]  eta: 0:23:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [  60/1251]  eta: 0:21:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [  70/1251]  eta: 0:20:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8054  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [  80/1251]  eta: 0:20:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8050  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [  90/1251]  eta: 0:19:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8037  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 100/1251]  eta: 0:18:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8073  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 110/1251]  eta: 0:18:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7962  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 120/1251]  eta: 0:17:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8053  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 130/1251]  eta: 0:17:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 140/1251]  eta: 0:17:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 150/1251]  eta: 0:16:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8079  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 160/1251]  eta: 0:16:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8057  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 170/1251]  eta: 0:16:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 180/1251]  eta: 0:16:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 190/1251]  eta: 0:15:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7979  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 200/1251]  eta: 0:15:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 210/1251]  eta: 0:15:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 220/1251]  eta: 0:15:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8009  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 230/1251]  eta: 0:14:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8031  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 240/1251]  eta: 0:14:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 250/1251]  eta: 0:14:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 260/1251]  eta: 0:14:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8074  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 270/1251]  eta: 0:14:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8061  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 280/1251]  eta: 0:13:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 290/1251]  eta: 0:13:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 300/1251]  eta: 0:13:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 310/1251]  eta: 0:13:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 320/1251]  eta: 0:13:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 330/1251]  eta: 0:13:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 340/1251]  eta: 0:12:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 350/1251]  eta: 0:12:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 360/1251]  eta: 0:12:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 370/1251]  eta: 0:12:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8008  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 380/1251]  eta: 0:12:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8005  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 390/1251]  eta: 0:12:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 400/1251]  eta: 0:11:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 410/1251]  eta: 0:11:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8166  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 420/1251]  eta: 0:11:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8141  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 430/1251]  eta: 0:11:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 440/1251]  eta: 0:11:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 450/1251]  eta: 0:11:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7966  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 460/1251]  eta: 0:11:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7929  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 470/1251]  eta: 0:10:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 480/1251]  eta: 0:10:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 490/1251]  eta: 0:10:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 500/1251]  eta: 0:10:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 510/1251]  eta: 0:10:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 520/1251]  eta: 0:10:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8044  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 530/1251]  eta: 0:10:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8073  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 540/1251]  eta: 0:09:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 550/1251]  eta: 0:09:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 560/1251]  eta: 0:09:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7994  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 570/1251]  eta: 0:09:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 580/1251]  eta: 0:09:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7869  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 590/1251]  eta: 0:09:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 600/1251]  eta: 0:08:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8006  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 610/1251]  eta: 0:08:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 620/1251]  eta: 0:08:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 630/1251]  eta: 0:08:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8016  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 640/1251]  eta: 0:08:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 650/1251]  eta: 0:08:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 660/1251]  eta: 0:08:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 670/1251]  eta: 0:07:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8034  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 680/1251]  eta: 0:07:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7999  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 690/1251]  eta: 0:07:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7816  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 700/1251]  eta: 0:07:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7979  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 710/1251]  eta: 0:07:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8039  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 720/1251]  eta: 0:07:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 730/1251]  eta: 0:07:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 740/1251]  eta: 0:06:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 750/1251]  eta: 0:06:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8035  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 760/1251]  eta: 0:06:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 770/1251]  eta: 0:06:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 780/1251]  eta: 0:06:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 790/1251]  eta: 0:06:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 800/1251]  eta: 0:06:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7974  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 810/1251]  eta: 0:06:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 820/1251]  eta: 0:05:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8048  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 830/1251]  eta: 0:05:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8064  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 840/1251]  eta: 0:05:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 850/1251]  eta: 0:05:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 860/1251]  eta: 0:05:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 870/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 880/1251]  eta: 0:05:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 890/1251]  eta: 0:04:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 900/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 910/1251]  eta: 0:04:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 920/1251]  eta: 0:04:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7851  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 930/1251]  eta: 0:04:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 940/1251]  eta: 0:04:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 950/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 960/1251]  eta: 0:03:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8013  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 970/1251]  eta: 0:03:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8050  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 980/1251]  eta: 0:03:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [ 990/1251]  eta: 0:03:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1000/1251]  eta: 0:03:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8014  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1010/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1020/1251]  eta: 0:03:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1030/1251]  eta: 0:02:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1040/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1050/1251]  eta: 0:02:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1060/1251]  eta: 0:02:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1070/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1080/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7883  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1090/1251]  eta: 0:02:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1100/1251]  eta: 0:02:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1110/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1120/1251]  eta: 0:01:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8033  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1130/1251]  eta: 0:01:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1140/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8065  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1150/1251]  eta: 0:01:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8018  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1160/1251]  eta: 0:01:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1170/1251]  eta: 0:01:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7911  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1180/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1190/1251]  eta: 0:00:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7898  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1200/1251]  eta: 0:00:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7780  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1210/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7789  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1220/1251]  eta: 0:00:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7730  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1230/1251]  eta: 0:00:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7680  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1240/1251]  eta: 0:00:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7770  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [32]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7769  data: 0.0001  max mem: 19734
Epoch: [32] Total time: 0:16:53 (0.8100 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:49:32  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 25.1825  data: 24.9631  max mem: 19734
Test:  [ 10/261]  eta: 0:13:55  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.3276  data: 2.9958  max mem: 19734
Test:  [ 20/261]  eta: 0:07:31  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.7099  data: 0.4095  max mem: 19734
Test:  [ 30/261]  eta: 0:05:15  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2880  data: 0.0170  max mem: 19734
Test:  [ 40/261]  eta: 0:04:29  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5272  data: 0.1797  max mem: 19734
Test:  [ 50/261]  eta: 0:03:38  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5266  data: 0.1790  max mem: 19734
Test:  [ 60/261]  eta: 0:03:02  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2684  data: 0.0145  max mem: 19734
Test:  [ 70/261]  eta: 0:02:34  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.2267  data: 0.0121  max mem: 19734
Test:  [ 80/261]  eta: 0:02:14  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.2376  data: 0.0107  max mem: 19734
Test:  [ 90/261]  eta: 0:02:00  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3242  data: 0.0134  max mem: 19734
Test:  [100/261]  eta: 0:02:00  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.7795  data: 0.4305  max mem: 19734
Test:  [110/261]  eta: 0:01:50  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.8819  data: 0.4321  max mem: 19734
Test:  [120/261]  eta: 0:01:38  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4446  data: 0.0122  max mem: 19734
Test:  [130/261]  eta: 0:01:34  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.6515  data: 0.2944  max mem: 19734
Test:  [140/261]  eta: 0:01:25  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.7575  data: 0.3045  max mem: 19734
Test:  [150/261]  eta: 0:01:16  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4866  data: 0.0229  max mem: 19734
Test:  [160/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3780  data: 0.0160  max mem: 19734
Test:  [170/261]  eta: 0:00:58  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2472  data: 0.0122  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1844  data: 0.0082  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1537  data: 0.0041  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1385  data: 0.0012  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1365  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1363  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1355  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1316  data: 0.0001  max mem: 19734
Test: Total time: 0:02:02 (0.4676 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [33]  [   0/1251]  eta: 4:49:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 13.8947  data: 3.8102  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [  10/1251]  eta: 0:51:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.4952  data: 0.8226  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [  20/1251]  eta: 0:34:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 1.0702  data: 0.2622  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [  30/1251]  eta: 0:28:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8182  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [  40/1251]  eta: 0:25:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8193  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [  50/1251]  eta: 0:23:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [  60/1251]  eta: 0:21:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [  70/1251]  eta: 0:21:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8113  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [  80/1251]  eta: 0:20:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8047  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [  90/1251]  eta: 0:19:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 100/1251]  eta: 0:18:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7925  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 110/1251]  eta: 0:18:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 120/1251]  eta: 0:17:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 130/1251]  eta: 0:17:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 140/1251]  eta: 0:17:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7982  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 150/1251]  eta: 0:16:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8091  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 160/1251]  eta: 0:16:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7998  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 170/1251]  eta: 0:16:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 180/1251]  eta: 0:16:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8068  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 190/1251]  eta: 0:15:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 200/1251]  eta: 0:15:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 210/1251]  eta: 0:15:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7895  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 220/1251]  eta: 0:15:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8152  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 230/1251]  eta: 0:14:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8114  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 240/1251]  eta: 0:14:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 250/1251]  eta: 0:14:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 260/1251]  eta: 0:14:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 270/1251]  eta: 0:14:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7966  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 280/1251]  eta: 0:13:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 290/1251]  eta: 0:13:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7987  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 300/1251]  eta: 0:13:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 310/1251]  eta: 0:13:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 320/1251]  eta: 0:13:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 330/1251]  eta: 0:13:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8015  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 340/1251]  eta: 0:12:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 350/1251]  eta: 0:12:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 360/1251]  eta: 0:12:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 370/1251]  eta: 0:12:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 380/1251]  eta: 0:12:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7931  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 390/1251]  eta: 0:12:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 400/1251]  eta: 0:11:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7830  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 410/1251]  eta: 0:11:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 420/1251]  eta: 0:11:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7956  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 430/1251]  eta: 0:11:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 440/1251]  eta: 0:11:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 450/1251]  eta: 0:11:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 460/1251]  eta: 0:11:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 470/1251]  eta: 0:10:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7973  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 480/1251]  eta: 0:10:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 490/1251]  eta: 0:10:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 500/1251]  eta: 0:10:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 510/1251]  eta: 0:10:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7884  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 520/1251]  eta: 0:10:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8012  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 530/1251]  eta: 0:09:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 540/1251]  eta: 0:09:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 550/1251]  eta: 0:09:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 560/1251]  eta: 0:09:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7972  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 570/1251]  eta: 0:09:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 580/1251]  eta: 0:09:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 590/1251]  eta: 0:09:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 600/1251]  eta: 0:08:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8009  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 610/1251]  eta: 0:08:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8089  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 620/1251]  eta: 0:08:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 630/1251]  eta: 0:08:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 640/1251]  eta: 0:08:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 650/1251]  eta: 0:08:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 660/1251]  eta: 0:08:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 670/1251]  eta: 0:07:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 680/1251]  eta: 0:07:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 690/1251]  eta: 0:07:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 700/1251]  eta: 0:07:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 710/1251]  eta: 0:07:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7991  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 720/1251]  eta: 0:07:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 730/1251]  eta: 0:07:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 740/1251]  eta: 0:06:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8026  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 750/1251]  eta: 0:06:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8061  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 760/1251]  eta: 0:06:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8094  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 770/1251]  eta: 0:06:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8043  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 780/1251]  eta: 0:06:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 790/1251]  eta: 0:06:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 800/1251]  eta: 0:06:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7971  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 810/1251]  eta: 0:06:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 820/1251]  eta: 0:05:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 830/1251]  eta: 0:05:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7907  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 840/1251]  eta: 0:05:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7863  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 850/1251]  eta: 0:05:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 860/1251]  eta: 0:05:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 870/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 880/1251]  eta: 0:05:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8006  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 890/1251]  eta: 0:04:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 900/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7985  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 910/1251]  eta: 0:04:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8120  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 920/1251]  eta: 0:04:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 930/1251]  eta: 0:04:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 940/1251]  eta: 0:04:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 950/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 960/1251]  eta: 0:03:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 970/1251]  eta: 0:03:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7887  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 980/1251]  eta: 0:03:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [ 990/1251]  eta: 0:03:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1000/1251]  eta: 0:03:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1010/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7946  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1020/1251]  eta: 0:03:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7979  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1030/1251]  eta: 0:02:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1040/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1050/1251]  eta: 0:02:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8094  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1060/1251]  eta: 0:02:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8077  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1070/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7838  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1080/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1090/1251]  eta: 0:02:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1100/1251]  eta: 0:02:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1110/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1120/1251]  eta: 0:01:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7932  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1130/1251]  eta: 0:01:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1140/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1150/1251]  eta: 0:01:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7952  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1160/1251]  eta: 0:01:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7966  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1170/1251]  eta: 0:01:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1180/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7962  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1190/1251]  eta: 0:00:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1200/1251]  eta: 0:00:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1210/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1220/1251]  eta: 0:00:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7667  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1230/1251]  eta: 0:00:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7685  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1240/1251]  eta: 0:00:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7758  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [33]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7735  data: 0.0002  max mem: 19734
Epoch: [33] Total time: 0:16:53 (0.8101 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 0:59:16  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 13.6266  data: 13.4921  max mem: 19734
Test:  [ 10/261]  eta: 0:10:26  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.4952  data: 2.1440  max mem: 19734
Test:  [ 20/261]  eta: 0:05:41  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.8084  data: 0.5092  max mem: 19734
Test:  [ 30/261]  eta: 0:04:01  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2485  data: 0.0086  max mem: 19734
Test:  [ 40/261]  eta: 0:03:26  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4286  data: 0.1897  max mem: 19734
Test:  [ 50/261]  eta: 0:02:52  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4669  data: 0.1893  max mem: 19734
Test:  [ 60/261]  eta: 0:02:26  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3068  data: 0.0088  max mem: 19734
Test:  [ 70/261]  eta: 0:02:31  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.7282  data: 0.4319  max mem: 19734
Test:  [ 80/261]  eta: 0:02:13  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.7589  data: 0.4600  max mem: 19734
Test:  [ 90/261]  eta: 0:01:56  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2787  data: 0.0372  max mem: 19734
Test:  [100/261]  eta: 0:01:47  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3948  data: 0.1331  max mem: 19734
Test:  [110/261]  eta: 0:01:42  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.6647  data: 0.2892  max mem: 19734
Test:  [120/261]  eta: 0:01:30  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.5004  data: 0.1703  max mem: 19734
Test:  [130/261]  eta: 0:01:20  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2512  data: 0.0185  max mem: 19734
Test:  [140/261]  eta: 0:01:13  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3967  data: 0.1168  max mem: 19734
Test:  [150/261]  eta: 0:01:04  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.3902  data: 0.1138  max mem: 19734
Test:  [160/261]  eta: 0:00:57  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2895  data: 0.0097  max mem: 19734
Test:  [170/261]  eta: 0:00:52  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.5588  data: 0.2477  max mem: 19734
Test:  [180/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.6380  data: 0.2500  max mem: 19734
Test:  [190/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.3751  data: 0.0170  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.3171  data: 0.1038  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.2699  data: 0.0937  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1551  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1397  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1362  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1358  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1320  data: 0.0001  max mem: 19734
Test: Total time: 0:01:59 (0.4565 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [34]  [   0/1251]  eta: 6:44:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 19.4148  data: 18.0877  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [  10/1251]  eta: 0:54:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.6294  data: 1.6449  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [  20/1251]  eta: 0:35:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8708  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [  30/1251]  eta: 0:29:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8004  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [  40/1251]  eta: 0:25:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7979  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [  50/1251]  eta: 0:23:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [  60/1251]  eta: 0:22:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8034  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [  70/1251]  eta: 0:21:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8047  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [  80/1251]  eta: 0:20:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [  90/1251]  eta: 0:19:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8149  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 100/1251]  eta: 0:19:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8119  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 110/1251]  eta: 0:18:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 120/1251]  eta: 0:18:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8038  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 130/1251]  eta: 0:17:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8026  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 140/1251]  eta: 0:17:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 150/1251]  eta: 0:17:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7986  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 160/1251]  eta: 0:16:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 170/1251]  eta: 0:16:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 180/1251]  eta: 0:16:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7979  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 190/1251]  eta: 0:15:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7983  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 200/1251]  eta: 0:15:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 210/1251]  eta: 0:15:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8092  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 220/1251]  eta: 0:15:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8048  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 230/1251]  eta: 0:15:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8038  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 240/1251]  eta: 0:14:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 250/1251]  eta: 0:14:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 260/1251]  eta: 0:14:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 270/1251]  eta: 0:14:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8026  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 280/1251]  eta: 0:14:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 290/1251]  eta: 0:13:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 300/1251]  eta: 0:13:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 310/1251]  eta: 0:13:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7814  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 320/1251]  eta: 0:13:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 330/1251]  eta: 0:13:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7984  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 340/1251]  eta: 0:13:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 350/1251]  eta: 0:12:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8041  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 360/1251]  eta: 0:12:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 370/1251]  eta: 0:12:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 380/1251]  eta: 0:12:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8083  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 390/1251]  eta: 0:12:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 400/1251]  eta: 0:12:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 410/1251]  eta: 0:11:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 420/1251]  eta: 0:11:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7978  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 430/1251]  eta: 0:11:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 440/1251]  eta: 0:11:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 450/1251]  eta: 0:11:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7855  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 460/1251]  eta: 0:11:04  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 470/1251]  eta: 0:10:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 480/1251]  eta: 0:10:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 490/1251]  eta: 0:10:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 500/1251]  eta: 0:10:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7958  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 510/1251]  eta: 0:10:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 520/1251]  eta: 0:10:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8016  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 530/1251]  eta: 0:10:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8073  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 540/1251]  eta: 0:09:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 550/1251]  eta: 0:09:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 560/1251]  eta: 0:09:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7982  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 570/1251]  eta: 0:09:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 580/1251]  eta: 0:09:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7818  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 590/1251]  eta: 0:09:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 600/1251]  eta: 0:08:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7875  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 610/1251]  eta: 0:08:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 620/1251]  eta: 0:08:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 630/1251]  eta: 0:08:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 640/1251]  eta: 0:08:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8058  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 650/1251]  eta: 0:08:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 660/1251]  eta: 0:08:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 670/1251]  eta: 0:07:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8053  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 680/1251]  eta: 0:07:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8046  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 690/1251]  eta: 0:07:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 700/1251]  eta: 0:07:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7841  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 710/1251]  eta: 0:07:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8012  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 720/1251]  eta: 0:07:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 730/1251]  eta: 0:07:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7800  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 740/1251]  eta: 0:07:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7859  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 750/1251]  eta: 0:06:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7903  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 760/1251]  eta: 0:06:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 770/1251]  eta: 0:06:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 780/1251]  eta: 0:06:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7881  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 790/1251]  eta: 0:06:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8004  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 800/1251]  eta: 0:06:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 810/1251]  eta: 0:06:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 820/1251]  eta: 0:05:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8069  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 830/1251]  eta: 0:05:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8026  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 840/1251]  eta: 0:05:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 850/1251]  eta: 0:05:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7823  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 860/1251]  eta: 0:05:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8016  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 870/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8018  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 880/1251]  eta: 0:05:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 890/1251]  eta: 0:04:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 900/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 910/1251]  eta: 0:04:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7893  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 920/1251]  eta: 0:04:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 930/1251]  eta: 0:04:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7923  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 940/1251]  eta: 0:04:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8040  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 950/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 960/1251]  eta: 0:03:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 970/1251]  eta: 0:03:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8038  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 980/1251]  eta: 0:03:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7937  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [ 990/1251]  eta: 0:03:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1000/1251]  eta: 0:03:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7934  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1010/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8042  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1020/1251]  eta: 0:03:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7941  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1030/1251]  eta: 0:02:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7812  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1040/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7789  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1050/1251]  eta: 0:02:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1060/1251]  eta: 0:02:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1070/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1080/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8020  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1090/1251]  eta: 0:02:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7977  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1100/1251]  eta: 0:02:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1110/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8015  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1120/1251]  eta: 0:01:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7990  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1130/1251]  eta: 0:01:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1140/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1150/1251]  eta: 0:01:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1160/1251]  eta: 0:01:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8010  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1170/1251]  eta: 0:01:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7936  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1180/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1190/1251]  eta: 0:00:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7798  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1200/1251]  eta: 0:00:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1210/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7775  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1220/1251]  eta: 0:00:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7715  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1230/1251]  eta: 0:00:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7781  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1240/1251]  eta: 0:00:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7759  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [34]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7708  data: 0.0002  max mem: 19734
Epoch: [34] Total time: 0:16:53 (0.8099 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:40:13  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 23.0409  data: 22.7377  max mem: 19734
Test:  [ 10/261]  eta: 0:11:17  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.6979  data: 2.4264  max mem: 19734
Test:  [ 20/261]  eta: 0:06:02  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.4266  data: 0.1987  max mem: 19734
Test:  [ 30/261]  eta: 0:04:07  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1774  data: 0.0075  max mem: 19734
Test:  [ 40/261]  eta: 0:03:39  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4584  data: 0.2755  max mem: 19734
Test:  [ 50/261]  eta: 0:02:57  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4844  data: 0.2721  max mem: 19734
Test:  [ 60/261]  eta: 0:02:32  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2750  data: 0.0081  max mem: 19734
Test:  [ 70/261]  eta: 0:02:22  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5002  data: 0.1935  max mem: 19734
Test:  [ 80/261]  eta: 0:02:03  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4486  data: 0.1930  max mem: 19734
Test:  [ 90/261]  eta: 0:01:48  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2497  data: 0.0112  max mem: 19734
Test:  [100/261]  eta: 0:01:54  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.8394  data: 0.6024  max mem: 19734
Test:  [110/261]  eta: 0:01:42  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.8612  data: 0.6157  max mem: 19734
Test:  [120/261]  eta: 0:01:31  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3124  data: 0.0307  max mem: 19734
Test:  [130/261]  eta: 0:01:20  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2841  data: 0.0183  max mem: 19734
Test:  [140/261]  eta: 0:01:12  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.2861  data: 0.0097  max mem: 19734
Test:  [150/261]  eta: 0:01:05  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.3849  data: 0.0902  max mem: 19734
Test:  [160/261]  eta: 0:00:57  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3770  data: 0.0923  max mem: 19734
Test:  [170/261]  eta: 0:00:54  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.6635  data: 0.3169  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.6768  data: 0.3514  max mem: 19734
Test:  [190/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.2515  data: 0.0456  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1920  data: 0.0300  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1718  data: 0.0233  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1361  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1358  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1357  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1315  data: 0.0001  max mem: 19734
Test: Total time: 0:01:57 (0.4490 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [35]  [   0/1251]  eta: 6:11:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 17.8130  data: 7.1627  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [  10/1251]  eta: 0:52:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.5182  data: 0.7442  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [  20/1251]  eta: 0:34:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8960  data: 0.0517  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [  30/1251]  eta: 0:28:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [  40/1251]  eta: 0:25:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8039  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [  50/1251]  eta: 0:23:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [  60/1251]  eta: 0:22:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [  70/1251]  eta: 0:20:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [  80/1251]  eta: 0:20:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [  90/1251]  eta: 0:19:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7955  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 100/1251]  eta: 0:18:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8065  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 110/1251]  eta: 0:18:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8092  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 120/1251]  eta: 0:18:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8002  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 130/1251]  eta: 0:17:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 140/1251]  eta: 0:17:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 150/1251]  eta: 0:16:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8042  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 160/1251]  eta: 0:16:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 170/1251]  eta: 0:16:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7917  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 180/1251]  eta: 0:16:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8023  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 190/1251]  eta: 0:15:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7989  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 200/1251]  eta: 0:15:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 210/1251]  eta: 0:15:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 220/1251]  eta: 0:15:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 230/1251]  eta: 0:14:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8079  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 240/1251]  eta: 0:14:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7995  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 250/1251]  eta: 0:14:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 260/1251]  eta: 0:14:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8072  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 270/1251]  eta: 0:14:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 280/1251]  eta: 0:14:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8034  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 290/1251]  eta: 0:13:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 300/1251]  eta: 0:13:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7972  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 310/1251]  eta: 0:13:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7964  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 320/1251]  eta: 0:13:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7852  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 330/1251]  eta: 0:13:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 340/1251]  eta: 0:12:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7965  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 350/1251]  eta: 0:12:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 360/1251]  eta: 0:12:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 370/1251]  eta: 0:12:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7847  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 380/1251]  eta: 0:12:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 390/1251]  eta: 0:12:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 400/1251]  eta: 0:11:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8005  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 410/1251]  eta: 0:11:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8052  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 420/1251]  eta: 0:11:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 430/1251]  eta: 0:11:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8005  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 440/1251]  eta: 0:11:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7987  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 450/1251]  eta: 0:11:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 460/1251]  eta: 0:11:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7943  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 470/1251]  eta: 0:10:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 480/1251]  eta: 0:10:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 490/1251]  eta: 0:10:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7915  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 500/1251]  eta: 0:10:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 510/1251]  eta: 0:10:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 520/1251]  eta: 0:10:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 530/1251]  eta: 0:09:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 540/1251]  eta: 0:09:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8046  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 550/1251]  eta: 0:09:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8045  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 560/1251]  eta: 0:09:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 570/1251]  eta: 0:09:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8002  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 580/1251]  eta: 0:09:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 590/1251]  eta: 0:09:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 600/1251]  eta: 0:08:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 610/1251]  eta: 0:08:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8066  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 620/1251]  eta: 0:08:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8092  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 630/1251]  eta: 0:08:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7944  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 640/1251]  eta: 0:08:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7890  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 650/1251]  eta: 0:08:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 660/1251]  eta: 0:08:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7801  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 670/1251]  eta: 0:07:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 680/1251]  eta: 0:07:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 690/1251]  eta: 0:07:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 700/1251]  eta: 0:07:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 710/1251]  eta: 0:07:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7909  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 720/1251]  eta: 0:07:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8004  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 730/1251]  eta: 0:07:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 740/1251]  eta: 0:06:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 750/1251]  eta: 0:06:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8019  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 760/1251]  eta: 0:06:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 770/1251]  eta: 0:06:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7900  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 780/1251]  eta: 0:06:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7831  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 790/1251]  eta: 0:06:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7864  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 800/1251]  eta: 0:06:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7856  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 810/1251]  eta: 0:06:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 820/1251]  eta: 0:05:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7940  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 830/1251]  eta: 0:05:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7899  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 840/1251]  eta: 0:05:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8001  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 850/1251]  eta: 0:05:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 860/1251]  eta: 0:05:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7993  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 870/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8032  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 880/1251]  eta: 0:05:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 890/1251]  eta: 0:04:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 900/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 910/1251]  eta: 0:04:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7981  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 920/1251]  eta: 0:04:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7878  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 930/1251]  eta: 0:04:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7858  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 940/1251]  eta: 0:04:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7873  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 950/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 960/1251]  eta: 0:03:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 970/1251]  eta: 0:03:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7935  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 980/1251]  eta: 0:03:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8129  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [ 990/1251]  eta: 0:03:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8117  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1000/1251]  eta: 0:03:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1010/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1020/1251]  eta: 0:03:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1030/1251]  eta: 0:02:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7910  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1040/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1050/1251]  eta: 0:02:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1060/1251]  eta: 0:02:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8000  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1070/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7906  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1080/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1090/1251]  eta: 0:02:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1100/1251]  eta: 0:02:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1110/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1120/1251]  eta: 0:01:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1130/1251]  eta: 0:01:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8112  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1140/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8100  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1150/1251]  eta: 0:01:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1160/1251]  eta: 0:01:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7979  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1170/1251]  eta: 0:01:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8053  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1180/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7970  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1190/1251]  eta: 0:00:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7806  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1200/1251]  eta: 0:00:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1210/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1220/1251]  eta: 0:00:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7689  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1230/1251]  eta: 0:00:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7664  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1240/1251]  eta: 0:00:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7704  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [35]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7808  data: 0.0001  max mem: 19734
Epoch: [35] Total time: 0:16:52 (0.8096 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:28:20  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 34.1025  data: 33.5927  max mem: 19734
Test:  [ 10/261]  eta: 0:14:34  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.4847  data: 3.1292  max mem: 19734
Test:  [ 20/261]  eta: 0:08:15  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.4558  data: 0.0511  max mem: 19734
Test:  [ 30/261]  eta: 0:05:53  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4580  data: 0.0190  max mem: 19734
Test:  [ 40/261]  eta: 0:04:59  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6167  data: 0.1754  max mem: 19734
Test:  [ 50/261]  eta: 0:04:10  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6520  data: 0.1707  max mem: 19734
Test:  [ 60/261]  eta: 0:03:30  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.4154  data: 0.0100  max mem: 19734
Test:  [ 70/261]  eta: 0:03:03  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3818  data: 0.0619  max mem: 19734
Test:  [ 80/261]  eta: 0:02:38  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3491  data: 0.0645  max mem: 19734
Test:  [ 90/261]  eta: 0:02:21  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3589  data: 0.0130  max mem: 19734
Test:  [100/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4033  data: 0.0119  max mem: 19734
Test:  [110/261]  eta: 0:01:54  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4575  data: 0.0138  max mem: 19734
Test:  [120/261]  eta: 0:01:43  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4971  data: 0.1083  max mem: 19734
Test:  [130/261]  eta: 0:01:30  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3211  data: 0.1045  max mem: 19734
Test:  [140/261]  eta: 0:01:23  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4134  data: 0.1854  max mem: 19734
Test:  [150/261]  eta: 0:01:13  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4699  data: 0.1884  max mem: 19734
Test:  [160/261]  eta: 0:01:05  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3366  data: 0.0144  max mem: 19734
Test:  [170/261]  eta: 0:00:58  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.4575  data: 0.1801  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.3656  data: 0.1753  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1582  data: 0.0061  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1427  data: 0.0040  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1374  data: 0.0014  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1361  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1359  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1356  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1358  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1317  data: 0.0001  max mem: 19734
Test: Total time: 0:02:02 (0.4685 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.28%
Loss is nan, stopping training this iteration.
Epoch: [36]  [   0/1251]  eta: 4:46:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 13.7521  data: 8.1704  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [  10/1251]  eta: 0:51:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.5020  data: 0.8318  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [  20/1251]  eta: 0:34:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 1.0865  data: 0.0492  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [  30/1251]  eta: 0:28:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [  40/1251]  eta: 0:25:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8012  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [  50/1251]  eta: 0:23:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8027  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [  60/1251]  eta: 0:21:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8033  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [  70/1251]  eta: 0:20:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8057  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [  80/1251]  eta: 0:20:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7930  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [  90/1251]  eta: 0:19:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 100/1251]  eta: 0:18:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 110/1251]  eta: 0:18:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 120/1251]  eta: 0:17:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7902  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 130/1251]  eta: 0:17:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7872  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 140/1251]  eta: 0:17:12  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 150/1251]  eta: 0:16:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7950  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 160/1251]  eta: 0:16:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 170/1251]  eta: 0:16:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8011  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 180/1251]  eta: 0:16:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8110  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 190/1251]  eta: 0:15:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8091  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 200/1251]  eta: 0:15:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 210/1251]  eta: 0:15:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7933  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 220/1251]  eta: 0:15:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7928  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 230/1251]  eta: 0:14:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8117  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 240/1251]  eta: 0:14:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8087  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 250/1251]  eta: 0:14:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 260/1251]  eta: 0:14:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7879  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 270/1251]  eta: 0:14:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 280/1251]  eta: 0:13:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7904  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 290/1251]  eta: 0:13:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7868  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 300/1251]  eta: 0:13:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 310/1251]  eta: 0:13:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8058  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 320/1251]  eta: 0:13:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 330/1251]  eta: 0:13:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 340/1251]  eta: 0:12:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8067  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 350/1251]  eta: 0:12:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7996  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 360/1251]  eta: 0:12:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8025  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 370/1251]  eta: 0:12:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7948  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 380/1251]  eta: 0:12:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7967  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 390/1251]  eta: 0:12:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 400/1251]  eta: 0:11:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 410/1251]  eta: 0:11:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 420/1251]  eta: 0:11:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7885  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 430/1251]  eta: 0:11:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7960  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 440/1251]  eta: 0:11:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 450/1251]  eta: 0:11:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7961  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 460/1251]  eta: 0:11:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8040  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 470/1251]  eta: 0:10:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 480/1251]  eta: 0:10:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8022  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 490/1251]  eta: 0:10:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8126  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 500/1251]  eta: 0:10:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7949  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 510/1251]  eta: 0:10:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7914  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 520/1251]  eta: 0:10:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7905  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 530/1251]  eta: 0:09:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7987  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 540/1251]  eta: 0:09:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 550/1251]  eta: 0:09:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 560/1251]  eta: 0:09:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 570/1251]  eta: 0:09:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 580/1251]  eta: 0:09:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7919  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 590/1251]  eta: 0:09:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 600/1251]  eta: 0:08:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8021  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 610/1251]  eta: 0:08:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8028  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 620/1251]  eta: 0:08:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 630/1251]  eta: 0:08:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7975  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 640/1251]  eta: 0:08:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8015  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 650/1251]  eta: 0:08:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8052  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 660/1251]  eta: 0:08:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 670/1251]  eta: 0:07:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7912  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 680/1251]  eta: 0:07:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 690/1251]  eta: 0:07:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 700/1251]  eta: 0:07:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 710/1251]  eta: 0:07:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 720/1251]  eta: 0:07:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 730/1251]  eta: 0:07:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 740/1251]  eta: 0:06:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 750/1251]  eta: 0:06:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8053  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 760/1251]  eta: 0:06:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8005  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [36]  [ 770/1251]  eta: 0:06:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.8071  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
