CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
| distributed init (rank 6): env://
| distributed init (rank 3): env://
| distributed init (rank 2): env://
| distributed init (rank 0): env://
| distributed init (rank 7): env://
| distributed init (rank 4): env://
| distributed init (rank 5): env://
| distributed init (rank 1): env://
Namespace(batch_size=128, epochs=100, bce_loss=False, update_freq=1, unscale_lr=False, model='vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.1, sched='cosine', lr=1e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='/cluster/work/cvl/guosun/shangye/pretrained/vim_t_midclstok_76p1acc.pth', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, cosub=False, finetune='', attn_only=False, data_path='/scratch/tmp.13698422.guosun/datasets/imagenet_full_size/', data_set='IMNET', inat_category='name', output_dir='/cluster/work/cvl/guosun/shangye/output/Vim_new/vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_300einit_100e_batch_size128_p0.8_lr0.00001_min_lr1e-6_decoder_pruning_loss3stage_weight0.1_mse_weight0.005_sort_keep_policy_pretrain_mae', log_dir=None, device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=32, pin_mem=True, distributed=True, world_size=8, dist_url='env://', if_amp=False, if_continue_inf=True, if_nan2num=False, if_random_cls_token_position=False, if_random_token_rank=False, base_rate=0.8, lr_scale=0.01, ratio_weight=2.0, pruning_weight=0.1, mse_weight=0.005, pretrained_mae_path='/cluster/work/cvl/guosun/shangye/output/pretrain_mae_vim/vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_mae_300e/checkpoint.pth', local_rank=0, rank=0, gpu=0, dist_backend='nccl')
Creating model: vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2
keep_rate [0.8, 0.6400000000000001, 0.5120000000000001]
VisionMambaPrunning(
  (token_merge_module): TPSModule()
  (score_predictor): ModuleList(
    (0-2): 3 x PredictorLG(
      (in_conv_local): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (in_conv_cls): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (out_conv): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=96, out_features=48, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=48, out_features=2, bias=True)
        (5): LogSoftmax(dim=-1)
      )
    )
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=192, out_features=1000, bias=True)
  (drop_path): Identity()
  (layers): ModuleList(
    (0-23): 24 x Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=192, out_features=768, bias=False)
        (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (act): SiLU()
        (x_proj): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj): Linear(in_features=12, out_features=384, bias=True)
        (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
        (out_proj): Linear(in_features=384, out_features=192, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): Identity()
    )
  )
  (decoder): MAE_Decoder(
    (decoder_embed): Linear(in_features=192, out_features=512, bias=True)
    (decoder_blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)
  )
  (norm_f): RMSNorm()
)
Weights of VisionMambaPrunning not initialized from pretrained model: ['score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'decoder.mask_token', 'decoder.decoder_pos_embed', 'decoder.decoder_embed.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_blocks.0.norm1.weight', 'decoder.decoder_blocks.0.norm1.bias', 'decoder.decoder_blocks.0.attn.qkv.weight', 'decoder.decoder_blocks.0.attn.qkv.bias', 'decoder.decoder_blocks.0.attn.proj.weight', 'decoder.decoder_blocks.0.attn.proj.bias', 'decoder.decoder_blocks.0.norm2.weight', 'decoder.decoder_blocks.0.norm2.bias', 'decoder.decoder_blocks.0.mlp.fc1.weight', 'decoder.decoder_blocks.0.mlp.fc1.bias', 'decoder.decoder_blocks.0.mlp.fc2.weight', 'decoder.decoder_blocks.0.mlp.fc2.bias', 'decoder.decoder_blocks.1.norm1.weight', 'decoder.decoder_blocks.1.norm1.bias', 'decoder.decoder_blocks.1.attn.qkv.weight', 'decoder.decoder_blocks.1.attn.qkv.bias', 'decoder.decoder_blocks.1.attn.proj.weight', 'decoder.decoder_blocks.1.attn.proj.bias', 'decoder.decoder_blocks.1.norm2.weight', 'decoder.decoder_blocks.1.norm2.bias', 'decoder.decoder_blocks.1.mlp.fc1.weight', 'decoder.decoder_blocks.1.mlp.fc1.bias', 'decoder.decoder_blocks.1.mlp.fc2.weight', 'decoder.decoder_blocks.1.mlp.fc2.bias', 'decoder.decoder_blocks.2.norm1.weight', 'decoder.decoder_blocks.2.norm1.bias', 'decoder.decoder_blocks.2.attn.qkv.weight', 'decoder.decoder_blocks.2.attn.qkv.bias', 'decoder.decoder_blocks.2.attn.proj.weight', 'decoder.decoder_blocks.2.attn.proj.bias', 'decoder.decoder_blocks.2.norm2.weight', 'decoder.decoder_blocks.2.norm2.bias', 'decoder.decoder_blocks.2.mlp.fc1.weight', 'decoder.decoder_blocks.2.mlp.fc1.bias', 'decoder.decoder_blocks.2.mlp.fc2.weight', 'decoder.decoder_blocks.2.mlp.fc2.bias', 'decoder.decoder_blocks.3.norm1.weight', 'decoder.decoder_blocks.3.norm1.bias', 'decoder.decoder_blocks.3.attn.qkv.weight', 'decoder.decoder_blocks.3.attn.qkv.bias', 'decoder.decoder_blocks.3.attn.proj.weight', 'decoder.decoder_blocks.3.attn.proj.bias', 'decoder.decoder_blocks.3.norm2.weight', 'decoder.decoder_blocks.3.norm2.bias', 'decoder.decoder_blocks.3.mlp.fc1.weight', 'decoder.decoder_blocks.3.mlp.fc1.bias', 'decoder.decoder_blocks.3.mlp.fc2.weight', 'decoder.decoder_blocks.3.mlp.fc2.bias', 'decoder.decoder_blocks.4.norm1.weight', 'decoder.decoder_blocks.4.norm1.bias', 'decoder.decoder_blocks.4.attn.qkv.weight', 'decoder.decoder_blocks.4.attn.qkv.bias', 'decoder.decoder_blocks.4.attn.proj.weight', 'decoder.decoder_blocks.4.attn.proj.bias', 'decoder.decoder_blocks.4.norm2.weight', 'decoder.decoder_blocks.4.norm2.bias', 'decoder.decoder_blocks.4.mlp.fc1.weight', 'decoder.decoder_blocks.4.mlp.fc1.bias', 'decoder.decoder_blocks.4.mlp.fc2.weight', 'decoder.decoder_blocks.4.mlp.fc2.bias', 'decoder.decoder_blocks.5.norm1.weight', 'decoder.decoder_blocks.5.norm1.bias', 'decoder.decoder_blocks.5.attn.qkv.weight', 'decoder.decoder_blocks.5.attn.qkv.bias', 'decoder.decoder_blocks.5.attn.proj.weight', 'decoder.decoder_blocks.5.attn.proj.bias', 'decoder.decoder_blocks.5.norm2.weight', 'decoder.decoder_blocks.5.norm2.bias', 'decoder.decoder_blocks.5.mlp.fc1.weight', 'decoder.decoder_blocks.5.mlp.fc1.bias', 'decoder.decoder_blocks.5.mlp.fc2.weight', 'decoder.decoder_blocks.5.mlp.fc2.bias', 'decoder.decoder_blocks.6.norm1.weight', 'decoder.decoder_blocks.6.norm1.bias', 'decoder.decoder_blocks.6.attn.qkv.weight', 'decoder.decoder_blocks.6.attn.qkv.bias', 'decoder.decoder_blocks.6.attn.proj.weight', 'decoder.decoder_blocks.6.attn.proj.bias', 'decoder.decoder_blocks.6.norm2.weight', 'decoder.decoder_blocks.6.norm2.bias', 'decoder.decoder_blocks.6.mlp.fc1.weight', 'decoder.decoder_blocks.6.mlp.fc1.bias', 'decoder.decoder_blocks.6.mlp.fc2.weight', 'decoder.decoder_blocks.6.mlp.fc2.bias', 'decoder.decoder_blocks.7.norm1.weight', 'decoder.decoder_blocks.7.norm1.bias', 'decoder.decoder_blocks.7.attn.qkv.weight', 'decoder.decoder_blocks.7.attn.qkv.bias', 'decoder.decoder_blocks.7.attn.proj.weight', 'decoder.decoder_blocks.7.attn.proj.bias', 'decoder.decoder_blocks.7.norm2.weight', 'decoder.decoder_blocks.7.norm2.bias', 'decoder.decoder_blocks.7.mlp.fc1.weight', 'decoder.decoder_blocks.7.mlp.fc1.bias', 'decoder.decoder_blocks.7.mlp.fc2.weight', 'decoder.decoder_blocks.7.mlp.fc2.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_pred.bias']
Weights of VisionMambaPrunning not initialized from pretrained model: ['cls_token', 'pos_embed', 'score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'head.weight', 'head.bias', 'layers.0.mixer.A_log', 'layers.0.mixer.D', 'layers.0.mixer.A_b_log', 'layers.0.mixer.D_b', 'layers.0.mixer.in_proj.weight', 'layers.0.mixer.conv1d.weight', 'layers.0.mixer.conv1d.bias', 'layers.0.mixer.x_proj.weight', 'layers.0.mixer.dt_proj.weight', 'layers.0.mixer.dt_proj.bias', 'layers.0.mixer.conv1d_b.weight', 'layers.0.mixer.conv1d_b.bias', 'layers.0.mixer.x_proj_b.weight', 'layers.0.mixer.dt_proj_b.weight', 'layers.0.mixer.dt_proj_b.bias', 'layers.0.mixer.out_proj.weight', 'layers.0.norm.weight', 'layers.1.mixer.A_log', 'layers.1.mixer.D', 'layers.1.mixer.A_b_log', 'layers.1.mixer.D_b', 'layers.1.mixer.in_proj.weight', 'layers.1.mixer.conv1d.weight', 'layers.1.mixer.conv1d.bias', 'layers.1.mixer.x_proj.weight', 'layers.1.mixer.dt_proj.weight', 'layers.1.mixer.dt_proj.bias', 'layers.1.mixer.conv1d_b.weight', 'layers.1.mixer.conv1d_b.bias', 'layers.1.mixer.x_proj_b.weight', 'layers.1.mixer.dt_proj_b.weight', 'layers.1.mixer.dt_proj_b.bias', 'layers.1.mixer.out_proj.weight', 'layers.1.norm.weight', 'layers.2.mixer.A_log', 'layers.2.mixer.D', 'layers.2.mixer.A_b_log', 'layers.2.mixer.D_b', 'layers.2.mixer.in_proj.weight', 'layers.2.mixer.conv1d.weight', 'layers.2.mixer.conv1d.bias', 'layers.2.mixer.x_proj.weight', 'layers.2.mixer.dt_proj.weight', 'layers.2.mixer.dt_proj.bias', 'layers.2.mixer.conv1d_b.weight', 'layers.2.mixer.conv1d_b.bias', 'layers.2.mixer.x_proj_b.weight', 'layers.2.mixer.dt_proj_b.weight', 'layers.2.mixer.dt_proj_b.bias', 'layers.2.mixer.out_proj.weight', 'layers.2.norm.weight', 'layers.3.mixer.A_log', 'layers.3.mixer.D', 'layers.3.mixer.A_b_log', 'layers.3.mixer.D_b', 'layers.3.mixer.in_proj.weight', 'layers.3.mixer.conv1d.weight', 'layers.3.mixer.conv1d.bias', 'layers.3.mixer.x_proj.weight', 'layers.3.mixer.dt_proj.weight', 'layers.3.mixer.dt_proj.bias', 'layers.3.mixer.conv1d_b.weight', 'layers.3.mixer.conv1d_b.bias', 'layers.3.mixer.x_proj_b.weight', 'layers.3.mixer.dt_proj_b.weight', 'layers.3.mixer.dt_proj_b.bias', 'layers.3.mixer.out_proj.weight', 'layers.3.norm.weight', 'layers.4.mixer.A_log', 'layers.4.mixer.D', 'layers.4.mixer.A_b_log', 'layers.4.mixer.D_b', 'layers.4.mixer.in_proj.weight', 'layers.4.mixer.conv1d.weight', 'layers.4.mixer.conv1d.bias', 'layers.4.mixer.x_proj.weight', 'layers.4.mixer.dt_proj.weight', 'layers.4.mixer.dt_proj.bias', 'layers.4.mixer.conv1d_b.weight', 'layers.4.mixer.conv1d_b.bias', 'layers.4.mixer.x_proj_b.weight', 'layers.4.mixer.dt_proj_b.weight', 'layers.4.mixer.dt_proj_b.bias', 'layers.4.mixer.out_proj.weight', 'layers.4.norm.weight', 'layers.5.mixer.A_log', 'layers.5.mixer.D', 'layers.5.mixer.A_b_log', 'layers.5.mixer.D_b', 'layers.5.mixer.in_proj.weight', 'layers.5.mixer.conv1d.weight', 'layers.5.mixer.conv1d.bias', 'layers.5.mixer.x_proj.weight', 'layers.5.mixer.dt_proj.weight', 'layers.5.mixer.dt_proj.bias', 'layers.5.mixer.conv1d_b.weight', 'layers.5.mixer.conv1d_b.bias', 'layers.5.mixer.x_proj_b.weight', 'layers.5.mixer.dt_proj_b.weight', 'layers.5.mixer.dt_proj_b.bias', 'layers.5.mixer.out_proj.weight', 'layers.5.norm.weight', 'layers.6.mixer.A_log', 'layers.6.mixer.D', 'layers.6.mixer.A_b_log', 'layers.6.mixer.D_b', 'layers.6.mixer.in_proj.weight', 'layers.6.mixer.conv1d.weight', 'layers.6.mixer.conv1d.bias', 'layers.6.mixer.x_proj.weight', 'layers.6.mixer.dt_proj.weight', 'layers.6.mixer.dt_proj.bias', 'layers.6.mixer.conv1d_b.weight', 'layers.6.mixer.conv1d_b.bias', 'layers.6.mixer.x_proj_b.weight', 'layers.6.mixer.dt_proj_b.weight', 'layers.6.mixer.dt_proj_b.bias', 'layers.6.mixer.out_proj.weight', 'layers.6.norm.weight', 'layers.7.mixer.A_log', 'layers.7.mixer.D', 'layers.7.mixer.A_b_log', 'layers.7.mixer.D_b', 'layers.7.mixer.in_proj.weight', 'layers.7.mixer.conv1d.weight', 'layers.7.mixer.conv1d.bias', 'layers.7.mixer.x_proj.weight', 'layers.7.mixer.dt_proj.weight', 'layers.7.mixer.dt_proj.bias', 'layers.7.mixer.conv1d_b.weight', 'layers.7.mixer.conv1d_b.bias', 'layers.7.mixer.x_proj_b.weight', 'layers.7.mixer.dt_proj_b.weight', 'layers.7.mixer.dt_proj_b.bias', 'layers.7.mixer.out_proj.weight', 'layers.7.norm.weight', 'layers.8.mixer.A_log', 'layers.8.mixer.D', 'layers.8.mixer.A_b_log', 'layers.8.mixer.D_b', 'layers.8.mixer.in_proj.weight', 'layers.8.mixer.conv1d.weight', 'layers.8.mixer.conv1d.bias', 'layers.8.mixer.x_proj.weight', 'layers.8.mixer.dt_proj.weight', 'layers.8.mixer.dt_proj.bias', 'layers.8.mixer.conv1d_b.weight', 'layers.8.mixer.conv1d_b.bias', 'layers.8.mixer.x_proj_b.weight', 'layers.8.mixer.dt_proj_b.weight', 'layers.8.mixer.dt_proj_b.bias', 'layers.8.mixer.out_proj.weight', 'layers.8.norm.weight', 'layers.9.mixer.A_log', 'layers.9.mixer.D', 'layers.9.mixer.A_b_log', 'layers.9.mixer.D_b', 'layers.9.mixer.in_proj.weight', 'layers.9.mixer.conv1d.weight', 'layers.9.mixer.conv1d.bias', 'layers.9.mixer.x_proj.weight', 'layers.9.mixer.dt_proj.weight', 'layers.9.mixer.dt_proj.bias', 'layers.9.mixer.conv1d_b.weight', 'layers.9.mixer.conv1d_b.bias', 'layers.9.mixer.x_proj_b.weight', 'layers.9.mixer.dt_proj_b.weight', 'layers.9.mixer.dt_proj_b.bias', 'layers.9.mixer.out_proj.weight', 'layers.9.norm.weight', 'layers.10.mixer.A_log', 'layers.10.mixer.D', 'layers.10.mixer.A_b_log', 'layers.10.mixer.D_b', 'layers.10.mixer.in_proj.weight', 'layers.10.mixer.conv1d.weight', 'layers.10.mixer.conv1d.bias', 'layers.10.mixer.x_proj.weight', 'layers.10.mixer.dt_proj.weight', 'layers.10.mixer.dt_proj.bias', 'layers.10.mixer.conv1d_b.weight', 'layers.10.mixer.conv1d_b.bias', 'layers.10.mixer.x_proj_b.weight', 'layers.10.mixer.dt_proj_b.weight', 'layers.10.mixer.dt_proj_b.bias', 'layers.10.mixer.out_proj.weight', 'layers.10.norm.weight', 'layers.11.mixer.A_log', 'layers.11.mixer.D', 'layers.11.mixer.A_b_log', 'layers.11.mixer.D_b', 'layers.11.mixer.in_proj.weight', 'layers.11.mixer.conv1d.weight', 'layers.11.mixer.conv1d.bias', 'layers.11.mixer.x_proj.weight', 'layers.11.mixer.dt_proj.weight', 'layers.11.mixer.dt_proj.bias', 'layers.11.mixer.conv1d_b.weight', 'layers.11.mixer.conv1d_b.bias', 'layers.11.mixer.x_proj_b.weight', 'layers.11.mixer.dt_proj_b.weight', 'layers.11.mixer.dt_proj_b.bias', 'layers.11.mixer.out_proj.weight', 'layers.11.norm.weight', 'layers.12.mixer.A_log', 'layers.12.mixer.D', 'layers.12.mixer.A_b_log', 'layers.12.mixer.D_b', 'layers.12.mixer.in_proj.weight', 'layers.12.mixer.conv1d.weight', 'layers.12.mixer.conv1d.bias', 'layers.12.mixer.x_proj.weight', 'layers.12.mixer.dt_proj.weight', 'layers.12.mixer.dt_proj.bias', 'layers.12.mixer.conv1d_b.weight', 'layers.12.mixer.conv1d_b.bias', 'layers.12.mixer.x_proj_b.weight', 'layers.12.mixer.dt_proj_b.weight', 'layers.12.mixer.dt_proj_b.bias', 'layers.12.mixer.out_proj.weight', 'layers.12.norm.weight', 'layers.13.mixer.A_log', 'layers.13.mixer.D', 'layers.13.mixer.A_b_log', 'layers.13.mixer.D_b', 'layers.13.mixer.in_proj.weight', 'layers.13.mixer.conv1d.weight', 'layers.13.mixer.conv1d.bias', 'layers.13.mixer.x_proj.weight', 'layers.13.mixer.dt_proj.weight', 'layers.13.mixer.dt_proj.bias', 'layers.13.mixer.conv1d_b.weight', 'layers.13.mixer.conv1d_b.bias', 'layers.13.mixer.x_proj_b.weight', 'layers.13.mixer.dt_proj_b.weight', 'layers.13.mixer.dt_proj_b.bias', 'layers.13.mixer.out_proj.weight', 'layers.13.norm.weight', 'layers.14.mixer.A_log', 'layers.14.mixer.D', 'layers.14.mixer.A_b_log', 'layers.14.mixer.D_b', 'layers.14.mixer.in_proj.weight', 'layers.14.mixer.conv1d.weight', 'layers.14.mixer.conv1d.bias', 'layers.14.mixer.x_proj.weight', 'layers.14.mixer.dt_proj.weight', 'layers.14.mixer.dt_proj.bias', 'layers.14.mixer.conv1d_b.weight', 'layers.14.mixer.conv1d_b.bias', 'layers.14.mixer.x_proj_b.weight', 'layers.14.mixer.dt_proj_b.weight', 'layers.14.mixer.dt_proj_b.bias', 'layers.14.mixer.out_proj.weight', 'layers.14.norm.weight', 'layers.15.mixer.A_log', 'layers.15.mixer.D', 'layers.15.mixer.A_b_log', 'layers.15.mixer.D_b', 'layers.15.mixer.in_proj.weight', 'layers.15.mixer.conv1d.weight', 'layers.15.mixer.conv1d.bias', 'layers.15.mixer.x_proj.weight', 'layers.15.mixer.dt_proj.weight', 'layers.15.mixer.dt_proj.bias', 'layers.15.mixer.conv1d_b.weight', 'layers.15.mixer.conv1d_b.bias', 'layers.15.mixer.x_proj_b.weight', 'layers.15.mixer.dt_proj_b.weight', 'layers.15.mixer.dt_proj_b.bias', 'layers.15.mixer.out_proj.weight', 'layers.15.norm.weight', 'layers.16.mixer.A_log', 'layers.16.mixer.D', 'layers.16.mixer.A_b_log', 'layers.16.mixer.D_b', 'layers.16.mixer.in_proj.weight', 'layers.16.mixer.conv1d.weight', 'layers.16.mixer.conv1d.bias', 'layers.16.mixer.x_proj.weight', 'layers.16.mixer.dt_proj.weight', 'layers.16.mixer.dt_proj.bias', 'layers.16.mixer.conv1d_b.weight', 'layers.16.mixer.conv1d_b.bias', 'layers.16.mixer.x_proj_b.weight', 'layers.16.mixer.dt_proj_b.weight', 'layers.16.mixer.dt_proj_b.bias', 'layers.16.mixer.out_proj.weight', 'layers.16.norm.weight', 'layers.17.mixer.A_log', 'layers.17.mixer.D', 'layers.17.mixer.A_b_log', 'layers.17.mixer.D_b', 'layers.17.mixer.in_proj.weight', 'layers.17.mixer.conv1d.weight', 'layers.17.mixer.conv1d.bias', 'layers.17.mixer.x_proj.weight', 'layers.17.mixer.dt_proj.weight', 'layers.17.mixer.dt_proj.bias', 'layers.17.mixer.conv1d_b.weight', 'layers.17.mixer.conv1d_b.bias', 'layers.17.mixer.x_proj_b.weight', 'layers.17.mixer.dt_proj_b.weight', 'layers.17.mixer.dt_proj_b.bias', 'layers.17.mixer.out_proj.weight', 'layers.17.norm.weight', 'layers.18.mixer.A_log', 'layers.18.mixer.D', 'layers.18.mixer.A_b_log', 'layers.18.mixer.D_b', 'layers.18.mixer.in_proj.weight', 'layers.18.mixer.conv1d.weight', 'layers.18.mixer.conv1d.bias', 'layers.18.mixer.x_proj.weight', 'layers.18.mixer.dt_proj.weight', 'layers.18.mixer.dt_proj.bias', 'layers.18.mixer.conv1d_b.weight', 'layers.18.mixer.conv1d_b.bias', 'layers.18.mixer.x_proj_b.weight', 'layers.18.mixer.dt_proj_b.weight', 'layers.18.mixer.dt_proj_b.bias', 'layers.18.mixer.out_proj.weight', 'layers.18.norm.weight', 'layers.19.mixer.A_log', 'layers.19.mixer.D', 'layers.19.mixer.A_b_log', 'layers.19.mixer.D_b', 'layers.19.mixer.in_proj.weight', 'layers.19.mixer.conv1d.weight', 'layers.19.mixer.conv1d.bias', 'layers.19.mixer.x_proj.weight', 'layers.19.mixer.dt_proj.weight', 'layers.19.mixer.dt_proj.bias', 'layers.19.mixer.conv1d_b.weight', 'layers.19.mixer.conv1d_b.bias', 'layers.19.mixer.x_proj_b.weight', 'layers.19.mixer.dt_proj_b.weight', 'layers.19.mixer.dt_proj_b.bias', 'layers.19.mixer.out_proj.weight', 'layers.19.norm.weight', 'layers.20.mixer.A_log', 'layers.20.mixer.D', 'layers.20.mixer.A_b_log', 'layers.20.mixer.D_b', 'layers.20.mixer.in_proj.weight', 'layers.20.mixer.conv1d.weight', 'layers.20.mixer.conv1d.bias', 'layers.20.mixer.x_proj.weight', 'layers.20.mixer.dt_proj.weight', 'layers.20.mixer.dt_proj.bias', 'layers.20.mixer.conv1d_b.weight', 'layers.20.mixer.conv1d_b.bias', 'layers.20.mixer.x_proj_b.weight', 'layers.20.mixer.dt_proj_b.weight', 'layers.20.mixer.dt_proj_b.bias', 'layers.20.mixer.out_proj.weight', 'layers.20.norm.weight', 'layers.21.mixer.A_log', 'layers.21.mixer.D', 'layers.21.mixer.A_b_log', 'layers.21.mixer.D_b', 'layers.21.mixer.in_proj.weight', 'layers.21.mixer.conv1d.weight', 'layers.21.mixer.conv1d.bias', 'layers.21.mixer.x_proj.weight', 'layers.21.mixer.dt_proj.weight', 'layers.21.mixer.dt_proj.bias', 'layers.21.mixer.conv1d_b.weight', 'layers.21.mixer.conv1d_b.bias', 'layers.21.mixer.x_proj_b.weight', 'layers.21.mixer.dt_proj_b.weight', 'layers.21.mixer.dt_proj_b.bias', 'layers.21.mixer.out_proj.weight', 'layers.21.norm.weight', 'layers.22.mixer.A_log', 'layers.22.mixer.D', 'layers.22.mixer.A_b_log', 'layers.22.mixer.D_b', 'layers.22.mixer.in_proj.weight', 'layers.22.mixer.conv1d.weight', 'layers.22.mixer.conv1d.bias', 'layers.22.mixer.x_proj.weight', 'layers.22.mixer.dt_proj.weight', 'layers.22.mixer.dt_proj.bias', 'layers.22.mixer.conv1d_b.weight', 'layers.22.mixer.conv1d_b.bias', 'layers.22.mixer.x_proj_b.weight', 'layers.22.mixer.dt_proj_b.weight', 'layers.22.mixer.dt_proj_b.bias', 'layers.22.mixer.out_proj.weight', 'layers.22.norm.weight', 'layers.23.mixer.A_log', 'layers.23.mixer.D', 'layers.23.mixer.A_b_log', 'layers.23.mixer.D_b', 'layers.23.mixer.in_proj.weight', 'layers.23.mixer.conv1d.weight', 'layers.23.mixer.conv1d.bias', 'layers.23.mixer.x_proj.weight', 'layers.23.mixer.dt_proj.weight', 'layers.23.mixer.dt_proj.bias', 'layers.23.mixer.conv1d_b.weight', 'layers.23.mixer.conv1d_b.bias', 'layers.23.mixer.x_proj_b.weight', 'layers.23.mixer.dt_proj_b.weight', 'layers.23.mixer.dt_proj_b.bias', 'layers.23.mixer.out_proj.weight', 'layers.23.norm.weight', 'norm_f.weight']
number of params: 33044734
ratio_weight= 2.0 reconstruction_weight 0.005 pruning_weight 0.1
Start training for 100 epochs
Epoch: [0]  [   0/1251]  eta: 11:26:36  lr: 0.000001  loss: 7.5813 (7.5813)  time: 32.9306  data: 16.6857  max mem: 19423
Epoch: [0]  [  10/1251]  eta: 1:17:53  lr: 0.000001  loss: 7.5231 (7.5044)  time: 3.7659  data: 1.5175  max mem: 19733
Epoch: [0]  [  20/1251]  eta: 0:48:15  lr: 0.000001  loss: 7.5025 (7.5107)  time: 0.8229  data: 0.0006  max mem: 19733
Epoch: [0]  [  30/1251]  eta: 0:37:34  lr: 0.000001  loss: 7.4803 (7.4891)  time: 0.7910  data: 0.0005  max mem: 19733
Epoch: [0]  [  40/1251]  eta: 0:32:10  lr: 0.000001  loss: 7.4030 (7.4683)  time: 0.7987  data: 0.0005  max mem: 19733
Epoch: [0]  [  50/1251]  eta: 0:28:45  lr: 0.000001  loss: 7.3825 (7.4428)  time: 0.8017  data: 0.0004  max mem: 19733
Epoch: [0]  [  60/1251]  eta: 0:26:31  lr: 0.000001  loss: 7.3399 (7.4256)  time: 0.8083  data: 0.0004  max mem: 19733
Epoch: [0]  [  70/1251]  eta: 0:24:48  lr: 0.000001  loss: 7.3127 (7.4033)  time: 0.8089  data: 0.0004  max mem: 19733
Epoch: [0]  [  80/1251]  eta: 0:23:27  lr: 0.000001  loss: 7.2331 (7.3815)  time: 0.7929  data: 0.0005  max mem: 19733
Epoch: [0]  [  90/1251]  eta: 0:22:23  lr: 0.000001  loss: 7.2259 (7.3666)  time: 0.7920  data: 0.0005  max mem: 19733
loss info: cls_loss=7.0541, ratio_loss=0.3934, pruning_loss=0.2690, mse_loss=1.2840
Epoch: [0]  [ 100/1251]  eta: 0:21:30  lr: 0.000001  loss: 7.1956 (7.3499)  time: 0.7928  data: 0.0005  max mem: 19733
Epoch: [0]  [ 110/1251]  eta: 0:20:45  lr: 0.000001  loss: 7.1707 (7.3310)  time: 0.7941  data: 0.0006  max mem: 19733
Epoch: [0]  [ 120/1251]  eta: 0:20:06  lr: 0.000001  loss: 7.1365 (7.3100)  time: 0.7940  data: 0.0006  max mem: 19733
Epoch: [0]  [ 130/1251]  eta: 0:19:32  lr: 0.000001  loss: 7.0050 (7.2883)  time: 0.7945  data: 0.0006  max mem: 19733
Epoch: [0]  [ 140/1251]  eta: 0:19:02  lr: 0.000001  loss: 6.9754 (7.2624)  time: 0.7946  data: 0.0006  max mem: 19733
Epoch: [0]  [ 150/1251]  eta: 0:18:35  lr: 0.000001  loss: 6.8901 (7.2346)  time: 0.7953  data: 0.0005  max mem: 19733
Epoch: [0]  [ 160/1251]  eta: 0:18:10  lr: 0.000001  loss: 6.8114 (7.2058)  time: 0.7962  data: 0.0006  max mem: 19733
Epoch: [0]  [ 170/1251]  eta: 0:17:47  lr: 0.000001  loss: 6.7843 (7.1772)  time: 0.7964  data: 0.0006  max mem: 19733
Epoch: [0]  [ 180/1251]  eta: 0:17:26  lr: 0.000001  loss: 6.6809 (7.1467)  time: 0.7961  data: 0.0004  max mem: 19733
Epoch: [0]  [ 190/1251]  eta: 0:17:07  lr: 0.000001  loss: 6.6313 (7.1147)  time: 0.8000  data: 0.0004  max mem: 19733
loss info: cls_loss=6.5247, ratio_loss=0.3879, pruning_loss=0.2687, mse_loss=1.3301
Epoch: [0]  [ 200/1251]  eta: 0:16:50  lr: 0.000001  loss: 6.5300 (7.0810)  time: 0.8183  data: 0.0004  max mem: 19733
Epoch: [0]  [ 210/1251]  eta: 0:16:32  lr: 0.000001  loss: 6.5114 (7.0522)  time: 0.8157  data: 0.0004  max mem: 19733
Epoch: [0]  [ 220/1251]  eta: 0:16:15  lr: 0.000001  loss: 6.5399 (7.0233)  time: 0.7980  data: 0.0004  max mem: 19733
Epoch: [0]  [ 230/1251]  eta: 0:16:00  lr: 0.000001  loss: 6.5399 (6.9932)  time: 0.8010  data: 0.0004  max mem: 19733
Epoch: [0]  [ 240/1251]  eta: 0:15:44  lr: 0.000001  loss: 6.3293 (6.9630)  time: 0.8037  data: 0.0004  max mem: 19733
Epoch: [0]  [ 250/1251]  eta: 0:15:29  lr: 0.000001  loss: 6.3293 (6.9293)  time: 0.7976  data: 0.0004  max mem: 19733
Epoch: [0]  [ 260/1251]  eta: 0:15:15  lr: 0.000001  loss: 6.0245 (6.8954)  time: 0.7964  data: 0.0004  max mem: 19733
Epoch: [0]  [ 270/1251]  eta: 0:15:01  lr: 0.000001  loss: 6.0057 (6.8614)  time: 0.7992  data: 0.0004  max mem: 19733
Epoch: [0]  [ 280/1251]  eta: 0:14:48  lr: 0.000001  loss: 6.3156 (6.8408)  time: 0.7997  data: 0.0004  max mem: 19733
Epoch: [0]  [ 290/1251]  eta: 0:14:35  lr: 0.000001  loss: 6.2988 (6.8034)  time: 0.8028  data: 0.0004  max mem: 19733
loss info: cls_loss=5.8621, ratio_loss=0.3832, pruning_loss=0.2768, mse_loss=1.3231
Epoch: [0]  [ 300/1251]  eta: 0:14:23  lr: 0.000001  loss: 5.6753 (6.7695)  time: 0.8006  data: 0.0003  max mem: 19733
Epoch: [0]  [ 310/1251]  eta: 0:14:10  lr: 0.000001  loss: 5.7975 (6.7464)  time: 0.7985  data: 0.0004  max mem: 19733
Epoch: [0]  [ 320/1251]  eta: 0:13:58  lr: 0.000001  loss: 6.4726 (6.7332)  time: 0.8007  data: 0.0004  max mem: 19733
Epoch: [0]  [ 330/1251]  eta: 0:13:47  lr: 0.000001  loss: 6.3701 (6.7094)  time: 0.8015  data: 0.0004  max mem: 19733
Epoch: [0]  [ 340/1251]  eta: 0:13:35  lr: 0.000001  loss: 6.0817 (6.6902)  time: 0.8045  data: 0.0004  max mem: 19733
Epoch: [0]  [ 350/1251]  eta: 0:13:24  lr: 0.000001  loss: 6.0817 (6.6678)  time: 0.8150  data: 0.0004  max mem: 19733
Epoch: [0]  [ 360/1251]  eta: 0:13:13  lr: 0.000001  loss: 6.0772 (6.6431)  time: 0.8106  data: 0.0004  max mem: 19733
Epoch: [0]  [ 370/1251]  eta: 0:13:02  lr: 0.000001  loss: 5.7619 (6.6198)  time: 0.7999  data: 0.0004  max mem: 19733
Epoch: [0]  [ 380/1251]  eta: 0:12:51  lr: 0.000001  loss: 5.7408 (6.5948)  time: 0.8005  data: 0.0004  max mem: 19733
Epoch: [0]  [ 390/1251]  eta: 0:12:40  lr: 0.000001  loss: 5.5828 (6.5680)  time: 0.8017  data: 0.0005  max mem: 19733
loss info: cls_loss=5.6116, ratio_loss=0.3795, pruning_loss=0.2739, mse_loss=1.2796
Epoch: [0]  [ 400/1251]  eta: 0:12:30  lr: 0.000001  loss: 5.8959 (6.5532)  time: 0.8024  data: 0.0006  max mem: 19733
Epoch: [0]  [ 410/1251]  eta: 0:12:19  lr: 0.000001  loss: 6.0496 (6.5406)  time: 0.8003  data: 0.0006  max mem: 19733
Epoch: [0]  [ 420/1251]  eta: 0:12:09  lr: 0.000001  loss: 6.0486 (6.5196)  time: 0.8001  data: 0.0005  max mem: 19733
Epoch: [0]  [ 430/1251]  eta: 0:11:59  lr: 0.000001  loss: 6.0486 (6.5065)  time: 0.8002  data: 0.0003  max mem: 19733
Epoch: [0]  [ 440/1251]  eta: 0:11:49  lr: 0.000001  loss: 5.9955 (6.4921)  time: 0.7988  data: 0.0004  max mem: 19733
Epoch: [0]  [ 450/1251]  eta: 0:11:39  lr: 0.000001  loss: 5.7258 (6.4718)  time: 0.8020  data: 0.0004  max mem: 19733
Epoch: [0]  [ 460/1251]  eta: 0:11:29  lr: 0.000001  loss: 5.4632 (6.4487)  time: 0.8059  data: 0.0004  max mem: 19733
Epoch: [0]  [ 470/1251]  eta: 0:11:19  lr: 0.000001  loss: 5.1800 (6.4206)  time: 0.8012  data: 0.0004  max mem: 19733
Epoch: [0]  [ 480/1251]  eta: 0:11:09  lr: 0.000001  loss: 5.3890 (6.4020)  time: 0.8021  data: 0.0004  max mem: 19733
Epoch: [0]  [ 490/1251]  eta: 0:10:59  lr: 0.000001  loss: 5.6331 (6.3922)  time: 0.8022  data: 0.0004  max mem: 19733
loss info: cls_loss=5.3738, ratio_loss=0.3741, pruning_loss=0.2778, mse_loss=1.2519
Epoch: [0]  [ 500/1251]  eta: 0:10:50  lr: 0.000001  loss: 5.6331 (6.3748)  time: 0.8187  data: 0.0004  max mem: 19733
Epoch: [0]  [ 510/1251]  eta: 0:10:41  lr: 0.000001  loss: 5.6609 (6.3648)  time: 0.8218  data: 0.0004  max mem: 19733
Epoch: [0]  [ 520/1251]  eta: 0:10:31  lr: 0.000001  loss: 5.8637 (6.3504)  time: 0.8013  data: 0.0004  max mem: 19733
Epoch: [0]  [ 530/1251]  eta: 0:10:22  lr: 0.000001  loss: 5.8076 (6.3298)  time: 0.7998  data: 0.0004  max mem: 19733
Epoch: [0]  [ 540/1251]  eta: 0:10:12  lr: 0.000001  loss: 5.8076 (6.3174)  time: 0.8010  data: 0.0004  max mem: 19733
Epoch: [0]  [ 550/1251]  eta: 0:10:03  lr: 0.000001  loss: 5.6774 (6.3024)  time: 0.8039  data: 0.0004  max mem: 19733
Epoch: [0]  [ 560/1251]  eta: 0:09:53  lr: 0.000001  loss: 5.8280 (6.2938)  time: 0.8032  data: 0.0004  max mem: 19733
Epoch: [0]  [ 570/1251]  eta: 0:09:44  lr: 0.000001  loss: 5.9536 (6.2846)  time: 0.8004  data: 0.0004  max mem: 19733
Epoch: [0]  [ 580/1251]  eta: 0:09:35  lr: 0.000001  loss: 5.8498 (6.2722)  time: 0.8010  data: 0.0004  max mem: 19733
Epoch: [0]  [ 590/1251]  eta: 0:09:26  lr: 0.000001  loss: 5.8852 (6.2620)  time: 0.8006  data: 0.0005  max mem: 19733
loss info: cls_loss=5.3513, ratio_loss=0.3711, pruning_loss=0.2784, mse_loss=1.2700
Epoch: [0]  [ 600/1251]  eta: 0:09:17  lr: 0.000001  loss: 5.8340 (6.2476)  time: 0.8006  data: 0.0005  max mem: 19733
Epoch: [0]  [ 610/1251]  eta: 0:09:07  lr: 0.000001  loss: 5.5841 (6.2360)  time: 0.7984  data: 0.0004  max mem: 19733
Epoch: [0]  [ 620/1251]  eta: 0:08:58  lr: 0.000001  loss: 5.5931 (6.2266)  time: 0.7972  data: 0.0006  max mem: 19733
Epoch: [0]  [ 630/1251]  eta: 0:08:49  lr: 0.000001  loss: 5.8021 (6.2156)  time: 0.8024  data: 0.0007  max mem: 19733
Epoch: [0]  [ 640/1251]  eta: 0:08:41  lr: 0.000001  loss: 5.2557 (6.2002)  time: 0.8314  data: 0.0005  max mem: 19733
Epoch: [0]  [ 650/1251]  eta: 0:08:32  lr: 0.000001  loss: 5.3674 (6.1894)  time: 0.8287  data: 0.0004  max mem: 19733
Epoch: [0]  [ 660/1251]  eta: 0:08:23  lr: 0.000001  loss: 5.2659 (6.1708)  time: 0.8015  data: 0.0004  max mem: 19733
Epoch: [0]  [ 670/1251]  eta: 0:08:14  lr: 0.000001  loss: 5.6043 (6.1648)  time: 0.8002  data: 0.0006  max mem: 19733
Epoch: [0]  [ 680/1251]  eta: 0:08:05  lr: 0.000001  loss: 5.7037 (6.1545)  time: 0.7974  data: 0.0006  max mem: 19733
Epoch: [0]  [ 690/1251]  eta: 0:07:56  lr: 0.000001  loss: 5.1456 (6.1394)  time: 0.7993  data: 0.0005  max mem: 19733
loss info: cls_loss=5.1033, ratio_loss=0.3675, pruning_loss=0.2841, mse_loss=1.2394
Epoch: [0]  [ 700/1251]  eta: 0:07:47  lr: 0.000001  loss: 5.1456 (6.1271)  time: 0.8003  data: 0.0005  max mem: 19733
Epoch: [0]  [ 710/1251]  eta: 0:07:38  lr: 0.000001  loss: 5.8021 (6.1151)  time: 0.7999  data: 0.0005  max mem: 19733
Epoch: [0]  [ 720/1251]  eta: 0:07:29  lr: 0.000001  loss: 5.6076 (6.1012)  time: 0.8002  data: 0.0006  max mem: 19733
Epoch: [0]  [ 730/1251]  eta: 0:07:21  lr: 0.000001  loss: 5.3093 (6.0917)  time: 0.7992  data: 0.0005  max mem: 19733
Epoch: [0]  [ 740/1251]  eta: 0:07:12  lr: 0.000001  loss: 5.6395 (6.0883)  time: 0.7997  data: 0.0004  max mem: 19733
Epoch: [0]  [ 750/1251]  eta: 0:07:03  lr: 0.000001  loss: 5.6953 (6.0761)  time: 0.7988  data: 0.0005  max mem: 19733
Epoch: [0]  [ 760/1251]  eta: 0:06:54  lr: 0.000001  loss: 5.6953 (6.0681)  time: 0.7997  data: 0.0005  max mem: 19733
Epoch: [0]  [ 770/1251]  eta: 0:06:46  lr: 0.000001  loss: 5.7056 (6.0591)  time: 0.8030  data: 0.0005  max mem: 19733
Epoch: [0]  [ 780/1251]  eta: 0:06:37  lr: 0.000001  loss: 5.4917 (6.0509)  time: 0.8099  data: 0.0005  max mem: 19733
Epoch: [0]  [ 790/1251]  eta: 0:06:28  lr: 0.000001  loss: 5.2618 (6.0406)  time: 0.8249  data: 0.0005  max mem: 19733
loss info: cls_loss=5.0965, ratio_loss=0.3646, pruning_loss=0.2837, mse_loss=1.2755
Epoch: [0]  [ 800/1251]  eta: 0:06:20  lr: 0.000001  loss: 5.3315 (6.0329)  time: 0.8170  data: 0.0005  max mem: 19733
Epoch: [0]  [ 810/1251]  eta: 0:06:11  lr: 0.000001  loss: 4.9518 (6.0158)  time: 0.7984  data: 0.0005  max mem: 19733
Epoch: [0]  [ 820/1251]  eta: 0:06:02  lr: 0.000001  loss: 4.9770 (6.0098)  time: 0.8034  data: 0.0006  max mem: 19733
Epoch: [0]  [ 830/1251]  eta: 0:05:54  lr: 0.000001  loss: 5.5745 (5.9968)  time: 0.8060  data: 0.0006  max mem: 19733
Epoch: [0]  [ 840/1251]  eta: 0:05:45  lr: 0.000001  loss: 4.7857 (5.9865)  time: 0.8025  data: 0.0006  max mem: 19733
Epoch: [0]  [ 850/1251]  eta: 0:05:37  lr: 0.000001  loss: 5.1622 (5.9765)  time: 0.8008  data: 0.0006  max mem: 19733
Epoch: [0]  [ 860/1251]  eta: 0:05:28  lr: 0.000001  loss: 5.5334 (5.9683)  time: 0.7970  data: 0.0006  max mem: 19733
Epoch: [0]  [ 870/1251]  eta: 0:05:19  lr: 0.000001  loss: 5.2430 (5.9616)  time: 0.7965  data: 0.0006  max mem: 19733
Epoch: [0]  [ 880/1251]  eta: 0:05:11  lr: 0.000001  loss: 5.2101 (5.9497)  time: 0.7968  data: 0.0006  max mem: 19733
Epoch: [0]  [ 890/1251]  eta: 0:05:02  lr: 0.000001  loss: 4.9834 (5.9394)  time: 0.7963  data: 0.0004  max mem: 19733
loss info: cls_loss=4.8550, ratio_loss=0.3606, pruning_loss=0.2898, mse_loss=1.2470
Epoch: [0]  [ 900/1251]  eta: 0:04:54  lr: 0.000001  loss: 5.3435 (5.9327)  time: 0.7993  data: 0.0004  max mem: 19733
Epoch: [0]  [ 910/1251]  eta: 0:04:45  lr: 0.000001  loss: 5.6122 (5.9264)  time: 0.8005  data: 0.0004  max mem: 19733
Epoch: [0]  [ 920/1251]  eta: 0:04:37  lr: 0.000001  loss: 5.6122 (5.9209)  time: 0.7991  data: 0.0005  max mem: 19733
Epoch: [0]  [ 930/1251]  eta: 0:04:28  lr: 0.000001  loss: 5.6005 (5.9181)  time: 0.8153  data: 0.0004  max mem: 19733
Epoch: [0]  [ 940/1251]  eta: 0:04:20  lr: 0.000001  loss: 5.5019 (5.9095)  time: 0.8293  data: 0.0004  max mem: 19733
Epoch: [0]  [ 950/1251]  eta: 0:04:11  lr: 0.000001  loss: 4.6845 (5.8949)  time: 0.8127  data: 0.0004  max mem: 19733
Epoch: [0]  [ 960/1251]  eta: 0:04:03  lr: 0.000001  loss: 4.9949 (5.8892)  time: 0.7988  data: 0.0004  max mem: 19733
Epoch: [0]  [ 970/1251]  eta: 0:03:54  lr: 0.000001  loss: 5.6207 (5.8841)  time: 0.7973  data: 0.0004  max mem: 19733
Epoch: [0]  [ 980/1251]  eta: 0:03:46  lr: 0.000001  loss: 5.4847 (5.8778)  time: 0.8048  data: 0.0004  max mem: 19733
Epoch: [0]  [ 990/1251]  eta: 0:03:38  lr: 0.000001  loss: 5.6103 (5.8717)  time: 0.8059  data: 0.0004  max mem: 19733
loss info: cls_loss=4.9895, ratio_loss=0.3568, pruning_loss=0.2862, mse_loss=1.2758
Epoch: [0]  [1000/1251]  eta: 0:03:29  lr: 0.000001  loss: 5.4665 (5.8648)  time: 0.7976  data: 0.0004  max mem: 19733
Epoch: [0]  [1010/1251]  eta: 0:03:21  lr: 0.000001  loss: 5.4036 (5.8607)  time: 0.8014  data: 0.0004  max mem: 19733
Epoch: [0]  [1020/1251]  eta: 0:03:12  lr: 0.000001  loss: 5.3655 (5.8531)  time: 0.8000  data: 0.0004  max mem: 19733
Epoch: [0]  [1030/1251]  eta: 0:03:04  lr: 0.000001  loss: 5.0815 (5.8427)  time: 0.7967  data: 0.0004  max mem: 19733
Epoch: [0]  [1040/1251]  eta: 0:02:55  lr: 0.000001  loss: 5.0815 (5.8386)  time: 0.7983  data: 0.0004  max mem: 19733
Epoch: [0]  [1050/1251]  eta: 0:02:47  lr: 0.000001  loss: 5.5336 (5.8318)  time: 0.8004  data: 0.0004  max mem: 19733
Epoch: [0]  [1060/1251]  eta: 0:02:39  lr: 0.000001  loss: 5.3904 (5.8278)  time: 0.7988  data: 0.0004  max mem: 19733
Epoch: [0]  [1070/1251]  eta: 0:02:30  lr: 0.000001  loss: 5.3534 (5.8208)  time: 0.7976  data: 0.0004  max mem: 19733
Epoch: [0]  [1080/1251]  eta: 0:02:22  lr: 0.000001  loss: 4.9998 (5.8147)  time: 0.8296  data: 0.0004  max mem: 19733
Epoch: [0]  [1090/1251]  eta: 0:02:14  lr: 0.000001  loss: 5.3453 (5.8086)  time: 0.8300  data: 0.0003  max mem: 19733
loss info: cls_loss=4.8982, ratio_loss=0.3527, pruning_loss=0.2874, mse_loss=1.2516
Epoch: [0]  [1100/1251]  eta: 0:02:05  lr: 0.000001  loss: 5.3453 (5.8022)  time: 0.8004  data: 0.0003  max mem: 19733
Epoch: [0]  [1110/1251]  eta: 0:01:57  lr: 0.000001  loss: 5.3986 (5.7975)  time: 0.8001  data: 0.0005  max mem: 19733
Epoch: [0]  [1120/1251]  eta: 0:01:48  lr: 0.000001  loss: 5.5570 (5.7942)  time: 0.8004  data: 0.0005  max mem: 19733
Epoch: [0]  [1130/1251]  eta: 0:01:40  lr: 0.000001  loss: 5.4571 (5.7903)  time: 0.8002  data: 0.0005  max mem: 19733
Epoch: [0]  [1140/1251]  eta: 0:01:32  lr: 0.000001  loss: 5.2594 (5.7829)  time: 0.8029  data: 0.0005  max mem: 19733
Epoch: [0]  [1150/1251]  eta: 0:01:23  lr: 0.000001  loss: 5.1188 (5.7778)  time: 0.8029  data: 0.0005  max mem: 19733
Epoch: [0]  [1160/1251]  eta: 0:01:15  lr: 0.000001  loss: 5.3915 (5.7721)  time: 0.7983  data: 0.0005  max mem: 19733
Epoch: [0]  [1170/1251]  eta: 0:01:07  lr: 0.000001  loss: 5.3915 (5.7671)  time: 0.7992  data: 0.0006  max mem: 19733
Epoch: [0]  [1180/1251]  eta: 0:00:58  lr: 0.000001  loss: 5.5679 (5.7652)  time: 0.8026  data: 0.0006  max mem: 19733
Epoch: [0]  [1190/1251]  eta: 0:00:50  lr: 0.000001  loss: 5.5679 (5.7620)  time: 0.8002  data: 0.0008  max mem: 19733
loss info: cls_loss=5.0151, ratio_loss=0.3487, pruning_loss=0.2824, mse_loss=1.2337
Epoch: [0]  [1200/1251]  eta: 0:00:42  lr: 0.000001  loss: 5.4146 (5.7586)  time: 0.7946  data: 0.0007  max mem: 19733
Epoch: [0]  [1210/1251]  eta: 0:00:34  lr: 0.000001  loss: 5.3151 (5.7528)  time: 0.7915  data: 0.0002  max mem: 19733
Epoch: [0]  [1220/1251]  eta: 0:00:25  lr: 0.000001  loss: 5.0747 (5.7468)  time: 0.7949  data: 0.0002  max mem: 19733
Epoch: [0]  [1230/1251]  eta: 0:00:17  lr: 0.000001  loss: 5.0747 (5.7417)  time: 0.8160  data: 0.0002  max mem: 19733
Epoch: [0]  [1240/1251]  eta: 0:00:09  lr: 0.000001  loss: 5.1198 (5.7357)  time: 0.8122  data: 0.0002  max mem: 19733
Epoch: [0]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 4.9772 (5.7285)  time: 0.7912  data: 0.0001  max mem: 19733
Epoch: [0] Total time: 0:17:17 (0.8292 s / it)
Averaged stats: lr: 0.000001  loss: 4.9772 (5.7304)
Test:  [  0/261]  eta: 1:53:50  loss: 1.5945 (1.5945)  acc1: 68.2292 (68.2292)  acc5: 88.0208 (88.0208)  time: 26.1709  data: 24.4603  max mem: 19733
Test:  [ 10/261]  eta: 0:13:19  loss: 1.5195 (1.5251)  acc1: 71.3542 (70.2178)  acc5: 89.5833 (89.2519)  time: 3.1841  data: 2.6891  max mem: 19733
Test:  [ 20/261]  eta: 0:07:41  loss: 1.6226 (1.6522)  acc1: 63.5417 (66.2946)  acc5: 88.0208 (87.6984)  time: 0.7005  data: 0.2662  max mem: 19733
Test:  [ 30/261]  eta: 0:05:26  loss: 1.6105 (1.5735)  acc1: 67.7083 (68.8172)  acc5: 86.4583 (88.1888)  time: 0.4408  data: 0.0187  max mem: 19733
Test:  [ 40/261]  eta: 0:04:37  loss: 1.4319 (1.5801)  acc1: 73.4375 (68.3308)  acc5: 89.5833 (88.2241)  time: 0.5659  data: 0.2288  max mem: 19733
Test:  [ 50/261]  eta: 0:03:45  loss: 1.7710 (1.6578)  acc1: 59.3750 (65.7782)  acc5: 84.8958 (87.1732)  time: 0.5289  data: 0.2293  max mem: 19733
Test:  [ 60/261]  eta: 0:03:11  loss: 1.9242 (1.6924)  acc1: 56.7708 (64.6004)  acc5: 84.3750 (86.8767)  time: 0.3333  data: 0.0206  max mem: 19733
Test:  [ 70/261]  eta: 0:02:47  loss: 1.9142 (1.7047)  acc1: 56.7708 (63.7911)  acc5: 85.9375 (86.9058)  time: 0.3942  data: 0.0230  max mem: 19733
Test:  [ 80/261]  eta: 0:02:30  loss: 1.6450 (1.6813)  acc1: 62.5000 (64.3840)  acc5: 88.0208 (87.3007)  time: 0.4542  data: 0.0242  max mem: 19733
Test:  [ 90/261]  eta: 0:02:16  loss: 1.5013 (1.6478)  acc1: 67.1875 (65.0240)  acc5: 90.1042 (87.7747)  time: 0.5243  data: 0.0214  max mem: 19733
Test:  [100/261]  eta: 0:02:01  loss: 1.3937 (1.6369)  acc1: 70.8333 (65.1867)  acc5: 89.5833 (87.8403)  time: 0.4462  data: 0.0154  max mem: 19733
Test:  [110/261]  eta: 0:01:50  loss: 1.5126 (1.6521)  acc1: 68.2292 (64.9916)  acc5: 87.5000 (87.4953)  time: 0.4326  data: 0.1082  max mem: 19733
Test:  [120/261]  eta: 0:01:39  loss: 1.9487 (1.6842)  acc1: 61.4583 (64.3552)  acc5: 80.7292 (86.8888)  time: 0.4701  data: 0.1051  max mem: 19733
Test:  [130/261]  eta: 0:01:29  loss: 2.0320 (1.7237)  acc1: 55.2083 (63.6450)  acc5: 79.6875 (86.2238)  time: 0.3835  data: 0.0109  max mem: 19733
Test:  [140/261]  eta: 0:01:20  loss: 2.0829 (1.7416)  acc1: 55.2083 (63.1538)  acc5: 80.7292 (85.9597)  time: 0.4158  data: 0.0663  max mem: 19733
Test:  [150/261]  eta: 0:01:12  loss: 1.8297 (1.7423)  acc1: 61.4583 (63.2830)  acc5: 83.3333 (85.7995)  time: 0.4471  data: 0.1260  max mem: 19733
Test:  [160/261]  eta: 0:01:03  loss: 1.7625 (1.7658)  acc1: 64.0625 (62.9723)  acc5: 82.8125 (85.3002)  time: 0.3467  data: 0.1377  max mem: 19733
Test:  [170/261]  eta: 0:00:54  loss: 2.1478 (1.7940)  acc1: 53.1250 (62.4178)  acc5: 77.0833 (84.8867)  time: 0.1985  data: 0.0722  max mem: 19733
Test:  [180/261]  eta: 0:00:46  loss: 2.0754 (1.8096)  acc1: 53.6458 (62.1144)  acc5: 78.6458 (84.6369)  time: 0.1181  data: 0.0017  max mem: 19733
Test:  [190/261]  eta: 0:00:38  loss: 2.0580 (1.8242)  acc1: 55.2083 (61.8046)  acc5: 81.2500 (84.4050)  time: 0.1298  data: 0.0040  max mem: 19733
Test:  [200/261]  eta: 0:00:32  loss: 2.0484 (1.8331)  acc1: 57.8125 (61.6553)  acc5: 81.2500 (84.2377)  time: 0.1424  data: 0.0040  max mem: 19733
Test:  [210/261]  eta: 0:00:25  loss: 1.9635 (1.8402)  acc1: 58.8542 (61.5423)  acc5: 81.7708 (84.0862)  time: 0.1472  data: 0.0004  max mem: 19733
Test:  [220/261]  eta: 0:00:20  loss: 2.1075 (1.8660)  acc1: 55.7292 (61.0011)  acc5: 79.6875 (83.6680)  time: 0.1338  data: 0.0002  max mem: 19733
Test:  [230/261]  eta: 0:00:14  loss: 2.1562 (1.8757)  acc1: 52.6042 (60.7436)  acc5: 77.0833 (83.4754)  time: 0.1144  data: 0.0001  max mem: 19733
Test:  [240/261]  eta: 0:00:09  loss: 1.9073 (1.8761)  acc1: 56.7708 (60.6328)  acc5: 81.2500 (83.4803)  time: 0.1148  data: 0.0001  max mem: 19733
Test:  [250/261]  eta: 0:00:04  loss: 1.8003 (1.8600)  acc1: 63.0208 (60.9811)  acc5: 84.8958 (83.7172)  time: 0.1151  data: 0.0001  max mem: 19733
Test:  [260/261]  eta: 0:00:00  loss: 1.4863 (1.8504)  acc1: 67.1875 (61.2040)  acc5: 90.6250 (83.9060)  time: 0.1123  data: 0.0001  max mem: 19733
Test: Total time: 0:01:53 (0.4352 s / it)
* Acc@1 61.205 Acc@5 83.906 loss 1.850
Accuracy of the network on the 50000 test images: 61.2%
Max accuracy: 61.20%
Epoch: [1]  [   0/1251]  eta: 5:36:47  lr: 0.000001  loss: 5.1209 (5.1209)  time: 16.1534  data: 14.6665  max mem: 19734
Epoch: [1]  [  10/1251]  eta: 0:47:42  lr: 0.000001  loss: 5.4118 (5.2553)  time: 2.3062  data: 1.3340  max mem: 19734
Epoch: [1]  [  20/1251]  eta: 0:32:29  lr: 0.000001  loss: 5.3483 (5.2201)  time: 0.8549  data: 0.0006  max mem: 19734
Epoch: [1]  [  30/1251]  eta: 0:26:59  lr: 0.000001  loss: 5.5425 (5.3153)  time: 0.7879  data: 0.0005  max mem: 19734
Epoch: [1]  [  40/1251]  eta: 0:24:08  lr: 0.000001  loss: 5.5042 (5.2775)  time: 0.7892  data: 0.0006  max mem: 19734
loss info: cls_loss=4.8741, ratio_loss=0.3440, pruning_loss=0.2830, mse_loss=1.2291
Epoch: [1]  [  50/1251]  eta: 0:22:21  lr: 0.000001  loss: 5.3516 (5.2619)  time: 0.7916  data: 0.0009  max mem: 19734
Epoch: [1]  [  60/1251]  eta: 0:21:09  lr: 0.000001  loss: 5.1552 (5.1982)  time: 0.8004  data: 0.0008  max mem: 19734
Epoch: [1]  [  70/1251]  eta: 0:20:14  lr: 0.000001  loss: 4.6533 (5.1003)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [1]  [  80/1251]  eta: 0:19:30  lr: 0.000001  loss: 4.9341 (5.1256)  time: 0.7944  data: 0.0005  max mem: 19734
Epoch: [1]  [  90/1251]  eta: 0:18:53  lr: 0.000001  loss: 5.5111 (5.1260)  time: 0.7922  data: 0.0005  max mem: 19734
Epoch: [1]  [ 100/1251]  eta: 0:18:23  lr: 0.000001  loss: 5.1940 (5.1401)  time: 0.7942  data: 0.0005  max mem: 19734
Epoch: [1]  [ 110/1251]  eta: 0:18:00  lr: 0.000001  loss: 5.3802 (5.1536)  time: 0.8146  data: 0.0006  max mem: 19734
Epoch: [1]  [ 120/1251]  eta: 0:17:38  lr: 0.000001  loss: 5.3936 (5.1644)  time: 0.8231  data: 0.0006  max mem: 19734
Epoch: [1]  [ 130/1251]  eta: 0:17:18  lr: 0.000001  loss: 5.3936 (5.1701)  time: 0.8083  data: 0.0007  max mem: 19734
Epoch: [1]  [ 140/1251]  eta: 0:16:58  lr: 0.000001  loss: 5.3614 (5.1538)  time: 0.7988  data: 0.0007  max mem: 19734
loss info: cls_loss=4.8347, ratio_loss=0.3387, pruning_loss=0.2822, mse_loss=1.2530
Epoch: [1]  [ 150/1251]  eta: 0:16:40  lr: 0.000001  loss: 5.1918 (5.1558)  time: 0.7962  data: 0.0005  max mem: 19734
Epoch: [1]  [ 160/1251]  eta: 0:16:24  lr: 0.000001  loss: 5.2137 (5.1621)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [1]  [ 170/1251]  eta: 0:16:08  lr: 0.000001  loss: 5.2115 (5.1589)  time: 0.7997  data: 0.0004  max mem: 19734
Epoch: [1]  [ 180/1251]  eta: 0:15:53  lr: 0.000001  loss: 5.1703 (5.1494)  time: 0.7973  data: 0.0005  max mem: 19734
Epoch: [1]  [ 190/1251]  eta: 0:15:39  lr: 0.000001  loss: 5.0347 (5.1377)  time: 0.7981  data: 0.0006  max mem: 19734
Epoch: [1]  [ 200/1251]  eta: 0:15:27  lr: 0.000001  loss: 5.2245 (5.1328)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [1]  [ 210/1251]  eta: 0:15:14  lr: 0.000001  loss: 5.3986 (5.1386)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [1]  [ 220/1251]  eta: 0:15:01  lr: 0.000001  loss: 5.2792 (5.1284)  time: 0.7974  data: 0.0004  max mem: 19734
Epoch: [1]  [ 230/1251]  eta: 0:14:49  lr: 0.000001  loss: 4.6128 (5.1036)  time: 0.7979  data: 0.0004  max mem: 19734
Epoch: [1]  [ 240/1251]  eta: 0:14:37  lr: 0.000001  loss: 4.5410 (5.0811)  time: 0.7971  data: 0.0005  max mem: 19734
loss info: cls_loss=4.6922, ratio_loss=0.3336, pruning_loss=0.2842, mse_loss=1.2230
Epoch: [1]  [ 250/1251]  eta: 0:14:26  lr: 0.000001  loss: 4.6779 (5.0698)  time: 0.7973  data: 0.0005  max mem: 19734
Epoch: [1]  [ 260/1251]  eta: 0:14:17  lr: 0.000001  loss: 5.0942 (5.0803)  time: 0.8376  data: 0.0005  max mem: 19734
Epoch: [1]  [ 270/1251]  eta: 0:14:06  lr: 0.000001  loss: 5.3465 (5.0846)  time: 0.8368  data: 0.0005  max mem: 19734
Epoch: [1]  [ 280/1251]  eta: 0:13:55  lr: 0.000001  loss: 5.2197 (5.0778)  time: 0.7977  data: 0.0005  max mem: 19734
Epoch: [1]  [ 290/1251]  eta: 0:13:45  lr: 0.000001  loss: 5.3100 (5.0781)  time: 0.7986  data: 0.0008  max mem: 19734
Epoch: [1]  [ 300/1251]  eta: 0:13:34  lr: 0.000001  loss: 4.9720 (5.0622)  time: 0.8010  data: 0.0008  max mem: 19734
Epoch: [1]  [ 310/1251]  eta: 0:13:24  lr: 0.000001  loss: 4.9720 (5.0551)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [1]  [ 320/1251]  eta: 0:13:14  lr: 0.000001  loss: 4.8052 (5.0463)  time: 0.8025  data: 0.0004  max mem: 19734
Epoch: [1]  [ 330/1251]  eta: 0:13:04  lr: 0.000001  loss: 4.8052 (5.0391)  time: 0.8049  data: 0.0004  max mem: 19734
Epoch: [1]  [ 340/1251]  eta: 0:12:55  lr: 0.000001  loss: 5.2027 (5.0441)  time: 0.8059  data: 0.0004  max mem: 19734
loss info: cls_loss=4.7479, ratio_loss=0.3281, pruning_loss=0.2837, mse_loss=1.2444
Epoch: [1]  [ 350/1251]  eta: 0:12:45  lr: 0.000001  loss: 5.3195 (5.0520)  time: 0.8059  data: 0.0004  max mem: 19734
Epoch: [1]  [ 360/1251]  eta: 0:12:35  lr: 0.000001  loss: 5.3547 (5.0533)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [1]  [ 370/1251]  eta: 0:12:25  lr: 0.000001  loss: 5.0592 (5.0474)  time: 0.7978  data: 0.0005  max mem: 19734
Epoch: [1]  [ 380/1251]  eta: 0:12:16  lr: 0.000001  loss: 5.1097 (5.0496)  time: 0.7964  data: 0.0005  max mem: 19734
Epoch: [1]  [ 390/1251]  eta: 0:12:06  lr: 0.000001  loss: 5.1109 (5.0469)  time: 0.7974  data: 0.0005  max mem: 19734
Epoch: [1]  [ 400/1251]  eta: 0:11:57  lr: 0.000001  loss: 4.8164 (5.0261)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [1]  [ 410/1251]  eta: 0:11:48  lr: 0.000001  loss: 4.3389 (5.0146)  time: 0.8210  data: 0.0004  max mem: 19734
Epoch: [1]  [ 420/1251]  eta: 0:11:39  lr: 0.000001  loss: 4.5561 (5.0089)  time: 0.8137  data: 0.0004  max mem: 19734
Epoch: [1]  [ 430/1251]  eta: 0:11:30  lr: 0.000001  loss: 5.2188 (5.0086)  time: 0.7978  data: 0.0005  max mem: 19734
Epoch: [1]  [ 440/1251]  eta: 0:11:21  lr: 0.000001  loss: 5.3396 (5.0109)  time: 0.8005  data: 0.0005  max mem: 19734
loss info: cls_loss=4.6368, ratio_loss=0.3238, pruning_loss=0.2831, mse_loss=1.2159
Epoch: [1]  [ 450/1251]  eta: 0:11:12  lr: 0.000001  loss: 5.3396 (5.0164)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [1]  [ 460/1251]  eta: 0:11:03  lr: 0.000001  loss: 5.1652 (5.0110)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [1]  [ 470/1251]  eta: 0:10:54  lr: 0.000001  loss: 4.7320 (5.0060)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [1]  [ 480/1251]  eta: 0:10:45  lr: 0.000001  loss: 5.3591 (5.0114)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [1]  [ 490/1251]  eta: 0:10:36  lr: 0.000001  loss: 5.3571 (5.0064)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [1]  [ 500/1251]  eta: 0:10:27  lr: 0.000001  loss: 4.5299 (4.9992)  time: 0.8152  data: 0.0004  max mem: 19734
Epoch: [1]  [ 510/1251]  eta: 0:10:19  lr: 0.000001  loss: 4.6994 (4.9969)  time: 0.8093  data: 0.0005  max mem: 19734
Epoch: [1]  [ 520/1251]  eta: 0:10:10  lr: 0.000001  loss: 4.9504 (4.9955)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [1]  [ 530/1251]  eta: 0:10:01  lr: 0.000001  loss: 5.2663 (4.9986)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [1]  [ 540/1251]  eta: 0:09:52  lr: 0.000001  loss: 5.1127 (4.9945)  time: 0.8053  data: 0.0005  max mem: 19734
loss info: cls_loss=4.6791, ratio_loss=0.3170, pruning_loss=0.2818, mse_loss=1.2353
Epoch: [1]  [ 550/1251]  eta: 0:09:44  lr: 0.000001  loss: 5.0363 (4.9971)  time: 0.8370  data: 0.0005  max mem: 19734
Epoch: [1]  [ 560/1251]  eta: 0:09:36  lr: 0.000001  loss: 5.0434 (4.9938)  time: 0.8312  data: 0.0004  max mem: 19734
Epoch: [1]  [ 570/1251]  eta: 0:09:27  lr: 0.000001  loss: 4.7979 (4.9913)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [1]  [ 580/1251]  eta: 0:09:18  lr: 0.000001  loss: 5.1907 (4.9955)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [1]  [ 590/1251]  eta: 0:09:09  lr: 0.000001  loss: 5.1706 (4.9905)  time: 0.8015  data: 0.0004  max mem: 19734
Epoch: [1]  [ 600/1251]  eta: 0:09:01  lr: 0.000001  loss: 4.6971 (4.9843)  time: 0.7993  data: 0.0004  max mem: 19734
Epoch: [1]  [ 610/1251]  eta: 0:08:52  lr: 0.000001  loss: 4.7073 (4.9828)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [1]  [ 620/1251]  eta: 0:08:43  lr: 0.000001  loss: 4.7073 (4.9757)  time: 0.8014  data: 0.0006  max mem: 19734
Epoch: [1]  [ 630/1251]  eta: 0:08:35  lr: 0.000001  loss: 5.1319 (4.9811)  time: 0.8014  data: 0.0006  max mem: 19734
Epoch: [1]  [ 640/1251]  eta: 0:08:26  lr: 0.000001  loss: 5.1142 (4.9728)  time: 0.8008  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5733, ratio_loss=0.3097, pruning_loss=0.2813, mse_loss=1.2379
Epoch: [1]  [ 650/1251]  eta: 0:08:18  lr: 0.000001  loss: 4.7310 (4.9706)  time: 0.8049  data: 0.0004  max mem: 19734
Epoch: [1]  [ 660/1251]  eta: 0:08:09  lr: 0.000001  loss: 5.0149 (4.9709)  time: 0.8058  data: 0.0004  max mem: 19734
Epoch: [1]  [ 670/1251]  eta: 0:08:01  lr: 0.000001  loss: 4.6642 (4.9610)  time: 0.8021  data: 0.0006  max mem: 19734
Epoch: [1]  [ 680/1251]  eta: 0:07:52  lr: 0.000001  loss: 4.4536 (4.9556)  time: 0.8016  data: 0.0006  max mem: 19734
Epoch: [1]  [ 690/1251]  eta: 0:07:44  lr: 0.000001  loss: 4.5985 (4.9519)  time: 0.8109  data: 0.0006  max mem: 19734
Epoch: [1]  [ 700/1251]  eta: 0:07:36  lr: 0.000001  loss: 4.8030 (4.9473)  time: 0.8315  data: 0.0005  max mem: 19734
Epoch: [1]  [ 710/1251]  eta: 0:07:27  lr: 0.000001  loss: 4.8797 (4.9439)  time: 0.8240  data: 0.0005  max mem: 19734
Epoch: [1]  [ 720/1251]  eta: 0:07:19  lr: 0.000001  loss: 4.7381 (4.9368)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [1]  [ 730/1251]  eta: 0:07:10  lr: 0.000001  loss: 4.3674 (4.9320)  time: 0.7993  data: 0.0006  max mem: 19734
Epoch: [1]  [ 740/1251]  eta: 0:07:02  lr: 0.000001  loss: 4.7454 (4.9295)  time: 0.8066  data: 0.0005  max mem: 19734
loss info: cls_loss=4.4059, ratio_loss=0.3037, pruning_loss=0.2905, mse_loss=1.2500
Epoch: [1]  [ 750/1251]  eta: 0:06:54  lr: 0.000001  loss: 4.8623 (4.9255)  time: 0.8083  data: 0.0004  max mem: 19734
Epoch: [1]  [ 760/1251]  eta: 0:06:45  lr: 0.000001  loss: 4.9953 (4.9303)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [1]  [ 770/1251]  eta: 0:06:37  lr: 0.000001  loss: 5.1303 (4.9276)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [1]  [ 780/1251]  eta: 0:06:28  lr: 0.000001  loss: 5.1554 (4.9292)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [1]  [ 790/1251]  eta: 0:06:20  lr: 0.000001  loss: 5.2261 (4.9281)  time: 0.8088  data: 0.0005  max mem: 19734
Epoch: [1]  [ 800/1251]  eta: 0:06:12  lr: 0.000001  loss: 4.7974 (4.9251)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [1]  [ 810/1251]  eta: 0:06:03  lr: 0.000001  loss: 4.6000 (4.9193)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [1]  [ 820/1251]  eta: 0:05:55  lr: 0.000001  loss: 4.7531 (4.9213)  time: 0.7971  data: 0.0005  max mem: 19734
Epoch: [1]  [ 830/1251]  eta: 0:05:46  lr: 0.000001  loss: 4.8021 (4.9163)  time: 0.7984  data: 0.0005  max mem: 19734
Epoch: [1]  [ 840/1251]  eta: 0:05:38  lr: 0.000001  loss: 4.8374 (4.9188)  time: 0.8226  data: 0.0005  max mem: 19734
loss info: cls_loss=4.6085, ratio_loss=0.2966, pruning_loss=0.2765, mse_loss=1.2049
Epoch: [1]  [ 850/1251]  eta: 0:05:30  lr: 0.000001  loss: 5.0606 (4.9158)  time: 0.8304  data: 0.0005  max mem: 19734
Epoch: [1]  [ 860/1251]  eta: 0:05:22  lr: 0.000001  loss: 4.9107 (4.9149)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [1]  [ 870/1251]  eta: 0:05:13  lr: 0.000001  loss: 5.1595 (4.9132)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [1]  [ 880/1251]  eta: 0:05:05  lr: 0.000001  loss: 5.0268 (4.9135)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [1]  [ 890/1251]  eta: 0:04:57  lr: 0.000001  loss: 5.1688 (4.9148)  time: 0.8005  data: 0.0004  max mem: 19734
Epoch: [1]  [ 900/1251]  eta: 0:04:48  lr: 0.000001  loss: 4.9850 (4.9124)  time: 0.8019  data: 0.0004  max mem: 19734
Epoch: [1]  [ 910/1251]  eta: 0:04:40  lr: 0.000001  loss: 4.9025 (4.9117)  time: 0.8008  data: 0.0004  max mem: 19734
Epoch: [1]  [ 920/1251]  eta: 0:04:32  lr: 0.000001  loss: 5.0231 (4.9114)  time: 0.8002  data: 0.0004  max mem: 19734
Epoch: [1]  [ 930/1251]  eta: 0:04:23  lr: 0.000001  loss: 4.8880 (4.9099)  time: 0.8002  data: 0.0004  max mem: 19734
Epoch: [1]  [ 940/1251]  eta: 0:04:15  lr: 0.000001  loss: 5.0640 (4.9098)  time: 0.8045  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5883, ratio_loss=0.2884, pruning_loss=0.2778, mse_loss=1.2263
Epoch: [1]  [ 950/1251]  eta: 0:04:07  lr: 0.000001  loss: 4.9059 (4.9057)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [1]  [ 960/1251]  eta: 0:03:59  lr: 0.000001  loss: 4.5817 (4.9033)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [1]  [ 970/1251]  eta: 0:03:50  lr: 0.000001  loss: 4.5169 (4.8985)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [1]  [ 980/1251]  eta: 0:03:42  lr: 0.000001  loss: 4.3292 (4.8935)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [1]  [ 990/1251]  eta: 0:03:34  lr: 0.000001  loss: 4.5311 (4.8907)  time: 0.8269  data: 0.0005  max mem: 19734
Epoch: [1]  [1000/1251]  eta: 0:03:26  lr: 0.000001  loss: 4.5311 (4.8875)  time: 0.8264  data: 0.0005  max mem: 19734
Epoch: [1]  [1010/1251]  eta: 0:03:17  lr: 0.000001  loss: 4.5104 (4.8842)  time: 0.7989  data: 0.0006  max mem: 19734
Epoch: [1]  [1020/1251]  eta: 0:03:09  lr: 0.000001  loss: 4.5509 (4.8831)  time: 0.7990  data: 0.0006  max mem: 19734
Epoch: [1]  [1030/1251]  eta: 0:03:01  lr: 0.000001  loss: 4.9194 (4.8831)  time: 0.7984  data: 0.0005  max mem: 19734
Epoch: [1]  [1040/1251]  eta: 0:02:53  lr: 0.000001  loss: 4.9194 (4.8807)  time: 0.8003  data: 0.0005  max mem: 19734
loss info: cls_loss=4.3875, ratio_loss=0.2794, pruning_loss=0.2818, mse_loss=1.2344
Epoch: [1]  [1050/1251]  eta: 0:02:44  lr: 0.000001  loss: 4.5089 (4.8754)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [1]  [1060/1251]  eta: 0:02:36  lr: 0.000001  loss: 4.7021 (4.8734)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [1]  [1070/1251]  eta: 0:02:28  lr: 0.000001  loss: 4.9009 (4.8739)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [1]  [1080/1251]  eta: 0:02:20  lr: 0.000001  loss: 5.1127 (4.8749)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [1]  [1090/1251]  eta: 0:02:11  lr: 0.000001  loss: 4.8682 (4.8722)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [1]  [1100/1251]  eta: 0:02:03  lr: 0.000001  loss: 4.6364 (4.8703)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [1]  [1110/1251]  eta: 0:01:55  lr: 0.000001  loss: 4.6235 (4.8670)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [1]  [1120/1251]  eta: 0:01:47  lr: 0.000001  loss: 4.6081 (4.8641)  time: 0.7995  data: 0.0006  max mem: 19734
Epoch: [1]  [1130/1251]  eta: 0:01:39  lr: 0.000001  loss: 4.7539 (4.8629)  time: 0.8115  data: 0.0006  max mem: 19734
Epoch: [1]  [1140/1251]  eta: 0:01:30  lr: 0.000001  loss: 4.6766 (4.8585)  time: 0.8271  data: 0.0005  max mem: 19734
loss info: cls_loss=4.4296, ratio_loss=0.2717, pruning_loss=0.2795, mse_loss=1.2262
Epoch: [1]  [1150/1251]  eta: 0:01:22  lr: 0.000001  loss: 4.3474 (4.8576)  time: 0.8151  data: 0.0005  max mem: 19734
Epoch: [1]  [1160/1251]  eta: 0:01:14  lr: 0.000001  loss: 5.0126 (4.8572)  time: 0.7988  data: 0.0005  max mem: 19734
Epoch: [1]  [1170/1251]  eta: 0:01:06  lr: 0.000001  loss: 4.6096 (4.8524)  time: 0.8003  data: 0.0004  max mem: 19734
Epoch: [1]  [1180/1251]  eta: 0:00:58  lr: 0.000001  loss: 4.7390 (4.8514)  time: 0.8041  data: 0.0004  max mem: 19734
Epoch: [1]  [1190/1251]  eta: 0:00:49  lr: 0.000001  loss: 4.8808 (4.8504)  time: 0.8015  data: 0.0006  max mem: 19734
Epoch: [1]  [1200/1251]  eta: 0:00:41  lr: 0.000001  loss: 4.8808 (4.8509)  time: 0.7942  data: 0.0004  max mem: 19734
Epoch: [1]  [1210/1251]  eta: 0:00:33  lr: 0.000001  loss: 5.0317 (4.8485)  time: 0.7908  data: 0.0001  max mem: 19734
Epoch: [1]  [1220/1251]  eta: 0:00:25  lr: 0.000001  loss: 4.9997 (4.8460)  time: 0.7908  data: 0.0001  max mem: 19734
Epoch: [1]  [1230/1251]  eta: 0:00:17  lr: 0.000001  loss: 4.3238 (4.8427)  time: 0.7920  data: 0.0002  max mem: 19734
Epoch: [1]  [1240/1251]  eta: 0:00:08  lr: 0.000001  loss: 4.8333 (4.8439)  time: 0.7970  data: 0.0002  max mem: 19734
loss info: cls_loss=4.4458, ratio_loss=0.2626, pruning_loss=0.2761, mse_loss=1.2094
Epoch: [1]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 4.8258 (4.8394)  time: 0.7962  data: 0.0001  max mem: 19734
Epoch: [1] Total time: 0:17:03 (0.8179 s / it)
Averaged stats: lr: 0.000001  loss: 4.8258 (4.8440)
Test:  [  0/261]  eta: 1:54:52  loss: 1.7185 (1.7185)  acc1: 64.0625 (64.0625)  acc5: 85.4167 (85.4167)  time: 26.4090  data: 26.2593  max mem: 19734
Test:  [ 10/261]  eta: 0:15:07  loss: 1.6537 (1.6370)  acc1: 66.6667 (66.1458)  acc5: 89.0625 (87.1212)  time: 3.6155  data: 3.2575  max mem: 19734
Test:  [ 20/261]  eta: 0:08:32  loss: 1.7449 (1.8216)  acc1: 62.5000 (61.6319)  acc5: 84.8958 (84.1766)  time: 0.9116  data: 0.4866  max mem: 19734
Test:  [ 30/261]  eta: 0:05:58  loss: 1.7692 (1.7406)  acc1: 62.5000 (63.9785)  acc5: 82.8125 (84.8286)  time: 0.4189  data: 0.0174  max mem: 19734
Test:  [ 40/261]  eta: 0:04:47  loss: 1.6248 (1.7482)  acc1: 66.6667 (63.5544)  acc5: 86.9792 (85.0483)  time: 0.4315  data: 0.1362  max mem: 19734
Test:  [ 50/261]  eta: 0:03:57  loss: 1.8681 (1.8279)  acc1: 58.8542 (61.1009)  acc5: 82.2917 (84.1912)  time: 0.4675  data: 0.1346  max mem: 19734
Test:  [ 60/261]  eta: 0:03:18  loss: 2.0435 (1.8691)  acc1: 51.5625 (59.7848)  acc5: 80.7292 (83.7944)  time: 0.3424  data: 0.0128  max mem: 19734
Test:  [ 70/261]  eta: 0:02:47  loss: 2.0211 (1.8768)  acc1: 53.6458 (59.3163)  acc5: 82.8125 (83.9495)  time: 0.2433  data: 0.0129  max mem: 19734
Test:  [ 80/261]  eta: 0:02:38  loss: 1.7679 (1.8497)  acc1: 60.4167 (60.0823)  acc5: 85.4167 (84.3557)  time: 0.5321  data: 0.3090  max mem: 19734
Test:  [ 90/261]  eta: 0:02:17  loss: 1.6714 (1.8134)  acc1: 63.5417 (60.9089)  acc5: 86.9792 (84.9130)  time: 0.5456  data: 0.3053  max mem: 19734
Test:  [100/261]  eta: 0:02:00  loss: 1.5151 (1.8015)  acc1: 68.7500 (61.2263)  acc5: 86.9792 (84.9732)  time: 0.2452  data: 0.0088  max mem: 19734
Test:  [110/261]  eta: 0:02:07  loss: 1.6507 (1.8110)  acc1: 63.5417 (61.0642)  acc5: 83.8542 (84.6659)  time: 1.0169  data: 0.7957  max mem: 19734
Test:  [120/261]  eta: 0:01:51  loss: 2.0257 (1.8354)  acc1: 58.8542 (60.6749)  acc5: 80.2083 (84.2416)  time: 0.9922  data: 0.7967  max mem: 19734
Test:  [130/261]  eta: 0:01:36  loss: 2.0714 (1.8662)  acc1: 53.6458 (60.1821)  acc5: 79.1667 (83.7349)  time: 0.1650  data: 0.0085  max mem: 19734
Test:  [140/261]  eta: 0:01:25  loss: 2.0830 (1.8767)  acc1: 52.6042 (59.7850)  acc5: 80.2083 (83.6178)  time: 0.2120  data: 0.0057  max mem: 19734
Test:  [150/261]  eta: 0:01:14  loss: 1.8745 (1.8727)  acc1: 61.4583 (60.0511)  acc5: 82.2917 (83.4920)  time: 0.2176  data: 0.0079  max mem: 19734
Test:  [160/261]  eta: 0:01:04  loss: 1.8172 (1.8921)  acc1: 61.9792 (59.8150)  acc5: 80.7292 (83.1457)  time: 0.1380  data: 0.0058  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 2.1576 (1.9183)  acc1: 52.6042 (59.3202)  acc5: 77.0833 (82.7211)  time: 0.1231  data: 0.0030  max mem: 19734
Test:  [180/261]  eta: 0:00:46  loss: 2.1968 (1.9322)  acc1: 52.6042 (59.0844)  acc5: 76.5625 (82.5046)  time: 0.1212  data: 0.0038  max mem: 19734
Test:  [190/261]  eta: 0:00:39  loss: 2.1968 (1.9416)  acc1: 55.2083 (58.9387)  acc5: 78.6458 (82.2998)  time: 0.1180  data: 0.0017  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: 2.1644 (1.9489)  acc1: 56.2500 (58.7920)  acc5: 78.6458 (82.1440)  time: 0.1160  data: 0.0008  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: 2.1216 (1.9543)  acc1: 57.2917 (58.7702)  acc5: 78.6458 (82.0350)  time: 0.1159  data: 0.0007  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: 2.2265 (1.9786)  acc1: 53.6458 (58.2461)  acc5: 75.5208 (81.6294)  time: 0.1155  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:14  loss: 2.2311 (1.9870)  acc1: 51.0417 (58.0605)  acc5: 74.4792 (81.4439)  time: 0.1156  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:09  loss: 2.0596 (1.9859)  acc1: 55.7292 (57.9767)  acc5: 79.6875 (81.4942)  time: 0.1157  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.7559 (1.9665)  acc1: 61.4583 (58.3333)  acc5: 84.8958 (81.7584)  time: 0.1156  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.5901 (1.9549)  acc1: 63.0208 (58.5780)  acc5: 88.5417 (81.9660)  time: 0.1123  data: 0.0001  max mem: 19734
Test: Total time: 0:01:54 (0.4383 s / it)
* Acc@1 58.578 Acc@5 81.966 loss 1.955
Accuracy of the network on the 50000 test images: 58.6%
Max accuracy: 61.20%
Epoch: [2]  [   0/1251]  eta: 7:07:52  lr: 0.000005  loss: 4.1747 (4.1747)  time: 20.5215  data: 5.7914  max mem: 19734
Epoch: [2]  [  10/1251]  eta: 0:57:55  lr: 0.000005  loss: 4.1928 (4.4911)  time: 2.8002  data: 0.5343  max mem: 19734
Epoch: [2]  [  20/1251]  eta: 0:38:34  lr: 0.000005  loss: 4.6480 (4.6026)  time: 0.9482  data: 0.0045  max mem: 19734
Epoch: [2]  [  30/1251]  eta: 0:31:28  lr: 0.000005  loss: 4.9527 (4.6779)  time: 0.8578  data: 0.0006  max mem: 19734
Epoch: [2]  [  40/1251]  eta: 0:27:35  lr: 0.000005  loss: 4.9418 (4.6505)  time: 0.8274  data: 0.0005  max mem: 19734
Epoch: [2]  [  50/1251]  eta: 0:25:08  lr: 0.000005  loss: 4.9418 (4.6832)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [2]  [  60/1251]  eta: 0:23:26  lr: 0.000005  loss: 4.9584 (4.7192)  time: 0.8009  data: 0.0007  max mem: 19734
Epoch: [2]  [  70/1251]  eta: 0:22:11  lr: 0.000005  loss: 4.9238 (4.7103)  time: 0.8016  data: 0.0007  max mem: 19734
Epoch: [2]  [  80/1251]  eta: 0:21:13  lr: 0.000005  loss: 4.6003 (4.6936)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [2]  [  90/1251]  eta: 0:20:26  lr: 0.000005  loss: 4.5630 (4.6536)  time: 0.8037  data: 0.0005  max mem: 19734
loss info: cls_loss=4.4461, ratio_loss=0.2331, pruning_loss=0.2682, mse_loss=1.2346
Epoch: [2]  [ 100/1251]  eta: 0:19:47  lr: 0.000005  loss: 4.6031 (4.6436)  time: 0.8063  data: 0.0005  max mem: 19734
Epoch: [2]  [ 110/1251]  eta: 0:19:13  lr: 0.000005  loss: 4.3370 (4.5876)  time: 0.8053  data: 0.0004  max mem: 19734
Epoch: [2]  [ 120/1251]  eta: 0:18:43  lr: 0.000005  loss: 4.3721 (4.5875)  time: 0.7996  data: 0.0004  max mem: 19734
Epoch: [2]  [ 130/1251]  eta: 0:18:17  lr: 0.000005  loss: 4.5376 (4.5730)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [2]  [ 140/1251]  eta: 0:17:53  lr: 0.000005  loss: 4.5376 (4.5705)  time: 0.8011  data: 0.0004  max mem: 19734
Epoch: [2]  [ 150/1251]  eta: 0:17:31  lr: 0.000005  loss: 4.5372 (4.5402)  time: 0.7984  data: 0.0006  max mem: 19734
Epoch: [2]  [ 160/1251]  eta: 0:17:11  lr: 0.000005  loss: 4.5091 (4.5381)  time: 0.7989  data: 0.0006  max mem: 19734
Epoch: [2]  [ 170/1251]  eta: 0:16:55  lr: 0.000005  loss: 4.4847 (4.5239)  time: 0.8240  data: 0.0008  max mem: 19734
Epoch: [2]  [ 180/1251]  eta: 0:16:40  lr: 0.000005  loss: 4.5544 (4.5224)  time: 0.8404  data: 0.0008  max mem: 19734
Epoch: [2]  [ 190/1251]  eta: 0:16:23  lr: 0.000005  loss: 4.6619 (4.5154)  time: 0.8150  data: 0.0006  max mem: 19734
loss info: cls_loss=4.2419, ratio_loss=0.1717, pruning_loss=0.2598, mse_loss=1.2055
Epoch: [2]  [ 200/1251]  eta: 0:16:07  lr: 0.000005  loss: 4.4476 (4.5140)  time: 0.7977  data: 0.0006  max mem: 19734
Epoch: [2]  [ 210/1251]  eta: 0:15:52  lr: 0.000005  loss: 4.5020 (4.5117)  time: 0.8053  data: 0.0006  max mem: 19734
Epoch: [2]  [ 220/1251]  eta: 0:15:38  lr: 0.000005  loss: 4.4035 (4.4951)  time: 0.8104  data: 0.0006  max mem: 19734
Epoch: [2]  [ 230/1251]  eta: 0:15:24  lr: 0.000005  loss: 4.4035 (4.4956)  time: 0.8050  data: 0.0007  max mem: 19734
Epoch: [2]  [ 240/1251]  eta: 0:15:11  lr: 0.000005  loss: 4.1863 (4.4685)  time: 0.8018  data: 0.0007  max mem: 19734
Epoch: [2]  [ 250/1251]  eta: 0:14:58  lr: 0.000005  loss: 3.9392 (4.4526)  time: 0.8079  data: 0.0006  max mem: 19734
Epoch: [2]  [ 260/1251]  eta: 0:14:46  lr: 0.000005  loss: 4.2059 (4.4380)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [2]  [ 270/1251]  eta: 0:14:33  lr: 0.000005  loss: 4.4606 (4.4387)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [2]  [ 280/1251]  eta: 0:14:21  lr: 0.000005  loss: 4.4432 (4.4217)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [2]  [ 290/1251]  eta: 0:14:10  lr: 0.000005  loss: 3.6792 (4.3933)  time: 0.8024  data: 0.0006  max mem: 19734
loss info: cls_loss=4.0243, ratio_loss=0.1063, pruning_loss=0.2578, mse_loss=1.2053
Epoch: [2]  [ 300/1251]  eta: 0:13:58  lr: 0.000005  loss: 3.7435 (4.3799)  time: 0.7973  data: 0.0007  max mem: 19734
Epoch: [2]  [ 310/1251]  eta: 0:13:48  lr: 0.000005  loss: 4.1778 (4.3680)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [2]  [ 320/1251]  eta: 0:13:38  lr: 0.000005  loss: 4.1160 (4.3599)  time: 0.8478  data: 0.0004  max mem: 19734
Epoch: [2]  [ 330/1251]  eta: 0:13:28  lr: 0.000005  loss: 3.8841 (4.3461)  time: 0.8323  data: 0.0005  max mem: 19734
Epoch: [2]  [ 340/1251]  eta: 0:13:17  lr: 0.000005  loss: 3.6332 (4.3312)  time: 0.8107  data: 0.0008  max mem: 19734
Epoch: [2]  [ 350/1251]  eta: 0:13:06  lr: 0.000005  loss: 3.6961 (4.3155)  time: 0.8034  data: 0.0007  max mem: 19734
Epoch: [2]  [ 360/1251]  eta: 0:12:56  lr: 0.000005  loss: 3.5759 (4.2851)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [2]  [ 370/1251]  eta: 0:12:46  lr: 0.000005  loss: 3.4212 (4.2717)  time: 0.8041  data: 0.0006  max mem: 19734
Epoch: [2]  [ 380/1251]  eta: 0:12:35  lr: 0.000005  loss: 3.9923 (4.2616)  time: 0.8042  data: 0.0006  max mem: 19734
Epoch: [2]  [ 390/1251]  eta: 0:12:25  lr: 0.000005  loss: 3.8861 (4.2458)  time: 0.8023  data: 0.0006  max mem: 19734
loss info: cls_loss=3.7526, ratio_loss=0.0525, pruning_loss=0.2549, mse_loss=1.2268
Epoch: [2]  [ 400/1251]  eta: 0:12:15  lr: 0.000005  loss: 3.8575 (4.2369)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [2]  [ 410/1251]  eta: 0:12:06  lr: 0.000005  loss: 3.9209 (4.2239)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [2]  [ 420/1251]  eta: 0:11:56  lr: 0.000005  loss: 3.9209 (4.2172)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [2]  [ 430/1251]  eta: 0:11:46  lr: 0.000005  loss: 4.0877 (4.2137)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [2]  [ 440/1251]  eta: 0:11:36  lr: 0.000005  loss: 4.1399 (4.2106)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [2]  [ 450/1251]  eta: 0:11:27  lr: 0.000005  loss: 3.8375 (4.1955)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [2]  [ 460/1251]  eta: 0:11:18  lr: 0.000005  loss: 3.6618 (4.1841)  time: 0.8322  data: 0.0005  max mem: 19734
Epoch: [2]  [ 470/1251]  eta: 0:11:09  lr: 0.000005  loss: 3.8679 (4.1803)  time: 0.8486  data: 0.0006  max mem: 19734
Epoch: [2]  [ 480/1251]  eta: 0:11:00  lr: 0.000005  loss: 3.9530 (4.1697)  time: 0.8184  data: 0.0006  max mem: 19734
Epoch: [2]  [ 490/1251]  eta: 0:10:50  lr: 0.000005  loss: 3.9974 (4.1687)  time: 0.8022  data: 0.0005  max mem: 19734
loss info: cls_loss=3.7959, ratio_loss=0.0225, pruning_loss=0.2474, mse_loss=1.1976
Epoch: [2]  [ 500/1251]  eta: 0:10:41  lr: 0.000005  loss: 4.1234 (4.1606)  time: 0.8010  data: 0.0006  max mem: 19734
Epoch: [2]  [ 510/1251]  eta: 0:10:32  lr: 0.000005  loss: 3.9130 (4.1559)  time: 0.8007  data: 0.0006  max mem: 19734
Epoch: [2]  [ 520/1251]  eta: 0:10:22  lr: 0.000005  loss: 3.8735 (4.1479)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [2]  [ 530/1251]  eta: 0:10:13  lr: 0.000005  loss: 3.8483 (4.1376)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [2]  [ 540/1251]  eta: 0:10:04  lr: 0.000005  loss: 3.9339 (4.1322)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [2]  [ 550/1251]  eta: 0:09:55  lr: 0.000005  loss: 4.0266 (4.1280)  time: 0.8055  data: 0.0005  max mem: 19734
Epoch: [2]  [ 560/1251]  eta: 0:09:46  lr: 0.000005  loss: 3.8269 (4.1191)  time: 0.7975  data: 0.0006  max mem: 19734
Epoch: [2]  [ 570/1251]  eta: 0:09:37  lr: 0.000005  loss: 3.7073 (4.1125)  time: 0.8005  data: 0.0007  max mem: 19734
Epoch: [2]  [ 580/1251]  eta: 0:09:28  lr: 0.000005  loss: 3.7073 (4.1026)  time: 0.8013  data: 0.0007  max mem: 19734
Epoch: [2]  [ 590/1251]  eta: 0:09:19  lr: 0.000005  loss: 3.7728 (4.0966)  time: 0.8023  data: 0.0006  max mem: 19734
loss info: cls_loss=3.7223, ratio_loss=0.0101, pruning_loss=0.2482, mse_loss=1.2328
Epoch: [2]  [ 600/1251]  eta: 0:09:10  lr: 0.000005  loss: 3.9476 (4.0913)  time: 0.8112  data: 0.0006  max mem: 19734
Epoch: [2]  [ 610/1251]  eta: 0:09:02  lr: 0.000005  loss: 3.8289 (4.0852)  time: 0.8376  data: 0.0005  max mem: 19734
Epoch: [2]  [ 620/1251]  eta: 0:08:53  lr: 0.000005  loss: 3.7980 (4.0818)  time: 0.8346  data: 0.0008  max mem: 19734
Epoch: [2]  [ 630/1251]  eta: 0:08:44  lr: 0.000005  loss: 3.8556 (4.0780)  time: 0.8093  data: 0.0008  max mem: 19734
Epoch: [2]  [ 640/1251]  eta: 0:08:35  lr: 0.000005  loss: 3.4535 (4.0639)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [2]  [ 650/1251]  eta: 0:08:26  lr: 0.000005  loss: 3.4225 (4.0580)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [2]  [ 660/1251]  eta: 0:08:18  lr: 0.000005  loss: 3.8309 (4.0519)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [2]  [ 670/1251]  eta: 0:08:09  lr: 0.000005  loss: 3.5918 (4.0467)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [2]  [ 680/1251]  eta: 0:08:00  lr: 0.000005  loss: 3.7644 (4.0415)  time: 0.8040  data: 0.0004  max mem: 19734
Epoch: [2]  [ 690/1251]  eta: 0:07:51  lr: 0.000005  loss: 3.7466 (4.0345)  time: 0.8090  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6233, ratio_loss=0.0056, pruning_loss=0.2455, mse_loss=1.2038
Epoch: [2]  [ 700/1251]  eta: 0:07:43  lr: 0.000005  loss: 3.8180 (4.0317)  time: 0.8068  data: 0.0004  max mem: 19734
Epoch: [2]  [ 710/1251]  eta: 0:07:34  lr: 0.000005  loss: 3.8855 (4.0295)  time: 0.8023  data: 0.0006  max mem: 19734
Epoch: [2]  [ 720/1251]  eta: 0:07:25  lr: 0.000005  loss: 3.7945 (4.0234)  time: 0.8027  data: 0.0006  max mem: 19734
Epoch: [2]  [ 730/1251]  eta: 0:07:17  lr: 0.000005  loss: 3.8220 (4.0229)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [2]  [ 740/1251]  eta: 0:07:08  lr: 0.000005  loss: 4.1169 (4.0229)  time: 0.8008  data: 0.0007  max mem: 19734
Epoch: [2]  [ 750/1251]  eta: 0:07:00  lr: 0.000005  loss: 4.1170 (4.0191)  time: 0.8181  data: 0.0007  max mem: 19734
Epoch: [2]  [ 760/1251]  eta: 0:06:51  lr: 0.000005  loss: 3.6825 (4.0174)  time: 0.8497  data: 0.0006  max mem: 19734
Epoch: [2]  [ 770/1251]  eta: 0:06:43  lr: 0.000005  loss: 3.6539 (4.0104)  time: 0.8404  data: 0.0005  max mem: 19734
Epoch: [2]  [ 780/1251]  eta: 0:06:34  lr: 0.000005  loss: 3.4150 (4.0063)  time: 0.8079  data: 0.0004  max mem: 19734
Epoch: [2]  [ 790/1251]  eta: 0:06:26  lr: 0.000005  loss: 3.6727 (4.0031)  time: 0.7993  data: 0.0005  max mem: 19734
loss info: cls_loss=3.7591, ratio_loss=0.0043, pruning_loss=0.2371, mse_loss=1.1212
Epoch: [2]  [ 800/1251]  eta: 0:06:17  lr: 0.000005  loss: 3.9494 (4.0006)  time: 0.8004  data: 0.0004  max mem: 19734
Epoch: [2]  [ 810/1251]  eta: 0:06:08  lr: 0.000005  loss: 3.9494 (3.9980)  time: 0.8016  data: 0.0004  max mem: 19734
Epoch: [2]  [ 820/1251]  eta: 0:06:00  lr: 0.000005  loss: 3.9572 (3.9947)  time: 0.8025  data: 0.0004  max mem: 19734
Epoch: [2]  [ 830/1251]  eta: 0:05:51  lr: 0.000005  loss: 3.9316 (3.9881)  time: 0.8033  data: 0.0004  max mem: 19734
Epoch: [2]  [ 840/1251]  eta: 0:05:43  lr: 0.000005  loss: 3.4712 (3.9818)  time: 0.8078  data: 0.0004  max mem: 19734
Epoch: [2]  [ 850/1251]  eta: 0:05:34  lr: 0.000005  loss: 3.8071 (3.9801)  time: 0.8054  data: 0.0004  max mem: 19734
Epoch: [2]  [ 860/1251]  eta: 0:05:26  lr: 0.000005  loss: 3.8795 (3.9754)  time: 0.8078  data: 0.0004  max mem: 19734
Epoch: [2]  [ 870/1251]  eta: 0:05:17  lr: 0.000005  loss: 3.5827 (3.9707)  time: 0.8103  data: 0.0004  max mem: 19734
Epoch: [2]  [ 880/1251]  eta: 0:05:09  lr: 0.000005  loss: 3.8274 (3.9694)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [2]  [ 890/1251]  eta: 0:05:01  lr: 0.000005  loss: 3.9416 (3.9679)  time: 0.8009  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6309, ratio_loss=0.0039, pruning_loss=0.2451, mse_loss=1.1547
Epoch: [2]  [ 900/1251]  eta: 0:04:52  lr: 0.000005  loss: 3.6909 (3.9633)  time: 0.8300  data: 0.0004  max mem: 19734
Epoch: [2]  [ 910/1251]  eta: 0:04:44  lr: 0.000005  loss: 3.6890 (3.9608)  time: 0.8375  data: 0.0004  max mem: 19734
Epoch: [2]  [ 920/1251]  eta: 0:04:35  lr: 0.000005  loss: 3.9782 (3.9598)  time: 0.8164  data: 0.0004  max mem: 19734
Epoch: [2]  [ 930/1251]  eta: 0:04:27  lr: 0.000005  loss: 3.7967 (3.9547)  time: 0.8091  data: 0.0004  max mem: 19734
Epoch: [2]  [ 940/1251]  eta: 0:04:19  lr: 0.000005  loss: 3.4532 (3.9498)  time: 0.8048  data: 0.0004  max mem: 19734
Epoch: [2]  [ 950/1251]  eta: 0:04:10  lr: 0.000005  loss: 3.4532 (3.9462)  time: 0.8056  data: 0.0004  max mem: 19734
Epoch: [2]  [ 960/1251]  eta: 0:04:02  lr: 0.000005  loss: 3.8516 (3.9437)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [2]  [ 970/1251]  eta: 0:03:53  lr: 0.000005  loss: 3.4351 (3.9373)  time: 0.8007  data: 0.0004  max mem: 19734
Epoch: [2]  [ 980/1251]  eta: 0:03:45  lr: 0.000005  loss: 3.4508 (3.9333)  time: 0.8004  data: 0.0004  max mem: 19734
Epoch: [2]  [ 990/1251]  eta: 0:03:37  lr: 0.000005  loss: 3.5980 (3.9317)  time: 0.8067  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5676, ratio_loss=0.0036, pruning_loss=0.2422, mse_loss=1.1846
Epoch: [2]  [1000/1251]  eta: 0:03:28  lr: 0.000005  loss: 3.6726 (3.9277)  time: 0.8081  data: 0.0004  max mem: 19734
Epoch: [2]  [1010/1251]  eta: 0:03:20  lr: 0.000005  loss: 3.6960 (3.9261)  time: 0.8009  data: 0.0004  max mem: 19734
Epoch: [2]  [1020/1251]  eta: 0:03:11  lr: 0.000005  loss: 3.5572 (3.9212)  time: 0.8021  data: 0.0004  max mem: 19734
Epoch: [2]  [1030/1251]  eta: 0:03:03  lr: 0.000005  loss: 3.6527 (3.9195)  time: 0.8043  data: 0.0003  max mem: 19734
Epoch: [2]  [1040/1251]  eta: 0:02:55  lr: 0.000005  loss: 3.6473 (3.9151)  time: 0.8161  data: 0.0004  max mem: 19734
Epoch: [2]  [1050/1251]  eta: 0:02:46  lr: 0.000005  loss: 3.6137 (3.9128)  time: 0.8435  data: 0.0004  max mem: 19734
Epoch: [2]  [1060/1251]  eta: 0:02:38  lr: 0.000005  loss: 3.9175 (3.9128)  time: 0.8384  data: 0.0004  max mem: 19734
Epoch: [2]  [1070/1251]  eta: 0:02:30  lr: 0.000005  loss: 3.9135 (3.9110)  time: 0.8094  data: 0.0006  max mem: 19734
Epoch: [2]  [1080/1251]  eta: 0:02:21  lr: 0.000005  loss: 3.7084 (3.9094)  time: 0.8002  data: 0.0006  max mem: 19734
Epoch: [2]  [1090/1251]  eta: 0:02:13  lr: 0.000005  loss: 3.7808 (3.9080)  time: 0.7993  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6622, ratio_loss=0.0036, pruning_loss=0.2372, mse_loss=1.1159
Epoch: [2]  [1100/1251]  eta: 0:02:05  lr: 0.000005  loss: 3.7808 (3.9041)  time: 0.8050  data: 0.0004  max mem: 19734
Epoch: [2]  [1110/1251]  eta: 0:01:56  lr: 0.000005  loss: 3.8144 (3.9040)  time: 0.8061  data: 0.0004  max mem: 19734
Epoch: [2]  [1120/1251]  eta: 0:01:48  lr: 0.000005  loss: 3.8787 (3.9026)  time: 0.8008  data: 0.0004  max mem: 19734
Epoch: [2]  [1130/1251]  eta: 0:01:40  lr: 0.000005  loss: 3.8292 (3.9028)  time: 0.8042  data: 0.0004  max mem: 19734
Epoch: [2]  [1140/1251]  eta: 0:01:31  lr: 0.000005  loss: 3.8759 (3.9005)  time: 0.8032  data: 0.0004  max mem: 19734
Epoch: [2]  [1150/1251]  eta: 0:01:23  lr: 0.000005  loss: 3.8470 (3.9003)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [2]  [1160/1251]  eta: 0:01:15  lr: 0.000005  loss: 3.8916 (3.8981)  time: 0.8088  data: 0.0005  max mem: 19734
Epoch: [2]  [1170/1251]  eta: 0:01:07  lr: 0.000005  loss: 4.0073 (3.8979)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [2]  [1180/1251]  eta: 0:00:58  lr: 0.000005  loss: 3.7735 (3.8963)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [2]  [1190/1251]  eta: 0:00:50  lr: 0.000005  loss: 3.6865 (3.8943)  time: 0.8235  data: 0.0009  max mem: 19734
loss info: cls_loss=3.6990, ratio_loss=0.0036, pruning_loss=0.2375, mse_loss=1.1911
Epoch: [2]  [1200/1251]  eta: 0:00:42  lr: 0.000005  loss: 3.7267 (3.8919)  time: 0.8351  data: 0.0008  max mem: 19734
Epoch: [2]  [1210/1251]  eta: 0:00:33  lr: 0.000005  loss: 3.7543 (3.8896)  time: 0.8138  data: 0.0002  max mem: 19734
Epoch: [2]  [1220/1251]  eta: 0:00:25  lr: 0.000005  loss: 3.8768 (3.8893)  time: 0.7972  data: 0.0002  max mem: 19734
Epoch: [2]  [1230/1251]  eta: 0:00:17  lr: 0.000005  loss: 3.9113 (3.8887)  time: 0.7919  data: 0.0002  max mem: 19734
Epoch: [2]  [1240/1251]  eta: 0:00:09  lr: 0.000005  loss: 3.8929 (3.8878)  time: 0.7924  data: 0.0002  max mem: 19734
Epoch: [2]  [1250/1251]  eta: 0:00:00  lr: 0.000005  loss: 3.8066 (3.8849)  time: 0.7910  data: 0.0002  max mem: 19734
Epoch: [2] Total time: 0:17:15 (0.8274 s / it)
Averaged stats: lr: 0.000005  loss: 3.8066 (3.8648)
Test:  [  0/261]  eta: 2:08:58  loss: 1.1663 (1.1663)  acc1: 76.5625 (76.5625)  acc5: 92.1875 (92.1875)  time: 29.6498  data: 29.2734  max mem: 19734
Test:  [ 10/261]  eta: 0:15:14  loss: 1.1663 (1.1903)  acc1: 77.0833 (75.5208)  acc5: 93.2292 (92.1875)  time: 3.6421  data: 3.3784  max mem: 19734
Test:  [ 20/261]  eta: 0:07:59  loss: 1.3626 (1.3760)  acc1: 70.3125 (69.6429)  acc5: 89.5833 (89.8065)  time: 0.6051  data: 0.3995  max mem: 19734
Test:  [ 30/261]  eta: 0:05:29  loss: 1.2188 (1.2721)  acc1: 72.9167 (73.2191)  acc5: 89.5833 (90.4570)  time: 0.2063  data: 0.0162  max mem: 19734
Test:  [ 40/261]  eta: 0:04:38  loss: 1.0253 (1.2479)  acc1: 80.7292 (73.8948)  acc5: 92.7083 (90.5996)  time: 0.4998  data: 0.2914  max mem: 19734
Test:  [ 50/261]  eta: 0:03:52  loss: 1.5828 (1.3578)  acc1: 62.5000 (70.5678)  acc5: 86.9792 (89.5118)  time: 0.6034  data: 0.3652  max mem: 19734
Test:  [ 60/261]  eta: 0:03:16  loss: 1.7326 (1.4138)  acc1: 60.4167 (68.6475)  acc5: 85.4167 (88.9942)  time: 0.3883  data: 0.0876  max mem: 19734
Test:  [ 70/261]  eta: 0:02:52  loss: 1.5961 (1.4173)  acc1: 61.4583 (67.9651)  acc5: 88.0208 (89.1505)  time: 0.3894  data: 0.1394  max mem: 19734
Test:  [ 80/261]  eta: 0:02:46  loss: 1.3344 (1.3983)  acc1: 68.7500 (68.3963)  acc5: 90.6250 (89.4997)  time: 0.7557  data: 0.5849  max mem: 19734
Test:  [ 90/261]  eta: 0:02:25  loss: 1.2882 (1.3674)  acc1: 72.9167 (69.2594)  acc5: 91.6667 (89.8523)  time: 0.6814  data: 0.4549  max mem: 19734
Test:  [100/261]  eta: 0:02:13  loss: 1.1829 (1.3649)  acc1: 72.9167 (69.2915)  acc5: 90.6250 (89.9598)  time: 0.4483  data: 0.2087  max mem: 19734
Test:  [110/261]  eta: 0:02:01  loss: 1.2840 (1.3782)  acc1: 69.7917 (69.1817)  acc5: 89.5833 (89.7382)  time: 0.5921  data: 0.3390  max mem: 19734
Test:  [120/261]  eta: 0:01:49  loss: 1.6768 (1.4110)  acc1: 63.5417 (68.5778)  acc5: 84.3750 (89.2605)  time: 0.5256  data: 0.1444  max mem: 19734
Test:  [130/261]  eta: 0:01:36  loss: 1.7449 (1.4502)  acc1: 59.3750 (67.8356)  acc5: 83.3333 (88.7444)  time: 0.3559  data: 0.0151  max mem: 19734
Test:  [140/261]  eta: 0:01:25  loss: 1.8407 (1.4718)  acc1: 58.3333 (67.3094)  acc5: 83.8542 (88.4715)  time: 0.2723  data: 0.0709  max mem: 19734
Test:  [150/261]  eta: 0:01:16  loss: 1.5411 (1.4710)  acc1: 65.6250 (67.4876)  acc5: 84.8958 (88.4278)  time: 0.3405  data: 0.1396  max mem: 19734
Test:  [160/261]  eta: 0:01:08  loss: 1.4114 (1.4909)  acc1: 70.8333 (67.2425)  acc5: 87.5000 (88.0855)  time: 0.5103  data: 0.3050  max mem: 19734
Test:  [170/261]  eta: 0:01:00  loss: 1.8660 (1.5199)  acc1: 59.3750 (66.6575)  acc5: 81.7708 (87.6888)  time: 0.5388  data: 0.3745  max mem: 19734
Test:  [180/261]  eta: 0:00:51  loss: 1.7923 (1.5339)  acc1: 59.3750 (66.3473)  acc5: 83.3333 (87.5374)  time: 0.2947  data: 0.1521  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: 1.7638 (1.5418)  acc1: 61.4583 (66.2249)  acc5: 84.3750 (87.4018)  time: 0.1396  data: 0.0105  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.7004 (1.5509)  acc1: 64.5833 (66.0733)  acc5: 84.3750 (87.2176)  time: 0.1258  data: 0.0043  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.8334 (1.5612)  acc1: 63.0208 (65.9261)  acc5: 83.3333 (87.0458)  time: 0.1156  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.8646 (1.5800)  acc1: 60.9375 (65.4742)  acc5: 81.7708 (86.7788)  time: 0.1154  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.8419 (1.5898)  acc1: 57.2917 (65.2823)  acc5: 81.7708 (86.6455)  time: 0.1154  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.7894 (1.5954)  acc1: 61.4583 (65.1409)  acc5: 84.3750 (86.6355)  time: 0.1154  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.4373 (1.5807)  acc1: 67.7083 (65.4818)  acc5: 89.5833 (86.8401)  time: 0.1152  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.2268 (1.5753)  acc1: 72.3958 (65.6100)  acc5: 91.1458 (86.9460)  time: 0.1120  data: 0.0001  max mem: 19734
Test: Total time: 0:02:05 (0.4800 s / it)
* Acc@1 65.610 Acc@5 86.946 loss 1.575
Accuracy of the network on the 50000 test images: 65.6%
Max accuracy: 65.61%
Epoch: [3]  [   0/1251]  eta: 1:24:57  lr: 0.000009  loss: 3.4780 (3.4780)  time: 4.0748  data: 3.2348  max mem: 19734
Epoch: [3]  [  10/1251]  eta: 0:22:48  lr: 0.000009  loss: 3.9962 (3.8910)  time: 1.1030  data: 0.2954  max mem: 19734
Epoch: [3]  [  20/1251]  eta: 0:19:41  lr: 0.000009  loss: 3.8820 (3.7262)  time: 0.8040  data: 0.0010  max mem: 19734
Epoch: [3]  [  30/1251]  eta: 0:18:29  lr: 0.000009  loss: 3.7254 (3.6915)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [3]  [  40/1251]  eta: 0:17:47  lr: 0.000009  loss: 3.7254 (3.7031)  time: 0.7998  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6930, ratio_loss=0.0036, pruning_loss=0.2308, mse_loss=1.1499
Epoch: [3]  [  50/1251]  eta: 0:17:19  lr: 0.000009  loss: 3.5832 (3.7085)  time: 0.7982  data: 0.0006  max mem: 19734
Epoch: [3]  [  60/1251]  eta: 0:16:56  lr: 0.000009  loss: 3.4619 (3.6559)  time: 0.7963  data: 0.0006  max mem: 19734
Epoch: [3]  [  70/1251]  eta: 0:16:41  lr: 0.000009  loss: 3.6892 (3.6677)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [3]  [  80/1251]  eta: 0:16:30  lr: 0.000009  loss: 3.9111 (3.6752)  time: 0.8215  data: 0.0006  max mem: 19734
Epoch: [3]  [  90/1251]  eta: 0:16:21  lr: 0.000009  loss: 3.8486 (3.6461)  time: 0.8354  data: 0.0006  max mem: 19734
Epoch: [3]  [ 100/1251]  eta: 0:16:08  lr: 0.000009  loss: 3.8389 (3.6700)  time: 0.8248  data: 0.0004  max mem: 19734
Epoch: [3]  [ 110/1251]  eta: 0:15:57  lr: 0.000009  loss: 3.7596 (3.6544)  time: 0.8116  data: 0.0005  max mem: 19734
Epoch: [3]  [ 120/1251]  eta: 0:15:45  lr: 0.000009  loss: 3.6175 (3.6490)  time: 0.8101  data: 0.0005  max mem: 19734
Epoch: [3]  [ 130/1251]  eta: 0:15:34  lr: 0.000009  loss: 3.4813 (3.6295)  time: 0.8055  data: 0.0004  max mem: 19734
Epoch: [3]  [ 140/1251]  eta: 0:15:23  lr: 0.000009  loss: 3.4813 (3.6270)  time: 0.8028  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5258, ratio_loss=0.0036, pruning_loss=0.2378, mse_loss=1.1667
Epoch: [3]  [ 150/1251]  eta: 0:15:13  lr: 0.000009  loss: 3.7154 (3.6135)  time: 0.8003  data: 0.0005  max mem: 19734
Epoch: [3]  [ 160/1251]  eta: 0:15:03  lr: 0.000009  loss: 3.5420 (3.6085)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [3]  [ 170/1251]  eta: 0:14:53  lr: 0.000009  loss: 3.5420 (3.6084)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [3]  [ 180/1251]  eta: 0:14:43  lr: 0.000009  loss: 3.8314 (3.6074)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [3]  [ 190/1251]  eta: 0:14:34  lr: 0.000009  loss: 3.2742 (3.5965)  time: 0.8054  data: 0.0006  max mem: 19734
Epoch: [3]  [ 200/1251]  eta: 0:14:25  lr: 0.000009  loss: 3.3423 (3.5923)  time: 0.8070  data: 0.0006  max mem: 19734
Epoch: [3]  [ 210/1251]  eta: 0:14:17  lr: 0.000009  loss: 3.6650 (3.5999)  time: 0.8144  data: 0.0006  max mem: 19734
Epoch: [3]  [ 220/1251]  eta: 0:14:09  lr: 0.000009  loss: 3.8767 (3.6116)  time: 0.8276  data: 0.0006  max mem: 19734
Epoch: [3]  [ 230/1251]  eta: 0:14:02  lr: 0.000009  loss: 3.8157 (3.6160)  time: 0.8405  data: 0.0005  max mem: 19734
Epoch: [3]  [ 240/1251]  eta: 0:13:53  lr: 0.000009  loss: 3.8485 (3.6254)  time: 0.8270  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5865, ratio_loss=0.0037, pruning_loss=0.2318, mse_loss=1.1063
Epoch: [3]  [ 250/1251]  eta: 0:13:43  lr: 0.000009  loss: 3.8799 (3.6185)  time: 0.8043  data: 0.0004  max mem: 19734
Epoch: [3]  [ 260/1251]  eta: 0:13:35  lr: 0.000009  loss: 3.5283 (3.6108)  time: 0.8051  data: 0.0004  max mem: 19734
Epoch: [3]  [ 270/1251]  eta: 0:13:26  lr: 0.000009  loss: 3.5767 (3.6150)  time: 0.8043  data: 0.0004  max mem: 19734
Epoch: [3]  [ 280/1251]  eta: 0:13:17  lr: 0.000009  loss: 3.6567 (3.6115)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [3]  [ 290/1251]  eta: 0:13:08  lr: 0.000009  loss: 3.7887 (3.6219)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [3]  [ 300/1251]  eta: 0:12:59  lr: 0.000009  loss: 3.8582 (3.6291)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [3]  [ 310/1251]  eta: 0:12:50  lr: 0.000009  loss: 3.8582 (3.6340)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [3]  [ 320/1251]  eta: 0:12:42  lr: 0.000009  loss: 3.4169 (3.6193)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [3]  [ 330/1251]  eta: 0:12:33  lr: 0.000009  loss: 3.1210 (3.6086)  time: 0.7988  data: 0.0004  max mem: 19734
Epoch: [3]  [ 340/1251]  eta: 0:12:24  lr: 0.000009  loss: 3.4015 (3.6103)  time: 0.8028  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5651, ratio_loss=0.0037, pruning_loss=0.2362, mse_loss=1.1403
Epoch: [3]  [ 350/1251]  eta: 0:12:16  lr: 0.000009  loss: 3.5842 (3.6060)  time: 0.8040  data: 0.0007  max mem: 19734
Epoch: [3]  [ 360/1251]  eta: 0:12:08  lr: 0.000009  loss: 3.6520 (3.6070)  time: 0.8134  data: 0.0005  max mem: 19734
Epoch: [3]  [ 370/1251]  eta: 0:12:00  lr: 0.000009  loss: 3.6520 (3.6060)  time: 0.8335  data: 0.0005  max mem: 19734
Epoch: [3]  [ 380/1251]  eta: 0:11:52  lr: 0.000009  loss: 3.4504 (3.6027)  time: 0.8316  data: 0.0005  max mem: 19734
Epoch: [3]  [ 390/1251]  eta: 0:11:44  lr: 0.000009  loss: 3.5974 (3.6051)  time: 0.8113  data: 0.0005  max mem: 19734
Epoch: [3]  [ 400/1251]  eta: 0:11:35  lr: 0.000009  loss: 3.6847 (3.6057)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [3]  [ 410/1251]  eta: 0:11:27  lr: 0.000009  loss: 3.6959 (3.6059)  time: 0.8120  data: 0.0004  max mem: 19734
Epoch: [3]  [ 420/1251]  eta: 0:11:19  lr: 0.000009  loss: 3.8476 (3.6123)  time: 0.8120  data: 0.0005  max mem: 19734
Epoch: [3]  [ 430/1251]  eta: 0:11:10  lr: 0.000009  loss: 3.8963 (3.6169)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [3]  [ 440/1251]  eta: 0:11:02  lr: 0.000009  loss: 3.5536 (3.6068)  time: 0.8057  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5788, ratio_loss=0.0038, pruning_loss=0.2317, mse_loss=1.1194
Epoch: [3]  [ 450/1251]  eta: 0:10:53  lr: 0.000009  loss: 3.4342 (3.6094)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [3]  [ 460/1251]  eta: 0:10:45  lr: 0.000009  loss: 3.8198 (3.6130)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [3]  [ 470/1251]  eta: 0:10:37  lr: 0.000009  loss: 3.8198 (3.6139)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [3]  [ 480/1251]  eta: 0:10:28  lr: 0.000009  loss: 3.7664 (3.6081)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [3]  [ 490/1251]  eta: 0:10:20  lr: 0.000009  loss: 3.7288 (3.6105)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [3]  [ 500/1251]  eta: 0:10:12  lr: 0.000009  loss: 3.7581 (3.6113)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [3]  [ 510/1251]  eta: 0:10:03  lr: 0.000009  loss: 3.5069 (3.6086)  time: 0.8062  data: 0.0004  max mem: 19734
Epoch: [3]  [ 520/1251]  eta: 0:09:56  lr: 0.000009  loss: 3.5069 (3.6084)  time: 0.8391  data: 0.0004  max mem: 19734
Epoch: [3]  [ 530/1251]  eta: 0:09:48  lr: 0.000009  loss: 3.6668 (3.6058)  time: 0.8412  data: 0.0005  max mem: 19734
Epoch: [3]  [ 540/1251]  eta: 0:09:40  lr: 0.000009  loss: 3.6714 (3.6068)  time: 0.8127  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5611, ratio_loss=0.0037, pruning_loss=0.2335, mse_loss=1.1941
Epoch: [3]  [ 550/1251]  eta: 0:09:31  lr: 0.000009  loss: 3.7144 (3.6021)  time: 0.8120  data: 0.0005  max mem: 19734
Epoch: [3]  [ 560/1251]  eta: 0:09:23  lr: 0.000009  loss: 3.7453 (3.6037)  time: 0.8076  data: 0.0004  max mem: 19734
Epoch: [3]  [ 570/1251]  eta: 0:09:15  lr: 0.000009  loss: 3.7864 (3.6095)  time: 0.8000  data: 0.0005  max mem: 19734
Epoch: [3]  [ 580/1251]  eta: 0:09:06  lr: 0.000009  loss: 3.7300 (3.6050)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [3]  [ 590/1251]  eta: 0:08:58  lr: 0.000009  loss: 3.4621 (3.6009)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [3]  [ 600/1251]  eta: 0:08:50  lr: 0.000009  loss: 3.8406 (3.6014)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [3]  [ 610/1251]  eta: 0:08:42  lr: 0.000009  loss: 3.8022 (3.5999)  time: 0.8074  data: 0.0005  max mem: 19734
Epoch: [3]  [ 620/1251]  eta: 0:08:33  lr: 0.000009  loss: 3.8103 (3.6043)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [3]  [ 630/1251]  eta: 0:08:25  lr: 0.000009  loss: 3.8103 (3.6020)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [3]  [ 640/1251]  eta: 0:08:17  lr: 0.000009  loss: 3.7623 (3.6043)  time: 0.8017  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5808, ratio_loss=0.0039, pruning_loss=0.2326, mse_loss=1.1831
Epoch: [3]  [ 650/1251]  eta: 0:08:09  lr: 0.000009  loss: 3.9072 (3.6084)  time: 0.8080  data: 0.0004  max mem: 19734
Epoch: [3]  [ 660/1251]  eta: 0:08:01  lr: 0.000009  loss: 3.9072 (3.6081)  time: 0.8306  data: 0.0004  max mem: 19734
Epoch: [3]  [ 670/1251]  eta: 0:07:53  lr: 0.000009  loss: 3.7812 (3.6091)  time: 0.8515  data: 0.0004  max mem: 19734
Epoch: [3]  [ 680/1251]  eta: 0:07:45  lr: 0.000009  loss: 3.7812 (3.6084)  time: 0.8285  data: 0.0004  max mem: 19734
Epoch: [3]  [ 690/1251]  eta: 0:07:37  lr: 0.000009  loss: 3.7083 (3.6086)  time: 0.8023  data: 0.0004  max mem: 19734
Epoch: [3]  [ 700/1251]  eta: 0:07:28  lr: 0.000009  loss: 3.8906 (3.6075)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [3]  [ 710/1251]  eta: 0:07:20  lr: 0.000009  loss: 3.8194 (3.6062)  time: 0.8086  data: 0.0004  max mem: 19734
Epoch: [3]  [ 720/1251]  eta: 0:07:12  lr: 0.000009  loss: 3.8911 (3.6125)  time: 0.8007  data: 0.0004  max mem: 19734
Epoch: [3]  [ 730/1251]  eta: 0:07:04  lr: 0.000009  loss: 3.8718 (3.6126)  time: 0.8006  data: 0.0004  max mem: 19734
Epoch: [3]  [ 740/1251]  eta: 0:06:55  lr: 0.000009  loss: 3.4858 (3.6079)  time: 0.8025  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5816, ratio_loss=0.0039, pruning_loss=0.2309, mse_loss=1.1531
Epoch: [3]  [ 750/1251]  eta: 0:06:47  lr: 0.000009  loss: 3.3533 (3.6075)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [3]  [ 760/1251]  eta: 0:06:39  lr: 0.000009  loss: 3.8318 (3.6098)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [3]  [ 770/1251]  eta: 0:06:31  lr: 0.000009  loss: 3.8318 (3.6094)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [3]  [ 780/1251]  eta: 0:06:23  lr: 0.000009  loss: 3.7052 (3.6093)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [3]  [ 790/1251]  eta: 0:06:14  lr: 0.000009  loss: 3.7126 (3.6087)  time: 0.8016  data: 0.0004  max mem: 19734
Epoch: [3]  [ 800/1251]  eta: 0:06:06  lr: 0.000009  loss: 3.5566 (3.6078)  time: 0.8103  data: 0.0005  max mem: 19734
Epoch: [3]  [ 810/1251]  eta: 0:05:58  lr: 0.000009  loss: 3.7128 (3.6100)  time: 0.8398  data: 0.0005  max mem: 19734
Epoch: [3]  [ 820/1251]  eta: 0:05:50  lr: 0.000009  loss: 3.7702 (3.6102)  time: 0.8490  data: 0.0005  max mem: 19734
Epoch: [3]  [ 830/1251]  eta: 0:05:42  lr: 0.000009  loss: 3.7702 (3.6109)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [3]  [ 840/1251]  eta: 0:05:34  lr: 0.000009  loss: 3.8967 (3.6136)  time: 0.8003  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6190, ratio_loss=0.0039, pruning_loss=0.2285, mse_loss=1.1373
Epoch: [3]  [ 850/1251]  eta: 0:05:26  lr: 0.000009  loss: 3.7648 (3.6144)  time: 0.8044  data: 0.0004  max mem: 19734
Epoch: [3]  [ 860/1251]  eta: 0:05:18  lr: 0.000009  loss: 3.6169 (3.6124)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [3]  [ 870/1251]  eta: 0:05:09  lr: 0.000009  loss: 3.5691 (3.6139)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [3]  [ 880/1251]  eta: 0:05:01  lr: 0.000009  loss: 3.7670 (3.6139)  time: 0.8029  data: 0.0004  max mem: 19734
Epoch: [3]  [ 890/1251]  eta: 0:04:53  lr: 0.000009  loss: 3.7734 (3.6156)  time: 0.7983  data: 0.0005  max mem: 19734
Epoch: [3]  [ 900/1251]  eta: 0:04:45  lr: 0.000009  loss: 3.7734 (3.6165)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [3]  [ 910/1251]  eta: 0:04:37  lr: 0.000009  loss: 3.6537 (3.6173)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [3]  [ 920/1251]  eta: 0:04:29  lr: 0.000009  loss: 3.6537 (3.6185)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [3]  [ 930/1251]  eta: 0:04:20  lr: 0.000009  loss: 3.7737 (3.6197)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [3]  [ 940/1251]  eta: 0:04:12  lr: 0.000009  loss: 3.9588 (3.6220)  time: 0.8035  data: 0.0006  max mem: 19734
loss info: cls_loss=3.6849, ratio_loss=0.0038, pruning_loss=0.2242, mse_loss=1.0813
Epoch: [3]  [ 950/1251]  eta: 0:04:04  lr: 0.000009  loss: 3.8406 (3.6243)  time: 0.8204  data: 0.0004  max mem: 19734
Epoch: [3]  [ 960/1251]  eta: 0:03:56  lr: 0.000009  loss: 3.6284 (3.6234)  time: 0.8386  data: 0.0005  max mem: 19734
Epoch: [3]  [ 970/1251]  eta: 0:03:48  lr: 0.000009  loss: 3.8043 (3.6249)  time: 0.8296  data: 0.0005  max mem: 19734
Epoch: [3]  [ 980/1251]  eta: 0:03:40  lr: 0.000009  loss: 3.8163 (3.6230)  time: 0.8105  data: 0.0004  max mem: 19734
Epoch: [3]  [ 990/1251]  eta: 0:03:32  lr: 0.000009  loss: 3.7025 (3.6236)  time: 0.8088  data: 0.0004  max mem: 19734
Epoch: [3]  [1000/1251]  eta: 0:03:24  lr: 0.000009  loss: 3.8229 (3.6240)  time: 0.8093  data: 0.0005  max mem: 19734
Epoch: [3]  [1010/1251]  eta: 0:03:15  lr: 0.000009  loss: 3.8229 (3.6243)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [3]  [1020/1251]  eta: 0:03:07  lr: 0.000009  loss: 3.7004 (3.6211)  time: 0.8003  data: 0.0004  max mem: 19734
Epoch: [3]  [1030/1251]  eta: 0:02:59  lr: 0.000009  loss: 3.5896 (3.6207)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [3]  [1040/1251]  eta: 0:02:51  lr: 0.000009  loss: 3.7671 (3.6211)  time: 0.8000  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5615, ratio_loss=0.0038, pruning_loss=0.2295, mse_loss=1.1481
Epoch: [3]  [1050/1251]  eta: 0:02:43  lr: 0.000009  loss: 3.8088 (3.6214)  time: 0.8013  data: 0.0004  max mem: 19734
Epoch: [3]  [1060/1251]  eta: 0:02:35  lr: 0.000009  loss: 3.6015 (3.6210)  time: 0.8067  data: 0.0005  max mem: 19734
Epoch: [3]  [1070/1251]  eta: 0:02:27  lr: 0.000009  loss: 3.4778 (3.6187)  time: 0.8082  data: 0.0006  max mem: 19734
Epoch: [3]  [1080/1251]  eta: 0:02:18  lr: 0.000009  loss: 3.7072 (3.6195)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [3]  [1090/1251]  eta: 0:02:10  lr: 0.000009  loss: 3.7290 (3.6196)  time: 0.8055  data: 0.0005  max mem: 19734
Epoch: [3]  [1100/1251]  eta: 0:02:02  lr: 0.000009  loss: 3.6381 (3.6175)  time: 0.8260  data: 0.0005  max mem: 19734
Epoch: [3]  [1110/1251]  eta: 0:01:54  lr: 0.000009  loss: 3.3564 (3.6138)  time: 0.8420  data: 0.0005  max mem: 19734
Epoch: [3]  [1120/1251]  eta: 0:01:46  lr: 0.000009  loss: 3.3564 (3.6121)  time: 0.8236  data: 0.0005  max mem: 19734
Epoch: [3]  [1130/1251]  eta: 0:01:38  lr: 0.000009  loss: 3.5396 (3.6115)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [3]  [1140/1251]  eta: 0:01:30  lr: 0.000009  loss: 3.6881 (3.6124)  time: 0.8058  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4834, ratio_loss=0.0040, pruning_loss=0.2306, mse_loss=1.1527
Epoch: [3]  [1150/1251]  eta: 0:01:22  lr: 0.000009  loss: 3.6465 (3.6112)  time: 0.8041  data: 0.0007  max mem: 19734
Epoch: [3]  [1160/1251]  eta: 0:01:13  lr: 0.000009  loss: 3.4399 (3.6109)  time: 0.8001  data: 0.0006  max mem: 19734
Epoch: [3]  [1170/1251]  eta: 0:01:05  lr: 0.000009  loss: 3.7473 (3.6115)  time: 0.8027  data: 0.0004  max mem: 19734
Epoch: [3]  [1180/1251]  eta: 0:00:57  lr: 0.000009  loss: 3.8105 (3.6117)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [3]  [1190/1251]  eta: 0:00:49  lr: 0.000009  loss: 3.8119 (3.6127)  time: 0.7987  data: 0.0007  max mem: 19734
Epoch: [3]  [1200/1251]  eta: 0:00:41  lr: 0.000009  loss: 3.7705 (3.6134)  time: 0.7940  data: 0.0005  max mem: 19734
Epoch: [3]  [1210/1251]  eta: 0:00:33  lr: 0.000009  loss: 3.5886 (3.6111)  time: 0.7915  data: 0.0002  max mem: 19734
Epoch: [3]  [1220/1251]  eta: 0:00:25  lr: 0.000009  loss: 3.5886 (3.6124)  time: 0.7919  data: 0.0001  max mem: 19734
Epoch: [3]  [1230/1251]  eta: 0:00:17  lr: 0.000009  loss: 3.9717 (3.6156)  time: 0.7918  data: 0.0001  max mem: 19734
Epoch: [3]  [1240/1251]  eta: 0:00:08  lr: 0.000009  loss: 3.9161 (3.6150)  time: 0.7964  data: 0.0002  max mem: 19734
loss info: cls_loss=3.6491, ratio_loss=0.0039, pruning_loss=0.2251, mse_loss=1.0956
Epoch: [3]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 3.7818 (3.6168)  time: 0.8151  data: 0.0001  max mem: 19734
Epoch: [3] Total time: 0:16:56 (0.8125 s / it)
Averaged stats: lr: 0.000009  loss: 3.7818 (3.6214)
Test:  [  0/261]  eta: 3:03:31  loss: 1.0805 (1.0805)  acc1: 81.7708 (81.7708)  acc5: 93.7500 (93.7500)  time: 42.1890  data: 41.8905  max mem: 19734
Test:  [ 10/261]  eta: 0:17:04  loss: 1.0805 (1.1534)  acc1: 82.8125 (81.0133)  acc5: 93.7500 (93.4659)  time: 4.0816  data: 3.8166  max mem: 19734
Test:  [ 20/261]  eta: 0:09:50  loss: 1.2786 (1.2561)  acc1: 75.5208 (76.5377)  acc5: 92.1875 (92.8075)  time: 0.4627  data: 0.0204  max mem: 19734
Test:  [ 30/261]  eta: 0:06:58  loss: 1.1592 (1.1861)  acc1: 81.2500 (79.2507)  acc5: 93.2292 (93.1956)  time: 0.5619  data: 0.0222  max mem: 19734
Test:  [ 40/261]  eta: 0:05:55  loss: 1.0124 (1.1589)  acc1: 83.8542 (79.9670)  acc5: 93.7500 (93.2546)  time: 0.7241  data: 0.3264  max mem: 19734
Test:  [ 50/261]  eta: 0:04:52  loss: 1.3453 (1.2340)  acc1: 73.9583 (77.4918)  acc5: 92.1875 (92.5858)  time: 0.7275  data: 0.3302  max mem: 19734
Test:  [ 60/261]  eta: 0:04:06  loss: 1.4114 (1.2463)  acc1: 70.3125 (76.4174)  acc5: 92.1875 (92.6827)  time: 0.4429  data: 0.0184  max mem: 19734
Test:  [ 70/261]  eta: 0:03:36  loss: 1.2731 (1.2422)  acc1: 70.3125 (75.9096)  acc5: 94.2708 (93.0311)  time: 0.4831  data: 0.1566  max mem: 19734
Test:  [ 80/261]  eta: 0:03:07  loss: 1.1997 (1.2459)  acc1: 78.6458 (76.1896)  acc5: 94.7917 (93.2099)  time: 0.4479  data: 0.1552  max mem: 19734
Test:  [ 90/261]  eta: 0:02:42  loss: 1.2058 (1.2366)  acc1: 78.6458 (76.6026)  acc5: 93.7500 (93.3207)  time: 0.3124  data: 0.0137  max mem: 19734
Test:  [100/261]  eta: 0:02:27  loss: 1.2596 (1.2416)  acc1: 78.6458 (76.5573)  acc5: 93.2292 (93.3220)  time: 0.4358  data: 0.1040  max mem: 19734
Test:  [110/261]  eta: 0:02:10  loss: 1.4163 (1.2738)  acc1: 71.8750 (76.1121)  acc5: 92.1875 (92.9007)  time: 0.4743  data: 0.1067  max mem: 19734
Test:  [120/261]  eta: 0:01:56  loss: 1.6187 (1.3127)  acc1: 66.6667 (75.1636)  acc5: 85.9375 (92.2779)  time: 0.3910  data: 0.1131  max mem: 19734
Test:  [130/261]  eta: 0:01:45  loss: 1.8017 (1.3618)  acc1: 61.4583 (74.2009)  acc5: 83.3333 (91.5633)  time: 0.4758  data: 0.2544  max mem: 19734
Test:  [140/261]  eta: 0:01:32  loss: 1.7087 (1.3872)  acc1: 63.5417 (73.5631)  acc5: 85.9375 (91.2382)  time: 0.3601  data: 0.1765  max mem: 19734
Test:  [150/261]  eta: 0:01:20  loss: 1.5996 (1.3975)  acc1: 68.2292 (73.5099)  acc5: 86.9792 (90.9941)  time: 0.1867  data: 0.0511  max mem: 19734
Test:  [160/261]  eta: 0:01:09  loss: 1.4934 (1.4210)  acc1: 73.4375 (73.1625)  acc5: 86.9792 (90.6832)  time: 0.1507  data: 0.0258  max mem: 19734
Test:  [170/261]  eta: 0:00:59  loss: 1.9353 (1.4558)  acc1: 59.8958 (72.2831)  acc5: 83.3333 (90.2504)  time: 0.1158  data: 0.0007  max mem: 19734
Test:  [180/261]  eta: 0:00:50  loss: 1.9230 (1.4752)  acc1: 59.8958 (71.9038)  acc5: 84.3750 (90.0553)  time: 0.1172  data: 0.0012  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.8071 (1.4941)  acc1: 63.5417 (71.6078)  acc5: 86.4583 (89.8151)  time: 0.1170  data: 0.0014  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.8023 (1.5096)  acc1: 66.1458 (71.3075)  acc5: 86.4583 (89.5885)  time: 0.1150  data: 0.0007  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.8188 (1.5275)  acc1: 64.5833 (70.9963)  acc5: 84.8958 (89.3686)  time: 0.1150  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.9933 (1.5462)  acc1: 61.9792 (70.4916)  acc5: 83.3333 (89.1191)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.9225 (1.5586)  acc1: 60.4167 (70.1659)  acc5: 84.3750 (88.9272)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.8281 (1.5671)  acc1: 63.5417 (69.9235)  acc5: 84.8958 (88.8594)  time: 0.1150  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.4397 (1.5554)  acc1: 68.7500 (70.1340)  acc5: 90.1042 (89.0065)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.3138 (1.5528)  acc1: 71.8750 (70.1700)  acc5: 93.2292 (89.1020)  time: 0.1116  data: 0.0001  max mem: 19734
Test: Total time: 0:02:02 (0.4684 s / it)
* Acc@1 70.170 Acc@5 89.102 loss 1.553
Accuracy of the network on the 50000 test images: 70.2%
Max accuracy: 70.17%
Epoch: [4]  [   0/1251]  eta: 3:10:51  lr: 0.000012  loss: 4.2696 (4.2696)  time: 9.1538  data: 8.2424  max mem: 19734
Epoch: [4]  [  10/1251]  eta: 0:35:59  lr: 0.000012  loss: 3.7915 (3.7025)  time: 1.7403  data: 0.8199  max mem: 19734
Epoch: [4]  [  20/1251]  eta: 0:26:34  lr: 0.000012  loss: 3.7754 (3.6155)  time: 0.9022  data: 0.0391  max mem: 19734
Epoch: [4]  [  30/1251]  eta: 0:23:07  lr: 0.000012  loss: 3.6037 (3.5147)  time: 0.8043  data: 0.0006  max mem: 19734
Epoch: [4]  [  40/1251]  eta: 0:21:17  lr: 0.000012  loss: 3.6396 (3.5628)  time: 0.8035  data: 0.0006  max mem: 19734
Epoch: [4]  [  50/1251]  eta: 0:20:07  lr: 0.000012  loss: 3.6968 (3.5508)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [4]  [  60/1251]  eta: 0:19:17  lr: 0.000012  loss: 3.7595 (3.5800)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [4]  [  70/1251]  eta: 0:18:40  lr: 0.000012  loss: 3.7499 (3.5502)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [4]  [  80/1251]  eta: 0:18:10  lr: 0.000012  loss: 3.5932 (3.5356)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [4]  [  90/1251]  eta: 0:17:45  lr: 0.000012  loss: 3.4986 (3.5187)  time: 0.8098  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4875, ratio_loss=0.0039, pruning_loss=0.2288, mse_loss=1.1294
Epoch: [4]  [ 100/1251]  eta: 0:17:23  lr: 0.000012  loss: 3.6166 (3.5407)  time: 0.8097  data: 0.0005  max mem: 19734
Epoch: [4]  [ 110/1251]  eta: 0:17:04  lr: 0.000012  loss: 3.6772 (3.5382)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [4]  [ 120/1251]  eta: 0:16:46  lr: 0.000012  loss: 3.3632 (3.5334)  time: 0.8062  data: 0.0005  max mem: 19734
Epoch: [4]  [ 130/1251]  eta: 0:16:36  lr: 0.000012  loss: 3.6052 (3.5485)  time: 0.8431  data: 0.0004  max mem: 19734
Epoch: [4]  [ 140/1251]  eta: 0:16:26  lr: 0.000012  loss: 3.8798 (3.5691)  time: 0.8713  data: 0.0004  max mem: 19734
Epoch: [4]  [ 150/1251]  eta: 0:16:10  lr: 0.000012  loss: 3.8138 (3.5635)  time: 0.8338  data: 0.0004  max mem: 19734
Epoch: [4]  [ 160/1251]  eta: 0:15:56  lr: 0.000012  loss: 3.6710 (3.5813)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [4]  [ 170/1251]  eta: 0:15:43  lr: 0.000012  loss: 3.6840 (3.5731)  time: 0.8008  data: 0.0004  max mem: 19734
Epoch: [4]  [ 180/1251]  eta: 0:15:29  lr: 0.000012  loss: 3.7207 (3.5805)  time: 0.7996  data: 0.0004  max mem: 19734
Epoch: [4]  [ 190/1251]  eta: 0:15:17  lr: 0.000012  loss: 3.6193 (3.5685)  time: 0.8002  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5991, ratio_loss=0.0040, pruning_loss=0.2245, mse_loss=1.1213
Epoch: [4]  [ 200/1251]  eta: 0:15:05  lr: 0.000012  loss: 3.6704 (3.5804)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [4]  [ 210/1251]  eta: 0:14:54  lr: 0.000012  loss: 3.9797 (3.5915)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [4]  [ 220/1251]  eta: 0:14:43  lr: 0.000012  loss: 3.8981 (3.5964)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [4]  [ 230/1251]  eta: 0:14:32  lr: 0.000012  loss: 3.7935 (3.5906)  time: 0.8066  data: 0.0006  max mem: 19734
Epoch: [4]  [ 240/1251]  eta: 0:14:21  lr: 0.000012  loss: 3.9409 (3.6083)  time: 0.8046  data: 0.0008  max mem: 19734
Epoch: [4]  [ 250/1251]  eta: 0:14:11  lr: 0.000012  loss: 3.9550 (3.6110)  time: 0.8073  data: 0.0007  max mem: 19734
Epoch: [4]  [ 260/1251]  eta: 0:14:01  lr: 0.000012  loss: 3.7783 (3.6120)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [4]  [ 270/1251]  eta: 0:13:51  lr: 0.000012  loss: 3.7783 (3.6132)  time: 0.8084  data: 0.0005  max mem: 19734
Epoch: [4]  [ 280/1251]  eta: 0:13:44  lr: 0.000012  loss: 3.9126 (3.6188)  time: 0.8471  data: 0.0004  max mem: 19734
Epoch: [4]  [ 290/1251]  eta: 0:13:34  lr: 0.000012  loss: 3.8983 (3.6158)  time: 0.8516  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6817, ratio_loss=0.0042, pruning_loss=0.2229, mse_loss=1.0651
Epoch: [4]  [ 300/1251]  eta: 0:13:24  lr: 0.000012  loss: 3.7103 (3.6238)  time: 0.8132  data: 0.0004  max mem: 19734
Epoch: [4]  [ 310/1251]  eta: 0:13:15  lr: 0.000012  loss: 3.7044 (3.6155)  time: 0.8017  data: 0.0004  max mem: 19734
Epoch: [4]  [ 320/1251]  eta: 0:13:05  lr: 0.000012  loss: 3.4729 (3.6107)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [4]  [ 330/1251]  eta: 0:12:56  lr: 0.000012  loss: 3.4259 (3.6028)  time: 0.8079  data: 0.0006  max mem: 19734
Epoch: [4]  [ 340/1251]  eta: 0:12:46  lr: 0.000012  loss: 3.7982 (3.6072)  time: 0.8123  data: 0.0005  max mem: 19734
Epoch: [4]  [ 350/1251]  eta: 0:12:37  lr: 0.000012  loss: 3.9009 (3.6104)  time: 0.8122  data: 0.0006  max mem: 19734
Epoch: [4]  [ 360/1251]  eta: 0:12:28  lr: 0.000012  loss: 3.7661 (3.6084)  time: 0.8130  data: 0.0006  max mem: 19734
Epoch: [4]  [ 370/1251]  eta: 0:12:19  lr: 0.000012  loss: 3.4860 (3.6073)  time: 0.8108  data: 0.0006  max mem: 19734
Epoch: [4]  [ 380/1251]  eta: 0:12:10  lr: 0.000012  loss: 3.5254 (3.6036)  time: 0.8081  data: 0.0006  max mem: 19734
Epoch: [4]  [ 390/1251]  eta: 0:12:01  lr: 0.000012  loss: 3.6555 (3.6014)  time: 0.8057  data: 0.0007  max mem: 19734
loss info: cls_loss=3.5309, ratio_loss=0.0043, pruning_loss=0.2286, mse_loss=1.1247
Epoch: [4]  [ 400/1251]  eta: 0:11:52  lr: 0.000012  loss: 3.7715 (3.6050)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [4]  [ 410/1251]  eta: 0:11:43  lr: 0.000012  loss: 3.7573 (3.6072)  time: 0.8097  data: 0.0006  max mem: 19734
Epoch: [4]  [ 420/1251]  eta: 0:11:35  lr: 0.000012  loss: 3.6328 (3.6023)  time: 0.8373  data: 0.0007  max mem: 19734
Epoch: [4]  [ 430/1251]  eta: 0:11:27  lr: 0.000012  loss: 3.7081 (3.6027)  time: 0.8681  data: 0.0005  max mem: 19734
Epoch: [4]  [ 440/1251]  eta: 0:11:18  lr: 0.000012  loss: 3.5134 (3.6004)  time: 0.8393  data: 0.0004  max mem: 19734
Epoch: [4]  [ 450/1251]  eta: 0:11:09  lr: 0.000012  loss: 3.5134 (3.6011)  time: 0.8021  data: 0.0004  max mem: 19734
Epoch: [4]  [ 460/1251]  eta: 0:11:00  lr: 0.000012  loss: 3.8031 (3.6046)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [4]  [ 470/1251]  eta: 0:10:52  lr: 0.000012  loss: 3.9527 (3.6081)  time: 0.8081  data: 0.0004  max mem: 19734
Epoch: [4]  [ 480/1251]  eta: 0:10:43  lr: 0.000012  loss: 3.5004 (3.6017)  time: 0.8089  data: 0.0004  max mem: 19734
Epoch: [4]  [ 490/1251]  eta: 0:10:34  lr: 0.000012  loss: 3.5860 (3.6054)  time: 0.8055  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5647, ratio_loss=0.0041, pruning_loss=0.2262, mse_loss=1.1387
Epoch: [4]  [ 500/1251]  eta: 0:10:25  lr: 0.000012  loss: 3.7160 (3.6057)  time: 0.8050  data: 0.0004  max mem: 19734
Epoch: [4]  [ 510/1251]  eta: 0:10:16  lr: 0.000012  loss: 3.4192 (3.6020)  time: 0.8035  data: 0.0004  max mem: 19734
Epoch: [4]  [ 520/1251]  eta: 0:10:08  lr: 0.000012  loss: 3.6714 (3.6043)  time: 0.8022  data: 0.0004  max mem: 19734
Epoch: [4]  [ 530/1251]  eta: 0:09:59  lr: 0.000012  loss: 3.4948 (3.5967)  time: 0.8050  data: 0.0004  max mem: 19734
Epoch: [4]  [ 540/1251]  eta: 0:09:50  lr: 0.000012  loss: 3.5054 (3.5955)  time: 0.8029  data: 0.0004  max mem: 19734
Epoch: [4]  [ 550/1251]  eta: 0:09:42  lr: 0.000012  loss: 3.7699 (3.5936)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [4]  [ 560/1251]  eta: 0:09:33  lr: 0.000012  loss: 3.7117 (3.5949)  time: 0.8047  data: 0.0006  max mem: 19734
Epoch: [4]  [ 570/1251]  eta: 0:09:25  lr: 0.000012  loss: 3.5292 (3.5916)  time: 0.8393  data: 0.0005  max mem: 19734
Epoch: [4]  [ 580/1251]  eta: 0:09:17  lr: 0.000012  loss: 3.6346 (3.5927)  time: 0.8641  data: 0.0004  max mem: 19734
Epoch: [4]  [ 590/1251]  eta: 0:09:09  lr: 0.000012  loss: 3.8150 (3.5968)  time: 0.8284  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5326, ratio_loss=0.0043, pruning_loss=0.2248, mse_loss=1.0570
Epoch: [4]  [ 600/1251]  eta: 0:09:00  lr: 0.000012  loss: 3.8150 (3.5936)  time: 0.8054  data: 0.0004  max mem: 19734
Epoch: [4]  [ 610/1251]  eta: 0:08:51  lr: 0.000012  loss: 3.6209 (3.5931)  time: 0.8040  data: 0.0004  max mem: 19734
Epoch: [4]  [ 620/1251]  eta: 0:08:43  lr: 0.000012  loss: 3.7442 (3.5934)  time: 0.8056  data: 0.0004  max mem: 19734
Epoch: [4]  [ 630/1251]  eta: 0:08:34  lr: 0.000012  loss: 3.9051 (3.5992)  time: 0.8062  data: 0.0004  max mem: 19734
Epoch: [4]  [ 640/1251]  eta: 0:08:26  lr: 0.000012  loss: 3.9051 (3.5983)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [4]  [ 650/1251]  eta: 0:08:17  lr: 0.000012  loss: 3.6426 (3.5976)  time: 0.8028  data: 0.0004  max mem: 19734
Epoch: [4]  [ 660/1251]  eta: 0:08:09  lr: 0.000012  loss: 3.4635 (3.5955)  time: 0.8057  data: 0.0004  max mem: 19734
Epoch: [4]  [ 670/1251]  eta: 0:08:00  lr: 0.000012  loss: 3.4635 (3.5939)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [4]  [ 680/1251]  eta: 0:07:52  lr: 0.000012  loss: 3.5274 (3.5956)  time: 0.8019  data: 0.0006  max mem: 19734
Epoch: [4]  [ 690/1251]  eta: 0:07:43  lr: 0.000012  loss: 3.7237 (3.5959)  time: 0.8027  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5622, ratio_loss=0.0041, pruning_loss=0.2228, mse_loss=1.0805
Epoch: [4]  [ 700/1251]  eta: 0:07:35  lr: 0.000012  loss: 3.8088 (3.5977)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [4]  [ 710/1251]  eta: 0:07:27  lr: 0.000012  loss: 3.8088 (3.5955)  time: 0.8200  data: 0.0005  max mem: 19734
Epoch: [4]  [ 720/1251]  eta: 0:07:19  lr: 0.000012  loss: 3.3585 (3.5915)  time: 0.8448  data: 0.0005  max mem: 19734
Epoch: [4]  [ 730/1251]  eta: 0:07:10  lr: 0.000012  loss: 3.4191 (3.5902)  time: 0.8378  data: 0.0005  max mem: 19734
Epoch: [4]  [ 740/1251]  eta: 0:07:02  lr: 0.000012  loss: 3.5338 (3.5889)  time: 0.8149  data: 0.0005  max mem: 19734
Epoch: [4]  [ 750/1251]  eta: 0:06:54  lr: 0.000012  loss: 3.6296 (3.5910)  time: 0.8045  data: 0.0004  max mem: 19734
Epoch: [4]  [ 760/1251]  eta: 0:06:45  lr: 0.000012  loss: 3.9230 (3.5934)  time: 0.8016  data: 0.0004  max mem: 19734
Epoch: [4]  [ 770/1251]  eta: 0:06:37  lr: 0.000012  loss: 3.6401 (3.5925)  time: 0.8007  data: 0.0004  max mem: 19734
Epoch: [4]  [ 780/1251]  eta: 0:06:28  lr: 0.000012  loss: 3.6401 (3.5919)  time: 0.8007  data: 0.0004  max mem: 19734
Epoch: [4]  [ 790/1251]  eta: 0:06:20  lr: 0.000012  loss: 3.5472 (3.5890)  time: 0.8008  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5081, ratio_loss=0.0044, pruning_loss=0.2214, mse_loss=1.0966
Epoch: [4]  [ 800/1251]  eta: 0:06:11  lr: 0.000012  loss: 3.5472 (3.5893)  time: 0.8023  data: 0.0004  max mem: 19734
Epoch: [4]  [ 810/1251]  eta: 0:06:03  lr: 0.000012  loss: 3.6184 (3.5891)  time: 0.8029  data: 0.0004  max mem: 19734
Epoch: [4]  [ 820/1251]  eta: 0:05:55  lr: 0.000012  loss: 3.6542 (3.5909)  time: 0.8023  data: 0.0003  max mem: 19734
Epoch: [4]  [ 830/1251]  eta: 0:05:46  lr: 0.000012  loss: 3.7642 (3.5920)  time: 0.8023  data: 0.0004  max mem: 19734
Epoch: [4]  [ 840/1251]  eta: 0:05:38  lr: 0.000012  loss: 3.7220 (3.5922)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [4]  [ 850/1251]  eta: 0:05:30  lr: 0.000012  loss: 3.5982 (3.5906)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [4]  [ 860/1251]  eta: 0:05:22  lr: 0.000012  loss: 3.8298 (3.5950)  time: 0.8367  data: 0.0005  max mem: 19734
Epoch: [4]  [ 870/1251]  eta: 0:05:14  lr: 0.000012  loss: 3.9529 (3.5986)  time: 0.8578  data: 0.0005  max mem: 19734
Epoch: [4]  [ 880/1251]  eta: 0:05:05  lr: 0.000012  loss: 3.8902 (3.5991)  time: 0.8297  data: 0.0004  max mem: 19734
Epoch: [4]  [ 890/1251]  eta: 0:04:57  lr: 0.000012  loss: 3.6511 (3.5973)  time: 0.8056  data: 0.0003  max mem: 19734
loss info: cls_loss=3.5897, ratio_loss=0.0042, pruning_loss=0.2188, mse_loss=1.0808
Epoch: [4]  [ 900/1251]  eta: 0:04:49  lr: 0.000012  loss: 3.2949 (3.5946)  time: 0.8023  data: 0.0004  max mem: 19734
Epoch: [4]  [ 910/1251]  eta: 0:04:40  lr: 0.000012  loss: 3.6729 (3.5969)  time: 0.8006  data: 0.0004  max mem: 19734
Epoch: [4]  [ 920/1251]  eta: 0:04:32  lr: 0.000012  loss: 3.9062 (3.5977)  time: 0.8016  data: 0.0004  max mem: 19734
Epoch: [4]  [ 930/1251]  eta: 0:04:24  lr: 0.000012  loss: 3.8903 (3.5983)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [4]  [ 940/1251]  eta: 0:04:15  lr: 0.000012  loss: 3.7816 (3.5992)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [4]  [ 950/1251]  eta: 0:04:07  lr: 0.000012  loss: 3.7158 (3.5984)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [4]  [ 960/1251]  eta: 0:03:59  lr: 0.000012  loss: 3.7177 (3.5969)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [4]  [ 970/1251]  eta: 0:03:51  lr: 0.000012  loss: 3.7801 (3.5977)  time: 0.8015  data: 0.0006  max mem: 19734
Epoch: [4]  [ 980/1251]  eta: 0:03:42  lr: 0.000012  loss: 3.7568 (3.5983)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [4]  [ 990/1251]  eta: 0:03:34  lr: 0.000012  loss: 3.5745 (3.5978)  time: 0.8028  data: 0.0007  max mem: 19734
loss info: cls_loss=3.6163, ratio_loss=0.0043, pruning_loss=0.2186, mse_loss=1.1044
Epoch: [4]  [1000/1251]  eta: 0:03:26  lr: 0.000012  loss: 3.7062 (3.5984)  time: 0.8206  data: 0.0008  max mem: 19734
Epoch: [4]  [1010/1251]  eta: 0:03:18  lr: 0.000012  loss: 3.7938 (3.6004)  time: 0.8349  data: 0.0005  max mem: 19734
Epoch: [4]  [1020/1251]  eta: 0:03:09  lr: 0.000012  loss: 3.8829 (3.5994)  time: 0.8272  data: 0.0003  max mem: 19734
Epoch: [4]  [1030/1251]  eta: 0:03:01  lr: 0.000012  loss: 3.2379 (3.5961)  time: 0.8093  data: 0.0004  max mem: 19734
Epoch: [4]  [1040/1251]  eta: 0:02:53  lr: 0.000012  loss: 3.4687 (3.5955)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [4]  [1050/1251]  eta: 0:02:45  lr: 0.000012  loss: 3.5333 (3.5953)  time: 0.8019  data: 0.0006  max mem: 19734
Epoch: [4]  [1060/1251]  eta: 0:02:36  lr: 0.000012  loss: 3.7882 (3.5961)  time: 0.8022  data: 0.0006  max mem: 19734
Epoch: [4]  [1070/1251]  eta: 0:02:28  lr: 0.000012  loss: 3.8052 (3.5976)  time: 0.8074  data: 0.0006  max mem: 19734
Epoch: [4]  [1080/1251]  eta: 0:02:20  lr: 0.000012  loss: 3.6998 (3.5976)  time: 0.8079  data: 0.0005  max mem: 19734
Epoch: [4]  [1090/1251]  eta: 0:02:12  lr: 0.000012  loss: 3.6843 (3.5976)  time: 0.8037  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5600, ratio_loss=0.0044, pruning_loss=0.2187, mse_loss=1.0836
Epoch: [4]  [1100/1251]  eta: 0:02:03  lr: 0.000012  loss: 3.5746 (3.5978)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [4]  [1110/1251]  eta: 0:01:55  lr: 0.000012  loss: 3.3498 (3.5947)  time: 0.7993  data: 0.0005  max mem: 19734
Epoch: [4]  [1120/1251]  eta: 0:01:47  lr: 0.000012  loss: 3.3996 (3.5940)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [4]  [1130/1251]  eta: 0:01:39  lr: 0.000012  loss: 3.5607 (3.5935)  time: 0.8035  data: 0.0006  max mem: 19734
Epoch: [4]  [1140/1251]  eta: 0:01:31  lr: 0.000012  loss: 3.3176 (3.5901)  time: 0.8015  data: 0.0006  max mem: 19734
Epoch: [4]  [1150/1251]  eta: 0:01:22  lr: 0.000012  loss: 3.3706 (3.5902)  time: 0.8293  data: 0.0005  max mem: 19734
Epoch: [4]  [1160/1251]  eta: 0:01:14  lr: 0.000012  loss: 3.5636 (3.5889)  time: 0.8580  data: 0.0004  max mem: 19734
Epoch: [4]  [1170/1251]  eta: 0:01:06  lr: 0.000012  loss: 3.6477 (3.5906)  time: 0.8313  data: 0.0005  max mem: 19734
Epoch: [4]  [1180/1251]  eta: 0:00:58  lr: 0.000012  loss: 3.6792 (3.5900)  time: 0.8042  data: 0.0007  max mem: 19734
Epoch: [4]  [1190/1251]  eta: 0:00:50  lr: 0.000012  loss: 3.6472 (3.5897)  time: 0.7988  data: 0.0012  max mem: 19734
loss info: cls_loss=3.4833, ratio_loss=0.0046, pruning_loss=0.2223, mse_loss=1.0865
Epoch: [4]  [1200/1251]  eta: 0:00:41  lr: 0.000012  loss: 3.6557 (3.5908)  time: 0.7941  data: 0.0007  max mem: 19734
Epoch: [4]  [1210/1251]  eta: 0:00:33  lr: 0.000012  loss: 3.3611 (3.5866)  time: 0.7956  data: 0.0002  max mem: 19734
Epoch: [4]  [1220/1251]  eta: 0:00:25  lr: 0.000012  loss: 3.3611 (3.5870)  time: 0.7960  data: 0.0002  max mem: 19734
Epoch: [4]  [1230/1251]  eta: 0:00:17  lr: 0.000012  loss: 3.5678 (3.5856)  time: 0.7928  data: 0.0002  max mem: 19734
Epoch: [4]  [1240/1251]  eta: 0:00:09  lr: 0.000012  loss: 3.8507 (3.5869)  time: 0.7930  data: 0.0002  max mem: 19734
Epoch: [4]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 3.8731 (3.5893)  time: 0.7928  data: 0.0002  max mem: 19734
Epoch: [4] Total time: 0:17:05 (0.8198 s / it)
Averaged stats: lr: 0.000012  loss: 3.8731 (3.5874)
Test:  [  0/261]  eta: 1:24:35  loss: 1.3167 (1.3167)  acc1: 76.5625 (76.5625)  acc5: 90.1042 (90.1042)  time: 19.4481  data: 19.1202  max mem: 19734
Test:  [ 10/261]  eta: 0:10:01  loss: 1.1977 (1.3000)  acc1: 82.2917 (78.7405)  acc5: 90.6250 (91.5246)  time: 2.3955  data: 2.0383  max mem: 19734
Test:  [ 20/261]  eta: 0:05:30  loss: 1.3672 (1.3665)  acc1: 76.5625 (75.0992)  acc5: 90.6250 (91.6171)  time: 0.4688  data: 0.1824  max mem: 19734
Test:  [ 30/261]  eta: 0:04:20  loss: 1.3125 (1.3007)  acc1: 78.1250 (77.9570)  acc5: 91.6667 (92.0699)  time: 0.4297  data: 0.0260  max mem: 19734
Test:  [ 40/261]  eta: 0:04:14  loss: 1.1222 (1.2697)  acc1: 84.3750 (78.7856)  acc5: 92.1875 (92.1494)  time: 0.9220  data: 0.4712  max mem: 19734
Test:  [ 50/261]  eta: 0:03:24  loss: 1.3419 (1.3249)  acc1: 74.4792 (76.7361)  acc5: 91.1458 (91.4931)  time: 0.7210  data: 0.4721  max mem: 19734
Test:  [ 60/261]  eta: 0:02:59  loss: 1.4392 (1.3319)  acc1: 71.3542 (76.1697)  acc5: 90.1042 (91.5727)  time: 0.3667  data: 0.0149  max mem: 19734
Test:  [ 70/261]  eta: 0:02:34  loss: 1.2872 (1.3293)  acc1: 72.9167 (75.5282)  acc5: 92.1875 (91.8574)  time: 0.4072  data: 0.0147  max mem: 19734
Test:  [ 80/261]  eta: 0:02:13  loss: 1.3799 (1.3500)  acc1: 73.9583 (75.6752)  acc5: 92.7083 (91.9689)  time: 0.2527  data: 0.0129  max mem: 19734
Test:  [ 90/261]  eta: 0:01:57  loss: 1.3799 (1.3421)  acc1: 77.0833 (76.0302)  acc5: 92.7083 (92.0788)  time: 0.2611  data: 0.0077  max mem: 19734
Test:  [100/261]  eta: 0:01:59  loss: 1.3197 (1.3430)  acc1: 77.0833 (76.0417)  acc5: 92.1875 (92.0844)  time: 0.7535  data: 0.5158  max mem: 19734
Test:  [110/261]  eta: 0:01:44  loss: 1.4289 (1.3682)  acc1: 72.9167 (75.6522)  acc5: 91.1458 (91.7371)  time: 0.7217  data: 0.5193  max mem: 19734
Test:  [120/261]  eta: 0:01:32  loss: 1.6275 (1.3983)  acc1: 68.7500 (74.8580)  acc5: 86.9792 (91.1932)  time: 0.2529  data: 0.0132  max mem: 19734
Test:  [130/261]  eta: 0:01:23  loss: 1.8119 (1.4411)  acc1: 64.0625 (73.9186)  acc5: 83.8542 (90.5773)  time: 0.3159  data: 0.0150  max mem: 19734
Test:  [140/261]  eta: 0:01:18  loss: 1.7688 (1.4617)  acc1: 63.5417 (73.2861)  acc5: 85.4167 (90.3664)  time: 0.5568  data: 0.2980  max mem: 19734
Test:  [150/261]  eta: 0:01:08  loss: 1.6290 (1.4656)  acc1: 68.2292 (73.3064)  acc5: 88.5417 (90.2076)  time: 0.4758  data: 0.2944  max mem: 19734
Test:  [160/261]  eta: 0:00:59  loss: 1.5340 (1.4832)  acc1: 72.3958 (72.9070)  acc5: 86.9792 (89.9521)  time: 0.1865  data: 0.0119  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.9860 (1.5154)  acc1: 58.8542 (72.0090)  acc5: 83.8542 (89.5468)  time: 0.5657  data: 0.3894  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: 2.0232 (1.5357)  acc1: 59.8958 (71.5844)  acc5: 83.3333 (89.3042)  time: 0.6736  data: 0.4941  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: 1.8681 (1.5500)  acc1: 63.0208 (71.3160)  acc5: 85.4167 (89.0925)  time: 0.2860  data: 0.1173  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.8471 (1.5642)  acc1: 65.1042 (70.9681)  acc5: 84.3750 (88.8630)  time: 0.2732  data: 0.0997  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.7654 (1.5787)  acc1: 64.0625 (70.6482)  acc5: 83.8542 (88.6305)  time: 0.2503  data: 0.0933  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.9507 (1.5970)  acc1: 59.8958 (70.1900)  acc5: 84.3750 (88.4474)  time: 0.1421  data: 0.0116  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.9507 (1.6079)  acc1: 59.8958 (69.8571)  acc5: 85.4167 (88.3117)  time: 0.1274  data: 0.0115  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.8111 (1.6158)  acc1: 61.4583 (69.5842)  acc5: 86.4583 (88.2369)  time: 0.1153  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.6233 (1.6065)  acc1: 69.7917 (69.8249)  acc5: 90.1042 (88.4026)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.5140 (1.6061)  acc1: 72.3958 (69.9020)  acc5: 91.6667 (88.4640)  time: 0.1120  data: 0.0001  max mem: 19734
Test: Total time: 0:02:01 (0.4651 s / it)
* Acc@1 69.902 Acc@5 88.464 loss 1.606
Accuracy of the network on the 50000 test images: 69.9%
Max accuracy: 70.17%
Epoch: [5]  [   0/1251]  eta: 6:00:47  lr: 0.000016  loss: 3.6583 (3.6583)  time: 17.3040  data: 13.8950  max mem: 19734
Epoch: [5]  [  10/1251]  eta: 0:52:21  lr: 0.000016  loss: 3.8547 (3.8243)  time: 2.5318  data: 1.3232  max mem: 19734
Epoch: [5]  [  20/1251]  eta: 0:35:04  lr: 0.000016  loss: 3.8193 (3.7387)  time: 0.9300  data: 0.0333  max mem: 19734
Epoch: [5]  [  30/1251]  eta: 0:28:55  lr: 0.000016  loss: 3.7283 (3.6811)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [5]  [  40/1251]  eta: 0:25:57  lr: 0.000016  loss: 3.7354 (3.7422)  time: 0.8407  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6132, ratio_loss=0.0044, pruning_loss=0.2176, mse_loss=1.0761
Epoch: [5]  [  50/1251]  eta: 0:23:57  lr: 0.000016  loss: 3.8536 (3.7593)  time: 0.8484  data: 0.0006  max mem: 19734
Epoch: [5]  [  60/1251]  eta: 0:22:30  lr: 0.000016  loss: 3.8536 (3.7491)  time: 0.8222  data: 0.0006  max mem: 19734
Epoch: [5]  [  70/1251]  eta: 0:21:23  lr: 0.000016  loss: 3.8329 (3.7456)  time: 0.8055  data: 0.0005  max mem: 19734
Epoch: [5]  [  80/1251]  eta: 0:20:30  lr: 0.000016  loss: 3.8185 (3.7238)  time: 0.7981  data: 0.0005  max mem: 19734
Epoch: [5]  [  90/1251]  eta: 0:19:47  lr: 0.000016  loss: 3.6256 (3.7164)  time: 0.7977  data: 0.0005  max mem: 19734
Epoch: [5]  [ 100/1251]  eta: 0:19:11  lr: 0.000016  loss: 3.5231 (3.6746)  time: 0.7959  data: 0.0004  max mem: 19734
Epoch: [5]  [ 110/1251]  eta: 0:18:40  lr: 0.000016  loss: 3.6612 (3.6749)  time: 0.7953  data: 0.0004  max mem: 19734
Epoch: [5]  [ 120/1251]  eta: 0:18:13  lr: 0.000016  loss: 3.8039 (3.6792)  time: 0.7967  data: 0.0004  max mem: 19734
Epoch: [5]  [ 130/1251]  eta: 0:17:49  lr: 0.000016  loss: 3.7866 (3.6736)  time: 0.7979  data: 0.0005  max mem: 19734
Epoch: [5]  [ 140/1251]  eta: 0:17:27  lr: 0.000016  loss: 3.5363 (3.6656)  time: 0.8003  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6017, ratio_loss=0.0046, pruning_loss=0.2161, mse_loss=1.0431
Epoch: [5]  [ 150/1251]  eta: 0:17:08  lr: 0.000016  loss: 3.7195 (3.6646)  time: 0.8016  data: 0.0006  max mem: 19734
Epoch: [5]  [ 160/1251]  eta: 0:16:49  lr: 0.000016  loss: 3.7344 (3.6615)  time: 0.8016  data: 0.0006  max mem: 19734
Epoch: [5]  [ 170/1251]  eta: 0:16:32  lr: 0.000016  loss: 3.7810 (3.6655)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [5]  [ 180/1251]  eta: 0:16:19  lr: 0.000016  loss: 3.4869 (3.6486)  time: 0.8223  data: 0.0005  max mem: 19734
Epoch: [5]  [ 190/1251]  eta: 0:16:06  lr: 0.000016  loss: 3.4240 (3.6404)  time: 0.8463  data: 0.0005  max mem: 19734
Epoch: [5]  [ 200/1251]  eta: 0:15:52  lr: 0.000016  loss: 3.6358 (3.6280)  time: 0.8332  data: 0.0005  max mem: 19734
Epoch: [5]  [ 210/1251]  eta: 0:15:37  lr: 0.000016  loss: 3.5913 (3.6119)  time: 0.8067  data: 0.0006  max mem: 19734
Epoch: [5]  [ 220/1251]  eta: 0:15:24  lr: 0.000016  loss: 3.2597 (3.5998)  time: 0.8027  data: 0.0006  max mem: 19734
Epoch: [5]  [ 230/1251]  eta: 0:15:11  lr: 0.000016  loss: 3.5020 (3.5945)  time: 0.8063  data: 0.0007  max mem: 19734
Epoch: [5]  [ 240/1251]  eta: 0:14:59  lr: 0.000016  loss: 3.6540 (3.5926)  time: 0.8088  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4202, ratio_loss=0.0045, pruning_loss=0.2213, mse_loss=1.0926
Epoch: [5]  [ 250/1251]  eta: 0:14:47  lr: 0.000016  loss: 3.7210 (3.5851)  time: 0.8104  data: 0.0004  max mem: 19734
Epoch: [5]  [ 260/1251]  eta: 0:14:35  lr: 0.000016  loss: 3.7302 (3.5908)  time: 0.8102  data: 0.0004  max mem: 19734
Epoch: [5]  [ 270/1251]  eta: 0:14:23  lr: 0.000016  loss: 3.6991 (3.5882)  time: 0.8081  data: 0.0004  max mem: 19734
Epoch: [5]  [ 280/1251]  eta: 0:14:12  lr: 0.000016  loss: 3.7272 (3.5886)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [5]  [ 290/1251]  eta: 0:14:00  lr: 0.000016  loss: 3.7334 (3.5933)  time: 0.7996  data: 0.0006  max mem: 19734
Epoch: [5]  [ 300/1251]  eta: 0:13:49  lr: 0.000016  loss: 3.6870 (3.5925)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [5]  [ 310/1251]  eta: 0:13:38  lr: 0.000016  loss: 3.6053 (3.5922)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [5]  [ 320/1251]  eta: 0:13:28  lr: 0.000016  loss: 3.6686 (3.5918)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [5]  [ 330/1251]  eta: 0:13:19  lr: 0.000016  loss: 3.6500 (3.5883)  time: 0.8324  data: 0.0006  max mem: 19734
Epoch: [5]  [ 340/1251]  eta: 0:13:10  lr: 0.000016  loss: 3.7039 (3.5916)  time: 0.8533  data: 0.0007  max mem: 19734
loss info: cls_loss=3.5999, ratio_loss=0.0050, pruning_loss=0.2134, mse_loss=1.0406
Epoch: [5]  [ 350/1251]  eta: 0:13:00  lr: 0.000016  loss: 3.5922 (3.5836)  time: 0.8342  data: 0.0007  max mem: 19734
Epoch: [5]  [ 360/1251]  eta: 0:12:50  lr: 0.000016  loss: 3.5886 (3.5828)  time: 0.8181  data: 0.0005  max mem: 19734
Epoch: [5]  [ 370/1251]  eta: 0:12:40  lr: 0.000016  loss: 3.6750 (3.5907)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [5]  [ 380/1251]  eta: 0:12:30  lr: 0.000016  loss: 3.7574 (3.5868)  time: 0.8061  data: 0.0006  max mem: 19734
Epoch: [5]  [ 390/1251]  eta: 0:12:20  lr: 0.000016  loss: 3.6292 (3.5871)  time: 0.8044  data: 0.0006  max mem: 19734
Epoch: [5]  [ 400/1251]  eta: 0:12:10  lr: 0.000016  loss: 3.4711 (3.5810)  time: 0.8037  data: 0.0004  max mem: 19734
Epoch: [5]  [ 410/1251]  eta: 0:12:01  lr: 0.000016  loss: 3.4711 (3.5796)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [5]  [ 420/1251]  eta: 0:11:51  lr: 0.000016  loss: 3.5354 (3.5734)  time: 0.8085  data: 0.0006  max mem: 19734
Epoch: [5]  [ 430/1251]  eta: 0:11:41  lr: 0.000016  loss: 3.2502 (3.5666)  time: 0.8054  data: 0.0006  max mem: 19734
Epoch: [5]  [ 440/1251]  eta: 0:11:32  lr: 0.000016  loss: 3.4595 (3.5638)  time: 0.8036  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4540, ratio_loss=0.0048, pruning_loss=0.2186, mse_loss=1.1179
Epoch: [5]  [ 450/1251]  eta: 0:11:23  lr: 0.000016  loss: 3.7585 (3.5698)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [5]  [ 460/1251]  eta: 0:11:13  lr: 0.000016  loss: 3.7415 (3.5724)  time: 0.8068  data: 0.0006  max mem: 19734
Epoch: [5]  [ 470/1251]  eta: 0:11:04  lr: 0.000016  loss: 3.7427 (3.5776)  time: 0.8080  data: 0.0007  max mem: 19734
Epoch: [5]  [ 480/1251]  eta: 0:10:56  lr: 0.000016  loss: 3.7948 (3.5776)  time: 0.8401  data: 0.0005  max mem: 19734
Epoch: [5]  [ 490/1251]  eta: 0:10:47  lr: 0.000016  loss: 3.4481 (3.5715)  time: 0.8468  data: 0.0005  max mem: 19734
Epoch: [5]  [ 500/1251]  eta: 0:10:38  lr: 0.000016  loss: 3.4481 (3.5704)  time: 0.8229  data: 0.0005  max mem: 19734
Epoch: [5]  [ 510/1251]  eta: 0:10:29  lr: 0.000016  loss: 3.5251 (3.5666)  time: 0.8104  data: 0.0004  max mem: 19734
Epoch: [5]  [ 520/1251]  eta: 0:10:20  lr: 0.000016  loss: 3.3707 (3.5680)  time: 0.8021  data: 0.0008  max mem: 19734
Epoch: [5]  [ 530/1251]  eta: 0:10:11  lr: 0.000016  loss: 3.8658 (3.5688)  time: 0.8046  data: 0.0009  max mem: 19734
Epoch: [5]  [ 540/1251]  eta: 0:10:02  lr: 0.000016  loss: 3.8004 (3.5739)  time: 0.8078  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5456, ratio_loss=0.0049, pruning_loss=0.2159, mse_loss=1.0863
Epoch: [5]  [ 550/1251]  eta: 0:09:53  lr: 0.000016  loss: 3.7221 (3.5670)  time: 0.8103  data: 0.0004  max mem: 19734
Epoch: [5]  [ 560/1251]  eta: 0:09:44  lr: 0.000016  loss: 3.6331 (3.5671)  time: 0.8075  data: 0.0005  max mem: 19734
Epoch: [5]  [ 570/1251]  eta: 0:09:35  lr: 0.000016  loss: 3.5889 (3.5674)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [5]  [ 580/1251]  eta: 0:09:26  lr: 0.000016  loss: 3.5084 (3.5646)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [5]  [ 590/1251]  eta: 0:09:17  lr: 0.000016  loss: 3.6134 (3.5665)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [5]  [ 600/1251]  eta: 0:09:08  lr: 0.000016  loss: 3.3966 (3.5598)  time: 0.8100  data: 0.0005  max mem: 19734
Epoch: [5]  [ 610/1251]  eta: 0:08:59  lr: 0.000016  loss: 3.2034 (3.5591)  time: 0.8084  data: 0.0005  max mem: 19734
Epoch: [5]  [ 620/1251]  eta: 0:08:51  lr: 0.000016  loss: 3.5066 (3.5624)  time: 0.8235  data: 0.0005  max mem: 19734
Epoch: [5]  [ 630/1251]  eta: 0:08:43  lr: 0.000016  loss: 3.8024 (3.5614)  time: 0.8455  data: 0.0005  max mem: 19734
Epoch: [5]  [ 640/1251]  eta: 0:08:34  lr: 0.000016  loss: 3.7324 (3.5611)  time: 0.8244  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4787, ratio_loss=0.0052, pruning_loss=0.2182, mse_loss=1.0287
Epoch: [5]  [ 650/1251]  eta: 0:08:25  lr: 0.000016  loss: 3.5670 (3.5557)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [5]  [ 660/1251]  eta: 0:08:16  lr: 0.000016  loss: 3.3130 (3.5565)  time: 0.8155  data: 0.0005  max mem: 19734
Epoch: [5]  [ 670/1251]  eta: 0:08:08  lr: 0.000016  loss: 3.7381 (3.5549)  time: 0.8045  data: 0.0006  max mem: 19734
Epoch: [5]  [ 680/1251]  eta: 0:07:59  lr: 0.000016  loss: 3.6188 (3.5584)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [5]  [ 690/1251]  eta: 0:07:50  lr: 0.000016  loss: 3.4796 (3.5537)  time: 0.8073  data: 0.0007  max mem: 19734
Epoch: [5]  [ 700/1251]  eta: 0:07:42  lr: 0.000016  loss: 3.2830 (3.5498)  time: 0.8059  data: 0.0008  max mem: 19734
Epoch: [5]  [ 710/1251]  eta: 0:07:33  lr: 0.000016  loss: 3.5639 (3.5504)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [5]  [ 720/1251]  eta: 0:07:24  lr: 0.000016  loss: 3.5944 (3.5488)  time: 0.8044  data: 0.0007  max mem: 19734
Epoch: [5]  [ 730/1251]  eta: 0:07:16  lr: 0.000016  loss: 3.4274 (3.5484)  time: 0.8033  data: 0.0007  max mem: 19734
Epoch: [5]  [ 740/1251]  eta: 0:07:07  lr: 0.000016  loss: 3.5420 (3.5464)  time: 0.8020  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4124, ratio_loss=0.0051, pruning_loss=0.2182, mse_loss=1.0524
Epoch: [5]  [ 750/1251]  eta: 0:06:59  lr: 0.000016  loss: 3.1215 (3.5407)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [5]  [ 760/1251]  eta: 0:06:50  lr: 0.000016  loss: 3.4107 (3.5423)  time: 0.8039  data: 0.0005  max mem: 19734
Epoch: [5]  [ 770/1251]  eta: 0:06:42  lr: 0.000016  loss: 3.7422 (3.5418)  time: 0.8307  data: 0.0005  max mem: 19734
Epoch: [5]  [ 780/1251]  eta: 0:06:33  lr: 0.000016  loss: 3.6965 (3.5430)  time: 0.8492  data: 0.0004  max mem: 19734
Epoch: [5]  [ 790/1251]  eta: 0:06:25  lr: 0.000016  loss: 3.5828 (3.5392)  time: 0.8334  data: 0.0005  max mem: 19734
Epoch: [5]  [ 800/1251]  eta: 0:06:16  lr: 0.000016  loss: 3.5828 (3.5381)  time: 0.8147  data: 0.0005  max mem: 19734
Epoch: [5]  [ 810/1251]  eta: 0:06:08  lr: 0.000016  loss: 3.7974 (3.5413)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [5]  [ 820/1251]  eta: 0:05:59  lr: 0.000016  loss: 3.6680 (3.5395)  time: 0.8102  data: 0.0005  max mem: 19734
Epoch: [5]  [ 830/1251]  eta: 0:05:51  lr: 0.000016  loss: 3.1946 (3.5365)  time: 0.8085  data: 0.0005  max mem: 19734
Epoch: [5]  [ 840/1251]  eta: 0:05:43  lr: 0.000016  loss: 3.4913 (3.5379)  time: 0.8076  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4735, ratio_loss=0.0052, pruning_loss=0.2181, mse_loss=1.1073
Epoch: [5]  [ 850/1251]  eta: 0:05:34  lr: 0.000016  loss: 3.6272 (3.5387)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [5]  [ 860/1251]  eta: 0:05:26  lr: 0.000016  loss: 3.4622 (3.5370)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [5]  [ 870/1251]  eta: 0:05:17  lr: 0.000016  loss: 3.4891 (3.5380)  time: 0.8034  data: 0.0006  max mem: 19734
Epoch: [5]  [ 880/1251]  eta: 0:05:09  lr: 0.000016  loss: 3.6226 (3.5387)  time: 0.8054  data: 0.0007  max mem: 19734
Epoch: [5]  [ 890/1251]  eta: 0:05:00  lr: 0.000016  loss: 3.6152 (3.5394)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [5]  [ 900/1251]  eta: 0:04:52  lr: 0.000016  loss: 3.9918 (3.5444)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [5]  [ 910/1251]  eta: 0:04:43  lr: 0.000016  loss: 3.9711 (3.5458)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [5]  [ 920/1251]  eta: 0:04:35  lr: 0.000016  loss: 3.9094 (3.5491)  time: 0.8415  data: 0.0005  max mem: 19734
Epoch: [5]  [ 930/1251]  eta: 0:04:27  lr: 0.000016  loss: 3.7765 (3.5451)  time: 0.8435  data: 0.0005  max mem: 19734
Epoch: [5]  [ 940/1251]  eta: 0:04:18  lr: 0.000016  loss: 3.4395 (3.5448)  time: 0.8150  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5622, ratio_loss=0.0053, pruning_loss=0.2136, mse_loss=1.0340
Epoch: [5]  [ 950/1251]  eta: 0:04:10  lr: 0.000016  loss: 3.4776 (3.5447)  time: 0.8103  data: 0.0006  max mem: 19734
Epoch: [5]  [ 960/1251]  eta: 0:04:02  lr: 0.000016  loss: 3.5623 (3.5436)  time: 0.8034  data: 0.0006  max mem: 19734
Epoch: [5]  [ 970/1251]  eta: 0:03:53  lr: 0.000016  loss: 3.6079 (3.5450)  time: 0.8039  data: 0.0005  max mem: 19734
Epoch: [5]  [ 980/1251]  eta: 0:03:45  lr: 0.000016  loss: 3.7268 (3.5452)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [5]  [ 990/1251]  eta: 0:03:36  lr: 0.000016  loss: 3.7258 (3.5466)  time: 0.8082  data: 0.0004  max mem: 19734
Epoch: [5]  [1000/1251]  eta: 0:03:28  lr: 0.000016  loss: 3.7258 (3.5460)  time: 0.8093  data: 0.0005  max mem: 19734
Epoch: [5]  [1010/1251]  eta: 0:03:20  lr: 0.000016  loss: 3.7995 (3.5491)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [5]  [1020/1251]  eta: 0:03:11  lr: 0.000016  loss: 3.7995 (3.5490)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [5]  [1030/1251]  eta: 0:03:03  lr: 0.000016  loss: 3.1601 (3.5458)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [5]  [1040/1251]  eta: 0:02:55  lr: 0.000016  loss: 3.4444 (3.5458)  time: 0.8044  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5161, ratio_loss=0.0054, pruning_loss=0.2106, mse_loss=1.0317
Epoch: [5]  [1050/1251]  eta: 0:02:46  lr: 0.000016  loss: 3.5999 (3.5469)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [5]  [1060/1251]  eta: 0:02:38  lr: 0.000016  loss: 3.5999 (3.5470)  time: 0.8309  data: 0.0005  max mem: 19734
Epoch: [5]  [1070/1251]  eta: 0:02:30  lr: 0.000016  loss: 3.4989 (3.5463)  time: 0.8498  data: 0.0006  max mem: 19734
Epoch: [5]  [1080/1251]  eta: 0:02:21  lr: 0.000016  loss: 3.4203 (3.5443)  time: 0.8318  data: 0.0005  max mem: 19734
Epoch: [5]  [1090/1251]  eta: 0:02:13  lr: 0.000016  loss: 3.6348 (3.5471)  time: 0.8158  data: 0.0004  max mem: 19734
Epoch: [5]  [1100/1251]  eta: 0:02:05  lr: 0.000016  loss: 3.6786 (3.5471)  time: 0.8041  data: 0.0004  max mem: 19734
Epoch: [5]  [1110/1251]  eta: 0:01:56  lr: 0.000016  loss: 3.7330 (3.5473)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [5]  [1120/1251]  eta: 0:01:48  lr: 0.000016  loss: 3.7020 (3.5463)  time: 0.8009  data: 0.0004  max mem: 19734
Epoch: [5]  [1130/1251]  eta: 0:01:40  lr: 0.000016  loss: 3.5486 (3.5461)  time: 0.8036  data: 0.0004  max mem: 19734
Epoch: [5]  [1140/1251]  eta: 0:01:31  lr: 0.000016  loss: 3.6584 (3.5466)  time: 0.8060  data: 0.0007  max mem: 19734
loss info: cls_loss=3.5061, ratio_loss=0.0057, pruning_loss=0.2123, mse_loss=1.0157
Epoch: [5]  [1150/1251]  eta: 0:01:23  lr: 0.000016  loss: 3.4881 (3.5446)  time: 0.8075  data: 0.0007  max mem: 19734
Epoch: [5]  [1160/1251]  eta: 0:01:15  lr: 0.000016  loss: 3.5652 (3.5444)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [5]  [1170/1251]  eta: 0:01:07  lr: 0.000016  loss: 3.6875 (3.5461)  time: 0.8034  data: 0.0004  max mem: 19734
Epoch: [5]  [1180/1251]  eta: 0:00:58  lr: 0.000016  loss: 3.5897 (3.5445)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [5]  [1190/1251]  eta: 0:00:50  lr: 0.000016  loss: 3.3769 (3.5418)  time: 0.8034  data: 0.0015  max mem: 19734
Epoch: [5]  [1200/1251]  eta: 0:00:42  lr: 0.000016  loss: 3.5509 (3.5420)  time: 0.8085  data: 0.0014  max mem: 19734
Epoch: [5]  [1210/1251]  eta: 0:00:33  lr: 0.000016  loss: 3.7192 (3.5432)  time: 0.8250  data: 0.0002  max mem: 19734
Epoch: [5]  [1220/1251]  eta: 0:00:25  lr: 0.000016  loss: 3.8584 (3.5443)  time: 0.8173  data: 0.0001  max mem: 19734
Epoch: [5]  [1230/1251]  eta: 0:00:17  lr: 0.000016  loss: 3.5150 (3.5441)  time: 0.8002  data: 0.0001  max mem: 19734
Epoch: [5]  [1240/1251]  eta: 0:00:09  lr: 0.000016  loss: 3.4509 (3.5426)  time: 0.7974  data: 0.0001  max mem: 19734
loss info: cls_loss=3.4839, ratio_loss=0.0058, pruning_loss=0.2116, mse_loss=1.0522
Epoch: [5]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.5945 (3.5423)  time: 0.7925  data: 0.0002  max mem: 19734
Epoch: [5] Total time: 0:17:15 (0.8276 s / it)
Averaged stats: lr: 0.000016  loss: 3.5945 (3.5492)
Test:  [  0/261]  eta: 1:29:08  loss: 1.1763 (1.1763)  acc1: 78.1250 (78.1250)  acc5: 91.6667 (91.6667)  time: 20.4909  data: 20.3047  max mem: 19734
Test:  [ 10/261]  eta: 0:08:26  loss: 0.9659 (1.1178)  acc1: 83.8542 (80.3977)  acc5: 93.7500 (92.8030)  time: 2.0169  data: 1.8500  max mem: 19734
Test:  [ 20/261]  eta: 0:04:43  loss: 1.1787 (1.2290)  acc1: 75.5208 (76.2153)  acc5: 91.6667 (92.3363)  time: 0.2128  data: 0.0363  max mem: 19734
Test:  [ 30/261]  eta: 0:03:23  loss: 1.0589 (1.1554)  acc1: 79.1667 (78.8138)  acc5: 92.1875 (92.7587)  time: 0.2563  data: 0.0400  max mem: 19734
Test:  [ 40/261]  eta: 0:02:55  loss: 0.9751 (1.1252)  acc1: 83.8542 (79.5224)  acc5: 93.7500 (92.8735)  time: 0.3940  data: 0.1588  max mem: 19734
Test:  [ 50/261]  eta: 0:02:26  loss: 1.2280 (1.1881)  acc1: 73.4375 (77.4918)  acc5: 91.1458 (92.2794)  time: 0.4004  data: 0.2140  max mem: 19734
Test:  [ 60/261]  eta: 0:02:02  loss: 1.3171 (1.1978)  acc1: 71.3542 (77.0236)  acc5: 91.6667 (92.4436)  time: 0.2287  data: 0.0644  max mem: 19734
Test:  [ 70/261]  eta: 0:02:05  loss: 1.1818 (1.1955)  acc1: 73.4375 (76.5845)  acc5: 93.2292 (92.7303)  time: 0.5646  data: 0.3730  max mem: 19734
Test:  [ 80/261]  eta: 0:01:57  loss: 1.1679 (1.2085)  acc1: 75.0000 (76.7040)  acc5: 93.7500 (92.8369)  time: 0.7767  data: 0.5574  max mem: 19734
Test:  [ 90/261]  eta: 0:01:50  loss: 1.2165 (1.1989)  acc1: 78.6458 (77.1062)  acc5: 92.7083 (92.9144)  time: 0.6141  data: 0.3175  max mem: 19734
Test:  [100/261]  eta: 0:01:42  loss: 1.1076 (1.1989)  acc1: 78.6458 (77.2329)  acc5: 92.7083 (92.9404)  time: 0.5680  data: 0.2248  max mem: 19734
Test:  [110/261]  eta: 0:01:33  loss: 1.2609 (1.2286)  acc1: 76.0417 (76.7690)  acc5: 91.6667 (92.5488)  time: 0.4979  data: 0.0998  max mem: 19734
Test:  [120/261]  eta: 0:01:24  loss: 1.6041 (1.2660)  acc1: 67.1875 (75.8523)  acc5: 86.4583 (91.9637)  time: 0.4031  data: 0.0120  max mem: 19734
Test:  [130/261]  eta: 0:01:17  loss: 1.7786 (1.3087)  acc1: 65.1042 (74.9881)  acc5: 83.8542 (91.3526)  time: 0.4248  data: 0.0147  max mem: 19734
Test:  [140/261]  eta: 0:01:11  loss: 1.6574 (1.3305)  acc1: 64.5833 (74.3462)  acc5: 87.5000 (91.1458)  time: 0.5515  data: 0.1303  max mem: 19734
Test:  [150/261]  eta: 0:01:03  loss: 1.4852 (1.3337)  acc1: 69.2708 (74.3688)  acc5: 89.5833 (91.0148)  time: 0.4721  data: 0.1323  max mem: 19734
Test:  [160/261]  eta: 0:00:57  loss: 1.4250 (1.3533)  acc1: 72.9167 (73.9939)  acc5: 89.5833 (90.7123)  time: 0.3841  data: 0.0165  max mem: 19734
Test:  [170/261]  eta: 0:00:50  loss: 1.7650 (1.3839)  acc1: 60.4167 (73.1482)  acc5: 84.3750 (90.3905)  time: 0.4048  data: 0.0449  max mem: 19734
Test:  [180/261]  eta: 0:00:45  loss: 1.8594 (1.4046)  acc1: 60.4167 (72.7037)  acc5: 84.8958 (90.1387)  time: 0.5525  data: 0.2447  max mem: 19734
Test:  [190/261]  eta: 0:00:39  loss: 1.7208 (1.4192)  acc1: 64.0625 (72.4313)  acc5: 85.9375 (89.9242)  time: 0.5378  data: 0.2164  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: 1.6884 (1.4349)  acc1: 67.1875 (72.1108)  acc5: 85.4167 (89.6481)  time: 0.3803  data: 0.0460  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.6609 (1.4484)  acc1: 66.1458 (71.8404)  acc5: 84.3750 (89.4673)  time: 0.4001  data: 0.0516  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.8230 (1.4689)  acc1: 61.9792 (71.3447)  acc5: 85.4167 (89.2416)  time: 0.3258  data: 0.0154  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.7657 (1.4786)  acc1: 61.4583 (71.0881)  acc5: 85.9375 (89.1189)  time: 0.2329  data: 0.0003  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.7329 (1.4892)  acc1: 61.9792 (70.7620)  acc5: 86.4583 (89.0452)  time: 0.2667  data: 0.0939  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.5905 (1.4826)  acc1: 68.2292 (70.9661)  acc5: 91.6667 (89.2015)  time: 0.2108  data: 0.0938  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.4398 (1.4837)  acc1: 73.9583 (70.9940)  acc5: 93.2292 (89.2580)  time: 0.1117  data: 0.0001  max mem: 19734
Test: Total time: 0:02:03 (0.4747 s / it)
* Acc@1 70.994 Acc@5 89.258 loss 1.484
Accuracy of the network on the 50000 test images: 71.0%
Max accuracy: 70.99%
Epoch: [6]  [   0/1251]  eta: 5:04:43  lr: 0.000020  loss: 2.8400 (2.8400)  time: 14.6154  data: 13.8686  max mem: 19734
Epoch: [6]  [  10/1251]  eta: 0:43:59  lr: 0.000020  loss: 3.3721 (3.3893)  time: 2.1272  data: 1.2613  max mem: 19734
Epoch: [6]  [  20/1251]  eta: 0:30:39  lr: 0.000020  loss: 3.4676 (3.4048)  time: 0.8379  data: 0.0006  max mem: 19734
Epoch: [6]  [  30/1251]  eta: 0:25:50  lr: 0.000020  loss: 3.5599 (3.4221)  time: 0.7982  data: 0.0006  max mem: 19734
Epoch: [6]  [  40/1251]  eta: 0:23:18  lr: 0.000020  loss: 3.6172 (3.4739)  time: 0.7980  data: 0.0006  max mem: 19734
Epoch: [6]  [  50/1251]  eta: 0:21:48  lr: 0.000020  loss: 3.6539 (3.4356)  time: 0.8101  data: 0.0006  max mem: 19734
Epoch: [6]  [  60/1251]  eta: 0:20:42  lr: 0.000020  loss: 3.1129 (3.3777)  time: 0.8149  data: 0.0005  max mem: 19734
Epoch: [6]  [  70/1251]  eta: 0:19:52  lr: 0.000020  loss: 3.4291 (3.4041)  time: 0.8048  data: 0.0004  max mem: 19734
Epoch: [6]  [  80/1251]  eta: 0:19:12  lr: 0.000020  loss: 3.6218 (3.4377)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [6]  [  90/1251]  eta: 0:18:38  lr: 0.000020  loss: 3.6218 (3.4378)  time: 0.8019  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4159, ratio_loss=0.0058, pruning_loss=0.2119, mse_loss=1.0462
Epoch: [6]  [ 100/1251]  eta: 0:18:21  lr: 0.000020  loss: 3.7180 (3.4547)  time: 0.8453  data: 0.0005  max mem: 19734
Epoch: [6]  [ 110/1251]  eta: 0:17:56  lr: 0.000020  loss: 3.7038 (3.4411)  time: 0.8512  data: 0.0005  max mem: 19734
Epoch: [6]  [ 120/1251]  eta: 0:17:33  lr: 0.000020  loss: 3.5861 (3.4494)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [6]  [ 130/1251]  eta: 0:17:13  lr: 0.000020  loss: 3.6935 (3.4623)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [6]  [ 140/1251]  eta: 0:16:55  lr: 0.000020  loss: 3.5589 (3.4682)  time: 0.8037  data: 0.0006  max mem: 19734
Epoch: [6]  [ 150/1251]  eta: 0:16:37  lr: 0.000020  loss: 3.5589 (3.4834)  time: 0.8047  data: 0.0006  max mem: 19734
Epoch: [6]  [ 160/1251]  eta: 0:16:22  lr: 0.000020  loss: 3.8400 (3.4968)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [6]  [ 170/1251]  eta: 0:16:07  lr: 0.000020  loss: 3.6213 (3.4875)  time: 0.8114  data: 0.0005  max mem: 19734
Epoch: [6]  [ 180/1251]  eta: 0:15:53  lr: 0.000020  loss: 3.4868 (3.4927)  time: 0.8110  data: 0.0006  max mem: 19734
Epoch: [6]  [ 190/1251]  eta: 0:15:39  lr: 0.000020  loss: 3.6300 (3.5027)  time: 0.8048  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5440, ratio_loss=0.0064, pruning_loss=0.2076, mse_loss=1.0454
Epoch: [6]  [ 200/1251]  eta: 0:15:27  lr: 0.000020  loss: 3.8377 (3.5181)  time: 0.8076  data: 0.0005  max mem: 19734
Epoch: [6]  [ 210/1251]  eta: 0:15:14  lr: 0.000020  loss: 3.6275 (3.5159)  time: 0.8083  data: 0.0005  max mem: 19734
Epoch: [6]  [ 220/1251]  eta: 0:15:01  lr: 0.000020  loss: 3.2621 (3.4924)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [6]  [ 230/1251]  eta: 0:14:49  lr: 0.000020  loss: 3.4020 (3.4962)  time: 0.8006  data: 0.0006  max mem: 19734
Epoch: [6]  [ 240/1251]  eta: 0:14:39  lr: 0.000020  loss: 3.5246 (3.4977)  time: 0.8172  data: 0.0006  max mem: 19734
Epoch: [6]  [ 250/1251]  eta: 0:14:29  lr: 0.000020  loss: 3.4641 (3.4936)  time: 0.8373  data: 0.0005  max mem: 19734
Epoch: [6]  [ 260/1251]  eta: 0:14:19  lr: 0.000020  loss: 3.4515 (3.4947)  time: 0.8289  data: 0.0006  max mem: 19734
Epoch: [6]  [ 270/1251]  eta: 0:14:08  lr: 0.000020  loss: 3.6582 (3.4970)  time: 0.8170  data: 0.0009  max mem: 19734
Epoch: [6]  [ 280/1251]  eta: 0:13:58  lr: 0.000020  loss: 3.3572 (3.4952)  time: 0.8161  data: 0.0008  max mem: 19734
Epoch: [6]  [ 290/1251]  eta: 0:13:47  lr: 0.000020  loss: 3.4983 (3.4982)  time: 0.8073  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4286, ratio_loss=0.0064, pruning_loss=0.2123, mse_loss=1.0279
Epoch: [6]  [ 300/1251]  eta: 0:13:36  lr: 0.000020  loss: 3.5473 (3.5000)  time: 0.7995  data: 0.0005  max mem: 19734
Epoch: [6]  [ 310/1251]  eta: 0:13:26  lr: 0.000020  loss: 3.6659 (3.5017)  time: 0.7979  data: 0.0006  max mem: 19734
Epoch: [6]  [ 320/1251]  eta: 0:13:16  lr: 0.000020  loss: 3.6392 (3.4960)  time: 0.7990  data: 0.0004  max mem: 19734
Epoch: [6]  [ 330/1251]  eta: 0:13:06  lr: 0.000020  loss: 3.6392 (3.5051)  time: 0.8007  data: 0.0004  max mem: 19734
Epoch: [6]  [ 340/1251]  eta: 0:12:56  lr: 0.000020  loss: 3.8423 (3.5061)  time: 0.8022  data: 0.0006  max mem: 19734
Epoch: [6]  [ 350/1251]  eta: 0:12:47  lr: 0.000020  loss: 3.6610 (3.5006)  time: 0.8156  data: 0.0008  max mem: 19734
Epoch: [6]  [ 360/1251]  eta: 0:12:37  lr: 0.000020  loss: 3.7291 (3.5066)  time: 0.8131  data: 0.0007  max mem: 19734
Epoch: [6]  [ 370/1251]  eta: 0:12:27  lr: 0.000020  loss: 3.8238 (3.5115)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [6]  [ 380/1251]  eta: 0:12:18  lr: 0.000020  loss: 3.7133 (3.5083)  time: 0.8012  data: 0.0004  max mem: 19734
Epoch: [6]  [ 390/1251]  eta: 0:12:10  lr: 0.000020  loss: 3.7178 (3.5119)  time: 0.8481  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5414, ratio_loss=0.0065, pruning_loss=0.2059, mse_loss=1.0059
Epoch: [6]  [ 400/1251]  eta: 0:12:01  lr: 0.000020  loss: 3.8646 (3.5183)  time: 0.8470  data: 0.0005  max mem: 19734
Epoch: [6]  [ 410/1251]  eta: 0:11:52  lr: 0.000020  loss: 3.4050 (3.5136)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [6]  [ 420/1251]  eta: 0:11:42  lr: 0.000020  loss: 3.4050 (3.5172)  time: 0.8098  data: 0.0005  max mem: 19734
Epoch: [6]  [ 430/1251]  eta: 0:11:33  lr: 0.000020  loss: 3.6244 (3.5157)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [6]  [ 440/1251]  eta: 0:11:24  lr: 0.000020  loss: 3.6151 (3.5188)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [6]  [ 450/1251]  eta: 0:11:15  lr: 0.000020  loss: 3.5748 (3.5132)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [6]  [ 460/1251]  eta: 0:11:06  lr: 0.000020  loss: 3.4779 (3.5106)  time: 0.8036  data: 0.0004  max mem: 19734
Epoch: [6]  [ 470/1251]  eta: 0:10:56  lr: 0.000020  loss: 3.5833 (3.5101)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [6]  [ 480/1251]  eta: 0:10:48  lr: 0.000020  loss: 3.3891 (3.5082)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [6]  [ 490/1251]  eta: 0:10:39  lr: 0.000020  loss: 3.3403 (3.5065)  time: 0.8095  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4445, ratio_loss=0.0065, pruning_loss=0.2074, mse_loss=1.0548
Epoch: [6]  [ 500/1251]  eta: 0:10:30  lr: 0.000020  loss: 3.5244 (3.5070)  time: 0.8097  data: 0.0005  max mem: 19734
Epoch: [6]  [ 510/1251]  eta: 0:10:21  lr: 0.000020  loss: 3.4772 (3.5037)  time: 0.8065  data: 0.0006  max mem: 19734
Epoch: [6]  [ 520/1251]  eta: 0:10:12  lr: 0.000020  loss: 3.2830 (3.5011)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [6]  [ 530/1251]  eta: 0:10:04  lr: 0.000020  loss: 3.5293 (3.5044)  time: 0.8214  data: 0.0005  max mem: 19734
Epoch: [6]  [ 540/1251]  eta: 0:09:55  lr: 0.000020  loss: 3.7555 (3.5038)  time: 0.8434  data: 0.0005  max mem: 19734
Epoch: [6]  [ 550/1251]  eta: 0:09:47  lr: 0.000020  loss: 3.3671 (3.4967)  time: 0.8360  data: 0.0005  max mem: 19734
Epoch: [6]  [ 560/1251]  eta: 0:09:38  lr: 0.000020  loss: 3.5888 (3.4962)  time: 0.8068  data: 0.0004  max mem: 19734
Epoch: [6]  [ 570/1251]  eta: 0:09:29  lr: 0.000020  loss: 3.7626 (3.5007)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [6]  [ 580/1251]  eta: 0:09:20  lr: 0.000020  loss: 3.7626 (3.5021)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [6]  [ 590/1251]  eta: 0:09:12  lr: 0.000020  loss: 3.7821 (3.5084)  time: 0.7999  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4805, ratio_loss=0.0067, pruning_loss=0.2043, mse_loss=1.0178
Epoch: [6]  [ 600/1251]  eta: 0:09:03  lr: 0.000020  loss: 3.6624 (3.5046)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [6]  [ 610/1251]  eta: 0:08:54  lr: 0.000020  loss: 3.3128 (3.5002)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [6]  [ 620/1251]  eta: 0:08:46  lr: 0.000020  loss: 3.5432 (3.5038)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [6]  [ 630/1251]  eta: 0:08:37  lr: 0.000020  loss: 3.5432 (3.5003)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [6]  [ 640/1251]  eta: 0:08:28  lr: 0.000020  loss: 3.5250 (3.5018)  time: 0.8071  data: 0.0005  max mem: 19734
Epoch: [6]  [ 650/1251]  eta: 0:08:20  lr: 0.000020  loss: 3.6071 (3.5005)  time: 0.8072  data: 0.0005  max mem: 19734
Epoch: [6]  [ 660/1251]  eta: 0:08:11  lr: 0.000020  loss: 3.3805 (3.4985)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [6]  [ 670/1251]  eta: 0:08:03  lr: 0.000020  loss: 3.5461 (3.5003)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [6]  [ 680/1251]  eta: 0:07:55  lr: 0.000020  loss: 3.6116 (3.5030)  time: 0.8331  data: 0.0005  max mem: 19734
Epoch: [6]  [ 690/1251]  eta: 0:07:46  lr: 0.000020  loss: 3.5542 (3.5010)  time: 0.8467  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4429, ratio_loss=0.0067, pruning_loss=0.2039, mse_loss=1.0315
Epoch: [6]  [ 700/1251]  eta: 0:07:38  lr: 0.000020  loss: 3.5803 (3.5046)  time: 0.8235  data: 0.0007  max mem: 19734
Epoch: [6]  [ 710/1251]  eta: 0:07:29  lr: 0.000020  loss: 3.7910 (3.5038)  time: 0.8083  data: 0.0007  max mem: 19734
Epoch: [6]  [ 720/1251]  eta: 0:07:21  lr: 0.000020  loss: 3.1931 (3.5011)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [6]  [ 730/1251]  eta: 0:07:12  lr: 0.000020  loss: 3.0108 (3.4988)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [6]  [ 740/1251]  eta: 0:07:04  lr: 0.000020  loss: 3.6867 (3.4996)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [6]  [ 750/1251]  eta: 0:06:55  lr: 0.000020  loss: 3.6867 (3.4997)  time: 0.8047  data: 0.0004  max mem: 19734
Epoch: [6]  [ 760/1251]  eta: 0:06:47  lr: 0.000020  loss: 3.6936 (3.5035)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [6]  [ 770/1251]  eta: 0:06:38  lr: 0.000020  loss: 3.7495 (3.5052)  time: 0.8068  data: 0.0005  max mem: 19734
Epoch: [6]  [ 780/1251]  eta: 0:06:30  lr: 0.000020  loss: 3.5637 (3.5063)  time: 0.8021  data: 0.0004  max mem: 19734
Epoch: [6]  [ 790/1251]  eta: 0:06:22  lr: 0.000020  loss: 3.4234 (3.5060)  time: 0.8095  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5025, ratio_loss=0.0078, pruning_loss=0.1985, mse_loss=0.9845
Epoch: [6]  [ 800/1251]  eta: 0:06:13  lr: 0.000020  loss: 3.3968 (3.5032)  time: 0.8130  data: 0.0006  max mem: 19734
Epoch: [6]  [ 810/1251]  eta: 0:06:05  lr: 0.000020  loss: 3.3112 (3.5022)  time: 0.8019  data: 0.0006  max mem: 19734
Epoch: [6]  [ 820/1251]  eta: 0:05:56  lr: 0.000020  loss: 3.5525 (3.5040)  time: 0.7990  data: 0.0004  max mem: 19734
Epoch: [6]  [ 830/1251]  eta: 0:05:48  lr: 0.000020  loss: 3.7465 (3.5055)  time: 0.8401  data: 0.0005  max mem: 19734
Epoch: [6]  [ 840/1251]  eta: 0:05:40  lr: 0.000020  loss: 3.5592 (3.5033)  time: 0.8410  data: 0.0005  max mem: 19734
Epoch: [6]  [ 850/1251]  eta: 0:05:31  lr: 0.000020  loss: 3.4909 (3.5040)  time: 0.8083  data: 0.0005  max mem: 19734
Epoch: [6]  [ 860/1251]  eta: 0:05:23  lr: 0.000020  loss: 3.7112 (3.5047)  time: 0.8123  data: 0.0005  max mem: 19734
Epoch: [6]  [ 870/1251]  eta: 0:05:15  lr: 0.000020  loss: 3.7011 (3.5055)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [6]  [ 880/1251]  eta: 0:05:06  lr: 0.000020  loss: 3.5981 (3.5076)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [6]  [ 890/1251]  eta: 0:04:58  lr: 0.000020  loss: 3.5248 (3.5095)  time: 0.8016  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5163, ratio_loss=0.0072, pruning_loss=0.1990, mse_loss=0.9720
Epoch: [6]  [ 900/1251]  eta: 0:04:50  lr: 0.000020  loss: 3.5248 (3.5105)  time: 0.8011  data: 0.0004  max mem: 19734
Epoch: [6]  [ 910/1251]  eta: 0:04:41  lr: 0.000020  loss: 3.4884 (3.5090)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [6]  [ 920/1251]  eta: 0:04:33  lr: 0.000020  loss: 3.3480 (3.5085)  time: 0.8003  data: 0.0005  max mem: 19734
Epoch: [6]  [ 930/1251]  eta: 0:04:25  lr: 0.000020  loss: 3.4726 (3.5075)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [6]  [ 940/1251]  eta: 0:04:16  lr: 0.000020  loss: 3.6873 (3.5089)  time: 0.8088  data: 0.0007  max mem: 19734
Epoch: [6]  [ 950/1251]  eta: 0:04:08  lr: 0.000020  loss: 3.7154 (3.5120)  time: 0.8074  data: 0.0007  max mem: 19734
Epoch: [6]  [ 960/1251]  eta: 0:04:00  lr: 0.000020  loss: 3.7050 (3.5129)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [6]  [ 970/1251]  eta: 0:03:51  lr: 0.000020  loss: 3.5003 (3.5108)  time: 0.8283  data: 0.0004  max mem: 19734
Epoch: [6]  [ 980/1251]  eta: 0:03:43  lr: 0.000020  loss: 3.2358 (3.5087)  time: 0.8593  data: 0.0004  max mem: 19734
Epoch: [6]  [ 990/1251]  eta: 0:03:35  lr: 0.000020  loss: 3.6591 (3.5132)  time: 0.8392  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5112, ratio_loss=0.0075, pruning_loss=0.1973, mse_loss=1.0190
Epoch: [6]  [1000/1251]  eta: 0:03:27  lr: 0.000020  loss: 3.7194 (3.5143)  time: 0.8075  data: 0.0005  max mem: 19734
Epoch: [6]  [1010/1251]  eta: 0:03:18  lr: 0.000020  loss: 3.6413 (3.5138)  time: 0.8014  data: 0.0007  max mem: 19734
Epoch: [6]  [1020/1251]  eta: 0:03:10  lr: 0.000020  loss: 3.6915 (3.5139)  time: 0.8037  data: 0.0007  max mem: 19734
Epoch: [6]  [1030/1251]  eta: 0:03:02  lr: 0.000020  loss: 3.6765 (3.5153)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [6]  [1040/1251]  eta: 0:02:53  lr: 0.000020  loss: 3.6765 (3.5172)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [6]  [1050/1251]  eta: 0:02:45  lr: 0.000020  loss: 3.7966 (3.5181)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [6]  [1060/1251]  eta: 0:02:37  lr: 0.000020  loss: 3.6424 (3.5181)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [6]  [1070/1251]  eta: 0:02:29  lr: 0.000020  loss: 3.5718 (3.5172)  time: 0.8051  data: 0.0004  max mem: 19734
Epoch: [6]  [1080/1251]  eta: 0:02:20  lr: 0.000020  loss: 3.4895 (3.5157)  time: 0.8047  data: 0.0004  max mem: 19734
Epoch: [6]  [1090/1251]  eta: 0:02:12  lr: 0.000020  loss: 3.8216 (3.5177)  time: 0.8044  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5385, ratio_loss=0.0074, pruning_loss=0.1948, mse_loss=0.9821
Epoch: [6]  [1100/1251]  eta: 0:02:04  lr: 0.000020  loss: 3.8691 (3.5191)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [6]  [1110/1251]  eta: 0:01:56  lr: 0.000020  loss: 3.5883 (3.5189)  time: 0.8006  data: 0.0004  max mem: 19734
Epoch: [6]  [1120/1251]  eta: 0:01:47  lr: 0.000020  loss: 3.5883 (3.5195)  time: 0.8251  data: 0.0005  max mem: 19734
Epoch: [6]  [1130/1251]  eta: 0:01:39  lr: 0.000020  loss: 3.6637 (3.5187)  time: 0.8394  data: 0.0004  max mem: 19734
Epoch: [6]  [1140/1251]  eta: 0:01:31  lr: 0.000020  loss: 3.6769 (3.5202)  time: 0.8257  data: 0.0004  max mem: 19734
Epoch: [6]  [1150/1251]  eta: 0:01:23  lr: 0.000020  loss: 3.6391 (3.5180)  time: 0.8133  data: 0.0004  max mem: 19734
Epoch: [6]  [1160/1251]  eta: 0:01:14  lr: 0.000020  loss: 3.5439 (3.5184)  time: 0.8030  data: 0.0004  max mem: 19734
Epoch: [6]  [1170/1251]  eta: 0:01:06  lr: 0.000020  loss: 3.6226 (3.5177)  time: 0.8035  data: 0.0004  max mem: 19734
Epoch: [6]  [1180/1251]  eta: 0:00:58  lr: 0.000020  loss: 3.4829 (3.5153)  time: 0.8045  data: 0.0004  max mem: 19734
Epoch: [6]  [1190/1251]  eta: 0:00:50  lr: 0.000020  loss: 3.1119 (3.5135)  time: 0.8005  data: 0.0008  max mem: 19734
loss info: cls_loss=3.4374, ratio_loss=0.0077, pruning_loss=0.1966, mse_loss=0.9943
Epoch: [6]  [1200/1251]  eta: 0:00:41  lr: 0.000020  loss: 3.4506 (3.5140)  time: 0.7945  data: 0.0007  max mem: 19734
Epoch: [6]  [1210/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.4506 (3.5135)  time: 0.7921  data: 0.0002  max mem: 19734
Epoch: [6]  [1220/1251]  eta: 0:00:25  lr: 0.000020  loss: 3.4505 (3.5142)  time: 0.7926  data: 0.0002  max mem: 19734
Epoch: [6]  [1230/1251]  eta: 0:00:17  lr: 0.000020  loss: 3.8010 (3.5158)  time: 0.7925  data: 0.0002  max mem: 19734
Epoch: [6]  [1240/1251]  eta: 0:00:09  lr: 0.000020  loss: 3.7910 (3.5171)  time: 0.7969  data: 0.0002  max mem: 19734
Epoch: [6]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.6701 (3.5185)  time: 0.7966  data: 0.0002  max mem: 19734
Epoch: [6] Total time: 0:17:08 (0.8222 s / it)
Averaged stats: lr: 0.000020  loss: 3.6701 (3.5264)
Test:  [  0/261]  eta: 0:53:14  loss: 0.9892 (0.9892)  acc1: 81.2500 (81.2500)  acc5: 93.7500 (93.7500)  time: 12.2402  data: 12.1168  max mem: 19734
Test:  [ 10/261]  eta: 0:13:02  loss: 0.8679 (1.0173)  acc1: 83.8542 (81.4394)  acc5: 94.7917 (93.7974)  time: 3.1169  data: 2.9721  max mem: 19734
Test:  [ 20/261]  eta: 0:07:00  loss: 1.1635 (1.1364)  acc1: 77.6042 (77.0833)  acc5: 91.6667 (92.8819)  time: 1.2215  data: 1.0342  max mem: 19734
Test:  [ 30/261]  eta: 0:04:44  loss: 1.0687 (1.0692)  acc1: 79.1667 (79.7379)  acc5: 91.6667 (93.1788)  time: 0.1922  data: 0.0107  max mem: 19734
Test:  [ 40/261]  eta: 0:03:53  loss: 0.8021 (1.0385)  acc1: 85.9375 (80.6021)  acc5: 94.7917 (93.4578)  time: 0.3299  data: 0.1545  max mem: 19734
Test:  [ 50/261]  eta: 0:03:05  loss: 1.1474 (1.1025)  acc1: 75.0000 (78.6663)  acc5: 92.7083 (92.9330)  time: 0.3318  data: 0.1523  max mem: 19734
Test:  [ 60/261]  eta: 0:02:43  loss: 1.2561 (1.1151)  acc1: 72.3958 (78.0567)  acc5: 92.1875 (93.0328)  time: 0.3144  data: 0.1336  max mem: 19734
Test:  [ 70/261]  eta: 0:02:23  loss: 1.1726 (1.1112)  acc1: 73.4375 (77.5748)  acc5: 94.2708 (93.2512)  time: 0.4239  data: 0.2052  max mem: 19734
Test:  [ 80/261]  eta: 0:02:25  loss: 1.1135 (1.1227)  acc1: 76.0417 (77.6556)  acc5: 94.7917 (93.3706)  time: 0.7871  data: 0.5553  max mem: 19734
Test:  [ 90/261]  eta: 0:02:11  loss: 1.0919 (1.1098)  acc1: 78.6458 (78.0449)  acc5: 93.7500 (93.4638)  time: 0.8403  data: 0.5558  max mem: 19734
Test:  [100/261]  eta: 0:01:58  loss: 1.0605 (1.1067)  acc1: 79.6875 (78.1198)  acc5: 93.7500 (93.5128)  time: 0.4458  data: 0.0848  max mem: 19734
Test:  [110/261]  eta: 0:01:43  loss: 1.1657 (1.1309)  acc1: 75.0000 (77.6605)  acc5: 92.7083 (93.2292)  time: 0.3008  data: 0.0130  max mem: 19734
Test:  [120/261]  eta: 0:01:33  loss: 1.4447 (1.1694)  acc1: 68.2292 (76.7777)  acc5: 88.0208 (92.7040)  time: 0.2958  data: 0.0369  max mem: 19734
Test:  [130/261]  eta: 0:01:21  loss: 1.6492 (1.2157)  acc1: 66.6667 (75.8548)  acc5: 85.4167 (92.0841)  time: 0.2685  data: 0.0368  max mem: 19734
Test:  [140/261]  eta: 0:01:17  loss: 1.6074 (1.2413)  acc1: 66.6667 (75.1921)  acc5: 89.0625 (91.8735)  time: 0.4800  data: 0.3030  max mem: 19734
Test:  [150/261]  eta: 0:01:11  loss: 1.4183 (1.2483)  acc1: 68.7500 (75.1345)  acc5: 90.1042 (91.7253)  time: 0.7791  data: 0.5164  max mem: 19734
Test:  [160/261]  eta: 0:01:02  loss: 1.3075 (1.2690)  acc1: 74.4792 (74.7380)  acc5: 90.1042 (91.4014)  time: 0.4675  data: 0.2206  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.6946 (1.3007)  acc1: 62.5000 (73.9187)  acc5: 85.4167 (91.0270)  time: 0.3333  data: 0.0545  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: 1.8162 (1.3232)  acc1: 61.9792 (73.4404)  acc5: 84.8958 (90.7746)  time: 0.3715  data: 0.0534  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: 1.6931 (1.3397)  acc1: 64.5833 (73.1321)  acc5: 87.5000 (90.5950)  time: 0.3824  data: 0.0578  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.6845 (1.3561)  acc1: 66.6667 (72.7586)  acc5: 86.4583 (90.3477)  time: 0.5660  data: 0.3068  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.6721 (1.3716)  acc1: 65.1042 (72.4748)  acc5: 84.8958 (90.1042)  time: 0.3965  data: 0.2520  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.7244 (1.3896)  acc1: 64.5833 (72.0635)  acc5: 84.8958 (89.9156)  time: 0.1653  data: 0.0359  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.7244 (1.4016)  acc1: 60.9375 (71.7758)  acc5: 86.4583 (89.7817)  time: 0.1508  data: 0.0358  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.6120 (1.4119)  acc1: 62.5000 (71.5119)  acc5: 87.5000 (89.6892)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.5545 (1.4067)  acc1: 68.2292 (71.6675)  acc5: 90.1042 (89.8012)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.3876 (1.4074)  acc1: 72.3958 (71.7040)  acc5: 93.2292 (89.8540)  time: 0.1113  data: 0.0001  max mem: 19734
Test: Total time: 0:02:05 (0.4824 s / it)
* Acc@1 71.704 Acc@5 89.854 loss 1.407
Accuracy of the network on the 50000 test images: 71.7%
Max accuracy: 71.70%
Epoch: [7]  [   0/1251]  eta: 3:40:55  lr: 0.000020  loss: 3.6589 (3.6589)  time: 10.5960  data: 9.5898  max mem: 19734
Epoch: [7]  [  10/1251]  eta: 0:38:38  lr: 0.000020  loss: 3.6589 (3.7264)  time: 1.8680  data: 0.8724  max mem: 19734
Epoch: [7]  [  20/1251]  eta: 0:28:10  lr: 0.000020  loss: 3.6065 (3.6504)  time: 0.9120  data: 0.0006  max mem: 19734
Epoch: [7]  [  30/1251]  eta: 0:24:08  lr: 0.000020  loss: 3.6826 (3.6331)  time: 0.8112  data: 0.0006  max mem: 19734
Epoch: [7]  [  40/1251]  eta: 0:22:00  lr: 0.000020  loss: 3.6826 (3.6359)  time: 0.7934  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5936, ratio_loss=0.0078, pruning_loss=0.1927, mse_loss=0.9970
Epoch: [7]  [  50/1251]  eta: 0:20:39  lr: 0.000020  loss: 3.5017 (3.5599)  time: 0.7942  data: 0.0005  max mem: 19734
Epoch: [7]  [  60/1251]  eta: 0:19:44  lr: 0.000020  loss: 3.5200 (3.5500)  time: 0.7975  data: 0.0004  max mem: 19734
Epoch: [7]  [  70/1251]  eta: 0:19:04  lr: 0.000020  loss: 3.5486 (3.5564)  time: 0.8068  data: 0.0005  max mem: 19734
Epoch: [7]  [  80/1251]  eta: 0:18:30  lr: 0.000020  loss: 3.6420 (3.5589)  time: 0.8075  data: 0.0005  max mem: 19734
Epoch: [7]  [  90/1251]  eta: 0:18:01  lr: 0.000020  loss: 3.6602 (3.5624)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [7]  [ 100/1251]  eta: 0:17:37  lr: 0.000020  loss: 3.5537 (3.5526)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [7]  [ 110/1251]  eta: 0:17:15  lr: 0.000020  loss: 3.3456 (3.5465)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [7]  [ 120/1251]  eta: 0:16:56  lr: 0.000020  loss: 3.5297 (3.5399)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [7]  [ 130/1251]  eta: 0:16:39  lr: 0.000020  loss: 3.7605 (3.5376)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [7]  [ 140/1251]  eta: 0:16:23  lr: 0.000020  loss: 3.6133 (3.5248)  time: 0.8018  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4370, ratio_loss=0.0078, pruning_loss=0.1953, mse_loss=1.0145
Epoch: [7]  [ 150/1251]  eta: 0:16:10  lr: 0.000020  loss: 3.4565 (3.5151)  time: 0.8139  data: 0.0005  max mem: 19734
Epoch: [7]  [ 160/1251]  eta: 0:15:58  lr: 0.000020  loss: 3.5948 (3.5228)  time: 0.8348  data: 0.0005  max mem: 19734
Epoch: [7]  [ 170/1251]  eta: 0:15:46  lr: 0.000020  loss: 3.5948 (3.5300)  time: 0.8319  data: 0.0005  max mem: 19734
Epoch: [7]  [ 180/1251]  eta: 0:15:33  lr: 0.000020  loss: 3.7099 (3.5439)  time: 0.8115  data: 0.0005  max mem: 19734
Epoch: [7]  [ 190/1251]  eta: 0:15:20  lr: 0.000020  loss: 3.7300 (3.5345)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [7]  [ 200/1251]  eta: 0:15:08  lr: 0.000020  loss: 3.2708 (3.5269)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [7]  [ 210/1251]  eta: 0:14:57  lr: 0.000020  loss: 3.7707 (3.5455)  time: 0.8106  data: 0.0005  max mem: 19734
Epoch: [7]  [ 220/1251]  eta: 0:14:46  lr: 0.000020  loss: 3.7707 (3.5393)  time: 0.8094  data: 0.0005  max mem: 19734
Epoch: [7]  [ 230/1251]  eta: 0:14:35  lr: 0.000020  loss: 3.6014 (3.5427)  time: 0.8004  data: 0.0006  max mem: 19734
Epoch: [7]  [ 240/1251]  eta: 0:14:24  lr: 0.000020  loss: 3.4105 (3.5311)  time: 0.8047  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5251, ratio_loss=0.0077, pruning_loss=0.1899, mse_loss=0.9703
Epoch: [7]  [ 250/1251]  eta: 0:14:13  lr: 0.000020  loss: 3.2433 (3.5132)  time: 0.8043  data: 0.0007  max mem: 19734
Epoch: [7]  [ 260/1251]  eta: 0:14:03  lr: 0.000020  loss: 3.3397 (3.5137)  time: 0.8034  data: 0.0007  max mem: 19734
Epoch: [7]  [ 270/1251]  eta: 0:13:53  lr: 0.000020  loss: 3.6495 (3.5206)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [7]  [ 280/1251]  eta: 0:13:43  lr: 0.000020  loss: 3.6495 (3.5168)  time: 0.8102  data: 0.0005  max mem: 19734
Epoch: [7]  [ 290/1251]  eta: 0:13:33  lr: 0.000020  loss: 3.1211 (3.5042)  time: 0.8100  data: 0.0005  max mem: 19734
Epoch: [7]  [ 300/1251]  eta: 0:13:25  lr: 0.000020  loss: 3.3127 (3.5029)  time: 0.8303  data: 0.0005  max mem: 19734
Epoch: [7]  [ 310/1251]  eta: 0:13:16  lr: 0.000020  loss: 3.5941 (3.4994)  time: 0.8428  data: 0.0005  max mem: 19734
Epoch: [7]  [ 320/1251]  eta: 0:13:07  lr: 0.000020  loss: 3.5595 (3.5007)  time: 0.8251  data: 0.0005  max mem: 19734
Epoch: [7]  [ 330/1251]  eta: 0:12:57  lr: 0.000020  loss: 3.3981 (3.4972)  time: 0.8109  data: 0.0005  max mem: 19734
Epoch: [7]  [ 340/1251]  eta: 0:12:48  lr: 0.000020  loss: 3.3924 (3.4938)  time: 0.8037  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3787, ratio_loss=0.0082, pruning_loss=0.1914, mse_loss=0.9909
Epoch: [7]  [ 350/1251]  eta: 0:12:38  lr: 0.000020  loss: 3.5308 (3.5013)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [7]  [ 360/1251]  eta: 0:12:29  lr: 0.000020  loss: 3.6199 (3.4965)  time: 0.8120  data: 0.0005  max mem: 19734
Epoch: [7]  [ 370/1251]  eta: 0:12:20  lr: 0.000020  loss: 3.4037 (3.4956)  time: 0.8121  data: 0.0007  max mem: 19734
Epoch: [7]  [ 380/1251]  eta: 0:12:11  lr: 0.000020  loss: 3.3663 (3.4939)  time: 0.8035  data: 0.0008  max mem: 19734
Epoch: [7]  [ 390/1251]  eta: 0:12:01  lr: 0.000020  loss: 3.5353 (3.4954)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [7]  [ 400/1251]  eta: 0:11:52  lr: 0.000020  loss: 3.3773 (3.4907)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [7]  [ 410/1251]  eta: 0:11:43  lr: 0.000020  loss: 3.3344 (3.4937)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [7]  [ 420/1251]  eta: 0:11:34  lr: 0.000020  loss: 3.6192 (3.4878)  time: 0.8101  data: 0.0005  max mem: 19734
Epoch: [7]  [ 430/1251]  eta: 0:11:25  lr: 0.000020  loss: 3.3323 (3.4851)  time: 0.8100  data: 0.0005  max mem: 19734
Epoch: [7]  [ 440/1251]  eta: 0:11:17  lr: 0.000020  loss: 3.3709 (3.4878)  time: 0.8153  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4392, ratio_loss=0.0078, pruning_loss=0.1933, mse_loss=1.0177
Epoch: [7]  [ 450/1251]  eta: 0:11:09  lr: 0.000020  loss: 3.6403 (3.4851)  time: 0.8452  data: 0.0005  max mem: 19734
Epoch: [7]  [ 460/1251]  eta: 0:11:00  lr: 0.000020  loss: 3.7423 (3.4917)  time: 0.8396  data: 0.0005  max mem: 19734
Epoch: [7]  [ 470/1251]  eta: 0:10:51  lr: 0.000020  loss: 3.7423 (3.4921)  time: 0.8101  data: 0.0005  max mem: 19734
Epoch: [7]  [ 480/1251]  eta: 0:10:43  lr: 0.000020  loss: 3.3428 (3.4881)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [7]  [ 490/1251]  eta: 0:10:34  lr: 0.000020  loss: 3.3410 (3.4855)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [7]  [ 500/1251]  eta: 0:10:25  lr: 0.000020  loss: 3.5904 (3.4889)  time: 0.8009  data: 0.0004  max mem: 19734
Epoch: [7]  [ 510/1251]  eta: 0:10:16  lr: 0.000020  loss: 3.7066 (3.4932)  time: 0.8082  data: 0.0004  max mem: 19734
Epoch: [7]  [ 520/1251]  eta: 0:10:08  lr: 0.000020  loss: 3.6681 (3.4934)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [7]  [ 530/1251]  eta: 0:09:59  lr: 0.000020  loss: 3.5743 (3.4931)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [7]  [ 540/1251]  eta: 0:09:50  lr: 0.000020  loss: 3.3236 (3.4889)  time: 0.8020  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4634, ratio_loss=0.0080, pruning_loss=0.1893, mse_loss=0.9806
Epoch: [7]  [ 550/1251]  eta: 0:09:42  lr: 0.000020  loss: 3.4842 (3.4911)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [7]  [ 560/1251]  eta: 0:09:33  lr: 0.000020  loss: 3.5384 (3.4887)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [7]  [ 570/1251]  eta: 0:09:24  lr: 0.000020  loss: 3.3757 (3.4852)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [7]  [ 580/1251]  eta: 0:09:16  lr: 0.000020  loss: 3.3798 (3.4837)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [7]  [ 590/1251]  eta: 0:09:08  lr: 0.000020  loss: 3.6000 (3.4832)  time: 0.8202  data: 0.0005  max mem: 19734
Epoch: [7]  [ 600/1251]  eta: 0:08:59  lr: 0.000020  loss: 3.5464 (3.4821)  time: 0.8363  data: 0.0005  max mem: 19734
Epoch: [7]  [ 610/1251]  eta: 0:08:51  lr: 0.000020  loss: 3.7780 (3.4876)  time: 0.8258  data: 0.0005  max mem: 19734
Epoch: [7]  [ 620/1251]  eta: 0:08:42  lr: 0.000020  loss: 3.7081 (3.4836)  time: 0.8103  data: 0.0005  max mem: 19734
Epoch: [7]  [ 630/1251]  eta: 0:08:34  lr: 0.000020  loss: 3.3673 (3.4845)  time: 0.8033  data: 0.0004  max mem: 19734
Epoch: [7]  [ 640/1251]  eta: 0:08:25  lr: 0.000020  loss: 3.7480 (3.4876)  time: 0.8047  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4434, ratio_loss=0.0080, pruning_loss=0.1886, mse_loss=0.9804
Epoch: [7]  [ 650/1251]  eta: 0:08:17  lr: 0.000020  loss: 3.7790 (3.4882)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [7]  [ 660/1251]  eta: 0:08:08  lr: 0.000020  loss: 3.8062 (3.4917)  time: 0.8088  data: 0.0004  max mem: 19734
Epoch: [7]  [ 670/1251]  eta: 0:08:00  lr: 0.000020  loss: 3.8632 (3.4957)  time: 0.8093  data: 0.0005  max mem: 19734
Epoch: [7]  [ 680/1251]  eta: 0:07:51  lr: 0.000020  loss: 3.7634 (3.4967)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [7]  [ 690/1251]  eta: 0:07:43  lr: 0.000020  loss: 3.7457 (3.4990)  time: 0.8036  data: 0.0004  max mem: 19734
Epoch: [7]  [ 700/1251]  eta: 0:07:35  lr: 0.000020  loss: 3.5484 (3.4970)  time: 0.8055  data: 0.0004  max mem: 19734
Epoch: [7]  [ 710/1251]  eta: 0:07:26  lr: 0.000020  loss: 3.5505 (3.4980)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [7]  [ 720/1251]  eta: 0:07:18  lr: 0.000020  loss: 3.6347 (3.4987)  time: 0.8003  data: 0.0005  max mem: 19734
Epoch: [7]  [ 730/1251]  eta: 0:07:09  lr: 0.000020  loss: 3.7973 (3.5027)  time: 0.8030  data: 0.0004  max mem: 19734
Epoch: [7]  [ 740/1251]  eta: 0:07:01  lr: 0.000020  loss: 3.7973 (3.5016)  time: 0.8344  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5652, ratio_loss=0.0079, pruning_loss=0.1837, mse_loss=0.9463
Epoch: [7]  [ 750/1251]  eta: 0:06:53  lr: 0.000020  loss: 3.0925 (3.4941)  time: 0.8450  data: 0.0005  max mem: 19734
Epoch: [7]  [ 760/1251]  eta: 0:06:45  lr: 0.000020  loss: 3.0164 (3.4899)  time: 0.8132  data: 0.0004  max mem: 19734
Epoch: [7]  [ 770/1251]  eta: 0:06:36  lr: 0.000020  loss: 3.5433 (3.4934)  time: 0.8008  data: 0.0006  max mem: 19734
Epoch: [7]  [ 780/1251]  eta: 0:06:28  lr: 0.000020  loss: 3.7682 (3.4956)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [7]  [ 790/1251]  eta: 0:06:19  lr: 0.000020  loss: 3.4613 (3.4949)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [7]  [ 800/1251]  eta: 0:06:11  lr: 0.000020  loss: 3.3903 (3.4946)  time: 0.8105  data: 0.0005  max mem: 19734
Epoch: [7]  [ 810/1251]  eta: 0:06:03  lr: 0.000020  loss: 3.6102 (3.4945)  time: 0.8080  data: 0.0006  max mem: 19734
Epoch: [7]  [ 820/1251]  eta: 0:05:54  lr: 0.000020  loss: 3.4526 (3.4941)  time: 0.8014  data: 0.0006  max mem: 19734
Epoch: [7]  [ 830/1251]  eta: 0:05:46  lr: 0.000020  loss: 3.5331 (3.4948)  time: 0.8010  data: 0.0004  max mem: 19734
Epoch: [7]  [ 840/1251]  eta: 0:05:38  lr: 0.000020  loss: 3.5599 (3.4925)  time: 0.8013  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4036, ratio_loss=0.0081, pruning_loss=0.1879, mse_loss=0.9385
Epoch: [7]  [ 850/1251]  eta: 0:05:29  lr: 0.000020  loss: 3.3768 (3.4920)  time: 0.8027  data: 0.0004  max mem: 19734
Epoch: [7]  [ 860/1251]  eta: 0:05:21  lr: 0.000020  loss: 3.4804 (3.4909)  time: 0.8000  data: 0.0004  max mem: 19734
Epoch: [7]  [ 870/1251]  eta: 0:05:13  lr: 0.000020  loss: 3.6105 (3.4926)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [7]  [ 880/1251]  eta: 0:05:05  lr: 0.000020  loss: 3.6176 (3.4921)  time: 0.8201  data: 0.0005  max mem: 19734
Epoch: [7]  [ 890/1251]  eta: 0:04:57  lr: 0.000020  loss: 3.4130 (3.4900)  time: 0.8455  data: 0.0005  max mem: 19734
Epoch: [7]  [ 900/1251]  eta: 0:04:48  lr: 0.000020  loss: 3.4248 (3.4915)  time: 0.8373  data: 0.0004  max mem: 19734
Epoch: [7]  [ 910/1251]  eta: 0:04:40  lr: 0.000020  loss: 3.7205 (3.4943)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [7]  [ 920/1251]  eta: 0:04:32  lr: 0.000020  loss: 3.6960 (3.4968)  time: 0.8007  data: 0.0004  max mem: 19734
Epoch: [7]  [ 930/1251]  eta: 0:04:23  lr: 0.000020  loss: 3.6632 (3.4983)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [7]  [ 940/1251]  eta: 0:04:15  lr: 0.000020  loss: 3.6079 (3.4996)  time: 0.8018  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5364, ratio_loss=0.0076, pruning_loss=0.1823, mse_loss=0.9782
Epoch: [7]  [ 950/1251]  eta: 0:04:07  lr: 0.000020  loss: 3.6217 (3.5013)  time: 0.8061  data: 0.0005  max mem: 19734
Epoch: [7]  [ 960/1251]  eta: 0:03:59  lr: 0.000020  loss: 3.6148 (3.5017)  time: 0.8067  data: 0.0005  max mem: 19734
Epoch: [7]  [ 970/1251]  eta: 0:03:50  lr: 0.000020  loss: 3.5492 (3.5000)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [7]  [ 980/1251]  eta: 0:03:42  lr: 0.000020  loss: 3.5809 (3.5000)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [7]  [ 990/1251]  eta: 0:03:34  lr: 0.000020  loss: 3.6423 (3.5004)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [7]  [1000/1251]  eta: 0:03:26  lr: 0.000020  loss: 3.5161 (3.4990)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [7]  [1010/1251]  eta: 0:03:17  lr: 0.000020  loss: 3.5254 (3.4987)  time: 0.8041  data: 0.0004  max mem: 19734
Epoch: [7]  [1020/1251]  eta: 0:03:09  lr: 0.000020  loss: 3.5252 (3.4971)  time: 0.8028  data: 0.0004  max mem: 19734
Epoch: [7]  [1030/1251]  eta: 0:03:01  lr: 0.000020  loss: 3.4692 (3.4974)  time: 0.8247  data: 0.0005  max mem: 19734
Epoch: [7]  [1040/1251]  eta: 0:02:53  lr: 0.000020  loss: 3.4999 (3.4978)  time: 0.8434  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4500, ratio_loss=0.0085, pruning_loss=0.1825, mse_loss=1.0011
Epoch: [7]  [1050/1251]  eta: 0:02:44  lr: 0.000020  loss: 3.5958 (3.4986)  time: 0.8195  data: 0.0004  max mem: 19734
Epoch: [7]  [1060/1251]  eta: 0:02:36  lr: 0.000020  loss: 3.4846 (3.4960)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [7]  [1070/1251]  eta: 0:02:28  lr: 0.000020  loss: 3.5684 (3.4973)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [7]  [1080/1251]  eta: 0:02:20  lr: 0.000020  loss: 3.5240 (3.4951)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [7]  [1090/1251]  eta: 0:02:12  lr: 0.000020  loss: 3.3357 (3.4939)  time: 0.8019  data: 0.0006  max mem: 19734
Epoch: [7]  [1100/1251]  eta: 0:02:03  lr: 0.000020  loss: 3.3620 (3.4923)  time: 0.8081  data: 0.0006  max mem: 19734
Epoch: [7]  [1110/1251]  eta: 0:01:55  lr: 0.000020  loss: 3.5437 (3.4924)  time: 0.8060  data: 0.0004  max mem: 19734
Epoch: [7]  [1120/1251]  eta: 0:01:47  lr: 0.000020  loss: 3.5587 (3.4923)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [7]  [1130/1251]  eta: 0:01:39  lr: 0.000020  loss: 3.6417 (3.4922)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [7]  [1140/1251]  eta: 0:01:30  lr: 0.000020  loss: 3.5250 (3.4906)  time: 0.8018  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3743, ratio_loss=0.0078, pruning_loss=0.1835, mse_loss=0.9967
Epoch: [7]  [1150/1251]  eta: 0:01:22  lr: 0.000020  loss: 3.3848 (3.4895)  time: 0.8019  data: 0.0004  max mem: 19734
Epoch: [7]  [1160/1251]  eta: 0:01:14  lr: 0.000020  loss: 3.6687 (3.4917)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [7]  [1170/1251]  eta: 0:01:06  lr: 0.000020  loss: 3.7706 (3.4916)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [7]  [1180/1251]  eta: 0:00:58  lr: 0.000020  loss: 3.5614 (3.4909)  time: 0.8370  data: 0.0005  max mem: 19734
Epoch: [7]  [1190/1251]  eta: 0:00:49  lr: 0.000020  loss: 3.5614 (3.4918)  time: 0.8372  data: 0.0008  max mem: 19734
Epoch: [7]  [1200/1251]  eta: 0:00:41  lr: 0.000020  loss: 3.5065 (3.4891)  time: 0.8062  data: 0.0006  max mem: 19734
Epoch: [7]  [1210/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.1882 (3.4892)  time: 0.7896  data: 0.0002  max mem: 19734
Epoch: [7]  [1220/1251]  eta: 0:00:25  lr: 0.000020  loss: 3.5239 (3.4885)  time: 0.7904  data: 0.0001  max mem: 19734
Epoch: [7]  [1230/1251]  eta: 0:00:17  lr: 0.000020  loss: 3.5239 (3.4883)  time: 0.7906  data: 0.0001  max mem: 19734
Epoch: [7]  [1240/1251]  eta: 0:00:09  lr: 0.000020  loss: 3.4613 (3.4872)  time: 0.7908  data: 0.0001  max mem: 19734
loss info: cls_loss=3.4343, ratio_loss=0.0085, pruning_loss=0.1823, mse_loss=0.9784
Epoch: [7]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.4728 (3.4883)  time: 0.7967  data: 0.0001  max mem: 19734
Epoch: [7] Total time: 0:17:04 (0.8191 s / it)
Averaged stats: lr: 0.000020  loss: 3.4728 (3.4848)
Test:  [  0/261]  eta: 1:41:08  loss: 0.8551 (0.8551)  acc1: 82.8125 (82.8125)  acc5: 95.3125 (95.3125)  time: 23.2497  data: 22.9355  max mem: 19734
Test:  [ 10/261]  eta: 0:14:36  loss: 0.7740 (0.8503)  acc1: 85.4167 (83.2860)  acc5: 96.3542 (95.3599)  time: 3.4938  data: 3.2719  max mem: 19734
Test:  [ 20/261]  eta: 0:07:41  loss: 1.0540 (0.9963)  acc1: 80.2083 (78.3482)  acc5: 92.7083 (93.9732)  time: 0.8496  data: 0.6590  max mem: 19734
Test:  [ 30/261]  eta: 0:05:21  loss: 0.8605 (0.9110)  acc1: 81.7708 (81.3004)  acc5: 93.2292 (94.5565)  time: 0.2327  data: 0.0137  max mem: 19734
Test:  [ 40/261]  eta: 0:04:45  loss: 0.6870 (0.8828)  acc1: 88.0208 (82.0122)  acc5: 95.8333 (94.8806)  time: 0.6365  data: 0.3957  max mem: 19734
Test:  [ 50/261]  eta: 0:03:55  loss: 1.0210 (0.9478)  acc1: 76.0417 (79.9735)  acc5: 93.2292 (94.3627)  time: 0.6930  data: 0.3937  max mem: 19734
Test:  [ 60/261]  eta: 0:03:16  loss: 1.0943 (0.9612)  acc1: 75.5208 (79.4484)  acc5: 93.2292 (94.3904)  time: 0.3283  data: 0.0149  max mem: 19734
Test:  [ 70/261]  eta: 0:03:14  loss: 1.0621 (0.9572)  acc1: 77.6042 (79.0420)  acc5: 94.7917 (94.6303)  time: 0.7600  data: 0.5245  max mem: 19734
Test:  [ 80/261]  eta: 0:02:47  loss: 0.9150 (0.9642)  acc1: 79.6875 (79.1345)  acc5: 95.8333 (94.7274)  time: 0.7645  data: 0.5220  max mem: 19734
Test:  [ 90/261]  eta: 0:02:27  loss: 0.9099 (0.9484)  acc1: 81.7708 (79.6131)  acc5: 95.3125 (94.8432)  time: 0.3052  data: 0.0116  max mem: 19734
Test:  [100/261]  eta: 0:02:07  loss: 0.9099 (0.9477)  acc1: 83.3333 (79.6205)  acc5: 94.7917 (94.9154)  time: 0.2593  data: 0.0134  max mem: 19734
Test:  [110/261]  eta: 0:02:04  loss: 1.0153 (0.9732)  acc1: 76.5625 (79.0963)  acc5: 94.2708 (94.5899)  time: 0.6498  data: 0.4195  max mem: 19734
Test:  [120/261]  eta: 0:01:49  loss: 1.3440 (1.0174)  acc1: 70.3125 (78.0777)  acc5: 89.0625 (93.9997)  time: 0.6979  data: 0.4141  max mem: 19734
Test:  [130/261]  eta: 0:01:36  loss: 1.5544 (1.0656)  acc1: 67.1875 (77.1589)  acc5: 85.4167 (93.3763)  time: 0.2551  data: 0.0097  max mem: 19734
Test:  [140/261]  eta: 0:01:28  loss: 1.4212 (1.0922)  acc1: 68.7500 (76.5145)  acc5: 90.1042 (93.1368)  time: 0.4512  data: 0.2591  max mem: 19734
Test:  [150/261]  eta: 0:01:16  loss: 1.3110 (1.0983)  acc1: 70.8333 (76.4935)  acc5: 91.1458 (92.9636)  time: 0.4146  data: 0.2582  max mem: 19734
Test:  [160/261]  eta: 0:01:06  loss: 1.1884 (1.1211)  acc1: 77.0833 (76.0967)  acc5: 91.6667 (92.6663)  time: 0.1739  data: 0.0107  max mem: 19734
Test:  [170/261]  eta: 0:01:00  loss: 1.5052 (1.1545)  acc1: 63.5417 (75.2345)  acc5: 85.9375 (92.2636)  time: 0.4212  data: 0.2598  max mem: 19734
Test:  [180/261]  eta: 0:00:51  loss: 1.6122 (1.1737)  acc1: 63.5417 (74.8015)  acc5: 85.9375 (92.0810)  time: 0.4038  data: 0.2567  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: 1.4776 (1.1875)  acc1: 66.6667 (74.5746)  acc5: 90.1042 (91.9148)  time: 0.1351  data: 0.0043  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.4776 (1.2041)  acc1: 69.2708 (74.2330)  acc5: 89.0625 (91.6822)  time: 0.1192  data: 0.0023  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.4835 (1.2191)  acc1: 66.6667 (73.9361)  acc5: 87.5000 (91.4692)  time: 0.1167  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.5758 (1.2381)  acc1: 64.0625 (73.4964)  acc5: 87.5000 (91.2802)  time: 0.1156  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.5767 (1.2482)  acc1: 63.5417 (73.2481)  acc5: 88.5417 (91.1391)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.5096 (1.2578)  acc1: 64.5833 (73.0096)  acc5: 89.0625 (91.0659)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.2690 (1.2516)  acc1: 71.8750 (73.2051)  acc5: 91.6667 (91.1832)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.1146 (1.2516)  acc1: 76.0417 (73.2380)  acc5: 94.7917 (91.2340)  time: 0.1115  data: 0.0001  max mem: 19734
Test: Total time: 0:02:03 (0.4751 s / it)
* Acc@1 73.238 Acc@5 91.234 loss 1.252
Accuracy of the network on the 50000 test images: 73.2%
Max accuracy: 73.24%
Epoch: [8]  [   0/1251]  eta: 3:10:44  lr: 0.000020  loss: 3.6435 (3.6435)  time: 9.1484  data: 8.3681  max mem: 19734
Epoch: [8]  [  10/1251]  eta: 0:33:17  lr: 0.000020  loss: 3.7074 (3.7107)  time: 1.6095  data: 0.7615  max mem: 19734
Epoch: [8]  [  20/1251]  eta: 0:25:03  lr: 0.000020  loss: 3.7201 (3.6965)  time: 0.8254  data: 0.0007  max mem: 19734
Epoch: [8]  [  30/1251]  eta: 0:22:02  lr: 0.000020  loss: 3.7831 (3.7108)  time: 0.7933  data: 0.0005  max mem: 19734
Epoch: [8]  [  40/1251]  eta: 0:20:27  lr: 0.000020  loss: 3.6953 (3.5940)  time: 0.7952  data: 0.0006  max mem: 19734
Epoch: [8]  [  50/1251]  eta: 0:19:27  lr: 0.000020  loss: 3.6915 (3.6279)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [8]  [  60/1251]  eta: 0:18:48  lr: 0.000020  loss: 3.6915 (3.6105)  time: 0.8117  data: 0.0004  max mem: 19734
Epoch: [8]  [  70/1251]  eta: 0:18:22  lr: 0.000020  loss: 3.5660 (3.6105)  time: 0.8341  data: 0.0004  max mem: 19734
Epoch: [8]  [  80/1251]  eta: 0:17:58  lr: 0.000020  loss: 3.6609 (3.6011)  time: 0.8401  data: 0.0004  max mem: 19734
Epoch: [8]  [  90/1251]  eta: 0:17:33  lr: 0.000020  loss: 3.6538 (3.5988)  time: 0.8165  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5684, ratio_loss=0.0080, pruning_loss=0.1762, mse_loss=0.9538
Epoch: [8]  [ 100/1251]  eta: 0:17:12  lr: 0.000020  loss: 3.5453 (3.5772)  time: 0.8012  data: 0.0004  max mem: 19734
Epoch: [8]  [ 110/1251]  eta: 0:16:53  lr: 0.000020  loss: 3.6522 (3.5888)  time: 0.8005  data: 0.0004  max mem: 19734
Epoch: [8]  [ 120/1251]  eta: 0:16:36  lr: 0.000020  loss: 3.6522 (3.5800)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [8]  [ 130/1251]  eta: 0:16:21  lr: 0.000020  loss: 3.5712 (3.5782)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [8]  [ 140/1251]  eta: 0:16:07  lr: 0.000020  loss: 3.5388 (3.5568)  time: 0.8077  data: 0.0004  max mem: 19734
Epoch: [8]  [ 150/1251]  eta: 0:15:54  lr: 0.000020  loss: 3.2312 (3.5454)  time: 0.8100  data: 0.0005  max mem: 19734
Epoch: [8]  [ 160/1251]  eta: 0:15:41  lr: 0.000020  loss: 3.5064 (3.5524)  time: 0.8067  data: 0.0005  max mem: 19734
Epoch: [8]  [ 170/1251]  eta: 0:15:28  lr: 0.000020  loss: 3.6573 (3.5595)  time: 0.8013  data: 0.0006  max mem: 19734
Epoch: [8]  [ 180/1251]  eta: 0:15:16  lr: 0.000020  loss: 3.7475 (3.5479)  time: 0.7981  data: 0.0006  max mem: 19734
Epoch: [8]  [ 190/1251]  eta: 0:15:04  lr: 0.000020  loss: 3.7158 (3.5566)  time: 0.7985  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4821, ratio_loss=0.0079, pruning_loss=0.1814, mse_loss=0.9521
Epoch: [8]  [ 200/1251]  eta: 0:14:53  lr: 0.000020  loss: 3.7132 (3.5603)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [8]  [ 210/1251]  eta: 0:14:44  lr: 0.000020  loss: 3.4892 (3.5479)  time: 0.8199  data: 0.0005  max mem: 19734
Epoch: [8]  [ 220/1251]  eta: 0:14:36  lr: 0.000020  loss: 3.1780 (3.5340)  time: 0.8448  data: 0.0005  max mem: 19734
Epoch: [8]  [ 230/1251]  eta: 0:14:25  lr: 0.000020  loss: 3.2455 (3.5319)  time: 0.8284  data: 0.0005  max mem: 19734
Epoch: [8]  [ 240/1251]  eta: 0:14:15  lr: 0.000020  loss: 3.7109 (3.5413)  time: 0.8054  data: 0.0006  max mem: 19734
Epoch: [8]  [ 250/1251]  eta: 0:14:05  lr: 0.000020  loss: 3.8367 (3.5525)  time: 0.8049  data: 0.0008  max mem: 19734
Epoch: [8]  [ 260/1251]  eta: 0:13:55  lr: 0.000020  loss: 3.8155 (3.5540)  time: 0.8058  data: 0.0009  max mem: 19734
Epoch: [8]  [ 270/1251]  eta: 0:13:45  lr: 0.000020  loss: 3.5459 (3.5457)  time: 0.8042  data: 0.0007  max mem: 19734
Epoch: [8]  [ 280/1251]  eta: 0:13:35  lr: 0.000020  loss: 3.5639 (3.5493)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [8]  [ 290/1251]  eta: 0:13:26  lr: 0.000020  loss: 3.5900 (3.5483)  time: 0.8014  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5153, ratio_loss=0.0078, pruning_loss=0.1775, mse_loss=0.9426
Epoch: [8]  [ 300/1251]  eta: 0:13:16  lr: 0.000020  loss: 3.6047 (3.5556)  time: 0.8044  data: 0.0006  max mem: 19734
Epoch: [8]  [ 310/1251]  eta: 0:13:07  lr: 0.000020  loss: 3.4407 (3.5464)  time: 0.8076  data: 0.0006  max mem: 19734
Epoch: [8]  [ 320/1251]  eta: 0:12:58  lr: 0.000020  loss: 3.2959 (3.5423)  time: 0.8085  data: 0.0005  max mem: 19734
Epoch: [8]  [ 330/1251]  eta: 0:12:48  lr: 0.000020  loss: 3.6622 (3.5463)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [8]  [ 340/1251]  eta: 0:12:39  lr: 0.000020  loss: 3.6622 (3.5455)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [8]  [ 350/1251]  eta: 0:12:31  lr: 0.000020  loss: 3.6966 (3.5461)  time: 0.8244  data: 0.0006  max mem: 19734
Epoch: [8]  [ 360/1251]  eta: 0:12:23  lr: 0.000020  loss: 3.6966 (3.5476)  time: 0.8389  data: 0.0005  max mem: 19734
Epoch: [8]  [ 370/1251]  eta: 0:12:15  lr: 0.000020  loss: 3.5132 (3.5365)  time: 0.8347  data: 0.0005  max mem: 19734
Epoch: [8]  [ 380/1251]  eta: 0:12:06  lr: 0.000020  loss: 3.1064 (3.5316)  time: 0.8244  data: 0.0004  max mem: 19734
Epoch: [8]  [ 390/1251]  eta: 0:11:57  lr: 0.000020  loss: 3.4584 (3.5297)  time: 0.8072  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4423, ratio_loss=0.0079, pruning_loss=0.1755, mse_loss=0.9291
Epoch: [8]  [ 400/1251]  eta: 0:11:48  lr: 0.000020  loss: 3.3807 (3.5226)  time: 0.8041  data: 0.0006  max mem: 19734
Epoch: [8]  [ 410/1251]  eta: 0:11:39  lr: 0.000020  loss: 3.3807 (3.5221)  time: 0.8063  data: 0.0007  max mem: 19734
Epoch: [8]  [ 420/1251]  eta: 0:11:30  lr: 0.000020  loss: 3.3237 (3.5146)  time: 0.8068  data: 0.0006  max mem: 19734
Epoch: [8]  [ 430/1251]  eta: 0:11:21  lr: 0.000020  loss: 3.3237 (3.5111)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [8]  [ 440/1251]  eta: 0:11:12  lr: 0.000020  loss: 3.3962 (3.5127)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [8]  [ 450/1251]  eta: 0:11:04  lr: 0.000020  loss: 3.5588 (3.5114)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [8]  [ 460/1251]  eta: 0:10:55  lr: 0.000020  loss: 3.6404 (3.5105)  time: 0.8075  data: 0.0005  max mem: 19734
Epoch: [8]  [ 470/1251]  eta: 0:10:47  lr: 0.000020  loss: 3.7119 (3.5140)  time: 0.8112  data: 0.0005  max mem: 19734
Epoch: [8]  [ 480/1251]  eta: 0:10:38  lr: 0.000020  loss: 3.5661 (3.5112)  time: 0.8087  data: 0.0006  max mem: 19734
Epoch: [8]  [ 490/1251]  eta: 0:10:29  lr: 0.000020  loss: 3.5385 (3.5121)  time: 0.8021  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4175, ratio_loss=0.0079, pruning_loss=0.1796, mse_loss=0.9420
Epoch: [8]  [ 500/1251]  eta: 0:10:21  lr: 0.000020  loss: 3.6503 (3.5146)  time: 0.8068  data: 0.0005  max mem: 19734
Epoch: [8]  [ 510/1251]  eta: 0:10:13  lr: 0.000020  loss: 3.5802 (3.5140)  time: 0.8342  data: 0.0004  max mem: 19734
Epoch: [8]  [ 520/1251]  eta: 0:10:05  lr: 0.000020  loss: 3.5175 (3.5159)  time: 0.8442  data: 0.0004  max mem: 19734
Epoch: [8]  [ 530/1251]  eta: 0:09:56  lr: 0.000020  loss: 3.2211 (3.5085)  time: 0.8169  data: 0.0004  max mem: 19734
Epoch: [8]  [ 540/1251]  eta: 0:09:47  lr: 0.000020  loss: 3.1060 (3.5105)  time: 0.8025  data: 0.0004  max mem: 19734
Epoch: [8]  [ 550/1251]  eta: 0:09:39  lr: 0.000020  loss: 3.6837 (3.5084)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [8]  [ 560/1251]  eta: 0:09:30  lr: 0.000020  loss: 3.1275 (3.4994)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [8]  [ 570/1251]  eta: 0:09:22  lr: 0.000020  loss: 3.1275 (3.4998)  time: 0.8050  data: 0.0004  max mem: 19734
Epoch: [8]  [ 580/1251]  eta: 0:09:13  lr: 0.000020  loss: 3.7237 (3.5007)  time: 0.8035  data: 0.0004  max mem: 19734
Epoch: [8]  [ 590/1251]  eta: 0:09:05  lr: 0.000020  loss: 3.4606 (3.4970)  time: 0.8036  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3964, ratio_loss=0.0075, pruning_loss=0.1782, mse_loss=0.9348
Epoch: [8]  [ 600/1251]  eta: 0:08:56  lr: 0.000020  loss: 3.4388 (3.4961)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [8]  [ 610/1251]  eta: 0:08:48  lr: 0.000020  loss: 3.4495 (3.4951)  time: 0.8062  data: 0.0005  max mem: 19734
Epoch: [8]  [ 620/1251]  eta: 0:08:39  lr: 0.000020  loss: 3.4334 (3.4937)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [8]  [ 630/1251]  eta: 0:08:31  lr: 0.000020  loss: 3.6798 (3.4995)  time: 0.8032  data: 0.0008  max mem: 19734
Epoch: [8]  [ 640/1251]  eta: 0:08:23  lr: 0.000020  loss: 3.6197 (3.4967)  time: 0.8117  data: 0.0008  max mem: 19734
Epoch: [8]  [ 650/1251]  eta: 0:08:15  lr: 0.000020  loss: 3.3512 (3.4952)  time: 0.8372  data: 0.0004  max mem: 19734
Epoch: [8]  [ 660/1251]  eta: 0:08:07  lr: 0.000020  loss: 3.4101 (3.4965)  time: 0.8484  data: 0.0005  max mem: 19734
Epoch: [8]  [ 670/1251]  eta: 0:07:58  lr: 0.000020  loss: 3.5458 (3.4961)  time: 0.8338  data: 0.0005  max mem: 19734
Epoch: [8]  [ 680/1251]  eta: 0:07:50  lr: 0.000020  loss: 3.7539 (3.5005)  time: 0.8164  data: 0.0005  max mem: 19734
Epoch: [8]  [ 690/1251]  eta: 0:07:42  lr: 0.000020  loss: 3.6954 (3.5007)  time: 0.8046  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4905, ratio_loss=0.0076, pruning_loss=0.1750, mse_loss=0.9488
Epoch: [8]  [ 700/1251]  eta: 0:07:33  lr: 0.000020  loss: 3.5872 (3.5027)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [8]  [ 710/1251]  eta: 0:07:25  lr: 0.000020  loss: 3.6184 (3.5002)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [8]  [ 720/1251]  eta: 0:07:17  lr: 0.000020  loss: 3.6184 (3.4990)  time: 0.8104  data: 0.0004  max mem: 19734
Epoch: [8]  [ 730/1251]  eta: 0:07:08  lr: 0.000020  loss: 3.7001 (3.5005)  time: 0.8042  data: 0.0006  max mem: 19734
Epoch: [8]  [ 740/1251]  eta: 0:07:00  lr: 0.000020  loss: 3.6889 (3.5014)  time: 0.8033  data: 0.0007  max mem: 19734
Epoch: [8]  [ 750/1251]  eta: 0:06:52  lr: 0.000020  loss: 3.5487 (3.5014)  time: 0.8044  data: 0.0006  max mem: 19734
Epoch: [8]  [ 760/1251]  eta: 0:06:43  lr: 0.000020  loss: 3.4181 (3.4995)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [8]  [ 770/1251]  eta: 0:06:35  lr: 0.000020  loss: 3.3443 (3.4975)  time: 0.8016  data: 0.0004  max mem: 19734
Epoch: [8]  [ 780/1251]  eta: 0:06:26  lr: 0.000020  loss: 3.5371 (3.4990)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [8]  [ 790/1251]  eta: 0:06:18  lr: 0.000020  loss: 3.4895 (3.4948)  time: 0.8092  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4357, ratio_loss=0.0078, pruning_loss=0.1748, mse_loss=0.9086
Epoch: [8]  [ 800/1251]  eta: 0:06:10  lr: 0.000020  loss: 3.4550 (3.4959)  time: 0.8419  data: 0.0005  max mem: 19734
Epoch: [8]  [ 810/1251]  eta: 0:06:02  lr: 0.000020  loss: 3.7417 (3.4953)  time: 0.8390  data: 0.0006  max mem: 19734
Epoch: [8]  [ 820/1251]  eta: 0:05:54  lr: 0.000020  loss: 3.4170 (3.4917)  time: 0.8148  data: 0.0005  max mem: 19734
Epoch: [8]  [ 830/1251]  eta: 0:05:45  lr: 0.000020  loss: 3.4886 (3.4926)  time: 0.8105  data: 0.0005  max mem: 19734
Epoch: [8]  [ 840/1251]  eta: 0:05:37  lr: 0.000020  loss: 3.6196 (3.4921)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [8]  [ 850/1251]  eta: 0:05:29  lr: 0.000020  loss: 3.7572 (3.4919)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [8]  [ 860/1251]  eta: 0:05:21  lr: 0.000020  loss: 3.6774 (3.4928)  time: 0.8046  data: 0.0006  max mem: 19734
Epoch: [8]  [ 870/1251]  eta: 0:05:12  lr: 0.000020  loss: 3.6385 (3.4935)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [8]  [ 880/1251]  eta: 0:05:04  lr: 0.000020  loss: 3.5976 (3.4928)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [8]  [ 890/1251]  eta: 0:04:56  lr: 0.000020  loss: 3.4680 (3.4912)  time: 0.8048  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4208, ratio_loss=0.0075, pruning_loss=0.1767, mse_loss=0.9387
Epoch: [8]  [ 900/1251]  eta: 0:04:47  lr: 0.000020  loss: 3.5789 (3.4924)  time: 0.8071  data: 0.0005  max mem: 19734
Epoch: [8]  [ 910/1251]  eta: 0:04:39  lr: 0.000020  loss: 3.7257 (3.4899)  time: 0.8081  data: 0.0009  max mem: 19734
Epoch: [8]  [ 920/1251]  eta: 0:04:31  lr: 0.000020  loss: 3.2246 (3.4877)  time: 0.8084  data: 0.0009  max mem: 19734
Epoch: [8]  [ 930/1251]  eta: 0:04:23  lr: 0.000020  loss: 3.2756 (3.4863)  time: 0.8205  data: 0.0004  max mem: 19734
Epoch: [8]  [ 940/1251]  eta: 0:04:15  lr: 0.000020  loss: 3.5728 (3.4882)  time: 0.8270  data: 0.0005  max mem: 19734
Epoch: [8]  [ 950/1251]  eta: 0:04:07  lr: 0.000020  loss: 3.6053 (3.4866)  time: 0.8546  data: 0.0006  max mem: 19734
Epoch: [8]  [ 960/1251]  eta: 0:03:58  lr: 0.000020  loss: 3.6053 (3.4887)  time: 0.8454  data: 0.0005  max mem: 19734
Epoch: [8]  [ 970/1251]  eta: 0:03:50  lr: 0.000020  loss: 3.6216 (3.4880)  time: 0.8140  data: 0.0006  max mem: 19734
Epoch: [8]  [ 980/1251]  eta: 0:03:42  lr: 0.000020  loss: 3.5885 (3.4873)  time: 0.8161  data: 0.0006  max mem: 19734
Epoch: [8]  [ 990/1251]  eta: 0:03:34  lr: 0.000020  loss: 3.5363 (3.4873)  time: 0.8070  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4373, ratio_loss=0.0077, pruning_loss=0.1733, mse_loss=0.9359
Epoch: [8]  [1000/1251]  eta: 0:03:25  lr: 0.000020  loss: 3.6394 (3.4903)  time: 0.8067  data: 0.0005  max mem: 19734
Epoch: [8]  [1010/1251]  eta: 0:03:17  lr: 0.000020  loss: 3.7668 (3.4917)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [8]  [1020/1251]  eta: 0:03:09  lr: 0.000020  loss: 3.5633 (3.4920)  time: 0.8025  data: 0.0004  max mem: 19734
Epoch: [8]  [1030/1251]  eta: 0:03:01  lr: 0.000020  loss: 3.5413 (3.4920)  time: 0.8039  data: 0.0004  max mem: 19734
Epoch: [8]  [1040/1251]  eta: 0:02:52  lr: 0.000020  loss: 3.4995 (3.4927)  time: 0.8036  data: 0.0004  max mem: 19734
Epoch: [8]  [1050/1251]  eta: 0:02:44  lr: 0.000020  loss: 3.4900 (3.4903)  time: 0.8036  data: 0.0004  max mem: 19734
Epoch: [8]  [1060/1251]  eta: 0:02:36  lr: 0.000020  loss: 3.5533 (3.4911)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [8]  [1070/1251]  eta: 0:02:28  lr: 0.000020  loss: 3.6359 (3.4903)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [8]  [1080/1251]  eta: 0:02:20  lr: 0.000020  loss: 3.2821 (3.4891)  time: 0.8137  data: 0.0005  max mem: 19734
Epoch: [8]  [1090/1251]  eta: 0:02:11  lr: 0.000020  loss: 3.6214 (3.4909)  time: 0.8346  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4944, ratio_loss=0.0082, pruning_loss=0.1720, mse_loss=0.9459
Epoch: [8]  [1100/1251]  eta: 0:02:03  lr: 0.000020  loss: 3.6046 (3.4885)  time: 0.8382  data: 0.0004  max mem: 19734
Epoch: [8]  [1110/1251]  eta: 0:01:55  lr: 0.000020  loss: 3.4118 (3.4887)  time: 0.8168  data: 0.0005  max mem: 19734
Epoch: [8]  [1120/1251]  eta: 0:01:47  lr: 0.000020  loss: 3.4890 (3.4890)  time: 0.8222  data: 0.0005  max mem: 19734
Epoch: [8]  [1130/1251]  eta: 0:01:39  lr: 0.000020  loss: 3.5265 (3.4878)  time: 0.8225  data: 0.0005  max mem: 19734
Epoch: [8]  [1140/1251]  eta: 0:01:30  lr: 0.000020  loss: 3.5265 (3.4889)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [8]  [1150/1251]  eta: 0:01:22  lr: 0.000020  loss: 3.5126 (3.4894)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [8]  [1160/1251]  eta: 0:01:14  lr: 0.000020  loss: 3.4628 (3.4887)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [8]  [1170/1251]  eta: 0:01:06  lr: 0.000020  loss: 3.4314 (3.4873)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [8]  [1180/1251]  eta: 0:00:58  lr: 0.000020  loss: 3.4314 (3.4852)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [8]  [1190/1251]  eta: 0:00:49  lr: 0.000020  loss: 3.6084 (3.4849)  time: 0.8049  data: 0.0010  max mem: 19734
loss info: cls_loss=3.4012, ratio_loss=0.0082, pruning_loss=0.1738, mse_loss=0.9214
Epoch: [8]  [1200/1251]  eta: 0:00:41  lr: 0.000020  loss: 3.6477 (3.4854)  time: 0.7995  data: 0.0008  max mem: 19734
Epoch: [8]  [1210/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.7706 (3.4864)  time: 0.7935  data: 0.0002  max mem: 19734
Epoch: [8]  [1220/1251]  eta: 0:00:25  lr: 0.000020  loss: 3.4632 (3.4852)  time: 0.8016  data: 0.0002  max mem: 19734
Epoch: [8]  [1230/1251]  eta: 0:00:17  lr: 0.000020  loss: 3.4339 (3.4858)  time: 0.8056  data: 0.0002  max mem: 19734
Epoch: [8]  [1240/1251]  eta: 0:00:09  lr: 0.000020  loss: 3.6958 (3.4850)  time: 0.8246  data: 0.0002  max mem: 19734
Epoch: [8]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.7807 (3.4866)  time: 0.8210  data: 0.0002  max mem: 19734
Epoch: [8] Total time: 0:17:04 (0.8192 s / it)
Averaged stats: lr: 0.000020  loss: 3.7807 (3.4765)
Test:  [  0/261]  eta: 2:13:46  loss: 0.7941 (0.7941)  acc1: 84.3750 (84.3750)  acc5: 94.7917 (94.7917)  time: 30.7523  data: 30.3157  max mem: 19734
Test:  [ 10/261]  eta: 0:12:58  loss: 0.7364 (0.8093)  acc1: 84.8958 (83.2860)  acc5: 95.3125 (95.4072)  time: 3.1002  data: 2.7639  max mem: 19734
Test:  [ 20/261]  eta: 0:06:56  loss: 1.0243 (0.9628)  acc1: 77.6042 (78.5714)  acc5: 93.2292 (94.1716)  time: 0.2762  data: 0.0118  max mem: 19734
Test:  [ 30/261]  eta: 0:05:05  loss: 0.8800 (0.8847)  acc1: 80.7292 (81.4348)  acc5: 93.7500 (94.6573)  time: 0.3437  data: 0.0137  max mem: 19734
Test:  [ 40/261]  eta: 0:04:21  loss: 0.6566 (0.8549)  acc1: 87.5000 (82.4441)  acc5: 95.8333 (94.8679)  time: 0.6124  data: 0.2083  max mem: 19734
Test:  [ 50/261]  eta: 0:03:40  loss: 0.9844 (0.9155)  acc1: 77.6042 (80.6577)  acc5: 95.3125 (94.4955)  time: 0.6127  data: 0.2229  max mem: 19734
Test:  [ 60/261]  eta: 0:03:05  loss: 1.0614 (0.9274)  acc1: 76.0417 (80.1742)  acc5: 93.2292 (94.5782)  time: 0.3940  data: 0.0294  max mem: 19734
Test:  [ 70/261]  eta: 0:02:55  loss: 1.0106 (0.9255)  acc1: 77.6042 (79.7315)  acc5: 95.3125 (94.7697)  time: 0.6036  data: 0.2770  max mem: 19734
Test:  [ 80/261]  eta: 0:02:35  loss: 0.8845 (0.9297)  acc1: 80.7292 (79.8354)  acc5: 95.8333 (94.8881)  time: 0.6651  data: 0.2771  max mem: 19734
Test:  [ 90/261]  eta: 0:02:19  loss: 0.8829 (0.9158)  acc1: 82.2917 (80.2370)  acc5: 94.7917 (94.9977)  time: 0.4350  data: 0.0163  max mem: 19734
Test:  [100/261]  eta: 0:02:09  loss: 0.8797 (0.9185)  acc1: 82.8125 (80.2135)  acc5: 94.2708 (95.0340)  time: 0.5767  data: 0.1564  max mem: 19734
Test:  [110/261]  eta: 0:01:54  loss: 0.9478 (0.9439)  acc1: 77.0833 (79.6547)  acc5: 94.2708 (94.7307)  time: 0.4988  data: 0.1566  max mem: 19734
Test:  [120/261]  eta: 0:01:43  loss: 1.3580 (0.9865)  acc1: 68.2292 (78.7061)  acc5: 89.0625 (94.1116)  time: 0.3605  data: 0.0137  max mem: 19734
Test:  [130/261]  eta: 0:01:35  loss: 1.4841 (1.0354)  acc1: 66.6667 (77.7115)  acc5: 86.9792 (93.5353)  time: 0.5771  data: 0.1958  max mem: 19734
Test:  [140/261]  eta: 0:01:24  loss: 1.4220 (1.0629)  acc1: 68.7500 (76.9688)  acc5: 89.0625 (93.2846)  time: 0.5282  data: 0.1955  max mem: 19734
Test:  [150/261]  eta: 0:01:18  loss: 1.2929 (1.0709)  acc1: 70.3125 (76.8971)  acc5: 90.6250 (93.0843)  time: 0.5502  data: 0.2636  max mem: 19734
Test:  [160/261]  eta: 0:01:08  loss: 1.1617 (1.0934)  acc1: 76.0417 (76.5010)  acc5: 90.6250 (92.7569)  time: 0.5388  data: 0.2699  max mem: 19734
Test:  [170/261]  eta: 0:01:00  loss: 1.4402 (1.1246)  acc1: 64.0625 (75.6792)  acc5: 86.4583 (92.3794)  time: 0.3142  data: 0.0239  max mem: 19734
Test:  [180/261]  eta: 0:00:51  loss: 1.5777 (1.1432)  acc1: 64.5833 (75.2791)  acc5: 86.9792 (92.1846)  time: 0.2338  data: 0.0144  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: 1.4555 (1.1574)  acc1: 67.1875 (75.0273)  acc5: 90.1042 (92.0293)  time: 0.1718  data: 0.0091  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.4475 (1.1749)  acc1: 69.7917 (74.6735)  acc5: 88.5417 (91.7755)  time: 0.1714  data: 0.0105  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.4475 (1.1888)  acc1: 68.2292 (74.4125)  acc5: 87.5000 (91.5827)  time: 0.1460  data: 0.0039  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.5363 (1.2078)  acc1: 65.6250 (73.9041)  acc5: 88.0208 (91.3956)  time: 0.1211  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.5363 (1.2179)  acc1: 65.6250 (73.6449)  acc5: 88.5417 (91.2608)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.4591 (1.2276)  acc1: 66.1458 (73.3964)  acc5: 89.0625 (91.1739)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1994 (1.2212)  acc1: 72.3958 (73.5558)  acc5: 92.1875 (91.2952)  time: 0.1149  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.1311 (1.2193)  acc1: 75.0000 (73.6020)  acc5: 94.7917 (91.3680)  time: 0.1119  data: 0.0002  max mem: 19734
Test: Total time: 0:02:05 (0.4795 s / it)
* Acc@1 73.602 Acc@5 91.368 loss 1.219
Accuracy of the network on the 50000 test images: 73.6%
Max accuracy: 73.60%
Epoch: [9]  [   0/1251]  eta: 1:06:53  lr: 0.000020  loss: 2.7123 (2.7123)  time: 3.2084  data: 2.2895  max mem: 19734
Epoch: [9]  [  10/1251]  eta: 0:23:09  lr: 0.000020  loss: 3.4275 (3.3571)  time: 1.1199  data: 0.2088  max mem: 19734
Epoch: [9]  [  20/1251]  eta: 0:19:49  lr: 0.000020  loss: 3.3197 (3.2790)  time: 0.8542  data: 0.0006  max mem: 19734
Epoch: [9]  [  30/1251]  eta: 0:18:33  lr: 0.000020  loss: 3.4451 (3.3857)  time: 0.7971  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4439, ratio_loss=0.0076, pruning_loss=0.1718, mse_loss=0.9197
Epoch: [9]  [  40/1251]  eta: 0:17:51  lr: 0.000020  loss: 3.6987 (3.4190)  time: 0.7989  data: 0.0005  max mem: 19734
Epoch: [9]  [  50/1251]  eta: 0:17:22  lr: 0.000020  loss: 3.7016 (3.4606)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [9]  [  60/1251]  eta: 0:16:59  lr: 0.000020  loss: 3.5497 (3.4507)  time: 0.7982  data: 0.0005  max mem: 19734
Epoch: [9]  [  70/1251]  eta: 0:16:41  lr: 0.000020  loss: 3.5109 (3.4786)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [9]  [  80/1251]  eta: 0:16:26  lr: 0.000020  loss: 3.5339 (3.4826)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [9]  [  90/1251]  eta: 0:16:13  lr: 0.000020  loss: 3.6898 (3.5136)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [9]  [ 100/1251]  eta: 0:16:00  lr: 0.000020  loss: 3.8206 (3.5165)  time: 0.8035  data: 0.0006  max mem: 19734
Epoch: [9]  [ 110/1251]  eta: 0:15:52  lr: 0.000020  loss: 3.6542 (3.5161)  time: 0.8192  data: 0.0006  max mem: 19734
Epoch: [9]  [ 120/1251]  eta: 0:15:45  lr: 0.000020  loss: 3.6493 (3.5217)  time: 0.8392  data: 0.0005  max mem: 19734
Epoch: [9]  [ 130/1251]  eta: 0:15:38  lr: 0.000020  loss: 3.5078 (3.5127)  time: 0.8496  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5279, ratio_loss=0.0075, pruning_loss=0.1700, mse_loss=0.9442
Epoch: [9]  [ 140/1251]  eta: 0:15:27  lr: 0.000020  loss: 3.5657 (3.5152)  time: 0.8295  data: 0.0005  max mem: 19734
Epoch: [9]  [ 150/1251]  eta: 0:15:16  lr: 0.000020  loss: 3.2713 (3.4879)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [9]  [ 160/1251]  eta: 0:15:07  lr: 0.000020  loss: 3.6933 (3.5189)  time: 0.8085  data: 0.0006  max mem: 19734
Epoch: [9]  [ 170/1251]  eta: 0:14:57  lr: 0.000020  loss: 3.7892 (3.5199)  time: 0.8107  data: 0.0006  max mem: 19734
Epoch: [9]  [ 180/1251]  eta: 0:14:47  lr: 0.000020  loss: 3.5998 (3.5235)  time: 0.8078  data: 0.0004  max mem: 19734
Epoch: [9]  [ 190/1251]  eta: 0:14:38  lr: 0.000020  loss: 3.7126 (3.5250)  time: 0.8069  data: 0.0004  max mem: 19734
Epoch: [9]  [ 200/1251]  eta: 0:14:28  lr: 0.000020  loss: 3.7126 (3.5289)  time: 0.8070  data: 0.0004  max mem: 19734
Epoch: [9]  [ 210/1251]  eta: 0:14:19  lr: 0.000020  loss: 3.6179 (3.5293)  time: 0.8066  data: 0.0010  max mem: 19734
Epoch: [9]  [ 220/1251]  eta: 0:14:10  lr: 0.000020  loss: 3.5380 (3.5264)  time: 0.8031  data: 0.0011  max mem: 19734
Epoch: [9]  [ 230/1251]  eta: 0:14:01  lr: 0.000020  loss: 3.7684 (3.5391)  time: 0.8042  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5686, ratio_loss=0.0074, pruning_loss=0.1680, mse_loss=0.9321
Epoch: [9]  [ 240/1251]  eta: 0:13:51  lr: 0.000020  loss: 3.7924 (3.5483)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [9]  [ 250/1251]  eta: 0:13:44  lr: 0.000020  loss: 3.7613 (3.5521)  time: 0.8222  data: 0.0004  max mem: 19734
Epoch: [9]  [ 260/1251]  eta: 0:13:36  lr: 0.000020  loss: 3.5846 (3.5483)  time: 0.8348  data: 0.0004  max mem: 19734
Epoch: [9]  [ 270/1251]  eta: 0:13:30  lr: 0.000020  loss: 3.5313 (3.5404)  time: 0.8543  data: 0.0004  max mem: 19734
Epoch: [9]  [ 280/1251]  eta: 0:13:21  lr: 0.000020  loss: 3.6367 (3.5434)  time: 0.8485  data: 0.0004  max mem: 19734
Epoch: [9]  [ 290/1251]  eta: 0:13:12  lr: 0.000020  loss: 3.6565 (3.5484)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [9]  [ 300/1251]  eta: 0:13:03  lr: 0.000020  loss: 3.3850 (3.5405)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [9]  [ 310/1251]  eta: 0:12:55  lr: 0.000020  loss: 3.2288 (3.5391)  time: 0.8053  data: 0.0004  max mem: 19734
Epoch: [9]  [ 320/1251]  eta: 0:12:46  lr: 0.000020  loss: 3.5507 (3.5350)  time: 0.8066  data: 0.0004  max mem: 19734
Epoch: [9]  [ 330/1251]  eta: 0:12:37  lr: 0.000020  loss: 3.5182 (3.5275)  time: 0.8101  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4451, ratio_loss=0.0074, pruning_loss=0.1715, mse_loss=0.9463
Epoch: [9]  [ 340/1251]  eta: 0:12:29  lr: 0.000020  loss: 3.5291 (3.5259)  time: 0.8075  data: 0.0005  max mem: 19734
Epoch: [9]  [ 350/1251]  eta: 0:12:20  lr: 0.000020  loss: 3.6371 (3.5233)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [9]  [ 360/1251]  eta: 0:12:11  lr: 0.000020  loss: 3.4949 (3.5227)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [9]  [ 370/1251]  eta: 0:12:03  lr: 0.000020  loss: 3.1890 (3.5140)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [9]  [ 380/1251]  eta: 0:11:54  lr: 0.000020  loss: 3.2724 (3.5158)  time: 0.8020  data: 0.0004  max mem: 19734
Epoch: [9]  [ 390/1251]  eta: 0:11:45  lr: 0.000020  loss: 3.6416 (3.5131)  time: 0.8053  data: 0.0004  max mem: 19734
Epoch: [9]  [ 400/1251]  eta: 0:11:37  lr: 0.000020  loss: 3.6361 (3.5120)  time: 0.8136  data: 0.0005  max mem: 19734
Epoch: [9]  [ 410/1251]  eta: 0:11:29  lr: 0.000020  loss: 3.4095 (3.5105)  time: 0.8293  data: 0.0006  max mem: 19734
Epoch: [9]  [ 420/1251]  eta: 0:11:22  lr: 0.000020  loss: 3.4184 (3.5137)  time: 0.8482  data: 0.0005  max mem: 19734
Epoch: [9]  [ 430/1251]  eta: 0:11:13  lr: 0.000020  loss: 3.5134 (3.5102)  time: 0.8280  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4431, ratio_loss=0.0077, pruning_loss=0.1694, mse_loss=0.9022
Epoch: [9]  [ 440/1251]  eta: 0:11:05  lr: 0.000020  loss: 3.5994 (3.5132)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [9]  [ 450/1251]  eta: 0:10:56  lr: 0.000020  loss: 3.5994 (3.5127)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [9]  [ 460/1251]  eta: 0:10:48  lr: 0.000020  loss: 3.5552 (3.5040)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [9]  [ 470/1251]  eta: 0:10:39  lr: 0.000020  loss: 3.2378 (3.5026)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [9]  [ 480/1251]  eta: 0:10:31  lr: 0.000020  loss: 3.4355 (3.5013)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [9]  [ 490/1251]  eta: 0:10:22  lr: 0.000020  loss: 3.3344 (3.4967)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [9]  [ 500/1251]  eta: 0:10:14  lr: 0.000020  loss: 3.3872 (3.4963)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [9]  [ 510/1251]  eta: 0:10:06  lr: 0.000020  loss: 3.5893 (3.4979)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [9]  [ 520/1251]  eta: 0:09:57  lr: 0.000020  loss: 3.5569 (3.4981)  time: 0.8034  data: 0.0006  max mem: 19734
Epoch: [9]  [ 530/1251]  eta: 0:09:49  lr: 0.000020  loss: 3.4978 (3.4971)  time: 0.8043  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3790, ratio_loss=0.0076, pruning_loss=0.1708, mse_loss=0.9716
Epoch: [9]  [ 540/1251]  eta: 0:09:41  lr: 0.000020  loss: 3.4029 (3.4934)  time: 0.8055  data: 0.0007  max mem: 19734
Epoch: [9]  [ 550/1251]  eta: 0:09:33  lr: 0.000020  loss: 3.4186 (3.4950)  time: 0.8222  data: 0.0007  max mem: 19734
Epoch: [9]  [ 560/1251]  eta: 0:09:25  lr: 0.000020  loss: 3.6576 (3.4954)  time: 0.8344  data: 0.0004  max mem: 19734
Epoch: [9]  [ 570/1251]  eta: 0:09:17  lr: 0.000020  loss: 3.6034 (3.4935)  time: 0.8353  data: 0.0005  max mem: 19734
Epoch: [9]  [ 580/1251]  eta: 0:09:08  lr: 0.000020  loss: 3.5152 (3.4916)  time: 0.8203  data: 0.0005  max mem: 19734
Epoch: [9]  [ 590/1251]  eta: 0:09:00  lr: 0.000020  loss: 3.5740 (3.4922)  time: 0.8034  data: 0.0004  max mem: 19734
Epoch: [9]  [ 600/1251]  eta: 0:08:52  lr: 0.000020  loss: 3.4182 (3.4890)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [9]  [ 610/1251]  eta: 0:08:43  lr: 0.000020  loss: 3.4405 (3.4877)  time: 0.8043  data: 0.0008  max mem: 19734
Epoch: [9]  [ 620/1251]  eta: 0:08:35  lr: 0.000020  loss: 3.5007 (3.4880)  time: 0.8053  data: 0.0008  max mem: 19734
Epoch: [9]  [ 630/1251]  eta: 0:08:27  lr: 0.000020  loss: 3.1432 (3.4841)  time: 0.8010  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4333, ratio_loss=0.0068, pruning_loss=0.1673, mse_loss=0.8523
Epoch: [9]  [ 640/1251]  eta: 0:08:18  lr: 0.000020  loss: 3.6879 (3.4880)  time: 0.8024  data: 0.0004  max mem: 19734
Epoch: [9]  [ 650/1251]  eta: 0:08:10  lr: 0.000020  loss: 3.6879 (3.4886)  time: 0.8063  data: 0.0005  max mem: 19734
Epoch: [9]  [ 660/1251]  eta: 0:08:02  lr: 0.000020  loss: 3.5239 (3.4880)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [9]  [ 670/1251]  eta: 0:07:54  lr: 0.000020  loss: 3.5151 (3.4884)  time: 0.8049  data: 0.0004  max mem: 19734
Epoch: [9]  [ 680/1251]  eta: 0:07:45  lr: 0.000020  loss: 3.3878 (3.4858)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [9]  [ 690/1251]  eta: 0:07:37  lr: 0.000020  loss: 3.5039 (3.4866)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [9]  [ 700/1251]  eta: 0:07:29  lr: 0.000020  loss: 3.5713 (3.4843)  time: 0.8268  data: 0.0005  max mem: 19734
Epoch: [9]  [ 710/1251]  eta: 0:07:21  lr: 0.000020  loss: 3.4676 (3.4839)  time: 0.8499  data: 0.0004  max mem: 19734
Epoch: [9]  [ 720/1251]  eta: 0:07:13  lr: 0.000020  loss: 3.4676 (3.4835)  time: 0.8365  data: 0.0005  max mem: 19734
Epoch: [9]  [ 730/1251]  eta: 0:07:05  lr: 0.000020  loss: 3.3876 (3.4804)  time: 0.8072  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3975, ratio_loss=0.0073, pruning_loss=0.1686, mse_loss=0.9472
Epoch: [9]  [ 740/1251]  eta: 0:06:57  lr: 0.000020  loss: 3.3876 (3.4794)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [9]  [ 750/1251]  eta: 0:06:48  lr: 0.000020  loss: 3.7852 (3.4808)  time: 0.8054  data: 0.0006  max mem: 19734
Epoch: [9]  [ 760/1251]  eta: 0:06:40  lr: 0.000020  loss: 3.6256 (3.4808)  time: 0.8024  data: 0.0006  max mem: 19734
Epoch: [9]  [ 770/1251]  eta: 0:06:32  lr: 0.000020  loss: 3.3819 (3.4801)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [9]  [ 780/1251]  eta: 0:06:24  lr: 0.000020  loss: 3.5845 (3.4808)  time: 0.8057  data: 0.0006  max mem: 19734
Epoch: [9]  [ 790/1251]  eta: 0:06:16  lr: 0.000020  loss: 3.6755 (3.4820)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [9]  [ 800/1251]  eta: 0:06:07  lr: 0.000020  loss: 3.6760 (3.4829)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [9]  [ 810/1251]  eta: 0:05:59  lr: 0.000020  loss: 3.6739 (3.4854)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [9]  [ 820/1251]  eta: 0:05:51  lr: 0.000020  loss: 3.6733 (3.4863)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [9]  [ 830/1251]  eta: 0:05:43  lr: 0.000020  loss: 3.4892 (3.4851)  time: 0.8028  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4634, ratio_loss=0.0071, pruning_loss=0.1670, mse_loss=0.9344
Epoch: [9]  [ 840/1251]  eta: 0:05:35  lr: 0.000020  loss: 3.3366 (3.4806)  time: 0.8140  data: 0.0004  max mem: 19734
Epoch: [9]  [ 850/1251]  eta: 0:05:27  lr: 0.000020  loss: 3.3356 (3.4786)  time: 0.8349  data: 0.0004  max mem: 19734
Epoch: [9]  [ 860/1251]  eta: 0:05:19  lr: 0.000020  loss: 3.3356 (3.4780)  time: 0.8539  data: 0.0005  max mem: 19734
Epoch: [9]  [ 870/1251]  eta: 0:05:10  lr: 0.000020  loss: 3.3795 (3.4768)  time: 0.8327  data: 0.0006  max mem: 19734
Epoch: [9]  [ 880/1251]  eta: 0:05:02  lr: 0.000020  loss: 3.3795 (3.4731)  time: 0.8037  data: 0.0006  max mem: 19734
Epoch: [9]  [ 890/1251]  eta: 0:04:54  lr: 0.000020  loss: 3.2177 (3.4740)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [9]  [ 900/1251]  eta: 0:04:46  lr: 0.000020  loss: 3.6885 (3.4735)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [9]  [ 910/1251]  eta: 0:04:38  lr: 0.000020  loss: 3.6999 (3.4757)  time: 0.8076  data: 0.0004  max mem: 19734
Epoch: [9]  [ 920/1251]  eta: 0:04:29  lr: 0.000020  loss: 3.5224 (3.4731)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [9]  [ 930/1251]  eta: 0:04:21  lr: 0.000020  loss: 3.2322 (3.4714)  time: 0.8040  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3317, ratio_loss=0.0074, pruning_loss=0.1710, mse_loss=0.9082
Epoch: [9]  [ 940/1251]  eta: 0:04:13  lr: 0.000020  loss: 3.2114 (3.4676)  time: 0.8053  data: 0.0007  max mem: 19734
Epoch: [9]  [ 950/1251]  eta: 0:04:05  lr: 0.000020  loss: 3.2594 (3.4665)  time: 0.8046  data: 0.0006  max mem: 19734
Epoch: [9]  [ 960/1251]  eta: 0:03:57  lr: 0.000020  loss: 3.4741 (3.4655)  time: 0.8058  data: 0.0006  max mem: 19734
Epoch: [9]  [ 970/1251]  eta: 0:03:48  lr: 0.000020  loss: 3.4596 (3.4661)  time: 0.8067  data: 0.0006  max mem: 19734
Epoch: [9]  [ 980/1251]  eta: 0:03:40  lr: 0.000020  loss: 3.5529 (3.4670)  time: 0.8146  data: 0.0005  max mem: 19734
Epoch: [9]  [ 990/1251]  eta: 0:03:32  lr: 0.000020  loss: 3.6838 (3.4668)  time: 0.8237  data: 0.0004  max mem: 19734
Epoch: [9]  [1000/1251]  eta: 0:03:24  lr: 0.000020  loss: 3.4809 (3.4679)  time: 0.8387  data: 0.0005  max mem: 19734
Epoch: [9]  [1010/1251]  eta: 0:03:16  lr: 0.000020  loss: 3.7506 (3.4690)  time: 0.8347  data: 0.0004  max mem: 19734
Epoch: [9]  [1020/1251]  eta: 0:03:08  lr: 0.000020  loss: 3.4802 (3.4673)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [9]  [1030/1251]  eta: 0:03:00  lr: 0.000020  loss: 3.4802 (3.4691)  time: 0.8037  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4693, ratio_loss=0.0071, pruning_loss=0.1646, mse_loss=0.8846
Epoch: [9]  [1040/1251]  eta: 0:02:51  lr: 0.000020  loss: 3.6387 (3.4702)  time: 0.8020  data: 0.0004  max mem: 19734
Epoch: [9]  [1050/1251]  eta: 0:02:43  lr: 0.000020  loss: 3.5637 (3.4704)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [9]  [1060/1251]  eta: 0:02:35  lr: 0.000020  loss: 3.5637 (3.4712)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [9]  [1070/1251]  eta: 0:02:27  lr: 0.000020  loss: 3.6784 (3.4727)  time: 0.8051  data: 0.0006  max mem: 19734
Epoch: [9]  [1080/1251]  eta: 0:02:19  lr: 0.000020  loss: 3.6527 (3.4710)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [9]  [1090/1251]  eta: 0:02:11  lr: 0.000020  loss: 3.5544 (3.4715)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [9]  [1100/1251]  eta: 0:02:02  lr: 0.000020  loss: 3.5544 (3.4706)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [9]  [1110/1251]  eta: 0:01:54  lr: 0.000020  loss: 3.2916 (3.4663)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [9]  [1120/1251]  eta: 0:01:46  lr: 0.000020  loss: 3.2916 (3.4661)  time: 0.8033  data: 0.0006  max mem: 19734
Epoch: [9]  [1130/1251]  eta: 0:01:38  lr: 0.000020  loss: 3.6029 (3.4655)  time: 0.8137  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3740, ratio_loss=0.0070, pruning_loss=0.1672, mse_loss=0.9362
Epoch: [9]  [1140/1251]  eta: 0:01:30  lr: 0.000020  loss: 3.4050 (3.4641)  time: 0.8370  data: 0.0005  max mem: 19734
Epoch: [9]  [1150/1251]  eta: 0:01:22  lr: 0.000020  loss: 3.4050 (3.4650)  time: 0.8627  data: 0.0006  max mem: 19734
Epoch: [9]  [1160/1251]  eta: 0:01:14  lr: 0.000020  loss: 3.4055 (3.4640)  time: 0.8396  data: 0.0005  max mem: 19734
Epoch: [9]  [1170/1251]  eta: 0:01:06  lr: 0.000020  loss: 3.4055 (3.4623)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [9]  [1180/1251]  eta: 0:00:57  lr: 0.000020  loss: 3.1672 (3.4595)  time: 0.8061  data: 0.0005  max mem: 19734
Epoch: [9]  [1190/1251]  eta: 0:00:49  lr: 0.000020  loss: 3.2674 (3.4592)  time: 0.8031  data: 0.0009  max mem: 19734
Epoch: [9]  [1200/1251]  eta: 0:00:41  lr: 0.000020  loss: 3.4365 (3.4569)  time: 0.7974  data: 0.0008  max mem: 19734
Epoch: [9]  [1210/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.6066 (3.4587)  time: 0.7918  data: 0.0001  max mem: 19734
Epoch: [9]  [1220/1251]  eta: 0:00:25  lr: 0.000020  loss: 3.6066 (3.4582)  time: 0.7919  data: 0.0001  max mem: 19734
Epoch: [9]  [1230/1251]  eta: 0:00:17  lr: 0.000020  loss: 3.4337 (3.4576)  time: 0.7923  data: 0.0002  max mem: 19734
loss info: cls_loss=3.3477, ratio_loss=0.0073, pruning_loss=0.1671, mse_loss=0.8852
Epoch: [9]  [1240/1251]  eta: 0:00:08  lr: 0.000020  loss: 3.3943 (3.4568)  time: 0.7925  data: 0.0002  max mem: 19734
Epoch: [9]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.6365 (3.4576)  time: 0.7941  data: 0.0002  max mem: 19734
Epoch: [9] Total time: 0:16:59 (0.8148 s / it)
Averaged stats: lr: 0.000020  loss: 3.6365 (3.4799)
Test:  [  0/261]  eta: 2:16:46  loss: 0.7966 (0.7966)  acc1: 81.2500 (81.2500)  acc5: 93.7500 (93.7500)  time: 31.4438  data: 31.0608  max mem: 19734
Test:  [ 10/261]  eta: 0:14:41  loss: 0.7578 (0.8014)  acc1: 83.8542 (83.3807)  acc5: 96.3542 (95.7860)  time: 3.5128  data: 3.1407  max mem: 19734
Test:  [ 20/261]  eta: 0:08:01  loss: 1.0174 (0.9511)  acc1: 78.6458 (78.9435)  acc5: 93.2292 (94.4692)  time: 0.5235  data: 0.1798  max mem: 19734
Test:  [ 30/261]  eta: 0:05:47  loss: 0.8267 (0.8674)  acc1: 83.3333 (81.6700)  acc5: 93.7500 (94.9261)  time: 0.3993  data: 0.0140  max mem: 19734
Test:  [ 40/261]  eta: 0:04:56  loss: 0.6483 (0.8364)  acc1: 88.0208 (82.4695)  acc5: 96.3542 (95.1855)  time: 0.6552  data: 0.2539  max mem: 19734
Test:  [ 50/261]  eta: 0:03:57  loss: 0.9491 (0.8982)  acc1: 77.0833 (80.6475)  acc5: 94.2708 (94.7202)  time: 0.5347  data: 0.2544  max mem: 19734
Test:  [ 60/261]  eta: 0:03:19  loss: 1.0476 (0.9124)  acc1: 76.5625 (80.1827)  acc5: 93.2292 (94.6636)  time: 0.2740  data: 0.0169  max mem: 19734
Test:  [ 70/261]  eta: 0:02:51  loss: 0.9890 (0.9138)  acc1: 77.0833 (79.6435)  acc5: 95.3125 (94.9310)  time: 0.3249  data: 0.0110  max mem: 19734
Test:  [ 80/261]  eta: 0:02:29  loss: 0.8899 (0.9170)  acc1: 79.6875 (79.7518)  acc5: 96.3542 (95.0424)  time: 0.3250  data: 0.0790  max mem: 19734
Test:  [ 90/261]  eta: 0:02:09  loss: 0.8552 (0.9011)  acc1: 82.8125 (80.1568)  acc5: 95.8333 (95.1408)  time: 0.2507  data: 0.0810  max mem: 19734
Test:  [100/261]  eta: 0:02:11  loss: 0.8552 (0.9035)  acc1: 82.2917 (80.0588)  acc5: 95.3125 (95.1784)  time: 0.7740  data: 0.5741  max mem: 19734
Test:  [110/261]  eta: 0:01:55  loss: 0.9588 (0.9290)  acc1: 76.0417 (79.5467)  acc5: 93.7500 (94.8667)  time: 0.7855  data: 0.5737  max mem: 19734
Test:  [120/261]  eta: 0:01:42  loss: 1.3115 (0.9700)  acc1: 68.2292 (78.6114)  acc5: 89.5833 (94.3139)  time: 0.2564  data: 0.0115  max mem: 19734
Test:  [130/261]  eta: 0:01:46  loss: 1.4128 (1.0165)  acc1: 66.6667 (77.6400)  acc5: 86.4583 (93.7182)  time: 1.1195  data: 0.8574  max mem: 19734
Test:  [140/261]  eta: 0:01:33  loss: 1.4338 (1.0440)  acc1: 67.1875 (76.9319)  acc5: 89.0625 (93.4360)  time: 1.0472  data: 0.8578  max mem: 19734
Test:  [150/261]  eta: 0:01:21  loss: 1.2777 (1.0488)  acc1: 70.3125 (76.8971)  acc5: 90.6250 (93.2809)  time: 0.1910  data: 0.0171  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: 1.0678 (1.0714)  acc1: 76.0417 (76.4978)  acc5: 92.1875 (92.9801)  time: 0.2144  data: 0.0172  max mem: 19734
Test:  [170/261]  eta: 0:01:01  loss: 1.3790 (1.1027)  acc1: 64.0625 (75.6640)  acc5: 86.4583 (92.6170)  time: 0.2152  data: 0.0417  max mem: 19734
Test:  [180/261]  eta: 0:00:52  loss: 1.5163 (1.1206)  acc1: 65.1042 (75.2647)  acc5: 87.5000 (92.4378)  time: 0.2055  data: 0.0410  max mem: 19734
Test:  [190/261]  eta: 0:00:44  loss: 1.3839 (1.1341)  acc1: 66.6667 (75.0136)  acc5: 90.6250 (92.2693)  time: 0.2161  data: 0.0123  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: 1.4233 (1.1506)  acc1: 69.7917 (74.6346)  acc5: 89.0625 (92.0320)  time: 0.2159  data: 0.0391  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.4771 (1.1659)  acc1: 67.1875 (74.3483)  acc5: 86.4583 (91.8074)  time: 0.1524  data: 0.0334  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.5519 (1.1846)  acc1: 65.6250 (73.8994)  acc5: 86.4583 (91.5960)  time: 0.1155  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.5321 (1.1950)  acc1: 65.1042 (73.6224)  acc5: 87.5000 (91.4637)  time: 0.1148  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.4217 (1.2038)  acc1: 65.1042 (73.4289)  acc5: 89.5833 (91.3987)  time: 0.1148  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1645 (1.1977)  acc1: 75.0000 (73.5994)  acc5: 92.1875 (91.5007)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0575 (1.1970)  acc1: 75.5208 (73.6280)  acc5: 94.7917 (91.5720)  time: 0.1118  data: 0.0001  max mem: 19734
Test: Total time: 0:02:08 (0.4924 s / it)
* Acc@1 73.628 Acc@5 91.572 loss 1.197
Accuracy of the network on the 50000 test images: 73.6%
Max accuracy: 73.63%
Epoch: [10]  [   0/1251]  eta: 3:51:16  lr: 0.000020  loss: 3.8774 (3.8774)  time: 11.0920  data: 10.1601  max mem: 19734
Epoch: [10]  [  10/1251]  eta: 0:37:56  lr: 0.000020  loss: 3.7833 (3.7305)  time: 1.8342  data: 0.9243  max mem: 19734
Epoch: [10]  [  20/1251]  eta: 0:27:50  lr: 0.000020  loss: 3.7220 (3.6403)  time: 0.8703  data: 0.0006  max mem: 19734
Epoch: [10]  [  30/1251]  eta: 0:24:08  lr: 0.000020  loss: 3.4901 (3.5424)  time: 0.8294  data: 0.0004  max mem: 19734
Epoch: [10]  [  40/1251]  eta: 0:22:09  lr: 0.000020  loss: 3.4115 (3.5068)  time: 0.8261  data: 0.0004  max mem: 19734
Epoch: [10]  [  50/1251]  eta: 0:20:50  lr: 0.000020  loss: 3.4987 (3.4758)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [10]  [  60/1251]  eta: 0:19:53  lr: 0.000020  loss: 3.4987 (3.4523)  time: 0.8062  data: 0.0004  max mem: 19734
Epoch: [10]  [  70/1251]  eta: 0:19:10  lr: 0.000020  loss: 3.7155 (3.4831)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [10]  [  80/1251]  eta: 0:18:35  lr: 0.000020  loss: 3.7936 (3.5120)  time: 0.8009  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4780, ratio_loss=0.0070, pruning_loss=0.1634, mse_loss=0.8907
Epoch: [10]  [  90/1251]  eta: 0:18:06  lr: 0.000020  loss: 3.6267 (3.4990)  time: 0.8005  data: 0.0006  max mem: 19734
Epoch: [10]  [ 100/1251]  eta: 0:17:41  lr: 0.000020  loss: 3.3370 (3.4790)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [10]  [ 110/1251]  eta: 0:17:20  lr: 0.000020  loss: 3.5567 (3.4793)  time: 0.8021  data: 0.0006  max mem: 19734
Epoch: [10]  [ 120/1251]  eta: 0:17:01  lr: 0.000020  loss: 3.5628 (3.4925)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [10]  [ 130/1251]  eta: 0:16:43  lr: 0.000020  loss: 3.5247 (3.4874)  time: 0.8065  data: 0.0004  max mem: 19734
Epoch: [10]  [ 140/1251]  eta: 0:16:29  lr: 0.000020  loss: 3.6369 (3.4883)  time: 0.8166  data: 0.0004  max mem: 19734
Epoch: [10]  [ 150/1251]  eta: 0:16:14  lr: 0.000020  loss: 3.5599 (3.4858)  time: 0.8170  data: 0.0006  max mem: 19734
Epoch: [10]  [ 160/1251]  eta: 0:16:00  lr: 0.000020  loss: 3.5599 (3.4920)  time: 0.8112  data: 0.0006  max mem: 19734
Epoch: [10]  [ 170/1251]  eta: 0:15:47  lr: 0.000020  loss: 3.7132 (3.4993)  time: 0.8125  data: 0.0005  max mem: 19734
Epoch: [10]  [ 180/1251]  eta: 0:15:36  lr: 0.000020  loss: 3.7143 (3.5034)  time: 0.8273  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4810, ratio_loss=0.0075, pruning_loss=0.1636, mse_loss=0.9156
Epoch: [10]  [ 190/1251]  eta: 0:15:25  lr: 0.000020  loss: 3.7375 (3.5038)  time: 0.8335  data: 0.0005  max mem: 19734
Epoch: [10]  [ 200/1251]  eta: 0:15:13  lr: 0.000020  loss: 3.6625 (3.4981)  time: 0.8139  data: 0.0005  max mem: 19734
Epoch: [10]  [ 210/1251]  eta: 0:15:01  lr: 0.000020  loss: 3.6304 (3.5050)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [10]  [ 220/1251]  eta: 0:14:50  lr: 0.000020  loss: 3.7160 (3.5057)  time: 0.8116  data: 0.0006  max mem: 19734
Epoch: [10]  [ 230/1251]  eta: 0:14:39  lr: 0.000020  loss: 3.7029 (3.5068)  time: 0.8066  data: 0.0006  max mem: 19734
Epoch: [10]  [ 240/1251]  eta: 0:14:27  lr: 0.000020  loss: 3.0790 (3.4925)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [10]  [ 250/1251]  eta: 0:14:17  lr: 0.000020  loss: 3.2789 (3.4961)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [10]  [ 260/1251]  eta: 0:14:06  lr: 0.000020  loss: 3.6511 (3.5058)  time: 0.8029  data: 0.0007  max mem: 19734
Epoch: [10]  [ 270/1251]  eta: 0:13:56  lr: 0.000020  loss: 3.5457 (3.4976)  time: 0.8007  data: 0.0007  max mem: 19734
Epoch: [10]  [ 280/1251]  eta: 0:13:45  lr: 0.000020  loss: 3.3109 (3.4902)  time: 0.8037  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4312, ratio_loss=0.0071, pruning_loss=0.1643, mse_loss=0.8856
Epoch: [10]  [ 290/1251]  eta: 0:13:36  lr: 0.000020  loss: 3.3181 (3.4860)  time: 0.8112  data: 0.0004  max mem: 19734
Epoch: [10]  [ 300/1251]  eta: 0:13:26  lr: 0.000020  loss: 3.3747 (3.4845)  time: 0.8124  data: 0.0005  max mem: 19734
Epoch: [10]  [ 310/1251]  eta: 0:13:17  lr: 0.000020  loss: 3.7262 (3.4876)  time: 0.8184  data: 0.0005  max mem: 19734
Epoch: [10]  [ 320/1251]  eta: 0:13:08  lr: 0.000020  loss: 3.6525 (3.4856)  time: 0.8239  data: 0.0005  max mem: 19734
Epoch: [10]  [ 330/1251]  eta: 0:12:59  lr: 0.000020  loss: 3.4713 (3.4802)  time: 0.8362  data: 0.0005  max mem: 19734
Epoch: [10]  [ 340/1251]  eta: 0:12:50  lr: 0.000020  loss: 3.5449 (3.4851)  time: 0.8300  data: 0.0005  max mem: 19734
Epoch: [10]  [ 350/1251]  eta: 0:12:40  lr: 0.000020  loss: 3.5449 (3.4780)  time: 0.8092  data: 0.0005  max mem: 19734
Epoch: [10]  [ 360/1251]  eta: 0:12:31  lr: 0.000020  loss: 3.4971 (3.4815)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [10]  [ 370/1251]  eta: 0:12:22  lr: 0.000020  loss: 3.4971 (3.4813)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [10]  [ 380/1251]  eta: 0:12:12  lr: 0.000020  loss: 3.4972 (3.4798)  time: 0.8060  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4393, ratio_loss=0.0070, pruning_loss=0.1645, mse_loss=0.8979
Epoch: [10]  [ 390/1251]  eta: 0:12:03  lr: 0.000020  loss: 3.6716 (3.4822)  time: 0.8062  data: 0.0005  max mem: 19734
Epoch: [10]  [ 400/1251]  eta: 0:11:54  lr: 0.000020  loss: 3.7057 (3.4867)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [10]  [ 410/1251]  eta: 0:11:45  lr: 0.000020  loss: 3.6978 (3.4909)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [10]  [ 420/1251]  eta: 0:11:36  lr: 0.000020  loss: 3.5442 (3.4892)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [10]  [ 430/1251]  eta: 0:11:27  lr: 0.000020  loss: 3.5442 (3.4893)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [10]  [ 440/1251]  eta: 0:11:18  lr: 0.000020  loss: 3.6295 (3.4873)  time: 0.8120  data: 0.0005  max mem: 19734
Epoch: [10]  [ 450/1251]  eta: 0:11:10  lr: 0.000020  loss: 3.5331 (3.4908)  time: 0.8232  data: 0.0005  max mem: 19734
Epoch: [10]  [ 460/1251]  eta: 0:11:01  lr: 0.000020  loss: 3.6895 (3.4936)  time: 0.8203  data: 0.0005  max mem: 19734
Epoch: [10]  [ 470/1251]  eta: 0:10:52  lr: 0.000020  loss: 3.6191 (3.4899)  time: 0.8239  data: 0.0005  max mem: 19734
Epoch: [10]  [ 480/1251]  eta: 0:10:44  lr: 0.000020  loss: 3.4045 (3.4825)  time: 0.8336  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4550, ratio_loss=0.0070, pruning_loss=0.1643, mse_loss=0.8664
Epoch: [10]  [ 490/1251]  eta: 0:10:35  lr: 0.000020  loss: 3.2957 (3.4784)  time: 0.8186  data: 0.0006  max mem: 19734
Epoch: [10]  [ 500/1251]  eta: 0:10:26  lr: 0.000020  loss: 3.7087 (3.4816)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [10]  [ 510/1251]  eta: 0:10:18  lr: 0.000020  loss: 3.8510 (3.4884)  time: 0.8069  data: 0.0004  max mem: 19734
Epoch: [10]  [ 520/1251]  eta: 0:10:09  lr: 0.000020  loss: 3.8228 (3.4899)  time: 0.8060  data: 0.0004  max mem: 19734
Epoch: [10]  [ 530/1251]  eta: 0:10:00  lr: 0.000020  loss: 3.7043 (3.4920)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [10]  [ 540/1251]  eta: 0:09:52  lr: 0.000020  loss: 3.7229 (3.4944)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [10]  [ 550/1251]  eta: 0:09:43  lr: 0.000020  loss: 3.6232 (3.4927)  time: 0.8082  data: 0.0007  max mem: 19734
Epoch: [10]  [ 560/1251]  eta: 0:09:34  lr: 0.000020  loss: 3.3168 (3.4904)  time: 0.8086  data: 0.0006  max mem: 19734
Epoch: [10]  [ 570/1251]  eta: 0:09:26  lr: 0.000020  loss: 3.1671 (3.4844)  time: 0.8074  data: 0.0005  max mem: 19734
Epoch: [10]  [ 580/1251]  eta: 0:09:17  lr: 0.000020  loss: 3.6326 (3.4874)  time: 0.8134  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5026, ratio_loss=0.0075, pruning_loss=0.1628, mse_loss=0.8463
Epoch: [10]  [ 590/1251]  eta: 0:09:09  lr: 0.000020  loss: 3.8789 (3.4899)  time: 0.8124  data: 0.0006  max mem: 19734
Epoch: [10]  [ 600/1251]  eta: 0:09:00  lr: 0.000020  loss: 3.6131 (3.4908)  time: 0.8177  data: 0.0007  max mem: 19734
Epoch: [10]  [ 610/1251]  eta: 0:08:52  lr: 0.000020  loss: 3.4391 (3.4872)  time: 0.8309  data: 0.0006  max mem: 19734
Epoch: [10]  [ 620/1251]  eta: 0:08:44  lr: 0.000020  loss: 3.3892 (3.4857)  time: 0.8461  data: 0.0005  max mem: 19734
Epoch: [10]  [ 630/1251]  eta: 0:08:36  lr: 0.000020  loss: 3.5320 (3.4860)  time: 0.8453  data: 0.0005  max mem: 19734
Epoch: [10]  [ 640/1251]  eta: 0:08:27  lr: 0.000020  loss: 3.7555 (3.4911)  time: 0.8176  data: 0.0005  max mem: 19734
Epoch: [10]  [ 650/1251]  eta: 0:08:19  lr: 0.000020  loss: 3.6954 (3.4888)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [10]  [ 660/1251]  eta: 0:08:10  lr: 0.000020  loss: 3.3563 (3.4850)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [10]  [ 670/1251]  eta: 0:08:01  lr: 0.000020  loss: 3.5636 (3.4882)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [10]  [ 680/1251]  eta: 0:07:53  lr: 0.000020  loss: 3.5185 (3.4858)  time: 0.8050  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4432, ratio_loss=0.0070, pruning_loss=0.1628, mse_loss=0.8591
Epoch: [10]  [ 690/1251]  eta: 0:07:45  lr: 0.000020  loss: 3.4388 (3.4865)  time: 0.8077  data: 0.0006  max mem: 19734
Epoch: [10]  [ 700/1251]  eta: 0:07:36  lr: 0.000020  loss: 3.6456 (3.4892)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [10]  [ 710/1251]  eta: 0:07:28  lr: 0.000020  loss: 3.6456 (3.4907)  time: 0.8057  data: 0.0004  max mem: 19734
Epoch: [10]  [ 720/1251]  eta: 0:07:19  lr: 0.000020  loss: 3.6330 (3.4920)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [10]  [ 730/1251]  eta: 0:07:11  lr: 0.000020  loss: 3.5163 (3.4892)  time: 0.8122  data: 0.0005  max mem: 19734
Epoch: [10]  [ 740/1251]  eta: 0:07:02  lr: 0.000020  loss: 3.4642 (3.4862)  time: 0.8130  data: 0.0005  max mem: 19734
Epoch: [10]  [ 750/1251]  eta: 0:06:54  lr: 0.000020  loss: 3.5252 (3.4860)  time: 0.8156  data: 0.0006  max mem: 19734
Epoch: [10]  [ 760/1251]  eta: 0:06:46  lr: 0.000020  loss: 3.4058 (3.4837)  time: 0.8217  data: 0.0006  max mem: 19734
Epoch: [10]  [ 770/1251]  eta: 0:06:38  lr: 0.000020  loss: 3.3270 (3.4819)  time: 0.8336  data: 0.0004  max mem: 19734
Epoch: [10]  [ 780/1251]  eta: 0:06:29  lr: 0.000020  loss: 3.4240 (3.4829)  time: 0.8345  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4331, ratio_loss=0.0074, pruning_loss=0.1636, mse_loss=0.8489
Epoch: [10]  [ 790/1251]  eta: 0:06:21  lr: 0.000020  loss: 3.7013 (3.4819)  time: 0.8181  data: 0.0004  max mem: 19734
Epoch: [10]  [ 800/1251]  eta: 0:06:13  lr: 0.000020  loss: 3.0937 (3.4760)  time: 0.8104  data: 0.0004  max mem: 19734
Epoch: [10]  [ 810/1251]  eta: 0:06:04  lr: 0.000020  loss: 2.9671 (3.4745)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [10]  [ 820/1251]  eta: 0:05:56  lr: 0.000020  loss: 3.4476 (3.4743)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [10]  [ 830/1251]  eta: 0:05:47  lr: 0.000020  loss: 3.4819 (3.4738)  time: 0.8054  data: 0.0006  max mem: 19734
Epoch: [10]  [ 840/1251]  eta: 0:05:39  lr: 0.000020  loss: 3.6723 (3.4746)  time: 0.8079  data: 0.0006  max mem: 19734
Epoch: [10]  [ 850/1251]  eta: 0:05:31  lr: 0.000020  loss: 3.6723 (3.4752)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [10]  [ 860/1251]  eta: 0:05:22  lr: 0.000020  loss: 3.6964 (3.4772)  time: 0.8095  data: 0.0007  max mem: 19734
Epoch: [10]  [ 870/1251]  eta: 0:05:14  lr: 0.000020  loss: 3.7922 (3.4812)  time: 0.8118  data: 0.0007  max mem: 19734
Epoch: [10]  [ 880/1251]  eta: 0:05:06  lr: 0.000020  loss: 3.5812 (3.4799)  time: 0.8191  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3894, ratio_loss=0.0068, pruning_loss=0.1627, mse_loss=0.8646
Epoch: [10]  [ 890/1251]  eta: 0:04:58  lr: 0.000020  loss: 3.1395 (3.4740)  time: 0.8271  data: 0.0004  max mem: 19734
Epoch: [10]  [ 900/1251]  eta: 0:04:49  lr: 0.000020  loss: 2.6555 (3.4691)  time: 0.8271  data: 0.0004  max mem: 19734
Epoch: [10]  [ 910/1251]  eta: 0:04:41  lr: 0.000020  loss: 3.1938 (3.4690)  time: 0.8242  data: 0.0004  max mem: 19734
Epoch: [10]  [ 920/1251]  eta: 0:04:33  lr: 0.000020  loss: 3.4592 (3.4698)  time: 0.8282  data: 0.0005  max mem: 19734
Epoch: [10]  [ 930/1251]  eta: 0:04:24  lr: 0.000020  loss: 3.7241 (3.4745)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [10]  [ 940/1251]  eta: 0:04:16  lr: 0.000020  loss: 3.7938 (3.4759)  time: 0.8049  data: 0.0004  max mem: 19734
Epoch: [10]  [ 950/1251]  eta: 0:04:08  lr: 0.000020  loss: 3.4869 (3.4749)  time: 0.8055  data: 0.0004  max mem: 19734
Epoch: [10]  [ 960/1251]  eta: 0:04:00  lr: 0.000020  loss: 3.6448 (3.4763)  time: 0.8050  data: 0.0004  max mem: 19734
Epoch: [10]  [ 970/1251]  eta: 0:03:51  lr: 0.000020  loss: 3.5609 (3.4747)  time: 0.8048  data: 0.0004  max mem: 19734
Epoch: [10]  [ 980/1251]  eta: 0:03:43  lr: 0.000020  loss: 3.4405 (3.4735)  time: 0.8047  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4272, ratio_loss=0.0070, pruning_loss=0.1618, mse_loss=0.8800
Epoch: [10]  [ 990/1251]  eta: 0:03:35  lr: 0.000020  loss: 3.4616 (3.4728)  time: 0.8076  data: 0.0004  max mem: 19734
Epoch: [10]  [1000/1251]  eta: 0:03:26  lr: 0.000020  loss: 3.4944 (3.4727)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [10]  [1010/1251]  eta: 0:03:18  lr: 0.000020  loss: 3.2101 (3.4685)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [10]  [1020/1251]  eta: 0:03:10  lr: 0.000020  loss: 3.3308 (3.4672)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [10]  [1030/1251]  eta: 0:03:02  lr: 0.000020  loss: 3.5298 (3.4674)  time: 0.8131  data: 0.0006  max mem: 19734
Epoch: [10]  [1040/1251]  eta: 0:02:53  lr: 0.000020  loss: 3.5062 (3.4665)  time: 0.8248  data: 0.0007  max mem: 19734
Epoch: [10]  [1050/1251]  eta: 0:02:45  lr: 0.000020  loss: 3.5062 (3.4666)  time: 0.8247  data: 0.0005  max mem: 19734
Epoch: [10]  [1060/1251]  eta: 0:02:37  lr: 0.000020  loss: 3.3662 (3.4642)  time: 0.8436  data: 0.0005  max mem: 19734
Epoch: [10]  [1070/1251]  eta: 0:02:29  lr: 0.000020  loss: 3.2198 (3.4638)  time: 0.8517  data: 0.0005  max mem: 19734
Epoch: [10]  [1080/1251]  eta: 0:02:20  lr: 0.000020  loss: 3.5793 (3.4645)  time: 0.8165  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3425, ratio_loss=0.0071, pruning_loss=0.1637, mse_loss=0.8568
Epoch: [10]  [1090/1251]  eta: 0:02:12  lr: 0.000020  loss: 3.1477 (3.4621)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [10]  [1100/1251]  eta: 0:02:04  lr: 0.000020  loss: 3.0994 (3.4613)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [10]  [1110/1251]  eta: 0:01:56  lr: 0.000020  loss: 3.4167 (3.4612)  time: 0.8059  data: 0.0004  max mem: 19734
Epoch: [10]  [1120/1251]  eta: 0:01:47  lr: 0.000020  loss: 3.7719 (3.4636)  time: 0.8093  data: 0.0004  max mem: 19734
Epoch: [10]  [1130/1251]  eta: 0:01:39  lr: 0.000020  loss: 3.5598 (3.4596)  time: 0.8071  data: 0.0004  max mem: 19734
Epoch: [10]  [1140/1251]  eta: 0:01:31  lr: 0.000020  loss: 3.0143 (3.4564)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [10]  [1150/1251]  eta: 0:01:23  lr: 0.000020  loss: 2.9851 (3.4542)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [10]  [1160/1251]  eta: 0:01:14  lr: 0.000020  loss: 3.5569 (3.4552)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [10]  [1170/1251]  eta: 0:01:06  lr: 0.000020  loss: 3.6906 (3.4567)  time: 0.8026  data: 0.0004  max mem: 19734
Epoch: [10]  [1180/1251]  eta: 0:00:58  lr: 0.000020  loss: 3.5577 (3.4551)  time: 0.8099  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3444, ratio_loss=0.0069, pruning_loss=0.1635, mse_loss=0.8934
Epoch: [10]  [1190/1251]  eta: 0:00:50  lr: 0.000020  loss: 3.3627 (3.4546)  time: 0.8202  data: 0.0007  max mem: 19734
Epoch: [10]  [1200/1251]  eta: 0:00:41  lr: 0.000020  loss: 3.5873 (3.4553)  time: 0.8262  data: 0.0005  max mem: 19734
Epoch: [10]  [1210/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.6096 (3.4560)  time: 0.8265  data: 0.0002  max mem: 19734
Epoch: [10]  [1220/1251]  eta: 0:00:25  lr: 0.000020  loss: 3.4297 (3.4550)  time: 0.8086  data: 0.0002  max mem: 19734
Epoch: [10]  [1230/1251]  eta: 0:00:17  lr: 0.000020  loss: 3.4297 (3.4552)  time: 0.7922  data: 0.0002  max mem: 19734
Epoch: [10]  [1240/1251]  eta: 0:00:09  lr: 0.000020  loss: 3.5089 (3.4571)  time: 0.7930  data: 0.0002  max mem: 19734
Epoch: [10]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.6649 (3.4580)  time: 0.7924  data: 0.0002  max mem: 19734
Epoch: [10] Total time: 0:17:09 (0.8227 s / it)
Averaged stats: lr: 0.000020  loss: 3.6649 (3.4676)
Test:  [  0/261]  eta: 1:59:55  loss: 0.8145 (0.8145)  acc1: 83.3333 (83.3333)  acc5: 94.7917 (94.7917)  time: 27.5705  data: 26.9422  max mem: 19734
Test:  [ 10/261]  eta: 0:15:28  loss: 0.7419 (0.7781)  acc1: 85.4167 (83.6648)  acc5: 95.8333 (95.7860)  time: 3.6989  data: 3.1613  max mem: 19734
Test:  [ 20/261]  eta: 0:08:46  loss: 0.9865 (0.9400)  acc1: 79.1667 (79.0427)  acc5: 93.2292 (94.3700)  time: 0.9162  data: 0.3977  max mem: 19734
Test:  [ 30/261]  eta: 0:06:05  loss: 0.8464 (0.8570)  acc1: 82.2917 (81.8212)  acc5: 93.7500 (94.8925)  time: 0.4157  data: 0.0170  max mem: 19734
Test:  [ 40/261]  eta: 0:04:46  loss: 0.6172 (0.8277)  acc1: 86.9792 (82.7236)  acc5: 96.8750 (95.1982)  time: 0.3672  data: 0.0736  max mem: 19734
Test:  [ 50/261]  eta: 0:04:19  loss: 0.9370 (0.8886)  acc1: 77.6042 (80.8517)  acc5: 94.7917 (94.7508)  time: 0.6848  data: 0.3226  max mem: 19734
Test:  [ 60/261]  eta: 0:03:38  loss: 0.9929 (0.9024)  acc1: 77.0833 (80.3364)  acc5: 93.7500 (94.8344)  time: 0.6540  data: 0.2654  max mem: 19734
Test:  [ 70/261]  eta: 0:03:07  loss: 0.9700 (0.9022)  acc1: 77.6042 (79.9149)  acc5: 95.8333 (95.0631)  time: 0.3425  data: 0.0146  max mem: 19734
Test:  [ 80/261]  eta: 0:02:51  loss: 0.8807 (0.9082)  acc1: 78.6458 (80.0026)  acc5: 96.3542 (95.0746)  time: 0.5246  data: 0.1652  max mem: 19734
Test:  [ 90/261]  eta: 0:02:29  loss: 0.8378 (0.8916)  acc1: 83.3333 (80.4373)  acc5: 96.3542 (95.2209)  time: 0.5059  data: 0.1670  max mem: 19734
Test:  [100/261]  eta: 0:02:10  loss: 0.8392 (0.8951)  acc1: 83.3333 (80.3837)  acc5: 95.3125 (95.2661)  time: 0.2680  data: 0.0202  max mem: 19734
Test:  [110/261]  eta: 0:01:56  loss: 0.8996 (0.9200)  acc1: 75.5208 (79.8611)  acc5: 93.7500 (94.9418)  time: 0.3000  data: 0.0170  max mem: 19734
Test:  [120/261]  eta: 0:01:43  loss: 1.2840 (0.9601)  acc1: 69.2708 (78.9213)  acc5: 89.0625 (94.3784)  time: 0.3291  data: 0.0495  max mem: 19734
Test:  [130/261]  eta: 0:01:34  loss: 1.3893 (1.0067)  acc1: 65.6250 (77.9222)  acc5: 86.9792 (93.7818)  time: 0.4443  data: 0.1558  max mem: 19734
Test:  [140/261]  eta: 0:01:23  loss: 1.3866 (1.0332)  acc1: 67.7083 (77.2052)  acc5: 89.0625 (93.5284)  time: 0.4157  data: 0.1194  max mem: 19734
Test:  [150/261]  eta: 0:01:15  loss: 1.2402 (1.0390)  acc1: 70.8333 (77.1661)  acc5: 90.6250 (93.3637)  time: 0.4200  data: 0.1310  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: 1.1363 (1.0604)  acc1: 76.0417 (76.8051)  acc5: 90.6250 (93.0642)  time: 0.7839  data: 0.5104  max mem: 19734
Test:  [170/261]  eta: 0:01:01  loss: 1.3887 (1.0915)  acc1: 65.1042 (75.9990)  acc5: 85.9375 (92.6809)  time: 0.6385  data: 0.4369  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 1.5256 (1.1104)  acc1: 63.5417 (75.5439)  acc5: 85.9375 (92.4580)  time: 0.2893  data: 0.1220  max mem: 19734
Test:  [190/261]  eta: 0:00:44  loss: 1.4536 (1.1247)  acc1: 65.6250 (75.2618)  acc5: 89.5833 (92.2611)  time: 0.1988  data: 0.0704  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: 1.4536 (1.1417)  acc1: 69.7917 (74.8653)  acc5: 89.0625 (92.0398)  time: 0.1167  data: 0.0018  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.4546 (1.1563)  acc1: 66.6667 (74.5927)  acc5: 87.5000 (91.8024)  time: 0.1158  data: 0.0014  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.5184 (1.1752)  acc1: 65.1042 (74.1469)  acc5: 86.9792 (91.5842)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.5348 (1.1850)  acc1: 65.1042 (73.8997)  acc5: 88.0208 (91.4840)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.4258 (1.1939)  acc1: 66.1458 (73.6493)  acc5: 90.1042 (91.4311)  time: 0.1152  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1546 (1.1867)  acc1: 73.9583 (73.7986)  acc5: 93.7500 (91.5505)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0051 (1.1864)  acc1: 76.5625 (73.8120)  acc5: 94.7917 (91.6040)  time: 0.1118  data: 0.0001  max mem: 19734
Test: Total time: 0:02:07 (0.4901 s / it)
* Acc@1 73.812 Acc@5 91.604 loss 1.186
Accuracy of the network on the 50000 test images: 73.8%
Max accuracy: 73.81%
Epoch: [11]  [   0/1251]  eta: 4:30:02  lr: 0.000020  loss: 3.8281 (3.8281)  time: 12.9519  data: 12.1693  max mem: 19734
Epoch: [11]  [  10/1251]  eta: 0:41:23  lr: 0.000020  loss: 3.4669 (3.5484)  time: 2.0014  data: 1.1070  max mem: 19734
Epoch: [11]  [  20/1251]  eta: 0:29:17  lr: 0.000020  loss: 3.7923 (3.6540)  time: 0.8514  data: 0.0006  max mem: 19734
Epoch: [11]  [  30/1251]  eta: 0:24:57  lr: 0.000020  loss: 3.6915 (3.5197)  time: 0.8005  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4899, ratio_loss=0.0072, pruning_loss=0.1599, mse_loss=0.8166
Epoch: [11]  [  40/1251]  eta: 0:22:38  lr: 0.000020  loss: 3.2593 (3.4994)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [11]  [  50/1251]  eta: 0:21:11  lr: 0.000020  loss: 3.2593 (3.4540)  time: 0.7985  data: 0.0005  max mem: 19734
Epoch: [11]  [  60/1251]  eta: 0:20:10  lr: 0.000020  loss: 3.4318 (3.4407)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [11]  [  70/1251]  eta: 0:19:28  lr: 0.000020  loss: 3.4318 (3.4196)  time: 0.8134  data: 0.0005  max mem: 19734
Epoch: [11]  [  80/1251]  eta: 0:18:53  lr: 0.000020  loss: 3.5292 (3.4571)  time: 0.8189  data: 0.0005  max mem: 19734
Epoch: [11]  [  90/1251]  eta: 0:18:27  lr: 0.000020  loss: 3.7351 (3.4516)  time: 0.8264  data: 0.0005  max mem: 19734
Epoch: [11]  [ 100/1251]  eta: 0:18:03  lr: 0.000020  loss: 3.5208 (3.4612)  time: 0.8342  data: 0.0005  max mem: 19734
Epoch: [11]  [ 110/1251]  eta: 0:17:39  lr: 0.000020  loss: 3.6223 (3.4637)  time: 0.8130  data: 0.0006  max mem: 19734
Epoch: [11]  [ 120/1251]  eta: 0:17:17  lr: 0.000020  loss: 3.4112 (3.4425)  time: 0.7980  data: 0.0005  max mem: 19734
Epoch: [11]  [ 130/1251]  eta: 0:16:58  lr: 0.000020  loss: 3.5649 (3.4554)  time: 0.7989  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3977, ratio_loss=0.0069, pruning_loss=0.1612, mse_loss=0.8394
Epoch: [11]  [ 140/1251]  eta: 0:16:41  lr: 0.000020  loss: 3.5787 (3.4483)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [11]  [ 150/1251]  eta: 0:16:25  lr: 0.000020  loss: 3.5268 (3.4524)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [11]  [ 160/1251]  eta: 0:16:11  lr: 0.000020  loss: 3.4792 (3.4386)  time: 0.8135  data: 0.0005  max mem: 19734
Epoch: [11]  [ 170/1251]  eta: 0:15:57  lr: 0.000020  loss: 3.4578 (3.4375)  time: 0.8150  data: 0.0004  max mem: 19734
Epoch: [11]  [ 180/1251]  eta: 0:15:43  lr: 0.000020  loss: 3.5219 (3.4358)  time: 0.8086  data: 0.0004  max mem: 19734
Epoch: [11]  [ 190/1251]  eta: 0:15:30  lr: 0.000020  loss: 3.4152 (3.4236)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [11]  [ 200/1251]  eta: 0:15:18  lr: 0.000020  loss: 3.3134 (3.4181)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [11]  [ 210/1251]  eta: 0:15:06  lr: 0.000020  loss: 3.5061 (3.4205)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [11]  [ 220/1251]  eta: 0:14:56  lr: 0.000020  loss: 3.5755 (3.4208)  time: 0.8271  data: 0.0005  max mem: 19734
Epoch: [11]  [ 230/1251]  eta: 0:14:44  lr: 0.000020  loss: 3.7498 (3.4352)  time: 0.8267  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4155, ratio_loss=0.0069, pruning_loss=0.1611, mse_loss=0.8792
Epoch: [11]  [ 240/1251]  eta: 0:14:36  lr: 0.000020  loss: 3.7498 (3.4462)  time: 0.8351  data: 0.0006  max mem: 19734
Epoch: [11]  [ 250/1251]  eta: 0:14:25  lr: 0.000020  loss: 3.6495 (3.4527)  time: 0.8430  data: 0.0005  max mem: 19734
Epoch: [11]  [ 260/1251]  eta: 0:14:14  lr: 0.000020  loss: 3.5705 (3.4511)  time: 0.8096  data: 0.0004  max mem: 19734
Epoch: [11]  [ 270/1251]  eta: 0:14:03  lr: 0.000020  loss: 3.5705 (3.4510)  time: 0.8019  data: 0.0004  max mem: 19734
Epoch: [11]  [ 280/1251]  eta: 0:13:53  lr: 0.000020  loss: 3.3845 (3.4481)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [11]  [ 290/1251]  eta: 0:13:42  lr: 0.000020  loss: 3.3845 (3.4522)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [11]  [ 300/1251]  eta: 0:13:32  lr: 0.000020  loss: 3.4841 (3.4490)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [11]  [ 310/1251]  eta: 0:13:23  lr: 0.000020  loss: 3.6854 (3.4602)  time: 0.8139  data: 0.0005  max mem: 19734
Epoch: [11]  [ 320/1251]  eta: 0:13:13  lr: 0.000020  loss: 3.6854 (3.4666)  time: 0.8143  data: 0.0005  max mem: 19734
Epoch: [11]  [ 330/1251]  eta: 0:13:03  lr: 0.000020  loss: 3.6534 (3.4723)  time: 0.8052  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5198, ratio_loss=0.0070, pruning_loss=0.1562, mse_loss=0.8446
Epoch: [11]  [ 340/1251]  eta: 0:12:53  lr: 0.000020  loss: 3.6482 (3.4721)  time: 0.8064  data: 0.0004  max mem: 19734
Epoch: [11]  [ 350/1251]  eta: 0:12:44  lr: 0.000020  loss: 3.2307 (3.4644)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [11]  [ 360/1251]  eta: 0:12:34  lr: 0.000020  loss: 3.2307 (3.4636)  time: 0.8061  data: 0.0007  max mem: 19734
Epoch: [11]  [ 370/1251]  eta: 0:12:25  lr: 0.000020  loss: 3.4937 (3.4645)  time: 0.8176  data: 0.0006  max mem: 19734
Epoch: [11]  [ 380/1251]  eta: 0:12:16  lr: 0.000020  loss: 3.6038 (3.4668)  time: 0.8233  data: 0.0005  max mem: 19734
Epoch: [11]  [ 390/1251]  eta: 0:12:08  lr: 0.000020  loss: 3.6790 (3.4716)  time: 0.8301  data: 0.0005  max mem: 19734
Epoch: [11]  [ 400/1251]  eta: 0:11:59  lr: 0.000020  loss: 3.6417 (3.4708)  time: 0.8317  data: 0.0005  max mem: 19734
Epoch: [11]  [ 410/1251]  eta: 0:11:49  lr: 0.000020  loss: 3.5394 (3.4683)  time: 0.8156  data: 0.0005  max mem: 19734
Epoch: [11]  [ 420/1251]  eta: 0:11:40  lr: 0.000020  loss: 3.2140 (3.4655)  time: 0.8121  data: 0.0005  max mem: 19734
Epoch: [11]  [ 430/1251]  eta: 0:11:31  lr: 0.000020  loss: 3.6852 (3.4725)  time: 0.8080  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4576, ratio_loss=0.0067, pruning_loss=0.1588, mse_loss=0.8374
Epoch: [11]  [ 440/1251]  eta: 0:11:22  lr: 0.000020  loss: 3.5749 (3.4747)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [11]  [ 450/1251]  eta: 0:11:13  lr: 0.000020  loss: 3.2899 (3.4643)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [11]  [ 460/1251]  eta: 0:11:04  lr: 0.000020  loss: 3.2657 (3.4607)  time: 0.8068  data: 0.0005  max mem: 19734
Epoch: [11]  [ 470/1251]  eta: 0:10:55  lr: 0.000020  loss: 3.5856 (3.4582)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [11]  [ 480/1251]  eta: 0:10:46  lr: 0.000020  loss: 3.5232 (3.4589)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [11]  [ 490/1251]  eta: 0:10:37  lr: 0.000020  loss: 3.5232 (3.4601)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [11]  [ 500/1251]  eta: 0:10:28  lr: 0.000020  loss: 3.4152 (3.4582)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [11]  [ 510/1251]  eta: 0:10:20  lr: 0.000020  loss: 3.3706 (3.4569)  time: 0.8135  data: 0.0006  max mem: 19734
Epoch: [11]  [ 520/1251]  eta: 0:10:11  lr: 0.000020  loss: 3.2193 (3.4552)  time: 0.8165  data: 0.0006  max mem: 19734
Epoch: [11]  [ 530/1251]  eta: 0:10:03  lr: 0.000020  loss: 3.2193 (3.4566)  time: 0.8247  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3430, ratio_loss=0.0065, pruning_loss=0.1631, mse_loss=0.8454
Epoch: [11]  [ 540/1251]  eta: 0:09:54  lr: 0.000020  loss: 3.4534 (3.4560)  time: 0.8351  data: 0.0005  max mem: 19734
Epoch: [11]  [ 550/1251]  eta: 0:09:45  lr: 0.000020  loss: 3.3915 (3.4530)  time: 0.8204  data: 0.0005  max mem: 19734
Epoch: [11]  [ 560/1251]  eta: 0:09:37  lr: 0.000020  loss: 3.6265 (3.4555)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [11]  [ 570/1251]  eta: 0:09:28  lr: 0.000020  loss: 3.6674 (3.4560)  time: 0.8023  data: 0.0004  max mem: 19734
Epoch: [11]  [ 580/1251]  eta: 0:09:19  lr: 0.000020  loss: 3.6569 (3.4577)  time: 0.8030  data: 0.0004  max mem: 19734
Epoch: [11]  [ 590/1251]  eta: 0:09:10  lr: 0.000020  loss: 3.6569 (3.4584)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [11]  [ 600/1251]  eta: 0:09:02  lr: 0.000020  loss: 3.5917 (3.4581)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [11]  [ 610/1251]  eta: 0:08:53  lr: 0.000020  loss: 3.7456 (3.4622)  time: 0.8143  data: 0.0006  max mem: 19734
Epoch: [11]  [ 620/1251]  eta: 0:08:45  lr: 0.000020  loss: 3.7200 (3.4628)  time: 0.8148  data: 0.0006  max mem: 19734
Epoch: [11]  [ 630/1251]  eta: 0:08:36  lr: 0.000020  loss: 3.5051 (3.4625)  time: 0.8031  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4956, ratio_loss=0.0066, pruning_loss=0.1594, mse_loss=0.8494
Epoch: [11]  [ 640/1251]  eta: 0:08:28  lr: 0.000020  loss: 3.4985 (3.4661)  time: 0.8028  data: 0.0004  max mem: 19734
Epoch: [11]  [ 650/1251]  eta: 0:08:19  lr: 0.000020  loss: 3.6744 (3.4683)  time: 0.8023  data: 0.0004  max mem: 19734
Epoch: [11]  [ 660/1251]  eta: 0:08:11  lr: 0.000020  loss: 3.6312 (3.4657)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [11]  [ 670/1251]  eta: 0:08:02  lr: 0.000020  loss: 3.6088 (3.4695)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [11]  [ 680/1251]  eta: 0:07:54  lr: 0.000020  loss: 3.6088 (3.4692)  time: 0.8381  data: 0.0005  max mem: 19734
Epoch: [11]  [ 690/1251]  eta: 0:07:46  lr: 0.000020  loss: 3.5150 (3.4699)  time: 0.8358  data: 0.0004  max mem: 19734
Epoch: [11]  [ 700/1251]  eta: 0:07:37  lr: 0.000020  loss: 3.7367 (3.4716)  time: 0.8020  data: 0.0004  max mem: 19734
Epoch: [11]  [ 710/1251]  eta: 0:07:29  lr: 0.000020  loss: 3.7519 (3.4757)  time: 0.8079  data: 0.0004  max mem: 19734
Epoch: [11]  [ 720/1251]  eta: 0:07:20  lr: 0.000020  loss: 3.7114 (3.4758)  time: 0.8075  data: 0.0004  max mem: 19734
Epoch: [11]  [ 730/1251]  eta: 0:07:12  lr: 0.000020  loss: 3.7084 (3.4813)  time: 0.8035  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5380, ratio_loss=0.0068, pruning_loss=0.1568, mse_loss=0.8331
Epoch: [11]  [ 740/1251]  eta: 0:07:03  lr: 0.000020  loss: 3.6834 (3.4802)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [11]  [ 750/1251]  eta: 0:06:55  lr: 0.000020  loss: 3.3254 (3.4784)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [11]  [ 760/1251]  eta: 0:06:46  lr: 0.000020  loss: 3.6418 (3.4821)  time: 0.8037  data: 0.0004  max mem: 19734
Epoch: [11]  [ 770/1251]  eta: 0:06:38  lr: 0.000020  loss: 3.8142 (3.4821)  time: 0.8032  data: 0.0004  max mem: 19734
Epoch: [11]  [ 780/1251]  eta: 0:06:29  lr: 0.000020  loss: 3.6196 (3.4806)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [11]  [ 790/1251]  eta: 0:06:21  lr: 0.000020  loss: 3.6198 (3.4811)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [11]  [ 800/1251]  eta: 0:06:13  lr: 0.000020  loss: 3.5728 (3.4818)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [11]  [ 810/1251]  eta: 0:06:04  lr: 0.000020  loss: 3.5345 (3.4789)  time: 0.8203  data: 0.0007  max mem: 19734
Epoch: [11]  [ 820/1251]  eta: 0:05:56  lr: 0.000020  loss: 3.5407 (3.4801)  time: 0.8276  data: 0.0007  max mem: 19734
Epoch: [11]  [ 830/1251]  eta: 0:05:48  lr: 0.000020  loss: 3.6364 (3.4806)  time: 0.8350  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4766, ratio_loss=0.0066, pruning_loss=0.1580, mse_loss=0.8145
Epoch: [11]  [ 840/1251]  eta: 0:05:39  lr: 0.000020  loss: 3.6364 (3.4818)  time: 0.8261  data: 0.0009  max mem: 19734
Epoch: [11]  [ 850/1251]  eta: 0:05:31  lr: 0.000020  loss: 3.7389 (3.4859)  time: 0.8028  data: 0.0009  max mem: 19734
Epoch: [11]  [ 860/1251]  eta: 0:05:23  lr: 0.000020  loss: 3.7310 (3.4871)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [11]  [ 870/1251]  eta: 0:05:14  lr: 0.000020  loss: 3.5720 (3.4864)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [11]  [ 880/1251]  eta: 0:05:06  lr: 0.000020  loss: 3.4557 (3.4826)  time: 0.8042  data: 0.0006  max mem: 19734
Epoch: [11]  [ 890/1251]  eta: 0:04:58  lr: 0.000020  loss: 3.4347 (3.4805)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [11]  [ 900/1251]  eta: 0:04:49  lr: 0.000020  loss: 3.4221 (3.4809)  time: 0.8125  data: 0.0005  max mem: 19734
Epoch: [11]  [ 910/1251]  eta: 0:04:41  lr: 0.000020  loss: 3.4221 (3.4781)  time: 0.8130  data: 0.0005  max mem: 19734
Epoch: [11]  [ 920/1251]  eta: 0:04:33  lr: 0.000020  loss: 3.4106 (3.4763)  time: 0.8031  data: 0.0006  max mem: 19734
Epoch: [11]  [ 930/1251]  eta: 0:04:24  lr: 0.000020  loss: 3.1839 (3.4737)  time: 0.8023  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3220, ratio_loss=0.0067, pruning_loss=0.1633, mse_loss=0.8320
Epoch: [11]  [ 940/1251]  eta: 0:04:16  lr: 0.000020  loss: 3.0044 (3.4686)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [11]  [ 950/1251]  eta: 0:04:08  lr: 0.000020  loss: 3.4024 (3.4680)  time: 0.8109  data: 0.0005  max mem: 19734
Epoch: [11]  [ 960/1251]  eta: 0:03:59  lr: 0.000020  loss: 3.4024 (3.4659)  time: 0.8107  data: 0.0005  max mem: 19734
Epoch: [11]  [ 970/1251]  eta: 0:03:51  lr: 0.000020  loss: 3.4815 (3.4668)  time: 0.8188  data: 0.0005  max mem: 19734
Epoch: [11]  [ 980/1251]  eta: 0:03:43  lr: 0.000020  loss: 3.6291 (3.4668)  time: 0.8366  data: 0.0005  max mem: 19734
Epoch: [11]  [ 990/1251]  eta: 0:03:35  lr: 0.000020  loss: 3.5853 (3.4681)  time: 0.8226  data: 0.0005  max mem: 19734
Epoch: [11]  [1000/1251]  eta: 0:03:26  lr: 0.000020  loss: 3.5667 (3.4666)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [11]  [1010/1251]  eta: 0:03:18  lr: 0.000020  loss: 3.2544 (3.4640)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [11]  [1020/1251]  eta: 0:03:10  lr: 0.000020  loss: 3.3123 (3.4638)  time: 0.8044  data: 0.0004  max mem: 19734
Epoch: [11]  [1030/1251]  eta: 0:03:02  lr: 0.000020  loss: 3.6574 (3.4664)  time: 0.8028  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4430, ratio_loss=0.0066, pruning_loss=0.1594, mse_loss=0.8251
Epoch: [11]  [1040/1251]  eta: 0:02:53  lr: 0.000020  loss: 3.6574 (3.4678)  time: 0.8008  data: 0.0006  max mem: 19734
Epoch: [11]  [1050/1251]  eta: 0:02:45  lr: 0.000020  loss: 3.4717 (3.4668)  time: 0.8063  data: 0.0005  max mem: 19734
Epoch: [11]  [1060/1251]  eta: 0:02:37  lr: 0.000020  loss: 3.2998 (3.4648)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [11]  [1070/1251]  eta: 0:02:28  lr: 0.000020  loss: 3.2281 (3.4632)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [11]  [1080/1251]  eta: 0:02:20  lr: 0.000020  loss: 3.4917 (3.4653)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [11]  [1090/1251]  eta: 0:02:12  lr: 0.000020  loss: 3.6764 (3.4664)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [11]  [1100/1251]  eta: 0:02:04  lr: 0.000020  loss: 3.6002 (3.4663)  time: 0.8214  data: 0.0005  max mem: 19734
Epoch: [11]  [1110/1251]  eta: 0:01:55  lr: 0.000020  loss: 3.5268 (3.4658)  time: 0.8201  data: 0.0005  max mem: 19734
Epoch: [11]  [1120/1251]  eta: 0:01:47  lr: 0.000020  loss: 3.5395 (3.4663)  time: 0.8253  data: 0.0005  max mem: 19734
Epoch: [11]  [1130/1251]  eta: 0:01:39  lr: 0.000020  loss: 3.5716 (3.4670)  time: 0.8270  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4283, ratio_loss=0.0070, pruning_loss=0.1577, mse_loss=0.8282
Epoch: [11]  [1140/1251]  eta: 0:01:31  lr: 0.000020  loss: 3.6118 (3.4661)  time: 0.8015  data: 0.0006  max mem: 19734
Epoch: [11]  [1150/1251]  eta: 0:01:23  lr: 0.000020  loss: 3.6601 (3.4666)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [11]  [1160/1251]  eta: 0:01:14  lr: 0.000020  loss: 3.6601 (3.4674)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [11]  [1170/1251]  eta: 0:01:06  lr: 0.000020  loss: 3.4012 (3.4645)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [11]  [1180/1251]  eta: 0:00:58  lr: 0.000020  loss: 3.4747 (3.4649)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [11]  [1190/1251]  eta: 0:00:50  lr: 0.000020  loss: 3.6698 (3.4659)  time: 0.7988  data: 0.0007  max mem: 19734
Epoch: [11]  [1200/1251]  eta: 0:00:41  lr: 0.000020  loss: 3.5467 (3.4653)  time: 0.7972  data: 0.0005  max mem: 19734
Epoch: [11]  [1210/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.4144 (3.4638)  time: 0.7941  data: 0.0002  max mem: 19734
Epoch: [11]  [1220/1251]  eta: 0:00:25  lr: 0.000020  loss: 3.3642 (3.4643)  time: 0.7916  data: 0.0002  max mem: 19734
Epoch: [11]  [1230/1251]  eta: 0:00:17  lr: 0.000020  loss: 3.7044 (3.4645)  time: 0.7924  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4283, ratio_loss=0.0070, pruning_loss=0.1571, mse_loss=0.8122
Epoch: [11]  [1240/1251]  eta: 0:00:09  lr: 0.000020  loss: 3.7326 (3.4660)  time: 0.7927  data: 0.0002  max mem: 19734
Epoch: [11]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.7326 (3.4664)  time: 0.8017  data: 0.0002  max mem: 19734
Epoch: [11] Total time: 0:17:07 (0.8215 s / it)
Averaged stats: lr: 0.000020  loss: 3.7326 (3.4480)
Test:  [  0/261]  eta: 2:05:43  loss: 0.7112 (0.7112)  acc1: 83.3333 (83.3333)  acc5: 96.3542 (96.3542)  time: 28.9037  data: 28.6379  max mem: 19734
Test:  [ 10/261]  eta: 0:11:53  loss: 0.7112 (0.7679)  acc1: 83.3333 (83.3807)  acc5: 96.8750 (95.9280)  time: 2.8418  data: 2.6131  max mem: 19734
Test:  [ 20/261]  eta: 0:06:39  loss: 0.9970 (0.9332)  acc1: 78.6458 (78.7202)  acc5: 94.2708 (94.3948)  time: 0.2939  data: 0.0102  max mem: 19734
Test:  [ 30/261]  eta: 0:04:59  loss: 0.8079 (0.8509)  acc1: 82.2917 (81.5692)  acc5: 94.7917 (94.8925)  time: 0.4472  data: 0.0107  max mem: 19734
Test:  [ 40/261]  eta: 0:04:14  loss: 0.5939 (0.8236)  acc1: 87.5000 (82.3679)  acc5: 96.3542 (95.1601)  time: 0.6262  data: 0.2325  max mem: 19734
Test:  [ 50/261]  eta: 0:03:33  loss: 0.9217 (0.8829)  acc1: 78.1250 (80.5147)  acc5: 94.7917 (94.7406)  time: 0.5734  data: 0.2333  max mem: 19734
Test:  [ 60/261]  eta: 0:03:01  loss: 0.9948 (0.8928)  acc1: 76.0417 (80.0973)  acc5: 93.7500 (94.8002)  time: 0.3955  data: 0.0128  max mem: 19734
Test:  [ 70/261]  eta: 0:02:43  loss: 0.9750 (0.8943)  acc1: 76.0417 (79.6288)  acc5: 95.8333 (95.0558)  time: 0.4546  data: 0.1355  max mem: 19734
Test:  [ 80/261]  eta: 0:02:21  loss: 0.8551 (0.8991)  acc1: 78.6458 (79.7325)  acc5: 96.3542 (95.1389)  time: 0.4085  data: 0.1899  max mem: 19734
Test:  [ 90/261]  eta: 0:02:03  loss: 0.8487 (0.8842)  acc1: 83.3333 (80.1797)  acc5: 96.3542 (95.2724)  time: 0.2357  data: 0.0644  max mem: 19734
Test:  [100/261]  eta: 0:01:47  loss: 0.8338 (0.8865)  acc1: 83.3333 (80.1516)  acc5: 95.8333 (95.3073)  time: 0.2051  data: 0.0071  max mem: 19734
Test:  [110/261]  eta: 0:01:58  loss: 0.9024 (0.9121)  acc1: 76.0417 (79.6547)  acc5: 93.7500 (94.9794)  time: 1.0863  data: 0.7977  max mem: 19734
Test:  [120/261]  eta: 0:01:46  loss: 1.2832 (0.9544)  acc1: 70.8333 (78.7491)  acc5: 89.0625 (94.4043)  time: 1.1797  data: 0.8047  max mem: 19734
Test:  [130/261]  eta: 0:01:33  loss: 1.4770 (1.0015)  acc1: 65.6250 (77.7632)  acc5: 86.9792 (93.7619)  time: 0.3393  data: 0.0162  max mem: 19734
Test:  [140/261]  eta: 0:01:25  loss: 1.3734 (1.0280)  acc1: 68.2292 (77.1166)  acc5: 90.1042 (93.5173)  time: 0.4412  data: 0.1172  max mem: 19734
Test:  [150/261]  eta: 0:01:16  loss: 1.2662 (1.0326)  acc1: 71.3542 (77.1040)  acc5: 91.1458 (93.3947)  time: 0.4927  data: 0.1196  max mem: 19734
Test:  [160/261]  eta: 0:01:08  loss: 1.1218 (1.0544)  acc1: 76.5625 (76.6984)  acc5: 91.1458 (93.1062)  time: 0.4587  data: 0.0157  max mem: 19734
Test:  [170/261]  eta: 0:00:59  loss: 1.3752 (1.0847)  acc1: 65.6250 (75.9320)  acc5: 86.9792 (92.7571)  time: 0.3965  data: 0.0568  max mem: 19734
Test:  [180/261]  eta: 0:00:50  loss: 1.5109 (1.1027)  acc1: 65.1042 (75.5323)  acc5: 87.5000 (92.5760)  time: 0.2399  data: 0.0533  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.4498 (1.1165)  acc1: 65.6250 (75.2618)  acc5: 90.1042 (92.4138)  time: 0.1870  data: 0.0099  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.4303 (1.1327)  acc1: 71.3542 (74.9430)  acc5: 89.0625 (92.1849)  time: 0.1505  data: 0.0111  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.4303 (1.1464)  acc1: 70.8333 (74.6618)  acc5: 86.9792 (91.9950)  time: 0.1509  data: 0.0299  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4635 (1.1653)  acc1: 65.6250 (74.1634)  acc5: 87.5000 (91.8034)  time: 0.1396  data: 0.0250  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.5058 (1.1750)  acc1: 65.6250 (73.9448)  acc5: 88.5417 (91.6937)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3365 (1.1840)  acc1: 69.7917 (73.7314)  acc5: 89.5833 (91.6364)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1406 (1.1781)  acc1: 75.0000 (73.9065)  acc5: 92.1875 (91.7455)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0034 (1.1771)  acc1: 76.5625 (73.9200)  acc5: 94.7917 (91.7940)  time: 0.1119  data: 0.0001  max mem: 19734
Test: Total time: 0:02:04 (0.4768 s / it)
* Acc@1 73.920 Acc@5 91.794 loss 1.177
Accuracy of the network on the 50000 test images: 73.9%
Max accuracy: 73.92%
Epoch: [12]  [   0/1251]  eta: 5:36:00  lr: 0.000019  loss: 4.3562 (4.3562)  time: 16.1157  data: 14.9900  max mem: 19734
Epoch: [12]  [  10/1251]  eta: 0:48:47  lr: 0.000019  loss: 3.3435 (3.4043)  time: 2.3593  data: 1.3634  max mem: 19734
Epoch: [12]  [  20/1251]  eta: 0:33:06  lr: 0.000019  loss: 3.4899 (3.3864)  time: 0.8887  data: 0.0008  max mem: 19734
Epoch: [12]  [  30/1251]  eta: 0:27:29  lr: 0.000019  loss: 3.5219 (3.4590)  time: 0.7962  data: 0.0009  max mem: 19734
Epoch: [12]  [  40/1251]  eta: 0:24:37  lr: 0.000019  loss: 3.5060 (3.4243)  time: 0.8069  data: 0.0007  max mem: 19734
Epoch: [12]  [  50/1251]  eta: 0:22:45  lr: 0.000019  loss: 3.1174 (3.3924)  time: 0.8059  data: 0.0006  max mem: 19734
Epoch: [12]  [  60/1251]  eta: 0:21:30  lr: 0.000019  loss: 3.1203 (3.3529)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [12]  [  70/1251]  eta: 0:20:33  lr: 0.000019  loss: 3.5768 (3.4073)  time: 0.8088  data: 0.0005  max mem: 19734
Epoch: [12]  [  80/1251]  eta: 0:19:48  lr: 0.000019  loss: 3.6354 (3.4279)  time: 0.8068  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4209, ratio_loss=0.0066, pruning_loss=0.1591, mse_loss=0.8006
Epoch: [12]  [  90/1251]  eta: 0:19:11  lr: 0.000019  loss: 3.5417 (3.4242)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [12]  [ 100/1251]  eta: 0:18:39  lr: 0.000019  loss: 3.5417 (3.4197)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [12]  [ 110/1251]  eta: 0:18:11  lr: 0.000019  loss: 3.5449 (3.4313)  time: 0.7978  data: 0.0005  max mem: 19734
Epoch: [12]  [ 120/1251]  eta: 0:17:47  lr: 0.000019  loss: 3.4029 (3.4170)  time: 0.7985  data: 0.0005  max mem: 19734
Epoch: [12]  [ 130/1251]  eta: 0:17:28  lr: 0.000019  loss: 3.3472 (3.4273)  time: 0.8179  data: 0.0005  max mem: 19734
Epoch: [12]  [ 140/1251]  eta: 0:17:09  lr: 0.000019  loss: 3.7893 (3.4541)  time: 0.8208  data: 0.0004  max mem: 19734
Epoch: [12]  [ 150/1251]  eta: 0:16:55  lr: 0.000019  loss: 3.5815 (3.4421)  time: 0.8368  data: 0.0006  max mem: 19734
Epoch: [12]  [ 160/1251]  eta: 0:16:39  lr: 0.000019  loss: 3.5659 (3.4443)  time: 0.8430  data: 0.0006  max mem: 19734
Epoch: [12]  [ 170/1251]  eta: 0:16:22  lr: 0.000019  loss: 3.6285 (3.4562)  time: 0.8086  data: 0.0006  max mem: 19734
Epoch: [12]  [ 180/1251]  eta: 0:16:08  lr: 0.000019  loss: 3.6285 (3.4469)  time: 0.8112  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4494, ratio_loss=0.0067, pruning_loss=0.1551, mse_loss=0.8126
Epoch: [12]  [ 190/1251]  eta: 0:15:53  lr: 0.000019  loss: 3.3816 (3.4449)  time: 0.8113  data: 0.0005  max mem: 19734
Epoch: [12]  [ 200/1251]  eta: 0:15:39  lr: 0.000019  loss: 3.5406 (3.4492)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [12]  [ 210/1251]  eta: 0:15:26  lr: 0.000019  loss: 3.5406 (3.4482)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [12]  [ 220/1251]  eta: 0:15:13  lr: 0.000019  loss: 3.3954 (3.4492)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [12]  [ 230/1251]  eta: 0:15:00  lr: 0.000019  loss: 3.3877 (3.4420)  time: 0.8000  data: 0.0006  max mem: 19734
Epoch: [12]  [ 240/1251]  eta: 0:14:48  lr: 0.000019  loss: 3.5481 (3.4514)  time: 0.8000  data: 0.0006  max mem: 19734
Epoch: [12]  [ 250/1251]  eta: 0:14:36  lr: 0.000019  loss: 3.6017 (3.4511)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [12]  [ 260/1251]  eta: 0:14:25  lr: 0.000019  loss: 3.5393 (3.4529)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [12]  [ 270/1251]  eta: 0:14:14  lr: 0.000019  loss: 3.7699 (3.4668)  time: 0.8103  data: 0.0006  max mem: 19734
Epoch: [12]  [ 280/1251]  eta: 0:14:03  lr: 0.000019  loss: 3.6952 (3.4692)  time: 0.8194  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4902, ratio_loss=0.0069, pruning_loss=0.1555, mse_loss=0.8442
Epoch: [12]  [ 290/1251]  eta: 0:13:53  lr: 0.000019  loss: 3.6079 (3.4677)  time: 0.8113  data: 0.0007  max mem: 19734
Epoch: [12]  [ 300/1251]  eta: 0:13:44  lr: 0.000019  loss: 3.6564 (3.4660)  time: 0.8328  data: 0.0006  max mem: 19734
Epoch: [12]  [ 310/1251]  eta: 0:13:33  lr: 0.000019  loss: 3.5397 (3.4582)  time: 0.8322  data: 0.0006  max mem: 19734
Epoch: [12]  [ 320/1251]  eta: 0:13:23  lr: 0.000019  loss: 3.3260 (3.4495)  time: 0.8010  data: 0.0006  max mem: 19734
Epoch: [12]  [ 330/1251]  eta: 0:13:13  lr: 0.000019  loss: 3.3561 (3.4477)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [12]  [ 340/1251]  eta: 0:13:02  lr: 0.000019  loss: 3.4121 (3.4512)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [12]  [ 350/1251]  eta: 0:12:52  lr: 0.000019  loss: 3.6355 (3.4526)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [12]  [ 360/1251]  eta: 0:12:42  lr: 0.000019  loss: 3.6136 (3.4553)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [12]  [ 370/1251]  eta: 0:12:32  lr: 0.000019  loss: 3.6136 (3.4602)  time: 0.8017  data: 0.0004  max mem: 19734
Epoch: [12]  [ 380/1251]  eta: 0:12:23  lr: 0.000019  loss: 3.3612 (3.4480)  time: 0.8007  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3600, ratio_loss=0.0067, pruning_loss=0.1585, mse_loss=0.8193
Epoch: [12]  [ 390/1251]  eta: 0:12:13  lr: 0.000019  loss: 3.0651 (3.4477)  time: 0.8019  data: 0.0004  max mem: 19734
Epoch: [12]  [ 400/1251]  eta: 0:12:04  lr: 0.000019  loss: 3.5175 (3.4489)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [12]  [ 410/1251]  eta: 0:11:54  lr: 0.000019  loss: 3.5175 (3.4492)  time: 0.8064  data: 0.0006  max mem: 19734
Epoch: [12]  [ 420/1251]  eta: 0:11:45  lr: 0.000019  loss: 3.2764 (3.4448)  time: 0.8123  data: 0.0006  max mem: 19734
Epoch: [12]  [ 430/1251]  eta: 0:11:36  lr: 0.000019  loss: 3.4982 (3.4470)  time: 0.8209  data: 0.0005  max mem: 19734
Epoch: [12]  [ 440/1251]  eta: 0:11:27  lr: 0.000019  loss: 3.5518 (3.4464)  time: 0.8309  data: 0.0005  max mem: 19734
Epoch: [12]  [ 450/1251]  eta: 0:11:19  lr: 0.000019  loss: 3.4905 (3.4450)  time: 0.8455  data: 0.0005  max mem: 19734
Epoch: [12]  [ 460/1251]  eta: 0:11:10  lr: 0.000019  loss: 3.2369 (3.4389)  time: 0.8281  data: 0.0006  max mem: 19734
Epoch: [12]  [ 470/1251]  eta: 0:11:01  lr: 0.000019  loss: 3.3483 (3.4373)  time: 0.8055  data: 0.0006  max mem: 19734
Epoch: [12]  [ 480/1251]  eta: 0:10:52  lr: 0.000019  loss: 3.5836 (3.4459)  time: 0.8095  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4158, ratio_loss=0.0064, pruning_loss=0.1573, mse_loss=0.8018
Epoch: [12]  [ 490/1251]  eta: 0:10:42  lr: 0.000019  loss: 3.7785 (3.4464)  time: 0.8083  data: 0.0005  max mem: 19734
Epoch: [12]  [ 500/1251]  eta: 0:10:33  lr: 0.000019  loss: 3.6614 (3.4474)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [12]  [ 510/1251]  eta: 0:10:24  lr: 0.000019  loss: 3.7485 (3.4517)  time: 0.8031  data: 0.0006  max mem: 19734
Epoch: [12]  [ 520/1251]  eta: 0:10:15  lr: 0.000019  loss: 3.5203 (3.4493)  time: 0.8045  data: 0.0006  max mem: 19734
Epoch: [12]  [ 530/1251]  eta: 0:10:07  lr: 0.000019  loss: 3.6391 (3.4518)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [12]  [ 540/1251]  eta: 0:09:58  lr: 0.000019  loss: 3.6462 (3.4539)  time: 0.8074  data: 0.0007  max mem: 19734
Epoch: [12]  [ 550/1251]  eta: 0:09:49  lr: 0.000019  loss: 3.5570 (3.4545)  time: 0.8060  data: 0.0006  max mem: 19734
Epoch: [12]  [ 560/1251]  eta: 0:09:40  lr: 0.000019  loss: 3.6026 (3.4585)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [12]  [ 570/1251]  eta: 0:09:31  lr: 0.000019  loss: 3.6756 (3.4562)  time: 0.8198  data: 0.0006  max mem: 19734
Epoch: [12]  [ 580/1251]  eta: 0:09:23  lr: 0.000019  loss: 3.1205 (3.4533)  time: 0.8201  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4564, ratio_loss=0.0065, pruning_loss=0.1558, mse_loss=0.8190
Epoch: [12]  [ 590/1251]  eta: 0:09:14  lr: 0.000019  loss: 3.4228 (3.4522)  time: 0.8321  data: 0.0005  max mem: 19734
Epoch: [12]  [ 600/1251]  eta: 0:09:06  lr: 0.000019  loss: 3.5150 (3.4544)  time: 0.8329  data: 0.0006  max mem: 19734
Epoch: [12]  [ 610/1251]  eta: 0:08:57  lr: 0.000019  loss: 3.5384 (3.4574)  time: 0.8051  data: 0.0006  max mem: 19734
Epoch: [12]  [ 620/1251]  eta: 0:08:48  lr: 0.000019  loss: 3.5384 (3.4582)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [12]  [ 630/1251]  eta: 0:08:40  lr: 0.000019  loss: 3.2790 (3.4556)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [12]  [ 640/1251]  eta: 0:08:31  lr: 0.000019  loss: 3.1903 (3.4537)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [12]  [ 650/1251]  eta: 0:08:22  lr: 0.000019  loss: 3.6650 (3.4566)  time: 0.8014  data: 0.0006  max mem: 19734
Epoch: [12]  [ 660/1251]  eta: 0:08:14  lr: 0.000019  loss: 3.6997 (3.4569)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [12]  [ 670/1251]  eta: 0:08:05  lr: 0.000019  loss: 3.5194 (3.4564)  time: 0.8050  data: 0.0006  max mem: 19734
Epoch: [12]  [ 680/1251]  eta: 0:07:56  lr: 0.000019  loss: 3.4296 (3.4550)  time: 0.8038  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4339, ratio_loss=0.0064, pruning_loss=0.1546, mse_loss=0.7942
Epoch: [12]  [ 690/1251]  eta: 0:07:48  lr: 0.000019  loss: 3.5622 (3.4555)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [12]  [ 700/1251]  eta: 0:07:39  lr: 0.000019  loss: 3.5624 (3.4552)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [12]  [ 710/1251]  eta: 0:07:31  lr: 0.000019  loss: 3.5624 (3.4561)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [12]  [ 720/1251]  eta: 0:07:22  lr: 0.000019  loss: 3.5670 (3.4555)  time: 0.8145  data: 0.0008  max mem: 19734
Epoch: [12]  [ 730/1251]  eta: 0:07:14  lr: 0.000019  loss: 3.6147 (3.4551)  time: 0.8265  data: 0.0008  max mem: 19734
Epoch: [12]  [ 740/1251]  eta: 0:07:06  lr: 0.000019  loss: 3.6147 (3.4559)  time: 0.8431  data: 0.0007  max mem: 19734
Epoch: [12]  [ 750/1251]  eta: 0:06:57  lr: 0.000019  loss: 3.5551 (3.4559)  time: 0.8323  data: 0.0007  max mem: 19734
Epoch: [12]  [ 760/1251]  eta: 0:06:49  lr: 0.000019  loss: 3.5551 (3.4545)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [12]  [ 770/1251]  eta: 0:06:40  lr: 0.000019  loss: 3.2719 (3.4510)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [12]  [ 780/1251]  eta: 0:06:32  lr: 0.000019  loss: 3.3765 (3.4526)  time: 0.8066  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4211, ratio_loss=0.0066, pruning_loss=0.1547, mse_loss=0.8062
Epoch: [12]  [ 790/1251]  eta: 0:06:23  lr: 0.000019  loss: 3.5945 (3.4526)  time: 0.8021  data: 0.0004  max mem: 19734
Epoch: [12]  [ 800/1251]  eta: 0:06:15  lr: 0.000019  loss: 3.4484 (3.4511)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [12]  [ 810/1251]  eta: 0:06:06  lr: 0.000019  loss: 3.6117 (3.4544)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [12]  [ 820/1251]  eta: 0:05:58  lr: 0.000019  loss: 3.7099 (3.4556)  time: 0.8049  data: 0.0004  max mem: 19734
Epoch: [12]  [ 830/1251]  eta: 0:05:49  lr: 0.000019  loss: 3.5245 (3.4571)  time: 0.8055  data: 0.0004  max mem: 19734
Epoch: [12]  [ 840/1251]  eta: 0:05:41  lr: 0.000019  loss: 3.5245 (3.4559)  time: 0.8035  data: 0.0004  max mem: 19734
Epoch: [12]  [ 850/1251]  eta: 0:05:32  lr: 0.000019  loss: 3.6486 (3.4582)  time: 0.8009  data: 0.0004  max mem: 19734
Epoch: [12]  [ 860/1251]  eta: 0:05:24  lr: 0.000019  loss: 3.6983 (3.4593)  time: 0.8229  data: 0.0004  max mem: 19734
Epoch: [12]  [ 870/1251]  eta: 0:05:16  lr: 0.000019  loss: 3.7986 (3.4632)  time: 0.8251  data: 0.0005  max mem: 19734
Epoch: [12]  [ 880/1251]  eta: 0:05:07  lr: 0.000019  loss: 3.7980 (3.4651)  time: 0.8157  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5378, ratio_loss=0.0063, pruning_loss=0.1519, mse_loss=0.7962
Epoch: [12]  [ 890/1251]  eta: 0:04:59  lr: 0.000019  loss: 3.6745 (3.4634)  time: 0.8299  data: 0.0004  max mem: 19734
Epoch: [12]  [ 900/1251]  eta: 0:04:51  lr: 0.000019  loss: 3.2488 (3.4621)  time: 0.8175  data: 0.0004  max mem: 19734
Epoch: [12]  [ 910/1251]  eta: 0:04:42  lr: 0.000019  loss: 3.5557 (3.4616)  time: 0.8025  data: 0.0004  max mem: 19734
Epoch: [12]  [ 920/1251]  eta: 0:04:34  lr: 0.000019  loss: 3.0729 (3.4553)  time: 0.8158  data: 0.0005  max mem: 19734
Epoch: [12]  [ 930/1251]  eta: 0:04:26  lr: 0.000019  loss: 3.2525 (3.4554)  time: 0.8154  data: 0.0005  max mem: 19734
Epoch: [12]  [ 940/1251]  eta: 0:04:17  lr: 0.000019  loss: 3.3200 (3.4525)  time: 0.8023  data: 0.0004  max mem: 19734
Epoch: [12]  [ 950/1251]  eta: 0:04:09  lr: 0.000019  loss: 3.2675 (3.4516)  time: 0.8041  data: 0.0004  max mem: 19734
Epoch: [12]  [ 960/1251]  eta: 0:04:01  lr: 0.000019  loss: 3.5011 (3.4501)  time: 0.8053  data: 0.0004  max mem: 19734
Epoch: [12]  [ 970/1251]  eta: 0:03:52  lr: 0.000019  loss: 3.5011 (3.4486)  time: 0.8058  data: 0.0004  max mem: 19734
Epoch: [12]  [ 980/1251]  eta: 0:03:44  lr: 0.000019  loss: 3.1718 (3.4469)  time: 0.8099  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2759, ratio_loss=0.0066, pruning_loss=0.1592, mse_loss=0.8207
Epoch: [12]  [ 990/1251]  eta: 0:03:36  lr: 0.000019  loss: 3.5684 (3.4492)  time: 0.8082  data: 0.0006  max mem: 19734
Epoch: [12]  [1000/1251]  eta: 0:03:27  lr: 0.000019  loss: 3.5684 (3.4471)  time: 0.8024  data: 0.0006  max mem: 19734
Epoch: [12]  [1010/1251]  eta: 0:03:19  lr: 0.000019  loss: 3.4928 (3.4478)  time: 0.8170  data: 0.0005  max mem: 19734
Epoch: [12]  [1020/1251]  eta: 0:03:11  lr: 0.000019  loss: 3.4444 (3.4464)  time: 0.8171  data: 0.0005  max mem: 19734
Epoch: [12]  [1030/1251]  eta: 0:03:02  lr: 0.000019  loss: 3.4154 (3.4452)  time: 0.8391  data: 0.0005  max mem: 19734
Epoch: [12]  [1040/1251]  eta: 0:02:54  lr: 0.000019  loss: 3.4473 (3.4447)  time: 0.8401  data: 0.0004  max mem: 19734
Epoch: [12]  [1050/1251]  eta: 0:02:46  lr: 0.000019  loss: 3.3149 (3.4434)  time: 0.8030  data: 0.0004  max mem: 19734
Epoch: [12]  [1060/1251]  eta: 0:02:37  lr: 0.000019  loss: 3.4942 (3.4437)  time: 0.8055  data: 0.0005  max mem: 19734
Epoch: [12]  [1070/1251]  eta: 0:02:29  lr: 0.000019  loss: 3.7276 (3.4460)  time: 0.8102  data: 0.0004  max mem: 19734
Epoch: [12]  [1080/1251]  eta: 0:02:21  lr: 0.000019  loss: 3.6177 (3.4451)  time: 0.8084  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3727, ratio_loss=0.0069, pruning_loss=0.1565, mse_loss=0.7954
Epoch: [12]  [1090/1251]  eta: 0:02:13  lr: 0.000019  loss: 3.3996 (3.4430)  time: 0.8055  data: 0.0006  max mem: 19734
Epoch: [12]  [1100/1251]  eta: 0:02:04  lr: 0.000019  loss: 3.4006 (3.4434)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [12]  [1110/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.4213 (3.4435)  time: 0.8058  data: 0.0004  max mem: 19734
Epoch: [12]  [1120/1251]  eta: 0:01:48  lr: 0.000019  loss: 3.4213 (3.4439)  time: 0.8114  data: 0.0004  max mem: 19734
Epoch: [12]  [1130/1251]  eta: 0:01:39  lr: 0.000019  loss: 3.5915 (3.4449)  time: 0.8122  data: 0.0004  max mem: 19734
Epoch: [12]  [1140/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.6242 (3.4466)  time: 0.8055  data: 0.0004  max mem: 19734
Epoch: [12]  [1150/1251]  eta: 0:01:23  lr: 0.000019  loss: 3.6041 (3.4471)  time: 0.8133  data: 0.0004  max mem: 19734
Epoch: [12]  [1160/1251]  eta: 0:01:15  lr: 0.000019  loss: 3.6278 (3.4470)  time: 0.8119  data: 0.0004  max mem: 19734
Epoch: [12]  [1170/1251]  eta: 0:01:06  lr: 0.000019  loss: 3.4344 (3.4446)  time: 0.8157  data: 0.0004  max mem: 19734
Epoch: [12]  [1180/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.4344 (3.4453)  time: 0.8343  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4438, ratio_loss=0.0063, pruning_loss=0.1562, mse_loss=0.8087
Epoch: [12]  [1190/1251]  eta: 0:00:50  lr: 0.000019  loss: 3.7477 (3.4469)  time: 0.8204  data: 0.0007  max mem: 19734
Epoch: [12]  [1200/1251]  eta: 0:00:42  lr: 0.000019  loss: 3.7477 (3.4480)  time: 0.7994  data: 0.0006  max mem: 19734
Epoch: [12]  [1210/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.7756 (3.4478)  time: 0.7941  data: 0.0001  max mem: 19734
Epoch: [12]  [1220/1251]  eta: 0:00:25  lr: 0.000019  loss: 3.5356 (3.4488)  time: 0.7965  data: 0.0001  max mem: 19734
Epoch: [12]  [1230/1251]  eta: 0:00:17  lr: 0.000019  loss: 3.4674 (3.4462)  time: 0.7960  data: 0.0001  max mem: 19734
Epoch: [12]  [1240/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.1025 (3.4458)  time: 0.7925  data: 0.0001  max mem: 19734
Epoch: [12]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.5154 (3.4467)  time: 0.7926  data: 0.0001  max mem: 19734
Epoch: [12] Total time: 0:17:11 (0.8248 s / it)
Averaged stats: lr: 0.000019  loss: 3.5154 (3.4590)
Test:  [  0/261]  eta: 1:28:08  loss: 0.7812 (0.7812)  acc1: 83.3333 (83.3333)  acc5: 95.3125 (95.3125)  time: 20.2640  data: 19.8386  max mem: 19734
Test:  [ 10/261]  eta: 0:11:13  loss: 0.7592 (0.7851)  acc1: 83.3333 (83.4280)  acc5: 95.8333 (95.7860)  time: 2.6822  data: 2.4843  max mem: 19734
Test:  [ 20/261]  eta: 0:05:59  loss: 0.9686 (0.9327)  acc1: 79.6875 (79.1171)  acc5: 93.2292 (94.4444)  time: 0.5529  data: 0.3778  max mem: 19734
Test:  [ 30/261]  eta: 0:04:15  loss: 0.7958 (0.8490)  acc1: 80.7292 (81.9388)  acc5: 93.2292 (94.9261)  time: 0.2386  data: 0.0078  max mem: 19734
Test:  [ 40/261]  eta: 0:03:24  loss: 0.5984 (0.8155)  acc1: 88.0208 (82.9014)  acc5: 96.3542 (95.2236)  time: 0.3341  data: 0.1080  max mem: 19734
Test:  [ 50/261]  eta: 0:02:53  loss: 0.9157 (0.8719)  acc1: 78.1250 (81.0458)  acc5: 94.2708 (94.7917)  time: 0.3846  data: 0.1066  max mem: 19734
Test:  [ 60/261]  eta: 0:02:38  loss: 1.0132 (0.8845)  acc1: 76.0417 (80.5157)  acc5: 94.2708 (94.8344)  time: 0.4984  data: 0.0139  max mem: 19734
Test:  [ 70/261]  eta: 0:02:40  loss: 0.9818 (0.8895)  acc1: 76.5625 (79.9296)  acc5: 95.8333 (95.0778)  time: 0.8835  data: 0.4267  max mem: 19734
Test:  [ 80/261]  eta: 0:02:21  loss: 0.8724 (0.8933)  acc1: 79.1667 (80.0990)  acc5: 96.3542 (95.1582)  time: 0.7567  data: 0.4209  max mem: 19734
Test:  [ 90/261]  eta: 0:02:07  loss: 0.8224 (0.8794)  acc1: 83.3333 (80.4487)  acc5: 96.3542 (95.2839)  time: 0.4029  data: 0.0152  max mem: 19734
Test:  [100/261]  eta: 0:01:54  loss: 0.8276 (0.8815)  acc1: 83.3333 (80.4094)  acc5: 95.8333 (95.3538)  time: 0.4375  data: 0.0234  max mem: 19734
Test:  [110/261]  eta: 0:01:42  loss: 0.9066 (0.9045)  acc1: 78.1250 (79.9690)  acc5: 94.7917 (95.0497)  time: 0.3771  data: 0.0598  max mem: 19734
Test:  [120/261]  eta: 0:01:29  loss: 1.2513 (0.9446)  acc1: 70.3125 (78.9988)  acc5: 89.0625 (94.4904)  time: 0.2642  data: 0.0541  max mem: 19734
Test:  [130/261]  eta: 0:01:20  loss: 1.4435 (0.9910)  acc1: 67.1875 (78.0415)  acc5: 86.9792 (93.8454)  time: 0.2831  data: 0.0168  max mem: 19734
Test:  [140/261]  eta: 0:01:16  loss: 1.3786 (1.0180)  acc1: 67.1875 (77.3124)  acc5: 89.0625 (93.6022)  time: 0.6136  data: 0.2034  max mem: 19734
Test:  [150/261]  eta: 0:01:08  loss: 1.2097 (1.0229)  acc1: 70.8333 (77.2972)  acc5: 91.1458 (93.4706)  time: 0.6453  data: 0.2010  max mem: 19734
Test:  [160/261]  eta: 0:01:01  loss: 1.0999 (1.0451)  acc1: 77.0833 (76.8828)  acc5: 91.1458 (93.1903)  time: 0.4332  data: 0.0136  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.3312 (1.0756)  acc1: 65.1042 (76.1056)  acc5: 87.5000 (92.8332)  time: 0.5068  data: 0.0148  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: 1.4853 (1.0928)  acc1: 65.1042 (75.7021)  acc5: 88.0208 (92.6681)  time: 0.5053  data: 0.0182  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: 1.4417 (1.1064)  acc1: 66.6667 (75.4254)  acc5: 90.1042 (92.5065)  time: 0.3140  data: 0.0160  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.3948 (1.1235)  acc1: 70.8333 (75.0959)  acc5: 89.5833 (92.2497)  time: 0.2246  data: 0.0092  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.3948 (1.1389)  acc1: 68.7500 (74.7729)  acc5: 86.9792 (92.0271)  time: 0.1866  data: 0.0034  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4629 (1.1578)  acc1: 65.6250 (74.2741)  acc5: 88.5417 (91.8387)  time: 0.1716  data: 0.0258  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.4695 (1.1671)  acc1: 65.6250 (74.0643)  acc5: 88.5417 (91.7185)  time: 0.2339  data: 0.0877  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.4114 (1.1760)  acc1: 66.1458 (73.8308)  acc5: 90.1042 (91.6818)  time: 0.1852  data: 0.0622  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1416 (1.1695)  acc1: 75.0000 (73.9708)  acc5: 92.1875 (91.7787)  time: 0.1163  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0300 (1.1684)  acc1: 76.0417 (74.0120)  acc5: 94.2708 (91.8380)  time: 0.1115  data: 0.0001  max mem: 19734
Test: Total time: 0:02:02 (0.4688 s / it)
* Acc@1 74.012 Acc@5 91.838 loss 1.168
Accuracy of the network on the 50000 test images: 74.0%
Max accuracy: 74.01%
Epoch: [13]  [   0/1251]  eta: 6:14:49  lr: 0.000019  loss: 2.4464 (2.4464)  time: 17.9772  data: 12.5306  max mem: 19734
Epoch: [13]  [  10/1251]  eta: 0:53:49  lr: 0.000019  loss: 3.4917 (3.1887)  time: 2.6023  data: 1.2890  max mem: 19734
Epoch: [13]  [  20/1251]  eta: 0:35:48  lr: 0.000019  loss: 3.5196 (3.3506)  time: 0.9340  data: 0.0827  max mem: 19734
Epoch: [13]  [  30/1251]  eta: 0:29:24  lr: 0.000019  loss: 3.3763 (3.3336)  time: 0.8087  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4048, ratio_loss=0.0066, pruning_loss=0.1568, mse_loss=0.7837
Epoch: [13]  [  40/1251]  eta: 0:26:08  lr: 0.000019  loss: 3.3170 (3.3703)  time: 0.8216  data: 0.0006  max mem: 19734
Epoch: [13]  [  50/1251]  eta: 0:23:57  lr: 0.000019  loss: 3.4668 (3.3913)  time: 0.8127  data: 0.0005  max mem: 19734
Epoch: [13]  [  60/1251]  eta: 0:22:33  lr: 0.000019  loss: 3.5111 (3.4199)  time: 0.8113  data: 0.0004  max mem: 19734
Epoch: [13]  [  70/1251]  eta: 0:21:30  lr: 0.000019  loss: 3.5111 (3.4161)  time: 0.8251  data: 0.0004  max mem: 19734
Epoch: [13]  [  80/1251]  eta: 0:20:37  lr: 0.000019  loss: 3.5528 (3.4219)  time: 0.8142  data: 0.0005  max mem: 19734
Epoch: [13]  [  90/1251]  eta: 0:19:53  lr: 0.000019  loss: 3.2291 (3.4038)  time: 0.8002  data: 0.0004  max mem: 19734
Epoch: [13]  [ 100/1251]  eta: 0:19:17  lr: 0.000019  loss: 3.2291 (3.4035)  time: 0.7971  data: 0.0005  max mem: 19734
Epoch: [13]  [ 110/1251]  eta: 0:18:46  lr: 0.000019  loss: 3.6455 (3.4029)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [13]  [ 120/1251]  eta: 0:18:18  lr: 0.000019  loss: 3.4889 (3.3930)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [13]  [ 130/1251]  eta: 0:17:54  lr: 0.000019  loss: 3.5392 (3.4042)  time: 0.7969  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3978, ratio_loss=0.0069, pruning_loss=0.1560, mse_loss=0.7985
Epoch: [13]  [ 140/1251]  eta: 0:17:31  lr: 0.000019  loss: 3.6850 (3.4044)  time: 0.7975  data: 0.0007  max mem: 19734
Epoch: [13]  [ 150/1251]  eta: 0:17:11  lr: 0.000019  loss: 3.3626 (3.4066)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [13]  [ 160/1251]  eta: 0:16:53  lr: 0.000019  loss: 3.2299 (3.3976)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [13]  [ 170/1251]  eta: 0:16:35  lr: 0.000019  loss: 3.2289 (3.3961)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [13]  [ 180/1251]  eta: 0:16:21  lr: 0.000019  loss: 3.6134 (3.4210)  time: 0.8151  data: 0.0005  max mem: 19734
Epoch: [13]  [ 190/1251]  eta: 0:16:06  lr: 0.000019  loss: 3.6134 (3.4085)  time: 0.8249  data: 0.0005  max mem: 19734
Epoch: [13]  [ 200/1251]  eta: 0:15:51  lr: 0.000019  loss: 3.0242 (3.4029)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [13]  [ 210/1251]  eta: 0:15:39  lr: 0.000019  loss: 3.5928 (3.4085)  time: 0.8211  data: 0.0005  max mem: 19734
Epoch: [13]  [ 220/1251]  eta: 0:15:26  lr: 0.000019  loss: 3.5052 (3.4059)  time: 0.8297  data: 0.0006  max mem: 19734
Epoch: [13]  [ 230/1251]  eta: 0:15:13  lr: 0.000019  loss: 3.3424 (3.4070)  time: 0.8073  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3983, ratio_loss=0.0064, pruning_loss=0.1543, mse_loss=0.7908
Epoch: [13]  [ 240/1251]  eta: 0:15:00  lr: 0.000019  loss: 3.5802 (3.4174)  time: 0.7974  data: 0.0004  max mem: 19734
Epoch: [13]  [ 250/1251]  eta: 0:14:47  lr: 0.000019  loss: 3.7323 (3.4305)  time: 0.8000  data: 0.0004  max mem: 19734
Epoch: [13]  [ 260/1251]  eta: 0:14:35  lr: 0.000019  loss: 3.6926 (3.4287)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [13]  [ 270/1251]  eta: 0:14:23  lr: 0.000019  loss: 3.2849 (3.4246)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [13]  [ 280/1251]  eta: 0:14:12  lr: 0.000019  loss: 3.2142 (3.4191)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [13]  [ 290/1251]  eta: 0:14:01  lr: 0.000019  loss: 3.4292 (3.4173)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [13]  [ 300/1251]  eta: 0:13:50  lr: 0.000019  loss: 3.4147 (3.4135)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [13]  [ 310/1251]  eta: 0:13:39  lr: 0.000019  loss: 3.3628 (3.4132)  time: 0.8029  data: 0.0004  max mem: 19734
Epoch: [13]  [ 320/1251]  eta: 0:13:28  lr: 0.000019  loss: 3.7392 (3.4187)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [13]  [ 330/1251]  eta: 0:13:18  lr: 0.000019  loss: 3.7976 (3.4260)  time: 0.8133  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4125, ratio_loss=0.0066, pruning_loss=0.1538, mse_loss=0.7769
Epoch: [13]  [ 340/1251]  eta: 0:13:08  lr: 0.000019  loss: 3.4056 (3.4198)  time: 0.8267  data: 0.0005  max mem: 19734
Epoch: [13]  [ 350/1251]  eta: 0:12:58  lr: 0.000019  loss: 3.3187 (3.4187)  time: 0.8163  data: 0.0005  max mem: 19734
Epoch: [13]  [ 360/1251]  eta: 0:12:49  lr: 0.000019  loss: 3.5690 (3.4171)  time: 0.8328  data: 0.0005  max mem: 19734
Epoch: [13]  [ 370/1251]  eta: 0:12:39  lr: 0.000019  loss: 3.6865 (3.4240)  time: 0.8302  data: 0.0005  max mem: 19734
Epoch: [13]  [ 380/1251]  eta: 0:12:29  lr: 0.000019  loss: 3.6865 (3.4256)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [13]  [ 390/1251]  eta: 0:12:19  lr: 0.000019  loss: 3.6124 (3.4221)  time: 0.8011  data: 0.0004  max mem: 19734
Epoch: [13]  [ 400/1251]  eta: 0:12:10  lr: 0.000019  loss: 3.6066 (3.4252)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [13]  [ 410/1251]  eta: 0:12:00  lr: 0.000019  loss: 3.4817 (3.4230)  time: 0.8014  data: 0.0006  max mem: 19734
Epoch: [13]  [ 420/1251]  eta: 0:11:50  lr: 0.000019  loss: 3.4194 (3.4228)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [13]  [ 430/1251]  eta: 0:11:41  lr: 0.000019  loss: 3.3883 (3.4256)  time: 0.8033  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4409, ratio_loss=0.0063, pruning_loss=0.1528, mse_loss=0.7648
Epoch: [13]  [ 440/1251]  eta: 0:11:31  lr: 0.000019  loss: 3.5819 (3.4294)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [13]  [ 450/1251]  eta: 0:11:22  lr: 0.000019  loss: 3.6968 (3.4324)  time: 0.8023  data: 0.0006  max mem: 19734
Epoch: [13]  [ 460/1251]  eta: 0:11:12  lr: 0.000019  loss: 3.4851 (3.4258)  time: 0.8016  data: 0.0006  max mem: 19734
Epoch: [13]  [ 470/1251]  eta: 0:11:04  lr: 0.000019  loss: 3.4243 (3.4262)  time: 0.8158  data: 0.0005  max mem: 19734
Epoch: [13]  [ 480/1251]  eta: 0:10:55  lr: 0.000019  loss: 3.6446 (3.4295)  time: 0.8270  data: 0.0005  max mem: 19734
Epoch: [13]  [ 490/1251]  eta: 0:10:45  lr: 0.000019  loss: 3.6635 (3.4337)  time: 0.8171  data: 0.0004  max mem: 19734
Epoch: [13]  [ 500/1251]  eta: 0:10:37  lr: 0.000019  loss: 3.6142 (3.4378)  time: 0.8204  data: 0.0004  max mem: 19734
Epoch: [13]  [ 510/1251]  eta: 0:10:29  lr: 0.000019  loss: 3.6967 (3.4440)  time: 0.8495  data: 0.0005  max mem: 19734
Epoch: [13]  [ 520/1251]  eta: 0:10:19  lr: 0.000019  loss: 3.6972 (3.4451)  time: 0.8349  data: 0.0004  max mem: 19734
Epoch: [13]  [ 530/1251]  eta: 0:10:10  lr: 0.000019  loss: 3.2333 (3.4382)  time: 0.8030  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4488, ratio_loss=0.0067, pruning_loss=0.1506, mse_loss=0.7847
Epoch: [13]  [ 540/1251]  eta: 0:10:01  lr: 0.000019  loss: 3.0513 (3.4367)  time: 0.8032  data: 0.0009  max mem: 19734
Epoch: [13]  [ 550/1251]  eta: 0:09:52  lr: 0.000019  loss: 3.4829 (3.4341)  time: 0.8060  data: 0.0009  max mem: 19734
Epoch: [13]  [ 560/1251]  eta: 0:09:43  lr: 0.000019  loss: 3.2891 (3.4333)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [13]  [ 570/1251]  eta: 0:09:34  lr: 0.000019  loss: 3.6029 (3.4370)  time: 0.8027  data: 0.0004  max mem: 19734
Epoch: [13]  [ 580/1251]  eta: 0:09:25  lr: 0.000019  loss: 3.6029 (3.4400)  time: 0.8026  data: 0.0004  max mem: 19734
Epoch: [13]  [ 590/1251]  eta: 0:09:17  lr: 0.000019  loss: 3.5329 (3.4415)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [13]  [ 600/1251]  eta: 0:09:08  lr: 0.000019  loss: 3.5329 (3.4388)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [13]  [ 610/1251]  eta: 0:08:59  lr: 0.000019  loss: 3.6729 (3.4415)  time: 0.8037  data: 0.0004  max mem: 19734
Epoch: [13]  [ 620/1251]  eta: 0:08:50  lr: 0.000019  loss: 3.5851 (3.4440)  time: 0.8224  data: 0.0005  max mem: 19734
Epoch: [13]  [ 630/1251]  eta: 0:08:42  lr: 0.000019  loss: 3.4541 (3.4415)  time: 0.8227  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4144, ratio_loss=0.0063, pruning_loss=0.1548, mse_loss=0.7628
Epoch: [13]  [ 640/1251]  eta: 0:08:33  lr: 0.000019  loss: 3.2634 (3.4387)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [13]  [ 650/1251]  eta: 0:08:25  lr: 0.000019  loss: 3.2634 (3.4353)  time: 0.8369  data: 0.0004  max mem: 19734
Epoch: [13]  [ 660/1251]  eta: 0:08:16  lr: 0.000019  loss: 3.5321 (3.4374)  time: 0.8302  data: 0.0005  max mem: 19734
Epoch: [13]  [ 670/1251]  eta: 0:08:07  lr: 0.000019  loss: 3.4916 (3.4345)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [13]  [ 680/1251]  eta: 0:07:59  lr: 0.000019  loss: 3.3925 (3.4334)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [13]  [ 690/1251]  eta: 0:07:50  lr: 0.000019  loss: 3.7176 (3.4372)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [13]  [ 700/1251]  eta: 0:07:41  lr: 0.000019  loss: 3.7039 (3.4373)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [13]  [ 710/1251]  eta: 0:07:33  lr: 0.000019  loss: 3.4220 (3.4367)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [13]  [ 720/1251]  eta: 0:07:24  lr: 0.000019  loss: 3.4220 (3.4351)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [13]  [ 730/1251]  eta: 0:07:15  lr: 0.000019  loss: 3.5660 (3.4349)  time: 0.8028  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4064, ratio_loss=0.0062, pruning_loss=0.1540, mse_loss=0.7742
Epoch: [13]  [ 740/1251]  eta: 0:07:07  lr: 0.000019  loss: 3.7178 (3.4370)  time: 0.8024  data: 0.0004  max mem: 19734
Epoch: [13]  [ 750/1251]  eta: 0:06:58  lr: 0.000019  loss: 3.6000 (3.4351)  time: 0.8014  data: 0.0004  max mem: 19734
Epoch: [13]  [ 760/1251]  eta: 0:06:50  lr: 0.000019  loss: 3.1722 (3.4313)  time: 0.8002  data: 0.0004  max mem: 19734
Epoch: [13]  [ 770/1251]  eta: 0:06:41  lr: 0.000019  loss: 3.0845 (3.4294)  time: 0.8166  data: 0.0004  max mem: 19734
Epoch: [13]  [ 780/1251]  eta: 0:06:33  lr: 0.000019  loss: 3.4240 (3.4297)  time: 0.8277  data: 0.0004  max mem: 19734
Epoch: [13]  [ 790/1251]  eta: 0:06:24  lr: 0.000019  loss: 3.4076 (3.4280)  time: 0.8191  data: 0.0004  max mem: 19734
Epoch: [13]  [ 800/1251]  eta: 0:06:16  lr: 0.000019  loss: 3.2492 (3.4295)  time: 0.8290  data: 0.0004  max mem: 19734
Epoch: [13]  [ 810/1251]  eta: 0:06:07  lr: 0.000019  loss: 3.6177 (3.4285)  time: 0.8215  data: 0.0005  max mem: 19734
Epoch: [13]  [ 820/1251]  eta: 0:05:59  lr: 0.000019  loss: 3.6314 (3.4304)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [13]  [ 830/1251]  eta: 0:05:50  lr: 0.000019  loss: 3.6598 (3.4317)  time: 0.8009  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3911, ratio_loss=0.0062, pruning_loss=0.1541, mse_loss=0.7649
Epoch: [13]  [ 840/1251]  eta: 0:05:42  lr: 0.000019  loss: 3.7094 (3.4348)  time: 0.8010  data: 0.0004  max mem: 19734
Epoch: [13]  [ 850/1251]  eta: 0:05:33  lr: 0.000019  loss: 3.6131 (3.4338)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [13]  [ 860/1251]  eta: 0:05:25  lr: 0.000019  loss: 3.6131 (3.4366)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [13]  [ 870/1251]  eta: 0:05:17  lr: 0.000019  loss: 3.6472 (3.4353)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [13]  [ 880/1251]  eta: 0:05:08  lr: 0.000019  loss: 3.4279 (3.4331)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [13]  [ 890/1251]  eta: 0:05:00  lr: 0.000019  loss: 3.5396 (3.4345)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [13]  [ 900/1251]  eta: 0:04:51  lr: 0.000019  loss: 3.7473 (3.4361)  time: 0.8019  data: 0.0004  max mem: 19734
Epoch: [13]  [ 910/1251]  eta: 0:04:43  lr: 0.000019  loss: 3.7473 (3.4374)  time: 0.8134  data: 0.0004  max mem: 19734
Epoch: [13]  [ 920/1251]  eta: 0:04:35  lr: 0.000019  loss: 3.3080 (3.4343)  time: 0.8242  data: 0.0005  max mem: 19734
Epoch: [13]  [ 930/1251]  eta: 0:04:26  lr: 0.000019  loss: 2.9807 (3.4305)  time: 0.8154  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3853, ratio_loss=0.0066, pruning_loss=0.1514, mse_loss=0.7597
Epoch: [13]  [ 940/1251]  eta: 0:04:18  lr: 0.000019  loss: 3.3383 (3.4316)  time: 0.8259  data: 0.0005  max mem: 19734
Epoch: [13]  [ 950/1251]  eta: 0:04:10  lr: 0.000019  loss: 3.4522 (3.4314)  time: 0.8352  data: 0.0005  max mem: 19734
Epoch: [13]  [ 960/1251]  eta: 0:04:01  lr: 0.000019  loss: 3.4522 (3.4306)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [13]  [ 970/1251]  eta: 0:03:53  lr: 0.000019  loss: 3.2996 (3.4285)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [13]  [ 980/1251]  eta: 0:03:44  lr: 0.000019  loss: 3.5494 (3.4307)  time: 0.8028  data: 0.0006  max mem: 19734
Epoch: [13]  [ 990/1251]  eta: 0:03:36  lr: 0.000019  loss: 3.5751 (3.4307)  time: 0.8029  data: 0.0007  max mem: 19734
Epoch: [13]  [1000/1251]  eta: 0:03:28  lr: 0.000019  loss: 3.5751 (3.4321)  time: 0.8037  data: 0.0006  max mem: 19734
Epoch: [13]  [1010/1251]  eta: 0:03:19  lr: 0.000019  loss: 3.5819 (3.4318)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [13]  [1020/1251]  eta: 0:03:11  lr: 0.000019  loss: 3.6030 (3.4316)  time: 0.8020  data: 0.0004  max mem: 19734
Epoch: [13]  [1030/1251]  eta: 0:03:03  lr: 0.000019  loss: 3.5907 (3.4321)  time: 0.8041  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4132, ratio_loss=0.0067, pruning_loss=0.1515, mse_loss=0.7626
Epoch: [13]  [1040/1251]  eta: 0:02:54  lr: 0.000019  loss: 3.5639 (3.4332)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [13]  [1050/1251]  eta: 0:02:46  lr: 0.000019  loss: 3.5639 (3.4343)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [13]  [1060/1251]  eta: 0:02:38  lr: 0.000019  loss: 3.5377 (3.4336)  time: 0.8148  data: 0.0005  max mem: 19734
Epoch: [13]  [1070/1251]  eta: 0:02:29  lr: 0.000019  loss: 3.6203 (3.4353)  time: 0.8156  data: 0.0006  max mem: 19734
Epoch: [13]  [1080/1251]  eta: 0:02:21  lr: 0.000019  loss: 3.5917 (3.4324)  time: 0.8181  data: 0.0005  max mem: 19734
Epoch: [13]  [1090/1251]  eta: 0:02:13  lr: 0.000019  loss: 3.4405 (3.4340)  time: 0.8352  data: 0.0005  max mem: 19734
Epoch: [13]  [1100/1251]  eta: 0:02:05  lr: 0.000019  loss: 3.5711 (3.4325)  time: 0.8251  data: 0.0006  max mem: 19734
Epoch: [13]  [1110/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.4299 (3.4314)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [13]  [1120/1251]  eta: 0:01:48  lr: 0.000019  loss: 3.0618 (3.4282)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [13]  [1130/1251]  eta: 0:01:40  lr: 0.000019  loss: 3.3933 (3.4291)  time: 0.8022  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3795, ratio_loss=0.0064, pruning_loss=0.1511, mse_loss=0.8028
Epoch: [13]  [1140/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.5791 (3.4293)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [13]  [1150/1251]  eta: 0:01:23  lr: 0.000019  loss: 3.4268 (3.4295)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [13]  [1160/1251]  eta: 0:01:15  lr: 0.000019  loss: 3.6613 (3.4318)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [13]  [1170/1251]  eta: 0:01:06  lr: 0.000019  loss: 3.6466 (3.4312)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [13]  [1180/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.4250 (3.4307)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [13]  [1190/1251]  eta: 0:00:50  lr: 0.000019  loss: 3.3390 (3.4287)  time: 0.8022  data: 0.0008  max mem: 19734
Epoch: [13]  [1200/1251]  eta: 0:00:42  lr: 0.000019  loss: 3.2496 (3.4271)  time: 0.7983  data: 0.0006  max mem: 19734
Epoch: [13]  [1210/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.4886 (3.4269)  time: 0.8108  data: 0.0001  max mem: 19734
Epoch: [13]  [1220/1251]  eta: 0:00:25  lr: 0.000019  loss: 3.5261 (3.4268)  time: 0.8104  data: 0.0001  max mem: 19734
Epoch: [13]  [1230/1251]  eta: 0:00:17  lr: 0.000019  loss: 3.5561 (3.4273)  time: 0.8030  data: 0.0002  max mem: 19734
loss info: cls_loss=3.3727, ratio_loss=0.0060, pruning_loss=0.1529, mse_loss=0.8073
Epoch: [13]  [1240/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.3104 (3.4248)  time: 0.8130  data: 0.0002  max mem: 19734
Epoch: [13]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.4818 (3.4263)  time: 0.8031  data: 0.0002  max mem: 19734
Epoch: [13] Total time: 0:17:13 (0.8260 s / it)
Averaged stats: lr: 0.000019  loss: 3.4818 (3.4396)
Test:  [  0/261]  eta: 1:52:49  loss: 0.8011 (0.8011)  acc1: 79.6875 (79.6875)  acc5: 94.2708 (94.2708)  time: 25.9376  data: 25.7518  max mem: 19734
Test:  [ 10/261]  eta: 0:11:16  loss: 0.7536 (0.7635)  acc1: 82.8125 (83.1439)  acc5: 95.3125 (95.5492)  time: 2.6942  data: 2.3510  max mem: 19734
Test:  [ 20/261]  eta: 0:06:51  loss: 0.9421 (0.9263)  acc1: 77.6042 (78.9435)  acc5: 93.2292 (94.0972)  time: 0.4944  data: 0.0173  max mem: 19734
Test:  [ 30/261]  eta: 0:05:23  loss: 0.8207 (0.8452)  acc1: 82.2917 (81.4684)  acc5: 93.7500 (94.7413)  time: 0.6857  data: 0.0245  max mem: 19734
Test:  [ 40/261]  eta: 0:04:07  loss: 0.6106 (0.8117)  acc1: 86.9792 (82.5330)  acc5: 96.8750 (95.0965)  time: 0.5018  data: 0.0169  max mem: 19734
Test:  [ 50/261]  eta: 0:03:22  loss: 0.9130 (0.8770)  acc1: 77.6042 (80.5351)  acc5: 94.7917 (94.6487)  time: 0.2767  data: 0.0127  max mem: 19734
Test:  [ 60/261]  eta: 0:02:50  loss: 0.9955 (0.8871)  acc1: 76.5625 (80.0461)  acc5: 93.7500 (94.6892)  time: 0.2995  data: 0.0139  max mem: 19734
Test:  [ 70/261]  eta: 0:02:40  loss: 0.9280 (0.8869)  acc1: 77.0833 (79.6435)  acc5: 95.8333 (94.9751)  time: 0.5390  data: 0.0163  max mem: 19734
Test:  [ 80/261]  eta: 0:02:17  loss: 0.8653 (0.8895)  acc1: 81.2500 (79.8161)  acc5: 96.3542 (95.0553)  time: 0.4772  data: 0.0167  max mem: 19734
Test:  [ 90/261]  eta: 0:02:03  loss: 0.7928 (0.8725)  acc1: 83.3333 (80.2827)  acc5: 96.3542 (95.1980)  time: 0.3010  data: 0.0122  max mem: 19734
Test:  [100/261]  eta: 0:01:56  loss: 0.7928 (0.8743)  acc1: 83.3333 (80.3115)  acc5: 95.3125 (95.2712)  time: 0.5701  data: 0.2232  max mem: 19734
Test:  [110/261]  eta: 0:01:45  loss: 0.8884 (0.8945)  acc1: 79.1667 (79.9268)  acc5: 94.2708 (94.9934)  time: 0.5974  data: 0.3600  max mem: 19734
Test:  [120/261]  eta: 0:01:32  loss: 1.2287 (0.9345)  acc1: 70.3125 (78.9127)  acc5: 89.0625 (94.4861)  time: 0.3460  data: 0.1480  max mem: 19734
Test:  [130/261]  eta: 0:01:25  loss: 1.3758 (0.9818)  acc1: 66.6667 (77.9063)  acc5: 86.9792 (93.8852)  time: 0.3940  data: 0.1677  max mem: 19734
Test:  [140/261]  eta: 0:01:16  loss: 1.3100 (1.0071)  acc1: 66.6667 (77.2569)  acc5: 88.5417 (93.6207)  time: 0.4444  data: 0.1708  max mem: 19734
Test:  [150/261]  eta: 0:01:11  loss: 1.2133 (1.0129)  acc1: 71.8750 (77.2179)  acc5: 91.1458 (93.4706)  time: 0.6116  data: 0.3533  max mem: 19734
Test:  [160/261]  eta: 0:01:03  loss: 1.0715 (1.0344)  acc1: 75.5208 (76.8084)  acc5: 91.6667 (93.1839)  time: 0.6565  data: 0.3537  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.3500 (1.0643)  acc1: 65.6250 (76.0264)  acc5: 86.4583 (92.8332)  time: 0.3158  data: 0.0160  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: 1.4828 (1.0823)  acc1: 64.5833 (75.5957)  acc5: 86.4583 (92.6537)  time: 0.2762  data: 0.0537  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: 1.3790 (1.0958)  acc1: 65.1042 (75.3381)  acc5: 90.6250 (92.4847)  time: 0.3001  data: 0.0508  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.3712 (1.1116)  acc1: 70.3125 (75.0000)  acc5: 89.5833 (92.2523)  time: 0.2513  data: 0.0069  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.3712 (1.1263)  acc1: 69.7917 (74.6791)  acc5: 88.0208 (92.0345)  time: 0.2155  data: 0.0211  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4559 (1.1441)  acc1: 67.1875 (74.2176)  acc5: 87.5000 (91.8316)  time: 0.2996  data: 0.0932  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4219 (1.1532)  acc1: 66.1458 (73.9719)  acc5: 88.5417 (91.7208)  time: 0.2716  data: 0.0837  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3799 (1.1617)  acc1: 67.1875 (73.7617)  acc5: 90.1042 (91.6645)  time: 0.1367  data: 0.0096  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1164 (1.1553)  acc1: 73.9583 (73.9210)  acc5: 93.2292 (91.7891)  time: 0.1215  data: 0.0043  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9660 (1.1548)  acc1: 76.0417 (73.9520)  acc5: 94.7917 (91.8460)  time: 0.1152  data: 0.0042  max mem: 19734
Test: Total time: 0:02:03 (0.4734 s / it)
* Acc@1 73.952 Acc@5 91.846 loss 1.155
Accuracy of the network on the 50000 test images: 74.0%
Max accuracy: 74.01%
Epoch: [14]  [   0/1251]  eta: 5:50:30  lr: 0.000019  loss: 3.9857 (3.9857)  time: 16.8113  data: 12.5092  max mem: 19734
Epoch: [14]  [  10/1251]  eta: 0:53:53  lr: 0.000019  loss: 3.8355 (3.6593)  time: 2.6054  data: 1.2521  max mem: 19734
Epoch: [14]  [  20/1251]  eta: 0:35:52  lr: 0.000019  loss: 3.6690 (3.5168)  time: 0.9953  data: 0.0634  max mem: 19734
Epoch: [14]  [  30/1251]  eta: 0:29:20  lr: 0.000019  loss: 3.6054 (3.5431)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [14]  [  40/1251]  eta: 0:25:56  lr: 0.000019  loss: 3.4119 (3.4603)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [14]  [  50/1251]  eta: 0:23:49  lr: 0.000019  loss: 3.3230 (3.4497)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [14]  [  60/1251]  eta: 0:22:22  lr: 0.000019  loss: 3.7570 (3.4962)  time: 0.8029  data: 0.0004  max mem: 19734
Epoch: [14]  [  70/1251]  eta: 0:21:16  lr: 0.000019  loss: 3.7331 (3.4820)  time: 0.8014  data: 0.0004  max mem: 19734
Epoch: [14]  [  80/1251]  eta: 0:20:25  lr: 0.000019  loss: 3.6019 (3.4891)  time: 0.8001  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4547, ratio_loss=0.0061, pruning_loss=0.1518, mse_loss=0.7951
Epoch: [14]  [  90/1251]  eta: 0:19:48  lr: 0.000019  loss: 3.6019 (3.4936)  time: 0.8186  data: 0.0005  max mem: 19734
Epoch: [14]  [ 100/1251]  eta: 0:19:13  lr: 0.000019  loss: 3.5817 (3.4974)  time: 0.8222  data: 0.0005  max mem: 19734
Epoch: [14]  [ 110/1251]  eta: 0:18:42  lr: 0.000019  loss: 3.6187 (3.4900)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [14]  [ 120/1251]  eta: 0:18:18  lr: 0.000019  loss: 3.5953 (3.4699)  time: 0.8165  data: 0.0005  max mem: 19734
Epoch: [14]  [ 130/1251]  eta: 0:17:56  lr: 0.000019  loss: 3.5960 (3.4816)  time: 0.8279  data: 0.0005  max mem: 19734
Epoch: [14]  [ 140/1251]  eta: 0:17:33  lr: 0.000019  loss: 3.6356 (3.4768)  time: 0.8104  data: 0.0007  max mem: 19734
Epoch: [14]  [ 150/1251]  eta: 0:17:13  lr: 0.000019  loss: 3.5521 (3.4660)  time: 0.7995  data: 0.0007  max mem: 19734
Epoch: [14]  [ 160/1251]  eta: 0:16:54  lr: 0.000019  loss: 3.6502 (3.4851)  time: 0.7989  data: 0.0005  max mem: 19734
Epoch: [14]  [ 170/1251]  eta: 0:16:37  lr: 0.000019  loss: 3.7279 (3.4612)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [14]  [ 180/1251]  eta: 0:16:20  lr: 0.000019  loss: 2.7219 (3.4272)  time: 0.8007  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3663, ratio_loss=0.0062, pruning_loss=0.1529, mse_loss=0.7747
Epoch: [14]  [ 190/1251]  eta: 0:16:05  lr: 0.000019  loss: 3.3258 (3.4458)  time: 0.7995  data: 0.0005  max mem: 19734
Epoch: [14]  [ 200/1251]  eta: 0:15:50  lr: 0.000019  loss: 3.6841 (3.4410)  time: 0.8010  data: 0.0004  max mem: 19734
Epoch: [14]  [ 210/1251]  eta: 0:15:36  lr: 0.000019  loss: 3.2406 (3.4339)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [14]  [ 220/1251]  eta: 0:15:22  lr: 0.000019  loss: 3.2071 (3.4267)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [14]  [ 230/1251]  eta: 0:15:10  lr: 0.000019  loss: 3.6339 (3.4333)  time: 0.8078  data: 0.0004  max mem: 19734
Epoch: [14]  [ 240/1251]  eta: 0:14:58  lr: 0.000019  loss: 3.7110 (3.4438)  time: 0.8230  data: 0.0004  max mem: 19734
Epoch: [14]  [ 250/1251]  eta: 0:14:46  lr: 0.000019  loss: 3.6806 (3.4453)  time: 0.8207  data: 0.0004  max mem: 19734
Epoch: [14]  [ 260/1251]  eta: 0:14:34  lr: 0.000019  loss: 3.4407 (3.4387)  time: 0.8114  data: 0.0004  max mem: 19734
Epoch: [14]  [ 270/1251]  eta: 0:14:24  lr: 0.000019  loss: 3.3242 (3.4322)  time: 0.8233  data: 0.0005  max mem: 19734
Epoch: [14]  [ 280/1251]  eta: 0:14:12  lr: 0.000019  loss: 3.4791 (3.4350)  time: 0.8176  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4032, ratio_loss=0.0060, pruning_loss=0.1513, mse_loss=0.7744
Epoch: [14]  [ 290/1251]  eta: 0:14:01  lr: 0.000019  loss: 3.5270 (3.4349)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [14]  [ 300/1251]  eta: 0:13:50  lr: 0.000019  loss: 3.5270 (3.4350)  time: 0.8019  data: 0.0006  max mem: 19734
Epoch: [14]  [ 310/1251]  eta: 0:13:39  lr: 0.000019  loss: 3.3656 (3.4285)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [14]  [ 320/1251]  eta: 0:13:29  lr: 0.000019  loss: 3.1835 (3.4239)  time: 0.8071  data: 0.0005  max mem: 19734
Epoch: [14]  [ 330/1251]  eta: 0:13:18  lr: 0.000019  loss: 3.5307 (3.4264)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [14]  [ 340/1251]  eta: 0:13:08  lr: 0.000019  loss: 3.3103 (3.4199)  time: 0.8094  data: 0.0005  max mem: 19734
Epoch: [14]  [ 350/1251]  eta: 0:12:58  lr: 0.000019  loss: 3.3003 (3.4185)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [14]  [ 360/1251]  eta: 0:12:48  lr: 0.000019  loss: 3.6187 (3.4219)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [14]  [ 370/1251]  eta: 0:12:38  lr: 0.000019  loss: 3.6346 (3.4173)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [14]  [ 380/1251]  eta: 0:12:28  lr: 0.000019  loss: 3.5877 (3.4208)  time: 0.8101  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3587, ratio_loss=0.0062, pruning_loss=0.1537, mse_loss=0.7904
Epoch: [14]  [ 390/1251]  eta: 0:12:18  lr: 0.000019  loss: 3.5748 (3.4221)  time: 0.8155  data: 0.0004  max mem: 19734
Epoch: [14]  [ 400/1251]  eta: 0:12:09  lr: 0.000019  loss: 3.5567 (3.4255)  time: 0.8099  data: 0.0005  max mem: 19734
Epoch: [14]  [ 410/1251]  eta: 0:11:59  lr: 0.000019  loss: 3.5730 (3.4296)  time: 0.8159  data: 0.0005  max mem: 19734
Epoch: [14]  [ 420/1251]  eta: 0:11:51  lr: 0.000019  loss: 3.5620 (3.4256)  time: 0.8309  data: 0.0006  max mem: 19734
Epoch: [14]  [ 430/1251]  eta: 0:11:41  lr: 0.000019  loss: 3.5139 (3.4281)  time: 0.8227  data: 0.0006  max mem: 19734
Epoch: [14]  [ 440/1251]  eta: 0:11:32  lr: 0.000019  loss: 3.4398 (3.4226)  time: 0.8081  data: 0.0004  max mem: 19734
Epoch: [14]  [ 450/1251]  eta: 0:11:22  lr: 0.000019  loss: 3.2077 (3.4234)  time: 0.8039  data: 0.0004  max mem: 19734
Epoch: [14]  [ 460/1251]  eta: 0:11:13  lr: 0.000019  loss: 3.7535 (3.4287)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [14]  [ 470/1251]  eta: 0:11:03  lr: 0.000019  loss: 3.5125 (3.4219)  time: 0.7995  data: 0.0005  max mem: 19734
Epoch: [14]  [ 480/1251]  eta: 0:10:54  lr: 0.000019  loss: 3.0629 (3.4182)  time: 0.8024  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3956, ratio_loss=0.0060, pruning_loss=0.1522, mse_loss=0.7676
Epoch: [14]  [ 490/1251]  eta: 0:10:45  lr: 0.000019  loss: 3.6453 (3.4207)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [14]  [ 500/1251]  eta: 0:10:36  lr: 0.000019  loss: 3.6453 (3.4220)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [14]  [ 510/1251]  eta: 0:10:27  lr: 0.000019  loss: 3.5768 (3.4249)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [14]  [ 520/1251]  eta: 0:10:18  lr: 0.000019  loss: 3.4901 (3.4237)  time: 0.8034  data: 0.0008  max mem: 19734
Epoch: [14]  [ 530/1251]  eta: 0:10:09  lr: 0.000019  loss: 3.4924 (3.4241)  time: 0.8203  data: 0.0008  max mem: 19734
Epoch: [14]  [ 540/1251]  eta: 0:10:00  lr: 0.000019  loss: 3.4924 (3.4192)  time: 0.8293  data: 0.0005  max mem: 19734
Epoch: [14]  [ 550/1251]  eta: 0:09:51  lr: 0.000019  loss: 3.5558 (3.4196)  time: 0.8159  data: 0.0005  max mem: 19734
Epoch: [14]  [ 560/1251]  eta: 0:09:43  lr: 0.000019  loss: 3.6140 (3.4224)  time: 0.8192  data: 0.0005  max mem: 19734
Epoch: [14]  [ 570/1251]  eta: 0:09:34  lr: 0.000019  loss: 3.6106 (3.4238)  time: 0.8318  data: 0.0004  max mem: 19734
Epoch: [14]  [ 580/1251]  eta: 0:09:25  lr: 0.000019  loss: 3.6498 (3.4294)  time: 0.8190  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4755, ratio_loss=0.0063, pruning_loss=0.1508, mse_loss=0.7703
Epoch: [14]  [ 590/1251]  eta: 0:09:16  lr: 0.000019  loss: 3.8743 (3.4334)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [14]  [ 600/1251]  eta: 0:09:07  lr: 0.000019  loss: 3.7393 (3.4356)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [14]  [ 610/1251]  eta: 0:08:59  lr: 0.000019  loss: 3.5801 (3.4339)  time: 0.7984  data: 0.0004  max mem: 19734
Epoch: [14]  [ 620/1251]  eta: 0:08:50  lr: 0.000019  loss: 3.3025 (3.4305)  time: 0.7983  data: 0.0004  max mem: 19734
Epoch: [14]  [ 630/1251]  eta: 0:08:41  lr: 0.000019  loss: 3.4720 (3.4318)  time: 0.7993  data: 0.0004  max mem: 19734
Epoch: [14]  [ 640/1251]  eta: 0:08:32  lr: 0.000019  loss: 3.5194 (3.4304)  time: 0.7999  data: 0.0004  max mem: 19734
Epoch: [14]  [ 650/1251]  eta: 0:08:23  lr: 0.000019  loss: 3.3747 (3.4278)  time: 0.7984  data: 0.0004  max mem: 19734
Epoch: [14]  [ 660/1251]  eta: 0:08:15  lr: 0.000019  loss: 3.6086 (3.4316)  time: 0.7997  data: 0.0004  max mem: 19734
Epoch: [14]  [ 670/1251]  eta: 0:08:06  lr: 0.000019  loss: 3.6086 (3.4318)  time: 0.8011  data: 0.0004  max mem: 19734
Epoch: [14]  [ 680/1251]  eta: 0:07:58  lr: 0.000019  loss: 3.5019 (3.4326)  time: 0.8162  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4181, ratio_loss=0.0064, pruning_loss=0.1505, mse_loss=0.7656
Epoch: [14]  [ 690/1251]  eta: 0:07:49  lr: 0.000019  loss: 3.5820 (3.4339)  time: 0.8165  data: 0.0004  max mem: 19734
Epoch: [14]  [ 700/1251]  eta: 0:07:40  lr: 0.000019  loss: 3.4987 (3.4353)  time: 0.8079  data: 0.0004  max mem: 19734
Epoch: [14]  [ 710/1251]  eta: 0:07:32  lr: 0.000019  loss: 3.4559 (3.4355)  time: 0.8319  data: 0.0004  max mem: 19734
Epoch: [14]  [ 720/1251]  eta: 0:07:24  lr: 0.000019  loss: 3.6460 (3.4376)  time: 0.8272  data: 0.0004  max mem: 19734
Epoch: [14]  [ 730/1251]  eta: 0:07:15  lr: 0.000019  loss: 3.7157 (3.4385)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [14]  [ 740/1251]  eta: 0:07:06  lr: 0.000019  loss: 3.6718 (3.4410)  time: 0.8031  data: 0.0004  max mem: 19734
Epoch: [14]  [ 750/1251]  eta: 0:06:58  lr: 0.000019  loss: 3.6583 (3.4448)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [14]  [ 760/1251]  eta: 0:06:49  lr: 0.000019  loss: 3.7482 (3.4479)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [14]  [ 770/1251]  eta: 0:06:41  lr: 0.000019  loss: 3.6839 (3.4493)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [14]  [ 780/1251]  eta: 0:06:32  lr: 0.000019  loss: 3.4632 (3.4461)  time: 0.8006  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5209, ratio_loss=0.0062, pruning_loss=0.1484, mse_loss=0.7453
Epoch: [14]  [ 790/1251]  eta: 0:06:24  lr: 0.000019  loss: 3.3297 (3.4483)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [14]  [ 800/1251]  eta: 0:06:15  lr: 0.000019  loss: 3.3721 (3.4463)  time: 0.8004  data: 0.0004  max mem: 19734
Epoch: [14]  [ 810/1251]  eta: 0:06:07  lr: 0.000019  loss: 3.5057 (3.4494)  time: 0.7986  data: 0.0004  max mem: 19734
Epoch: [14]  [ 820/1251]  eta: 0:05:58  lr: 0.000019  loss: 3.6561 (3.4486)  time: 0.8139  data: 0.0004  max mem: 19734
Epoch: [14]  [ 830/1251]  eta: 0:05:50  lr: 0.000019  loss: 3.4648 (3.4480)  time: 0.8199  data: 0.0003  max mem: 19734
Epoch: [14]  [ 840/1251]  eta: 0:05:41  lr: 0.000019  loss: 3.2898 (3.4454)  time: 0.8057  data: 0.0003  max mem: 19734
Epoch: [14]  [ 850/1251]  eta: 0:05:33  lr: 0.000019  loss: 3.3843 (3.4445)  time: 0.8159  data: 0.0004  max mem: 19734
Epoch: [14]  [ 860/1251]  eta: 0:05:25  lr: 0.000019  loss: 3.4503 (3.4454)  time: 0.8390  data: 0.0004  max mem: 19734
Epoch: [14]  [ 870/1251]  eta: 0:05:16  lr: 0.000019  loss: 3.6628 (3.4472)  time: 0.8235  data: 0.0004  max mem: 19734
Epoch: [14]  [ 880/1251]  eta: 0:05:08  lr: 0.000019  loss: 3.8046 (3.4468)  time: 0.7977  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4333, ratio_loss=0.0062, pruning_loss=0.1504, mse_loss=0.7479
Epoch: [14]  [ 890/1251]  eta: 0:04:59  lr: 0.000019  loss: 3.7347 (3.4494)  time: 0.7985  data: 0.0004  max mem: 19734
Epoch: [14]  [ 900/1251]  eta: 0:04:51  lr: 0.000019  loss: 3.7347 (3.4512)  time: 0.8013  data: 0.0004  max mem: 19734
Epoch: [14]  [ 910/1251]  eta: 0:04:43  lr: 0.000019  loss: 3.6088 (3.4516)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [14]  [ 920/1251]  eta: 0:04:34  lr: 0.000019  loss: 3.4810 (3.4501)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [14]  [ 930/1251]  eta: 0:04:26  lr: 0.000019  loss: 3.4108 (3.4487)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [14]  [ 940/1251]  eta: 0:04:17  lr: 0.000019  loss: 3.2265 (3.4467)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [14]  [ 950/1251]  eta: 0:04:09  lr: 0.000019  loss: 3.2265 (3.4464)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [14]  [ 960/1251]  eta: 0:04:01  lr: 0.000019  loss: 3.6050 (3.4490)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [14]  [ 970/1251]  eta: 0:03:52  lr: 0.000019  loss: 3.7093 (3.4493)  time: 0.8252  data: 0.0006  max mem: 19734
Epoch: [14]  [ 980/1251]  eta: 0:03:44  lr: 0.000019  loss: 3.5459 (3.4512)  time: 0.8180  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4554, ratio_loss=0.0062, pruning_loss=0.1520, mse_loss=0.7218
Epoch: [14]  [ 990/1251]  eta: 0:03:36  lr: 0.000019  loss: 3.6429 (3.4521)  time: 0.8021  data: 0.0007  max mem: 19734
Epoch: [14]  [1000/1251]  eta: 0:03:27  lr: 0.000019  loss: 3.6428 (3.4523)  time: 0.8294  data: 0.0006  max mem: 19734
Epoch: [14]  [1010/1251]  eta: 0:03:19  lr: 0.000019  loss: 3.6069 (3.4530)  time: 0.8460  data: 0.0006  max mem: 19734
Epoch: [14]  [1020/1251]  eta: 0:03:11  lr: 0.000019  loss: 3.3768 (3.4526)  time: 0.8194  data: 0.0006  max mem: 19734
Epoch: [14]  [1030/1251]  eta: 0:03:02  lr: 0.000019  loss: 3.0911 (3.4471)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [14]  [1040/1251]  eta: 0:02:54  lr: 0.000019  loss: 3.2121 (3.4491)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [14]  [1050/1251]  eta: 0:02:46  lr: 0.000019  loss: 3.6465 (3.4477)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [14]  [1060/1251]  eta: 0:02:38  lr: 0.000019  loss: 3.5227 (3.4473)  time: 0.7991  data: 0.0005  max mem: 19734
Epoch: [14]  [1070/1251]  eta: 0:02:29  lr: 0.000019  loss: 3.5411 (3.4473)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [14]  [1080/1251]  eta: 0:02:21  lr: 0.000019  loss: 3.5305 (3.4467)  time: 0.7997  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3708, ratio_loss=0.0062, pruning_loss=0.1515, mse_loss=0.7537
Epoch: [14]  [1090/1251]  eta: 0:02:13  lr: 0.000019  loss: 3.4471 (3.4470)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [14]  [1100/1251]  eta: 0:02:04  lr: 0.000019  loss: 3.5856 (3.4468)  time: 0.8020  data: 0.0004  max mem: 19734
Epoch: [14]  [1110/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.4286 (3.4442)  time: 0.8116  data: 0.0005  max mem: 19734
Epoch: [14]  [1120/1251]  eta: 0:01:48  lr: 0.000019  loss: 3.2696 (3.4423)  time: 0.8222  data: 0.0005  max mem: 19734
Epoch: [14]  [1130/1251]  eta: 0:01:39  lr: 0.000019  loss: 3.4005 (3.4444)  time: 0.8125  data: 0.0005  max mem: 19734
Epoch: [14]  [1140/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.6478 (3.4446)  time: 0.8140  data: 0.0005  max mem: 19734
Epoch: [14]  [1150/1251]  eta: 0:01:23  lr: 0.000019  loss: 3.5617 (3.4446)  time: 0.8290  data: 0.0005  max mem: 19734
Epoch: [14]  [1160/1251]  eta: 0:01:15  lr: 0.000019  loss: 3.5015 (3.4438)  time: 0.8268  data: 0.0005  max mem: 19734
Epoch: [14]  [1170/1251]  eta: 0:01:06  lr: 0.000019  loss: 3.5374 (3.4443)  time: 0.8113  data: 0.0005  max mem: 19734
Epoch: [14]  [1180/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.6592 (3.4465)  time: 0.8012  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4163, ratio_loss=0.0062, pruning_loss=0.1512, mse_loss=0.7334
Epoch: [14]  [1190/1251]  eta: 0:00:50  lr: 0.000019  loss: 3.6592 (3.4454)  time: 0.8034  data: 0.0008  max mem: 19734
Epoch: [14]  [1200/1251]  eta: 0:00:42  lr: 0.000019  loss: 3.2701 (3.4429)  time: 0.7970  data: 0.0007  max mem: 19734
Epoch: [14]  [1210/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.2701 (3.4422)  time: 0.7921  data: 0.0001  max mem: 19734
Epoch: [14]  [1220/1251]  eta: 0:00:25  lr: 0.000019  loss: 3.2407 (3.4404)  time: 0.7915  data: 0.0001  max mem: 19734
Epoch: [14]  [1230/1251]  eta: 0:00:17  lr: 0.000019  loss: 3.2407 (3.4405)  time: 0.7906  data: 0.0002  max mem: 19734
Epoch: [14]  [1240/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.3975 (3.4393)  time: 0.7919  data: 0.0002  max mem: 19734
Epoch: [14]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.3689 (3.4389)  time: 0.7917  data: 0.0002  max mem: 19734
Epoch: [14] Total time: 0:17:11 (0.8248 s / it)
Averaged stats: lr: 0.000019  loss: 3.3689 (3.4453)
Test:  [  0/261]  eta: 1:50:43  loss: 0.7324 (0.7324)  acc1: 83.8542 (83.8542)  acc5: 95.8333 (95.8333)  time: 25.4548  data: 25.1020  max mem: 19734
Test:  [ 10/261]  eta: 0:13:25  loss: 0.7324 (0.7510)  acc1: 83.8542 (83.4754)  acc5: 96.3542 (96.0227)  time: 3.2093  data: 3.0110  max mem: 19734
Test:  [ 20/261]  eta: 0:07:08  loss: 0.9700 (0.9139)  acc1: 77.0833 (78.9435)  acc5: 93.7500 (94.6181)  time: 0.5955  data: 0.4073  max mem: 19734
Test:  [ 30/261]  eta: 0:04:53  loss: 0.8139 (0.8289)  acc1: 84.3750 (81.8380)  acc5: 94.7917 (95.2285)  time: 0.2060  data: 0.0101  max mem: 19734
Test:  [ 40/261]  eta: 0:04:16  loss: 0.5725 (0.7994)  acc1: 88.0208 (82.7109)  acc5: 96.3542 (95.4776)  time: 0.5147  data: 0.2963  max mem: 19734
Test:  [ 50/261]  eta: 0:03:39  loss: 0.9182 (0.8604)  acc1: 77.6042 (81.0458)  acc5: 94.2708 (95.0266)  time: 0.6840  data: 0.4416  max mem: 19734
Test:  [ 60/261]  eta: 0:03:08  loss: 0.9824 (0.8731)  acc1: 76.0417 (80.5755)  acc5: 94.2708 (95.0649)  time: 0.4790  data: 0.2672  max mem: 19734
Test:  [ 70/261]  eta: 0:02:57  loss: 0.9733 (0.8754)  acc1: 77.0833 (80.0543)  acc5: 95.8333 (95.2465)  time: 0.6380  data: 0.4632  max mem: 19734
Test:  [ 80/261]  eta: 0:02:35  loss: 0.8875 (0.8787)  acc1: 78.1250 (80.2341)  acc5: 96.3542 (95.3318)  time: 0.6268  data: 0.3579  max mem: 19734
Test:  [ 90/261]  eta: 0:02:15  loss: 0.8284 (0.8654)  acc1: 83.3333 (80.6376)  acc5: 95.8333 (95.4155)  time: 0.2998  data: 0.0243  max mem: 19734
Test:  [100/261]  eta: 0:02:02  loss: 0.8034 (0.8677)  acc1: 84.3750 (80.6312)  acc5: 95.8333 (95.4517)  time: 0.3430  data: 0.1385  max mem: 19734
Test:  [110/261]  eta: 0:01:48  loss: 0.9015 (0.8920)  acc1: 77.0833 (80.1333)  acc5: 94.2708 (95.1483)  time: 0.3769  data: 0.1335  max mem: 19734
Test:  [120/261]  eta: 0:01:35  loss: 1.2346 (0.9333)  acc1: 70.8333 (79.1451)  acc5: 89.0625 (94.5980)  time: 0.2488  data: 0.0106  max mem: 19734
Test:  [130/261]  eta: 0:01:29  loss: 1.4297 (0.9801)  acc1: 66.6667 (78.1250)  acc5: 86.9792 (94.0005)  time: 0.4815  data: 0.1992  max mem: 19734
Test:  [140/261]  eta: 0:01:18  loss: 1.3684 (1.0061)  acc1: 68.2292 (77.4564)  acc5: 89.5833 (93.7205)  time: 0.5124  data: 0.2010  max mem: 19734
Test:  [150/261]  eta: 0:01:09  loss: 1.2224 (1.0126)  acc1: 71.8750 (77.4248)  acc5: 90.6250 (93.5430)  time: 0.2628  data: 0.0118  max mem: 19734
Test:  [160/261]  eta: 0:01:06  loss: 1.0253 (1.0328)  acc1: 76.5625 (77.0380)  acc5: 91.1458 (93.2324)  time: 0.7176  data: 0.4547  max mem: 19734
Test:  [170/261]  eta: 0:00:57  loss: 1.3459 (1.0643)  acc1: 65.6250 (76.2427)  acc5: 86.4583 (92.8637)  time: 0.6814  data: 0.4578  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: 1.4577 (1.0810)  acc1: 63.5417 (75.8172)  acc5: 88.5417 (92.7112)  time: 0.1692  data: 0.0137  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: 1.4062 (1.0942)  acc1: 66.1458 (75.5508)  acc5: 90.6250 (92.5529)  time: 0.1558  data: 0.0102  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.3326 (1.1089)  acc1: 72.3958 (75.2513)  acc5: 88.5417 (92.3352)  time: 0.2086  data: 0.0786  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.3524 (1.1227)  acc1: 70.8333 (74.9975)  acc5: 88.5417 (92.1258)  time: 0.1999  data: 0.0775  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4353 (1.1417)  acc1: 67.1875 (74.5216)  acc5: 87.5000 (91.9330)  time: 0.1209  data: 0.0043  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.4281 (1.1505)  acc1: 66.6667 (74.3056)  acc5: 88.5417 (91.8313)  time: 0.1155  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3448 (1.1606)  acc1: 66.6667 (74.0664)  acc5: 91.1458 (91.7726)  time: 0.1153  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1095 (1.1540)  acc1: 75.0000 (74.2177)  acc5: 93.2292 (91.8887)  time: 0.1155  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9881 (1.1538)  acc1: 75.0000 (74.2400)  acc5: 94.7917 (91.9400)  time: 0.1122  data: 0.0001  max mem: 19734
Test: Total time: 0:02:00 (0.4634 s / it)
* Acc@1 74.240 Acc@5 91.940 loss 1.154
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.24%
Epoch: [15]  [   0/1251]  eta: 4:45:48  lr: 0.000019  loss: 3.5058 (3.5058)  time: 13.7080  data: 12.7260  max mem: 19734
Epoch: [15]  [  10/1251]  eta: 0:43:28  lr: 0.000019  loss: 3.6404 (3.6110)  time: 2.1016  data: 1.2212  max mem: 19734
Epoch: [15]  [  20/1251]  eta: 0:30:23  lr: 0.000019  loss: 3.6404 (3.6134)  time: 0.8702  data: 0.0356  max mem: 19734
Epoch: [15]  [  30/1251]  eta: 0:25:46  lr: 0.000019  loss: 3.5597 (3.5641)  time: 0.8077  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3688, ratio_loss=0.0058, pruning_loss=0.1517, mse_loss=0.7845
Epoch: [15]  [  40/1251]  eta: 0:23:21  lr: 0.000019  loss: 3.4666 (3.5166)  time: 0.8175  data: 0.0005  max mem: 19734
Epoch: [15]  [  50/1251]  eta: 0:21:49  lr: 0.000019  loss: 3.4666 (3.5363)  time: 0.8177  data: 0.0004  max mem: 19734
Epoch: [15]  [  60/1251]  eta: 0:20:42  lr: 0.000019  loss: 3.3774 (3.4966)  time: 0.8081  data: 0.0004  max mem: 19734
Epoch: [15]  [  70/1251]  eta: 0:19:50  lr: 0.000019  loss: 3.2124 (3.4613)  time: 0.7974  data: 0.0004  max mem: 19734
Epoch: [15]  [  80/1251]  eta: 0:19:09  lr: 0.000019  loss: 3.4943 (3.4526)  time: 0.7961  data: 0.0004  max mem: 19734
Epoch: [15]  [  90/1251]  eta: 0:18:36  lr: 0.000019  loss: 3.3801 (3.4324)  time: 0.7971  data: 0.0005  max mem: 19734
Epoch: [15]  [ 100/1251]  eta: 0:18:10  lr: 0.000019  loss: 3.5907 (3.4626)  time: 0.8071  data: 0.0006  max mem: 19734
Epoch: [15]  [ 110/1251]  eta: 0:17:45  lr: 0.000019  loss: 3.6621 (3.4516)  time: 0.8093  data: 0.0005  max mem: 19734
Epoch: [15]  [ 120/1251]  eta: 0:17:24  lr: 0.000019  loss: 3.6079 (3.4667)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [15]  [ 130/1251]  eta: 0:17:05  lr: 0.000019  loss: 3.7140 (3.4932)  time: 0.8043  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4406, ratio_loss=0.0062, pruning_loss=0.1512, mse_loss=0.7729
Epoch: [15]  [ 140/1251]  eta: 0:16:48  lr: 0.000019  loss: 3.7254 (3.4963)  time: 0.8112  data: 0.0006  max mem: 19734
Epoch: [15]  [ 150/1251]  eta: 0:16:32  lr: 0.000019  loss: 3.4138 (3.4840)  time: 0.8196  data: 0.0005  max mem: 19734
Epoch: [15]  [ 160/1251]  eta: 0:16:17  lr: 0.000019  loss: 3.4038 (3.4826)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [15]  [ 170/1251]  eta: 0:16:02  lr: 0.000019  loss: 3.5612 (3.4767)  time: 0.8018  data: 0.0006  max mem: 19734
Epoch: [15]  [ 180/1251]  eta: 0:15:50  lr: 0.000019  loss: 3.4197 (3.4631)  time: 0.8201  data: 0.0005  max mem: 19734
Epoch: [15]  [ 190/1251]  eta: 0:15:38  lr: 0.000019  loss: 3.2884 (3.4453)  time: 0.8378  data: 0.0005  max mem: 19734
Epoch: [15]  [ 200/1251]  eta: 0:15:25  lr: 0.000019  loss: 3.4847 (3.4474)  time: 0.8226  data: 0.0007  max mem: 19734
Epoch: [15]  [ 210/1251]  eta: 0:15:13  lr: 0.000019  loss: 3.6990 (3.4620)  time: 0.8047  data: 0.0008  max mem: 19734
Epoch: [15]  [ 220/1251]  eta: 0:15:01  lr: 0.000019  loss: 3.7228 (3.4532)  time: 0.8077  data: 0.0008  max mem: 19734
Epoch: [15]  [ 230/1251]  eta: 0:14:49  lr: 0.000019  loss: 3.4865 (3.4492)  time: 0.8106  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3620, ratio_loss=0.0064, pruning_loss=0.1507, mse_loss=0.7564
Epoch: [15]  [ 240/1251]  eta: 0:14:38  lr: 0.000019  loss: 3.4865 (3.4473)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [15]  [ 250/1251]  eta: 0:14:27  lr: 0.000019  loss: 3.5172 (3.4476)  time: 0.8129  data: 0.0004  max mem: 19734
Epoch: [15]  [ 260/1251]  eta: 0:14:16  lr: 0.000019  loss: 3.5182 (3.4448)  time: 0.8115  data: 0.0004  max mem: 19734
Epoch: [15]  [ 270/1251]  eta: 0:14:05  lr: 0.000019  loss: 3.4884 (3.4400)  time: 0.8032  data: 0.0008  max mem: 19734
Epoch: [15]  [ 280/1251]  eta: 0:13:54  lr: 0.000019  loss: 3.2341 (3.4328)  time: 0.8021  data: 0.0009  max mem: 19734
Epoch: [15]  [ 290/1251]  eta: 0:13:44  lr: 0.000019  loss: 3.5616 (3.4358)  time: 0.8087  data: 0.0006  max mem: 19734
Epoch: [15]  [ 300/1251]  eta: 0:13:34  lr: 0.000019  loss: 3.6628 (3.4404)  time: 0.8167  data: 0.0004  max mem: 19734
Epoch: [15]  [ 310/1251]  eta: 0:13:24  lr: 0.000019  loss: 3.4992 (3.4410)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [15]  [ 320/1251]  eta: 0:13:14  lr: 0.000019  loss: 3.6370 (3.4479)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [15]  [ 330/1251]  eta: 0:13:05  lr: 0.000019  loss: 3.6370 (3.4470)  time: 0.8244  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4506, ratio_loss=0.0060, pruning_loss=0.1490, mse_loss=0.7623
Epoch: [15]  [ 340/1251]  eta: 0:12:56  lr: 0.000019  loss: 3.6167 (3.4576)  time: 0.8273  data: 0.0005  max mem: 19734
Epoch: [15]  [ 350/1251]  eta: 0:12:46  lr: 0.000019  loss: 3.7686 (3.4568)  time: 0.8093  data: 0.0006  max mem: 19734
Epoch: [15]  [ 360/1251]  eta: 0:12:36  lr: 0.000019  loss: 3.4780 (3.4551)  time: 0.8065  data: 0.0006  max mem: 19734
Epoch: [15]  [ 370/1251]  eta: 0:12:27  lr: 0.000019  loss: 3.3678 (3.4440)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [15]  [ 380/1251]  eta: 0:12:17  lr: 0.000019  loss: 3.4081 (3.4460)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [15]  [ 390/1251]  eta: 0:12:08  lr: 0.000019  loss: 3.5839 (3.4431)  time: 0.7989  data: 0.0005  max mem: 19734
Epoch: [15]  [ 400/1251]  eta: 0:11:59  lr: 0.000019  loss: 3.5536 (3.4389)  time: 0.8063  data: 0.0005  max mem: 19734
Epoch: [15]  [ 410/1251]  eta: 0:11:49  lr: 0.000019  loss: 3.7553 (3.4490)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [15]  [ 420/1251]  eta: 0:11:40  lr: 0.000019  loss: 3.7856 (3.4502)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [15]  [ 430/1251]  eta: 0:11:31  lr: 0.000019  loss: 3.5402 (3.4499)  time: 0.8032  data: 0.0009  max mem: 19734
loss info: cls_loss=3.4213, ratio_loss=0.0059, pruning_loss=0.1518, mse_loss=0.7558
Epoch: [15]  [ 440/1251]  eta: 0:11:22  lr: 0.000019  loss: 3.5402 (3.4497)  time: 0.8156  data: 0.0009  max mem: 19734
Epoch: [15]  [ 450/1251]  eta: 0:11:13  lr: 0.000019  loss: 3.5642 (3.4542)  time: 0.8152  data: 0.0005  max mem: 19734
Epoch: [15]  [ 460/1251]  eta: 0:11:04  lr: 0.000019  loss: 3.5679 (3.4585)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [15]  [ 470/1251]  eta: 0:10:55  lr: 0.000019  loss: 3.5392 (3.4611)  time: 0.8175  data: 0.0005  max mem: 19734
Epoch: [15]  [ 480/1251]  eta: 0:10:47  lr: 0.000019  loss: 3.4516 (3.4566)  time: 0.8423  data: 0.0006  max mem: 19734
Epoch: [15]  [ 490/1251]  eta: 0:10:38  lr: 0.000019  loss: 3.3335 (3.4526)  time: 0.8275  data: 0.0006  max mem: 19734
Epoch: [15]  [ 500/1251]  eta: 0:10:29  lr: 0.000019  loss: 3.5126 (3.4505)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [15]  [ 510/1251]  eta: 0:10:20  lr: 0.000019  loss: 3.5126 (3.4465)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [15]  [ 520/1251]  eta: 0:10:11  lr: 0.000019  loss: 3.5759 (3.4438)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [15]  [ 530/1251]  eta: 0:10:03  lr: 0.000019  loss: 3.5989 (3.4445)  time: 0.8028  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3967, ratio_loss=0.0060, pruning_loss=0.1507, mse_loss=0.7367
Epoch: [15]  [ 540/1251]  eta: 0:09:54  lr: 0.000019  loss: 3.4815 (3.4456)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [15]  [ 550/1251]  eta: 0:09:45  lr: 0.000019  loss: 3.3898 (3.4443)  time: 0.8116  data: 0.0006  max mem: 19734
Epoch: [15]  [ 560/1251]  eta: 0:09:36  lr: 0.000019  loss: 3.3893 (3.4417)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [15]  [ 570/1251]  eta: 0:09:28  lr: 0.000019  loss: 3.3853 (3.4397)  time: 0.7997  data: 0.0006  max mem: 19734
Epoch: [15]  [ 580/1251]  eta: 0:09:19  lr: 0.000019  loss: 3.5827 (3.4444)  time: 0.8105  data: 0.0007  max mem: 19734
Epoch: [15]  [ 590/1251]  eta: 0:09:11  lr: 0.000019  loss: 3.6424 (3.4432)  time: 0.8184  data: 0.0005  max mem: 19734
Epoch: [15]  [ 600/1251]  eta: 0:09:02  lr: 0.000019  loss: 3.6144 (3.4457)  time: 0.8110  data: 0.0005  max mem: 19734
Epoch: [15]  [ 610/1251]  eta: 0:08:53  lr: 0.000019  loss: 3.5288 (3.4428)  time: 0.8082  data: 0.0005  max mem: 19734
Epoch: [15]  [ 620/1251]  eta: 0:08:45  lr: 0.000019  loss: 3.1038 (3.4380)  time: 0.8220  data: 0.0004  max mem: 19734
Epoch: [15]  [ 630/1251]  eta: 0:08:37  lr: 0.000019  loss: 3.1037 (3.4318)  time: 0.8352  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3437, ratio_loss=0.0058, pruning_loss=0.1528, mse_loss=0.7563
Epoch: [15]  [ 640/1251]  eta: 0:08:28  lr: 0.000019  loss: 3.3618 (3.4336)  time: 0.8189  data: 0.0005  max mem: 19734
Epoch: [15]  [ 650/1251]  eta: 0:08:20  lr: 0.000019  loss: 3.6299 (3.4354)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [15]  [ 660/1251]  eta: 0:08:11  lr: 0.000019  loss: 3.6538 (3.4395)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [15]  [ 670/1251]  eta: 0:08:02  lr: 0.000019  loss: 3.5473 (3.4389)  time: 0.8023  data: 0.0006  max mem: 19734
Epoch: [15]  [ 680/1251]  eta: 0:07:54  lr: 0.000019  loss: 3.4135 (3.4390)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [15]  [ 690/1251]  eta: 0:07:45  lr: 0.000019  loss: 3.5627 (3.4429)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [15]  [ 700/1251]  eta: 0:07:37  lr: 0.000019  loss: 3.6983 (3.4440)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [15]  [ 710/1251]  eta: 0:07:28  lr: 0.000019  loss: 3.5659 (3.4439)  time: 0.8062  data: 0.0005  max mem: 19734
Epoch: [15]  [ 720/1251]  eta: 0:07:20  lr: 0.000019  loss: 3.6004 (3.4468)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [15]  [ 730/1251]  eta: 0:07:12  lr: 0.000019  loss: 3.6335 (3.4459)  time: 0.8138  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5089, ratio_loss=0.0062, pruning_loss=0.1475, mse_loss=0.7305
Epoch: [15]  [ 740/1251]  eta: 0:07:03  lr: 0.000019  loss: 3.2152 (3.4429)  time: 0.8158  data: 0.0005  max mem: 19734
Epoch: [15]  [ 750/1251]  eta: 0:06:55  lr: 0.000019  loss: 3.4413 (3.4420)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [15]  [ 760/1251]  eta: 0:06:46  lr: 0.000019  loss: 3.5707 (3.4432)  time: 0.8118  data: 0.0005  max mem: 19734
Epoch: [15]  [ 770/1251]  eta: 0:06:38  lr: 0.000019  loss: 3.4968 (3.4451)  time: 0.8317  data: 0.0006  max mem: 19734
Epoch: [15]  [ 780/1251]  eta: 0:06:30  lr: 0.000019  loss: 3.6071 (3.4468)  time: 0.8314  data: 0.0006  max mem: 19734
Epoch: [15]  [ 790/1251]  eta: 0:06:21  lr: 0.000019  loss: 3.5763 (3.4453)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [15]  [ 800/1251]  eta: 0:06:13  lr: 0.000019  loss: 3.4340 (3.4470)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [15]  [ 810/1251]  eta: 0:06:05  lr: 0.000019  loss: 3.6249 (3.4493)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [15]  [ 820/1251]  eta: 0:05:56  lr: 0.000019  loss: 3.7423 (3.4478)  time: 0.8030  data: 0.0006  max mem: 19734
Epoch: [15]  [ 830/1251]  eta: 0:05:48  lr: 0.000019  loss: 3.3468 (3.4492)  time: 0.8004  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4491, ratio_loss=0.0060, pruning_loss=0.1488, mse_loss=0.7374
Epoch: [15]  [ 840/1251]  eta: 0:05:39  lr: 0.000019  loss: 3.6252 (3.4506)  time: 0.8060  data: 0.0004  max mem: 19734
Epoch: [15]  [ 850/1251]  eta: 0:05:31  lr: 0.000019  loss: 3.4931 (3.4501)  time: 0.8060  data: 0.0004  max mem: 19734
Epoch: [15]  [ 860/1251]  eta: 0:05:23  lr: 0.000019  loss: 3.4931 (3.4510)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [15]  [ 870/1251]  eta: 0:05:14  lr: 0.000019  loss: 3.7273 (3.4497)  time: 0.8142  data: 0.0005  max mem: 19734
Epoch: [15]  [ 880/1251]  eta: 0:05:06  lr: 0.000019  loss: 3.5692 (3.4492)  time: 0.8197  data: 0.0005  max mem: 19734
Epoch: [15]  [ 890/1251]  eta: 0:04:58  lr: 0.000019  loss: 3.2953 (3.4459)  time: 0.8072  data: 0.0006  max mem: 19734
Epoch: [15]  [ 900/1251]  eta: 0:04:49  lr: 0.000019  loss: 3.2953 (3.4448)  time: 0.8084  data: 0.0005  max mem: 19734
Epoch: [15]  [ 910/1251]  eta: 0:04:41  lr: 0.000019  loss: 3.3956 (3.4451)  time: 0.8173  data: 0.0004  max mem: 19734
Epoch: [15]  [ 920/1251]  eta: 0:04:33  lr: 0.000019  loss: 3.5840 (3.4452)  time: 0.8311  data: 0.0005  max mem: 19734
Epoch: [15]  [ 930/1251]  eta: 0:04:25  lr: 0.000019  loss: 3.5840 (3.4439)  time: 0.8208  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4030, ratio_loss=0.0059, pruning_loss=0.1517, mse_loss=0.7192
Epoch: [15]  [ 940/1251]  eta: 0:04:16  lr: 0.000019  loss: 3.6789 (3.4459)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [15]  [ 950/1251]  eta: 0:04:08  lr: 0.000019  loss: 3.5509 (3.4431)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [15]  [ 960/1251]  eta: 0:04:00  lr: 0.000019  loss: 3.4487 (3.4461)  time: 0.8017  data: 0.0004  max mem: 19734
Epoch: [15]  [ 970/1251]  eta: 0:03:51  lr: 0.000019  loss: 3.5971 (3.4467)  time: 0.8034  data: 0.0004  max mem: 19734
Epoch: [15]  [ 980/1251]  eta: 0:03:43  lr: 0.000019  loss: 3.5166 (3.4469)  time: 0.8035  data: 0.0004  max mem: 19734
Epoch: [15]  [ 990/1251]  eta: 0:03:35  lr: 0.000019  loss: 3.6419 (3.4469)  time: 0.8092  data: 0.0005  max mem: 19734
Epoch: [15]  [1000/1251]  eta: 0:03:26  lr: 0.000019  loss: 3.6222 (3.4481)  time: 0.8078  data: 0.0007  max mem: 19734
Epoch: [15]  [1010/1251]  eta: 0:03:18  lr: 0.000019  loss: 3.5596 (3.4460)  time: 0.8038  data: 0.0008  max mem: 19734
Epoch: [15]  [1020/1251]  eta: 0:03:10  lr: 0.000019  loss: 3.2971 (3.4443)  time: 0.8113  data: 0.0005  max mem: 19734
Epoch: [15]  [1030/1251]  eta: 0:03:02  lr: 0.000019  loss: 3.5850 (3.4443)  time: 0.8177  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4094, ratio_loss=0.0057, pruning_loss=0.1494, mse_loss=0.7647
Epoch: [15]  [1040/1251]  eta: 0:02:53  lr: 0.000019  loss: 3.7139 (3.4437)  time: 0.8124  data: 0.0004  max mem: 19734
Epoch: [15]  [1050/1251]  eta: 0:02:45  lr: 0.000019  loss: 3.3690 (3.4413)  time: 0.8113  data: 0.0005  max mem: 19734
Epoch: [15]  [1060/1251]  eta: 0:02:37  lr: 0.000019  loss: 3.3468 (3.4395)  time: 0.8273  data: 0.0007  max mem: 19734
Epoch: [15]  [1070/1251]  eta: 0:02:29  lr: 0.000019  loss: 3.6022 (3.4399)  time: 0.8409  data: 0.0007  max mem: 19734
Epoch: [15]  [1080/1251]  eta: 0:02:20  lr: 0.000019  loss: 3.6055 (3.4407)  time: 0.8240  data: 0.0004  max mem: 19734
Epoch: [15]  [1090/1251]  eta: 0:02:12  lr: 0.000019  loss: 3.5297 (3.4390)  time: 0.8047  data: 0.0004  max mem: 19734
Epoch: [15]  [1100/1251]  eta: 0:02:04  lr: 0.000019  loss: 3.6052 (3.4412)  time: 0.8046  data: 0.0004  max mem: 19734
Epoch: [15]  [1110/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.6261 (3.4410)  time: 0.8033  data: 0.0004  max mem: 19734
Epoch: [15]  [1120/1251]  eta: 0:01:47  lr: 0.000019  loss: 3.6040 (3.4421)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [15]  [1130/1251]  eta: 0:01:39  lr: 0.000019  loss: 3.6040 (3.4432)  time: 0.8031  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4058, ratio_loss=0.0062, pruning_loss=0.1511, mse_loss=0.7388
Epoch: [15]  [1140/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.4305 (3.4430)  time: 0.8099  data: 0.0005  max mem: 19734
Epoch: [15]  [1150/1251]  eta: 0:01:23  lr: 0.000019  loss: 3.6958 (3.4446)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [15]  [1160/1251]  eta: 0:01:14  lr: 0.000019  loss: 3.8575 (3.4461)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [15]  [1170/1251]  eta: 0:01:06  lr: 0.000019  loss: 3.5005 (3.4471)  time: 0.8195  data: 0.0006  max mem: 19734
Epoch: [15]  [1180/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.4129 (3.4459)  time: 0.8201  data: 0.0006  max mem: 19734
Epoch: [15]  [1190/1251]  eta: 0:00:50  lr: 0.000019  loss: 3.4629 (3.4468)  time: 0.8025  data: 0.0007  max mem: 19734
Epoch: [15]  [1200/1251]  eta: 0:00:41  lr: 0.000019  loss: 3.4629 (3.4464)  time: 0.8040  data: 0.0006  max mem: 19734
Epoch: [15]  [1210/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.3891 (3.4448)  time: 0.8191  data: 0.0001  max mem: 19734
Epoch: [15]  [1220/1251]  eta: 0:00:25  lr: 0.000019  loss: 3.1992 (3.4440)  time: 0.8123  data: 0.0002  max mem: 19734
Epoch: [15]  [1230/1251]  eta: 0:00:17  lr: 0.000019  loss: 3.4094 (3.4437)  time: 0.7937  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4199, ratio_loss=0.0061, pruning_loss=0.1505, mse_loss=0.7320
Epoch: [15]  [1240/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.4133 (3.4442)  time: 0.7929  data: 0.0001  max mem: 19734
Epoch: [15]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.3952 (3.4432)  time: 0.7923  data: 0.0001  max mem: 19734
Epoch: [15] Total time: 0:17:08 (0.8222 s / it)
Averaged stats: lr: 0.000019  loss: 3.3952 (3.4399)
Test:  [  0/261]  eta: 2:18:37  loss: 0.7249 (0.7249)  acc1: 83.3333 (83.3333)  acc5: 94.7917 (94.7917)  time: 31.8691  data: 30.7050  max mem: 19734
Test:  [ 10/261]  eta: 0:15:47  loss: 0.7249 (0.7333)  acc1: 83.3333 (83.9962)  acc5: 96.3542 (96.1174)  time: 3.7730  data: 3.0926  max mem: 19734
Test:  [ 20/261]  eta: 0:09:25  loss: 0.9094 (0.9084)  acc1: 79.6875 (79.4395)  acc5: 93.7500 (94.5437)  time: 0.8719  data: 0.1784  max mem: 19734
Test:  [ 30/261]  eta: 0:06:49  loss: 0.8310 (0.8262)  acc1: 82.2917 (82.1405)  acc5: 94.7917 (95.0605)  time: 0.6725  data: 0.0279  max mem: 19734
Test:  [ 40/261]  eta: 0:05:13  loss: 0.5895 (0.7994)  acc1: 87.5000 (82.9141)  acc5: 96.3542 (95.2998)  time: 0.4399  data: 0.0230  max mem: 19734
Test:  [ 50/261]  eta: 0:04:12  loss: 0.9270 (0.8571)  acc1: 78.1250 (81.2500)  acc5: 94.7917 (94.9346)  time: 0.3098  data: 0.0178  max mem: 19734
Test:  [ 60/261]  eta: 0:03:31  loss: 1.0156 (0.8724)  acc1: 76.0417 (80.6523)  acc5: 93.7500 (95.0137)  time: 0.3037  data: 0.0180  max mem: 19734
Test:  [ 70/261]  eta: 0:02:57  loss: 0.9645 (0.8734)  acc1: 76.0417 (80.1423)  acc5: 96.3542 (95.2245)  time: 0.2500  data: 0.0157  max mem: 19734
Test:  [ 80/261]  eta: 0:02:32  loss: 0.8831 (0.8771)  acc1: 79.6875 (80.2019)  acc5: 96.3542 (95.3061)  time: 0.2142  data: 0.0121  max mem: 19734
Test:  [ 90/261]  eta: 0:02:13  loss: 0.8115 (0.8617)  acc1: 84.3750 (80.6090)  acc5: 96.3542 (95.4270)  time: 0.2499  data: 0.0082  max mem: 19734
Test:  [100/261]  eta: 0:02:08  loss: 0.8109 (0.8650)  acc1: 84.3750 (80.5384)  acc5: 95.8333 (95.4724)  time: 0.6115  data: 0.2735  max mem: 19734
Test:  [110/261]  eta: 0:01:53  loss: 0.9122 (0.8881)  acc1: 77.0833 (80.0582)  acc5: 94.2708 (95.1436)  time: 0.6211  data: 0.2767  max mem: 19734
Test:  [120/261]  eta: 0:01:46  loss: 1.2646 (0.9295)  acc1: 70.3125 (79.0203)  acc5: 89.0625 (94.5764)  time: 0.5425  data: 0.0185  max mem: 19734
Test:  [130/261]  eta: 0:01:39  loss: 1.4053 (0.9755)  acc1: 68.7500 (78.0336)  acc5: 86.9792 (94.0283)  time: 0.7974  data: 0.0249  max mem: 19734
Test:  [140/261]  eta: 0:01:27  loss: 1.3883 (1.0048)  acc1: 67.1875 (77.3087)  acc5: 89.5833 (93.7611)  time: 0.5308  data: 0.0247  max mem: 19734
Test:  [150/261]  eta: 0:01:17  loss: 1.2576 (1.0108)  acc1: 71.8750 (77.3041)  acc5: 90.6250 (93.5810)  time: 0.2863  data: 0.0179  max mem: 19734
Test:  [160/261]  eta: 0:01:07  loss: 1.1041 (1.0311)  acc1: 77.0833 (76.9539)  acc5: 90.1042 (93.2971)  time: 0.2685  data: 0.0530  max mem: 19734
Test:  [170/261]  eta: 0:00:57  loss: 1.3011 (1.0622)  acc1: 63.0208 (76.1544)  acc5: 86.4583 (92.9550)  time: 0.1833  data: 0.0506  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: 1.4687 (1.0794)  acc1: 64.5833 (75.7338)  acc5: 87.5000 (92.7601)  time: 0.1486  data: 0.0091  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.4257 (1.0926)  acc1: 66.6667 (75.4936)  acc5: 91.1458 (92.6238)  time: 0.2360  data: 0.1009  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.4215 (1.1077)  acc1: 71.8750 (75.1995)  acc5: 90.1042 (92.3922)  time: 0.2413  data: 0.1229  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.4215 (1.1221)  acc1: 67.7083 (74.9284)  acc5: 87.5000 (92.1924)  time: 0.1422  data: 0.0271  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4544 (1.1412)  acc1: 67.1875 (74.4273)  acc5: 87.5000 (91.9778)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.4850 (1.1509)  acc1: 63.5417 (74.2064)  acc5: 88.5417 (91.8809)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3515 (1.1603)  acc1: 68.7500 (73.9713)  acc5: 90.6250 (91.8028)  time: 0.1154  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0806 (1.1535)  acc1: 75.5208 (74.1430)  acc5: 93.2292 (91.9177)  time: 0.1152  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9686 (1.1529)  acc1: 76.5625 (74.1900)  acc5: 95.3125 (91.9640)  time: 0.1118  data: 0.0001  max mem: 19734
Test: Total time: 0:02:02 (0.4689 s / it)
* Acc@1 74.190 Acc@5 91.963 loss 1.153
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.24%
Epoch: [16]  [   0/1251]  eta: 6:39:27  lr: 0.000019  loss: 3.7822 (3.7822)  time: 19.1589  data: 11.7686  max mem: 19734
Epoch: [16]  [  10/1251]  eta: 0:54:25  lr: 0.000019  loss: 3.6252 (3.4177)  time: 2.6315  data: 1.0722  max mem: 19734
Epoch: [16]  [  20/1251]  eta: 0:36:10  lr: 0.000019  loss: 3.6193 (3.5237)  time: 0.8931  data: 0.0015  max mem: 19734
Epoch: [16]  [  30/1251]  eta: 0:29:33  lr: 0.000019  loss: 3.6068 (3.5573)  time: 0.8039  data: 0.0004  max mem: 19734
Epoch: [16]  [  40/1251]  eta: 0:26:05  lr: 0.000019  loss: 3.6046 (3.5467)  time: 0.7995  data: 0.0005  max mem: 19734
Epoch: [16]  [  50/1251]  eta: 0:24:01  lr: 0.000019  loss: 3.6011 (3.5259)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [16]  [  60/1251]  eta: 0:22:34  lr: 0.000019  loss: 3.5948 (3.5436)  time: 0.8188  data: 0.0005  max mem: 19734
Epoch: [16]  [  70/1251]  eta: 0:21:26  lr: 0.000019  loss: 3.5852 (3.5386)  time: 0.8080  data: 0.0006  max mem: 19734
Epoch: [16]  [  80/1251]  eta: 0:20:36  lr: 0.000019  loss: 3.5867 (3.5530)  time: 0.8076  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5160, ratio_loss=0.0061, pruning_loss=0.1459, mse_loss=0.7323
Epoch: [16]  [  90/1251]  eta: 0:19:53  lr: 0.000019  loss: 3.6217 (3.5273)  time: 0.8092  data: 0.0005  max mem: 19734
Epoch: [16]  [ 100/1251]  eta: 0:19:21  lr: 0.000019  loss: 3.2938 (3.4997)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [16]  [ 110/1251]  eta: 0:18:49  lr: 0.000019  loss: 3.3239 (3.4850)  time: 0.8174  data: 0.0005  max mem: 19734
Epoch: [16]  [ 120/1251]  eta: 0:18:22  lr: 0.000019  loss: 3.3555 (3.4661)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [16]  [ 130/1251]  eta: 0:17:58  lr: 0.000019  loss: 3.3555 (3.4567)  time: 0.8098  data: 0.0004  max mem: 19734
Epoch: [16]  [ 140/1251]  eta: 0:17:36  lr: 0.000019  loss: 3.4962 (3.4655)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [16]  [ 150/1251]  eta: 0:17:17  lr: 0.000019  loss: 3.5641 (3.4617)  time: 0.8083  data: 0.0006  max mem: 19734
Epoch: [16]  [ 160/1251]  eta: 0:16:58  lr: 0.000019  loss: 3.3403 (3.4595)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [16]  [ 170/1251]  eta: 0:16:40  lr: 0.000019  loss: 3.2654 (3.4401)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [16]  [ 180/1251]  eta: 0:16:24  lr: 0.000019  loss: 3.1763 (3.4386)  time: 0.8014  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3027, ratio_loss=0.0058, pruning_loss=0.1520, mse_loss=0.7675
Epoch: [16]  [ 190/1251]  eta: 0:16:08  lr: 0.000019  loss: 3.6395 (3.4331)  time: 0.8045  data: 0.0004  max mem: 19734
Epoch: [16]  [ 200/1251]  eta: 0:15:54  lr: 0.000019  loss: 3.4674 (3.4273)  time: 0.8098  data: 0.0004  max mem: 19734
Epoch: [16]  [ 210/1251]  eta: 0:15:41  lr: 0.000019  loss: 3.4681 (3.4341)  time: 0.8184  data: 0.0005  max mem: 19734
Epoch: [16]  [ 220/1251]  eta: 0:15:27  lr: 0.000019  loss: 3.4893 (3.4325)  time: 0.8136  data: 0.0006  max mem: 19734
Epoch: [16]  [ 230/1251]  eta: 0:15:14  lr: 0.000019  loss: 3.6346 (3.4374)  time: 0.8132  data: 0.0006  max mem: 19734
Epoch: [16]  [ 240/1251]  eta: 0:15:04  lr: 0.000019  loss: 3.6016 (3.4230)  time: 0.8412  data: 0.0005  max mem: 19734
Epoch: [16]  [ 250/1251]  eta: 0:14:52  lr: 0.000019  loss: 3.0985 (3.4243)  time: 0.8386  data: 0.0005  max mem: 19734
Epoch: [16]  [ 260/1251]  eta: 0:14:40  lr: 0.000019  loss: 3.4396 (3.4242)  time: 0.8113  data: 0.0006  max mem: 19734
Epoch: [16]  [ 270/1251]  eta: 0:14:28  lr: 0.000019  loss: 3.6069 (3.4317)  time: 0.8064  data: 0.0006  max mem: 19734
Epoch: [16]  [ 280/1251]  eta: 0:14:17  lr: 0.000019  loss: 3.7795 (3.4439)  time: 0.8085  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4324, ratio_loss=0.0061, pruning_loss=0.1500, mse_loss=0.7334
Epoch: [16]  [ 290/1251]  eta: 0:14:05  lr: 0.000019  loss: 3.4725 (3.4272)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [16]  [ 300/1251]  eta: 0:13:54  lr: 0.000019  loss: 3.4592 (3.4343)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [16]  [ 310/1251]  eta: 0:13:43  lr: 0.000019  loss: 3.5524 (3.4344)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [16]  [ 320/1251]  eta: 0:13:32  lr: 0.000019  loss: 3.6129 (3.4418)  time: 0.7983  data: 0.0005  max mem: 19734
Epoch: [16]  [ 330/1251]  eta: 0:13:21  lr: 0.000019  loss: 3.3959 (3.4318)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [16]  [ 340/1251]  eta: 0:13:11  lr: 0.000019  loss: 2.9786 (3.4290)  time: 0.8034  data: 0.0006  max mem: 19734
Epoch: [16]  [ 350/1251]  eta: 0:13:01  lr: 0.000019  loss: 3.1743 (3.4258)  time: 0.8236  data: 0.0006  max mem: 19734
Epoch: [16]  [ 360/1251]  eta: 0:12:51  lr: 0.000019  loss: 3.5340 (3.4304)  time: 0.8223  data: 0.0005  max mem: 19734
Epoch: [16]  [ 370/1251]  eta: 0:12:41  lr: 0.000019  loss: 3.5636 (3.4296)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [16]  [ 380/1251]  eta: 0:12:31  lr: 0.000019  loss: 3.5083 (3.4337)  time: 0.8067  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3947, ratio_loss=0.0057, pruning_loss=0.1489, mse_loss=0.7390
Epoch: [16]  [ 390/1251]  eta: 0:12:22  lr: 0.000019  loss: 3.4864 (3.4301)  time: 0.8364  data: 0.0005  max mem: 19734
Epoch: [16]  [ 400/1251]  eta: 0:12:13  lr: 0.000019  loss: 3.4955 (3.4299)  time: 0.8353  data: 0.0005  max mem: 19734
Epoch: [16]  [ 410/1251]  eta: 0:12:03  lr: 0.000019  loss: 3.6689 (3.4352)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [16]  [ 420/1251]  eta: 0:11:53  lr: 0.000019  loss: 3.6614 (3.4346)  time: 0.8022  data: 0.0006  max mem: 19734
Epoch: [16]  [ 430/1251]  eta: 0:11:43  lr: 0.000019  loss: 3.3748 (3.4310)  time: 0.8048  data: 0.0006  max mem: 19734
Epoch: [16]  [ 440/1251]  eta: 0:11:34  lr: 0.000019  loss: 3.4476 (3.4332)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [16]  [ 450/1251]  eta: 0:11:25  lr: 0.000019  loss: 3.4822 (3.4326)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [16]  [ 460/1251]  eta: 0:11:15  lr: 0.000019  loss: 3.5450 (3.4357)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [16]  [ 470/1251]  eta: 0:11:06  lr: 0.000019  loss: 3.6614 (3.4405)  time: 0.8086  data: 0.0004  max mem: 19734
Epoch: [16]  [ 480/1251]  eta: 0:10:57  lr: 0.000019  loss: 3.6376 (3.4372)  time: 0.8091  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4213, ratio_loss=0.0059, pruning_loss=0.1486, mse_loss=0.7312
Epoch: [16]  [ 490/1251]  eta: 0:10:47  lr: 0.000019  loss: 3.4196 (3.4369)  time: 0.8058  data: 0.0004  max mem: 19734
Epoch: [16]  [ 500/1251]  eta: 0:10:38  lr: 0.000019  loss: 3.6098 (3.4385)  time: 0.8152  data: 0.0004  max mem: 19734
Epoch: [16]  [ 510/1251]  eta: 0:10:29  lr: 0.000019  loss: 3.5676 (3.4392)  time: 0.8104  data: 0.0004  max mem: 19734
Epoch: [16]  [ 520/1251]  eta: 0:10:20  lr: 0.000019  loss: 3.5226 (3.4408)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [16]  [ 530/1251]  eta: 0:10:11  lr: 0.000019  loss: 3.5484 (3.4442)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [16]  [ 540/1251]  eta: 0:10:03  lr: 0.000019  loss: 3.4672 (3.4396)  time: 0.8284  data: 0.0005  max mem: 19734
Epoch: [16]  [ 550/1251]  eta: 0:09:54  lr: 0.000019  loss: 3.4778 (3.4421)  time: 0.8224  data: 0.0008  max mem: 19734
Epoch: [16]  [ 560/1251]  eta: 0:09:44  lr: 0.000019  loss: 3.5633 (3.4440)  time: 0.7996  data: 0.0008  max mem: 19734
Epoch: [16]  [ 570/1251]  eta: 0:09:35  lr: 0.000019  loss: 3.6050 (3.4474)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [16]  [ 580/1251]  eta: 0:09:27  lr: 0.000019  loss: 3.7099 (3.4465)  time: 0.8044  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4903, ratio_loss=0.0059, pruning_loss=0.1454, mse_loss=0.7509
Epoch: [16]  [ 590/1251]  eta: 0:09:18  lr: 0.000019  loss: 3.4898 (3.4483)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [16]  [ 600/1251]  eta: 0:09:09  lr: 0.000019  loss: 3.6221 (3.4545)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [16]  [ 610/1251]  eta: 0:09:00  lr: 0.000019  loss: 3.6365 (3.4539)  time: 0.8017  data: 0.0006  max mem: 19734
Epoch: [16]  [ 620/1251]  eta: 0:08:51  lr: 0.000019  loss: 3.4276 (3.4529)  time: 0.8017  data: 0.0007  max mem: 19734
Epoch: [16]  [ 630/1251]  eta: 0:08:42  lr: 0.000019  loss: 3.4276 (3.4516)  time: 0.8040  data: 0.0006  max mem: 19734
Epoch: [16]  [ 640/1251]  eta: 0:08:34  lr: 0.000019  loss: 3.5336 (3.4537)  time: 0.8122  data: 0.0005  max mem: 19734
Epoch: [16]  [ 650/1251]  eta: 0:08:25  lr: 0.000019  loss: 3.4643 (3.4504)  time: 0.8158  data: 0.0005  max mem: 19734
Epoch: [16]  [ 660/1251]  eta: 0:08:16  lr: 0.000019  loss: 3.2430 (3.4500)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [16]  [ 670/1251]  eta: 0:08:07  lr: 0.000019  loss: 3.4331 (3.4507)  time: 0.8063  data: 0.0005  max mem: 19734
Epoch: [16]  [ 680/1251]  eta: 0:07:59  lr: 0.000019  loss: 3.5612 (3.4527)  time: 0.8300  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4608, ratio_loss=0.0058, pruning_loss=0.1472, mse_loss=0.7498
Epoch: [16]  [ 690/1251]  eta: 0:07:51  lr: 0.000019  loss: 3.5582 (3.4528)  time: 0.8347  data: 0.0005  max mem: 19734
Epoch: [16]  [ 700/1251]  eta: 0:07:42  lr: 0.000019  loss: 3.5198 (3.4524)  time: 0.8082  data: 0.0005  max mem: 19734
Epoch: [16]  [ 710/1251]  eta: 0:07:33  lr: 0.000019  loss: 3.5833 (3.4545)  time: 0.7992  data: 0.0006  max mem: 19734
Epoch: [16]  [ 720/1251]  eta: 0:07:25  lr: 0.000019  loss: 3.4416 (3.4518)  time: 0.7981  data: 0.0006  max mem: 19734
Epoch: [16]  [ 730/1251]  eta: 0:07:16  lr: 0.000019  loss: 3.4402 (3.4529)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [16]  [ 740/1251]  eta: 0:07:07  lr: 0.000019  loss: 3.5886 (3.4527)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [16]  [ 750/1251]  eta: 0:06:59  lr: 0.000019  loss: 3.5886 (3.4530)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [16]  [ 760/1251]  eta: 0:06:50  lr: 0.000019  loss: 3.6043 (3.4532)  time: 0.7968  data: 0.0006  max mem: 19734
Epoch: [16]  [ 770/1251]  eta: 0:06:41  lr: 0.000019  loss: 3.7635 (3.4567)  time: 0.7970  data: 0.0005  max mem: 19734
Epoch: [16]  [ 780/1251]  eta: 0:06:33  lr: 0.000019  loss: 3.5851 (3.4512)  time: 0.8064  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4024, ratio_loss=0.0059, pruning_loss=0.1471, mse_loss=0.7296
Epoch: [16]  [ 790/1251]  eta: 0:06:24  lr: 0.000019  loss: 3.2344 (3.4493)  time: 0.8150  data: 0.0004  max mem: 19734
Epoch: [16]  [ 800/1251]  eta: 0:06:16  lr: 0.000019  loss: 3.5879 (3.4506)  time: 0.8069  data: 0.0004  max mem: 19734
Epoch: [16]  [ 810/1251]  eta: 0:06:07  lr: 0.000019  loss: 3.5879 (3.4500)  time: 0.7975  data: 0.0007  max mem: 19734
Epoch: [16]  [ 820/1251]  eta: 0:05:59  lr: 0.000019  loss: 3.6741 (3.4522)  time: 0.8045  data: 0.0007  max mem: 19734
Epoch: [16]  [ 830/1251]  eta: 0:05:51  lr: 0.000019  loss: 3.6760 (3.4536)  time: 0.8395  data: 0.0004  max mem: 19734
Epoch: [16]  [ 840/1251]  eta: 0:05:42  lr: 0.000019  loss: 3.5182 (3.4524)  time: 0.8314  data: 0.0004  max mem: 19734
Epoch: [16]  [ 850/1251]  eta: 0:05:34  lr: 0.000019  loss: 3.3670 (3.4486)  time: 0.7976  data: 0.0004  max mem: 19734
Epoch: [16]  [ 860/1251]  eta: 0:05:25  lr: 0.000019  loss: 3.4594 (3.4494)  time: 0.7992  data: 0.0004  max mem: 19734
Epoch: [16]  [ 870/1251]  eta: 0:05:17  lr: 0.000019  loss: 3.3362 (3.4458)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [16]  [ 880/1251]  eta: 0:05:08  lr: 0.000019  loss: 3.3362 (3.4465)  time: 0.7986  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3931, ratio_loss=0.0059, pruning_loss=0.1477, mse_loss=0.7414
Epoch: [16]  [ 890/1251]  eta: 0:05:00  lr: 0.000019  loss: 3.3998 (3.4468)  time: 0.8059  data: 0.0004  max mem: 19734
Epoch: [16]  [ 900/1251]  eta: 0:04:51  lr: 0.000019  loss: 3.3998 (3.4472)  time: 0.8075  data: 0.0004  max mem: 19734
Epoch: [16]  [ 910/1251]  eta: 0:04:43  lr: 0.000019  loss: 3.5618 (3.4464)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [16]  [ 920/1251]  eta: 0:04:35  lr: 0.000019  loss: 3.5383 (3.4460)  time: 0.7996  data: 0.0004  max mem: 19734
Epoch: [16]  [ 930/1251]  eta: 0:04:26  lr: 0.000019  loss: 3.1000 (3.4416)  time: 0.8079  data: 0.0004  max mem: 19734
Epoch: [16]  [ 940/1251]  eta: 0:04:18  lr: 0.000019  loss: 3.3414 (3.4450)  time: 0.8169  data: 0.0004  max mem: 19734
Epoch: [16]  [ 950/1251]  eta: 0:04:09  lr: 0.000019  loss: 3.7995 (3.4443)  time: 0.8094  data: 0.0004  max mem: 19734
Epoch: [16]  [ 960/1251]  eta: 0:04:01  lr: 0.000019  loss: 3.6176 (3.4431)  time: 0.8051  data: 0.0004  max mem: 19734
Epoch: [16]  [ 970/1251]  eta: 0:03:53  lr: 0.000019  loss: 3.4548 (3.4425)  time: 0.8231  data: 0.0004  max mem: 19734
Epoch: [16]  [ 980/1251]  eta: 0:03:44  lr: 0.000019  loss: 3.5721 (3.4445)  time: 0.8251  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4040, ratio_loss=0.0058, pruning_loss=0.1499, mse_loss=0.7324
Epoch: [16]  [ 990/1251]  eta: 0:03:36  lr: 0.000019  loss: 3.5918 (3.4459)  time: 0.8059  data: 0.0004  max mem: 19734
Epoch: [16]  [1000/1251]  eta: 0:03:28  lr: 0.000019  loss: 3.5518 (3.4433)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [16]  [1010/1251]  eta: 0:03:19  lr: 0.000019  loss: 3.0141 (3.4419)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [16]  [1020/1251]  eta: 0:03:11  lr: 0.000019  loss: 3.5331 (3.4417)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [16]  [1030/1251]  eta: 0:03:03  lr: 0.000019  loss: 3.4540 (3.4421)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [16]  [1040/1251]  eta: 0:02:54  lr: 0.000019  loss: 3.3514 (3.4404)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [16]  [1050/1251]  eta: 0:02:46  lr: 0.000019  loss: 3.3514 (3.4409)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [16]  [1060/1251]  eta: 0:02:38  lr: 0.000019  loss: 3.5439 (3.4391)  time: 0.7983  data: 0.0006  max mem: 19734
Epoch: [16]  [1070/1251]  eta: 0:02:29  lr: 0.000019  loss: 3.3424 (3.4388)  time: 0.8070  data: 0.0006  max mem: 19734
Epoch: [16]  [1080/1251]  eta: 0:02:21  lr: 0.000019  loss: 3.5158 (3.4376)  time: 0.8116  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3434, ratio_loss=0.0061, pruning_loss=0.1482, mse_loss=0.7064
Epoch: [16]  [1090/1251]  eta: 0:02:13  lr: 0.000019  loss: 3.4863 (3.4356)  time: 0.8034  data: 0.0006  max mem: 19734
Epoch: [16]  [1100/1251]  eta: 0:02:04  lr: 0.000019  loss: 3.2712 (3.4354)  time: 0.7980  data: 0.0006  max mem: 19734
Epoch: [16]  [1110/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.2712 (3.4341)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [16]  [1120/1251]  eta: 0:01:48  lr: 0.000019  loss: 3.2503 (3.4315)  time: 0.8260  data: 0.0004  max mem: 19734
Epoch: [16]  [1130/1251]  eta: 0:01:40  lr: 0.000019  loss: 3.5416 (3.4332)  time: 0.8217  data: 0.0004  max mem: 19734
Epoch: [16]  [1140/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.6176 (3.4337)  time: 0.8000  data: 0.0004  max mem: 19734
Epoch: [16]  [1150/1251]  eta: 0:01:23  lr: 0.000019  loss: 3.6069 (3.4339)  time: 0.8021  data: 0.0004  max mem: 19734
Epoch: [16]  [1160/1251]  eta: 0:01:15  lr: 0.000019  loss: 3.3302 (3.4325)  time: 0.7991  data: 0.0005  max mem: 19734
Epoch: [16]  [1170/1251]  eta: 0:01:06  lr: 0.000019  loss: 3.6512 (3.4338)  time: 0.7968  data: 0.0005  max mem: 19734
Epoch: [16]  [1180/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.7059 (3.4357)  time: 0.8063  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3865, ratio_loss=0.0059, pruning_loss=0.1491, mse_loss=0.7513
Epoch: [16]  [1190/1251]  eta: 0:00:50  lr: 0.000019  loss: 3.6628 (3.4358)  time: 0.8055  data: 0.0010  max mem: 19734
Epoch: [16]  [1200/1251]  eta: 0:00:42  lr: 0.000019  loss: 3.7074 (3.4380)  time: 0.7939  data: 0.0008  max mem: 19734
Epoch: [16]  [1210/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.7893 (3.4379)  time: 0.7899  data: 0.0001  max mem: 19734
Epoch: [16]  [1220/1251]  eta: 0:00:25  lr: 0.000019  loss: 3.5855 (3.4375)  time: 0.7986  data: 0.0001  max mem: 19734
Epoch: [16]  [1230/1251]  eta: 0:00:17  lr: 0.000019  loss: 3.5473 (3.4384)  time: 0.8077  data: 0.0001  max mem: 19734
Epoch: [16]  [1240/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.5473 (3.4380)  time: 0.8013  data: 0.0001  max mem: 19734
Epoch: [16]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.5943 (3.4383)  time: 0.7924  data: 0.0001  max mem: 19734
Epoch: [16] Total time: 0:17:12 (0.8251 s / it)
Averaged stats: lr: 0.000019  loss: 3.5943 (3.4364)
Test:  [  0/261]  eta: 1:46:59  loss: 0.7134 (0.7134)  acc1: 82.2917 (82.2917)  acc5: 96.3542 (96.3542)  time: 24.5956  data: 24.4113  max mem: 19734
Test:  [ 10/261]  eta: 0:12:05  loss: 0.7134 (0.7378)  acc1: 82.2917 (83.3807)  acc5: 96.3542 (96.2121)  time: 2.8891  data: 2.6711  max mem: 19734
Test:  [ 20/261]  eta: 0:06:50  loss: 0.9526 (0.9101)  acc1: 78.1250 (78.6706)  acc5: 93.7500 (94.4940)  time: 0.5572  data: 0.2706  max mem: 19734
Test:  [ 30/261]  eta: 0:04:40  loss: 0.8096 (0.8261)  acc1: 81.7708 (81.4180)  acc5: 94.7917 (95.0437)  time: 0.2948  data: 0.0286  max mem: 19734
Test:  [ 40/261]  eta: 0:04:15  loss: 0.5923 (0.7932)  acc1: 88.0208 (82.5457)  acc5: 96.3542 (95.3379)  time: 0.5808  data: 0.3920  max mem: 19734
Test:  [ 50/261]  eta: 0:03:36  loss: 0.9112 (0.8570)  acc1: 78.1250 (80.6475)  acc5: 94.7917 (94.9244)  time: 0.7290  data: 0.3970  max mem: 19734
Test:  [ 60/261]  eta: 0:03:00  loss: 0.9865 (0.8695)  acc1: 75.5208 (80.1059)  acc5: 93.7500 (94.9539)  time: 0.3756  data: 0.0194  max mem: 19734
Test:  [ 70/261]  eta: 0:03:00  loss: 0.9476 (0.8684)  acc1: 76.5625 (79.7389)  acc5: 95.8333 (95.1658)  time: 0.7468  data: 0.4456  max mem: 19734
Test:  [ 80/261]  eta: 0:02:40  loss: 0.8220 (0.8718)  acc1: 81.7708 (79.9318)  acc5: 96.3542 (95.2675)  time: 0.8533  data: 0.4467  max mem: 19734
Test:  [ 90/261]  eta: 0:02:22  loss: 0.8161 (0.8570)  acc1: 83.8542 (80.3800)  acc5: 96.3542 (95.3869)  time: 0.4309  data: 0.0193  max mem: 19734
Test:  [100/261]  eta: 0:02:19  loss: 0.8161 (0.8605)  acc1: 84.3750 (80.3476)  acc5: 96.3542 (95.4414)  time: 0.7761  data: 0.4640  max mem: 19734
Test:  [110/261]  eta: 0:02:03  loss: 0.8772 (0.8821)  acc1: 76.5625 (79.9174)  acc5: 94.7917 (95.1577)  time: 0.7510  data: 0.4622  max mem: 19734
Test:  [120/261]  eta: 0:01:50  loss: 1.2332 (0.9221)  acc1: 69.7917 (78.9299)  acc5: 90.1042 (94.6238)  time: 0.3723  data: 0.0182  max mem: 19734
Test:  [130/261]  eta: 0:01:39  loss: 1.3655 (0.9697)  acc1: 65.6250 (77.9381)  acc5: 86.9792 (94.0482)  time: 0.4478  data: 0.0173  max mem: 19734
Test:  [140/261]  eta: 0:01:29  loss: 1.3318 (0.9958)  acc1: 67.1875 (77.2902)  acc5: 89.5833 (93.8017)  time: 0.4475  data: 0.0214  max mem: 19734
Test:  [150/261]  eta: 0:01:18  loss: 1.2582 (1.0018)  acc1: 71.8750 (77.2144)  acc5: 91.1458 (93.6776)  time: 0.3540  data: 0.0222  max mem: 19734
Test:  [160/261]  eta: 0:01:09  loss: 1.1148 (1.0233)  acc1: 76.0417 (76.8634)  acc5: 91.6667 (93.3650)  time: 0.3850  data: 0.0185  max mem: 19734
Test:  [170/261]  eta: 0:01:02  loss: 1.3017 (1.0533)  acc1: 64.5833 (76.0752)  acc5: 88.0208 (93.0190)  time: 0.5069  data: 0.2190  max mem: 19734
Test:  [180/261]  eta: 0:00:52  loss: 1.4099 (1.0708)  acc1: 64.5833 (75.6705)  acc5: 87.5000 (92.8292)  time: 0.3414  data: 0.2131  max mem: 19734
Test:  [190/261]  eta: 0:00:44  loss: 1.3910 (1.0848)  acc1: 68.2292 (75.4199)  acc5: 90.6250 (92.6756)  time: 0.1363  data: 0.0074  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: 1.3393 (1.1001)  acc1: 70.8333 (75.1244)  acc5: 90.6250 (92.4570)  time: 0.1379  data: 0.0154  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.3676 (1.1138)  acc1: 70.3125 (74.8568)  acc5: 88.0208 (92.2764)  time: 0.1276  data: 0.0122  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4295 (1.1328)  acc1: 67.1875 (74.4014)  acc5: 88.0208 (92.0744)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4549 (1.1422)  acc1: 65.6250 (74.1883)  acc5: 88.5417 (91.9801)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.2936 (1.1513)  acc1: 68.2292 (73.9929)  acc5: 91.1458 (91.8957)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0484 (1.1447)  acc1: 75.0000 (74.1513)  acc5: 92.7083 (92.0173)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9243 (1.1437)  acc1: 77.0833 (74.1920)  acc5: 95.3125 (92.0640)  time: 0.1115  data: 0.0001  max mem: 19734
Test: Total time: 0:02:08 (0.4911 s / it)
* Acc@1 74.192 Acc@5 92.064 loss 1.144
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.24%
Epoch: [17]  [   0/1251]  eta: 5:23:53  lr: 0.000019  loss: 3.1851 (3.1851)  time: 15.5340  data: 12.9774  max mem: 19734
Epoch: [17]  [  10/1251]  eta: 0:55:02  lr: 0.000019  loss: 3.6276 (3.6031)  time: 2.6614  data: 1.1812  max mem: 19734
Epoch: [17]  [  20/1251]  eta: 0:36:22  lr: 0.000019  loss: 3.6276 (3.5784)  time: 1.0848  data: 0.0011  max mem: 19734
Epoch: [17]  [  30/1251]  eta: 0:29:39  lr: 0.000019  loss: 3.3421 (3.4206)  time: 0.7955  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4523, ratio_loss=0.0058, pruning_loss=0.1457, mse_loss=0.7200
Epoch: [17]  [  40/1251]  eta: 0:26:16  lr: 0.000019  loss: 3.0451 (3.3485)  time: 0.8076  data: 0.0005  max mem: 19734
Epoch: [17]  [  50/1251]  eta: 0:24:04  lr: 0.000019  loss: 2.7734 (3.2639)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [17]  [  60/1251]  eta: 0:22:35  lr: 0.000019  loss: 3.0783 (3.3056)  time: 0.8010  data: 0.0004  max mem: 19734
Epoch: [17]  [  70/1251]  eta: 0:21:26  lr: 0.000019  loss: 3.6407 (3.3396)  time: 0.8010  data: 0.0004  max mem: 19734
Epoch: [17]  [  80/1251]  eta: 0:20:33  lr: 0.000019  loss: 3.6407 (3.3704)  time: 0.7967  data: 0.0005  max mem: 19734
Epoch: [17]  [  90/1251]  eta: 0:19:51  lr: 0.000019  loss: 3.5651 (3.3925)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [17]  [ 100/1251]  eta: 0:19:17  lr: 0.000019  loss: 3.5651 (3.4216)  time: 0.8116  data: 0.0009  max mem: 19734
Epoch: [17]  [ 110/1251]  eta: 0:18:45  lr: 0.000019  loss: 3.5272 (3.4169)  time: 0.8076  data: 0.0008  max mem: 19734
Epoch: [17]  [ 120/1251]  eta: 0:18:20  lr: 0.000019  loss: 3.4477 (3.4102)  time: 0.8088  data: 0.0004  max mem: 19734
Epoch: [17]  [ 130/1251]  eta: 0:17:56  lr: 0.000019  loss: 3.3392 (3.4146)  time: 0.8152  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3956, ratio_loss=0.0058, pruning_loss=0.1487, mse_loss=0.7442
Epoch: [17]  [ 140/1251]  eta: 0:17:36  lr: 0.000019  loss: 3.3392 (3.4121)  time: 0.8182  data: 0.0004  max mem: 19734
Epoch: [17]  [ 150/1251]  eta: 0:17:18  lr: 0.000019  loss: 3.3362 (3.3990)  time: 0.8287  data: 0.0005  max mem: 19734
Epoch: [17]  [ 160/1251]  eta: 0:17:01  lr: 0.000019  loss: 3.5732 (3.4056)  time: 0.8333  data: 0.0005  max mem: 19734
Epoch: [17]  [ 170/1251]  eta: 0:16:43  lr: 0.000019  loss: 3.4506 (3.4025)  time: 0.8209  data: 0.0005  max mem: 19734
Epoch: [17]  [ 180/1251]  eta: 0:16:27  lr: 0.000019  loss: 3.3700 (3.4044)  time: 0.8024  data: 0.0006  max mem: 19734
Epoch: [17]  [ 190/1251]  eta: 0:16:11  lr: 0.000019  loss: 3.4845 (3.4022)  time: 0.8074  data: 0.0006  max mem: 19734
Epoch: [17]  [ 200/1251]  eta: 0:15:56  lr: 0.000019  loss: 3.4332 (3.3954)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [17]  [ 210/1251]  eta: 0:15:42  lr: 0.000019  loss: 3.4477 (3.4037)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [17]  [ 220/1251]  eta: 0:15:28  lr: 0.000019  loss: 3.5809 (3.4036)  time: 0.8064  data: 0.0007  max mem: 19734
Epoch: [17]  [ 230/1251]  eta: 0:15:15  lr: 0.000019  loss: 3.3662 (3.3986)  time: 0.8064  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3456, ratio_loss=0.0060, pruning_loss=0.1511, mse_loss=0.6950
Epoch: [17]  [ 240/1251]  eta: 0:15:02  lr: 0.000019  loss: 3.2264 (3.3948)  time: 0.8033  data: 0.0008  max mem: 19734
Epoch: [17]  [ 250/1251]  eta: 0:14:50  lr: 0.000019  loss: 3.1772 (3.3893)  time: 0.8066  data: 0.0008  max mem: 19734
Epoch: [17]  [ 260/1251]  eta: 0:14:38  lr: 0.000019  loss: 3.6011 (3.4032)  time: 0.8156  data: 0.0006  max mem: 19734
Epoch: [17]  [ 270/1251]  eta: 0:14:27  lr: 0.000019  loss: 3.5743 (3.3943)  time: 0.8157  data: 0.0006  max mem: 19734
Epoch: [17]  [ 280/1251]  eta: 0:14:15  lr: 0.000019  loss: 2.8844 (3.3874)  time: 0.8104  data: 0.0006  max mem: 19734
Epoch: [17]  [ 290/1251]  eta: 0:14:05  lr: 0.000019  loss: 3.0998 (3.3793)  time: 0.8166  data: 0.0005  max mem: 19734
Epoch: [17]  [ 300/1251]  eta: 0:13:55  lr: 0.000019  loss: 3.3976 (3.3833)  time: 0.8373  data: 0.0005  max mem: 19734
Epoch: [17]  [ 310/1251]  eta: 0:13:44  lr: 0.000019  loss: 3.5224 (3.3908)  time: 0.8264  data: 0.0005  max mem: 19734
Epoch: [17]  [ 320/1251]  eta: 0:13:33  lr: 0.000019  loss: 3.6261 (3.3929)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [17]  [ 330/1251]  eta: 0:13:22  lr: 0.000019  loss: 3.4816 (3.3786)  time: 0.8036  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3009, ratio_loss=0.0059, pruning_loss=0.1513, mse_loss=0.7270
Epoch: [17]  [ 340/1251]  eta: 0:13:12  lr: 0.000019  loss: 3.3276 (3.3764)  time: 0.8160  data: 0.0006  max mem: 19734
Epoch: [17]  [ 350/1251]  eta: 0:13:02  lr: 0.000019  loss: 2.9897 (3.3655)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [17]  [ 360/1251]  eta: 0:12:52  lr: 0.000019  loss: 3.4742 (3.3725)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [17]  [ 370/1251]  eta: 0:12:42  lr: 0.000019  loss: 3.8469 (3.3796)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [17]  [ 380/1251]  eta: 0:12:32  lr: 0.000019  loss: 3.8046 (3.3859)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [17]  [ 390/1251]  eta: 0:12:22  lr: 0.000019  loss: 3.4748 (3.3860)  time: 0.8068  data: 0.0004  max mem: 19734
Epoch: [17]  [ 400/1251]  eta: 0:12:13  lr: 0.000019  loss: 3.2031 (3.3856)  time: 0.8245  data: 0.0005  max mem: 19734
Epoch: [17]  [ 410/1251]  eta: 0:12:03  lr: 0.000019  loss: 3.2736 (3.3862)  time: 0.8328  data: 0.0005  max mem: 19734
Epoch: [17]  [ 420/1251]  eta: 0:11:54  lr: 0.000019  loss: 3.4369 (3.3853)  time: 0.8169  data: 0.0004  max mem: 19734
Epoch: [17]  [ 430/1251]  eta: 0:11:44  lr: 0.000019  loss: 3.4372 (3.3868)  time: 0.8126  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4055, ratio_loss=0.0062, pruning_loss=0.1492, mse_loss=0.7100
Epoch: [17]  [ 440/1251]  eta: 0:11:35  lr: 0.000019  loss: 3.4561 (3.3869)  time: 0.8136  data: 0.0004  max mem: 19734
Epoch: [17]  [ 450/1251]  eta: 0:11:26  lr: 0.000019  loss: 3.4561 (3.3900)  time: 0.8248  data: 0.0005  max mem: 19734
Epoch: [17]  [ 460/1251]  eta: 0:11:16  lr: 0.000019  loss: 3.5168 (3.3929)  time: 0.8184  data: 0.0005  max mem: 19734
Epoch: [17]  [ 470/1251]  eta: 0:11:07  lr: 0.000019  loss: 3.5168 (3.3917)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [17]  [ 480/1251]  eta: 0:10:58  lr: 0.000019  loss: 3.6680 (3.3928)  time: 0.8037  data: 0.0006  max mem: 19734
Epoch: [17]  [ 490/1251]  eta: 0:10:49  lr: 0.000019  loss: 3.6680 (3.3965)  time: 0.8161  data: 0.0006  max mem: 19734
Epoch: [17]  [ 500/1251]  eta: 0:10:39  lr: 0.000019  loss: 3.6947 (3.4022)  time: 0.8177  data: 0.0006  max mem: 19734
Epoch: [17]  [ 510/1251]  eta: 0:10:30  lr: 0.000019  loss: 3.6531 (3.4003)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [17]  [ 520/1251]  eta: 0:10:21  lr: 0.000019  loss: 3.7952 (3.4074)  time: 0.8114  data: 0.0005  max mem: 19734
Epoch: [17]  [ 530/1251]  eta: 0:10:12  lr: 0.000019  loss: 3.3649 (3.3981)  time: 0.8122  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4305, ratio_loss=0.0058, pruning_loss=0.1489, mse_loss=0.7450
Epoch: [17]  [ 540/1251]  eta: 0:10:03  lr: 0.000019  loss: 3.3460 (3.4028)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [17]  [ 550/1251]  eta: 0:09:54  lr: 0.000019  loss: 3.7076 (3.4093)  time: 0.8119  data: 0.0006  max mem: 19734
Epoch: [17]  [ 560/1251]  eta: 0:09:45  lr: 0.000019  loss: 3.7779 (3.4132)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [17]  [ 570/1251]  eta: 0:09:36  lr: 0.000019  loss: 3.6560 (3.4109)  time: 0.8118  data: 0.0005  max mem: 19734
Epoch: [17]  [ 580/1251]  eta: 0:09:27  lr: 0.000019  loss: 3.3375 (3.4115)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [17]  [ 590/1251]  eta: 0:09:19  lr: 0.000019  loss: 3.3286 (3.4063)  time: 0.8305  data: 0.0006  max mem: 19734
Epoch: [17]  [ 600/1251]  eta: 0:09:10  lr: 0.000019  loss: 3.1238 (3.4075)  time: 0.8429  data: 0.0004  max mem: 19734
Epoch: [17]  [ 610/1251]  eta: 0:09:02  lr: 0.000019  loss: 3.5959 (3.4093)  time: 0.8209  data: 0.0005  max mem: 19734
Epoch: [17]  [ 620/1251]  eta: 0:08:53  lr: 0.000019  loss: 3.3436 (3.4049)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [17]  [ 630/1251]  eta: 0:08:44  lr: 0.000019  loss: 3.4429 (3.4104)  time: 0.8024  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4518, ratio_loss=0.0058, pruning_loss=0.1440, mse_loss=0.6916
Epoch: [17]  [ 640/1251]  eta: 0:08:35  lr: 0.000019  loss: 3.6537 (3.4098)  time: 0.8063  data: 0.0004  max mem: 19734
Epoch: [17]  [ 650/1251]  eta: 0:08:26  lr: 0.000019  loss: 3.4984 (3.4086)  time: 0.8077  data: 0.0004  max mem: 19734
Epoch: [17]  [ 660/1251]  eta: 0:08:17  lr: 0.000019  loss: 3.6052 (3.4115)  time: 0.8033  data: 0.0004  max mem: 19734
Epoch: [17]  [ 670/1251]  eta: 0:08:09  lr: 0.000019  loss: 3.5776 (3.4111)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [17]  [ 680/1251]  eta: 0:08:00  lr: 0.000019  loss: 3.5697 (3.4136)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [17]  [ 690/1251]  eta: 0:07:51  lr: 0.000019  loss: 3.5768 (3.4124)  time: 0.8170  data: 0.0005  max mem: 19734
Epoch: [17]  [ 700/1251]  eta: 0:07:43  lr: 0.000019  loss: 3.2295 (3.4110)  time: 0.8292  data: 0.0005  max mem: 19734
Epoch: [17]  [ 710/1251]  eta: 0:07:34  lr: 0.000019  loss: 3.2860 (3.4146)  time: 0.8136  data: 0.0005  max mem: 19734
Epoch: [17]  [ 720/1251]  eta: 0:07:26  lr: 0.000019  loss: 3.7566 (3.4169)  time: 0.8062  data: 0.0006  max mem: 19734
Epoch: [17]  [ 730/1251]  eta: 0:07:17  lr: 0.000019  loss: 3.5658 (3.4168)  time: 0.8141  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4334, ratio_loss=0.0058, pruning_loss=0.1471, mse_loss=0.7280
Epoch: [17]  [ 740/1251]  eta: 0:07:09  lr: 0.000019  loss: 3.6619 (3.4212)  time: 0.8393  data: 0.0006  max mem: 19734
Epoch: [17]  [ 750/1251]  eta: 0:07:00  lr: 0.000019  loss: 3.6619 (3.4200)  time: 0.8349  data: 0.0007  max mem: 19734
Epoch: [17]  [ 760/1251]  eta: 0:06:51  lr: 0.000019  loss: 3.2297 (3.4183)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [17]  [ 770/1251]  eta: 0:06:43  lr: 0.000019  loss: 3.2049 (3.4147)  time: 0.7988  data: 0.0005  max mem: 19734
Epoch: [17]  [ 780/1251]  eta: 0:06:34  lr: 0.000019  loss: 3.5642 (3.4175)  time: 0.8126  data: 0.0005  max mem: 19734
Epoch: [17]  [ 790/1251]  eta: 0:06:26  lr: 0.000019  loss: 3.6556 (3.4182)  time: 0.8161  data: 0.0006  max mem: 19734
Epoch: [17]  [ 800/1251]  eta: 0:06:17  lr: 0.000019  loss: 3.6780 (3.4199)  time: 0.8045  data: 0.0007  max mem: 19734
Epoch: [17]  [ 810/1251]  eta: 0:06:09  lr: 0.000019  loss: 3.6396 (3.4183)  time: 0.8031  data: 0.0007  max mem: 19734
Epoch: [17]  [ 820/1251]  eta: 0:06:00  lr: 0.000019  loss: 3.4085 (3.4169)  time: 0.8038  data: 0.0007  max mem: 19734
Epoch: [17]  [ 830/1251]  eta: 0:05:52  lr: 0.000019  loss: 3.3640 (3.4171)  time: 0.8063  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3956, ratio_loss=0.0058, pruning_loss=0.1483, mse_loss=0.7196
Epoch: [17]  [ 840/1251]  eta: 0:05:43  lr: 0.000019  loss: 3.5854 (3.4200)  time: 0.8192  data: 0.0005  max mem: 19734
Epoch: [17]  [ 850/1251]  eta: 0:05:35  lr: 0.000019  loss: 3.4144 (3.4162)  time: 0.8251  data: 0.0005  max mem: 19734
Epoch: [17]  [ 860/1251]  eta: 0:05:26  lr: 0.000019  loss: 3.1270 (3.4167)  time: 0.8130  data: 0.0005  max mem: 19734
Epoch: [17]  [ 870/1251]  eta: 0:05:18  lr: 0.000019  loss: 3.3153 (3.4168)  time: 0.8156  data: 0.0007  max mem: 19734
Epoch: [17]  [ 880/1251]  eta: 0:05:09  lr: 0.000019  loss: 3.3904 (3.4167)  time: 0.8212  data: 0.0007  max mem: 19734
Epoch: [17]  [ 890/1251]  eta: 0:05:01  lr: 0.000019  loss: 3.5707 (3.4162)  time: 0.8270  data: 0.0005  max mem: 19734
Epoch: [17]  [ 900/1251]  eta: 0:04:53  lr: 0.000019  loss: 3.4531 (3.4142)  time: 0.8167  data: 0.0004  max mem: 19734
Epoch: [17]  [ 910/1251]  eta: 0:04:44  lr: 0.000019  loss: 3.6546 (3.4159)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [17]  [ 920/1251]  eta: 0:04:36  lr: 0.000019  loss: 3.7136 (3.4195)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [17]  [ 930/1251]  eta: 0:04:27  lr: 0.000019  loss: 3.6855 (3.4172)  time: 0.8142  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3923, ratio_loss=0.0059, pruning_loss=0.1478, mse_loss=0.6881
Epoch: [17]  [ 940/1251]  eta: 0:04:19  lr: 0.000019  loss: 3.2461 (3.4174)  time: 0.8145  data: 0.0005  max mem: 19734
Epoch: [17]  [ 950/1251]  eta: 0:04:10  lr: 0.000019  loss: 3.5486 (3.4159)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [17]  [ 960/1251]  eta: 0:04:02  lr: 0.000019  loss: 3.5486 (3.4177)  time: 0.7994  data: 0.0004  max mem: 19734
Epoch: [17]  [ 970/1251]  eta: 0:03:54  lr: 0.000019  loss: 3.4432 (3.4170)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [17]  [ 980/1251]  eta: 0:03:45  lr: 0.000019  loss: 3.4768 (3.4191)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [17]  [ 990/1251]  eta: 0:03:37  lr: 0.000019  loss: 3.6092 (3.4211)  time: 0.8149  data: 0.0005  max mem: 19734
Epoch: [17]  [1000/1251]  eta: 0:03:28  lr: 0.000019  loss: 3.7888 (3.4247)  time: 0.8169  data: 0.0006  max mem: 19734
Epoch: [17]  [1010/1251]  eta: 0:03:20  lr: 0.000019  loss: 3.8560 (3.4277)  time: 0.8172  data: 0.0006  max mem: 19734
Epoch: [17]  [1020/1251]  eta: 0:03:12  lr: 0.000019  loss: 3.7479 (3.4278)  time: 0.8118  data: 0.0005  max mem: 19734
Epoch: [17]  [1030/1251]  eta: 0:03:03  lr: 0.000019  loss: 3.4472 (3.4280)  time: 0.8249  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5042, ratio_loss=0.0058, pruning_loss=0.1446, mse_loss=0.7000
Epoch: [17]  [1040/1251]  eta: 0:02:55  lr: 0.000019  loss: 3.3912 (3.4276)  time: 0.8261  data: 0.0005  max mem: 19734
Epoch: [17]  [1050/1251]  eta: 0:02:47  lr: 0.000019  loss: 3.4442 (3.4289)  time: 0.8112  data: 0.0005  max mem: 19734
Epoch: [17]  [1060/1251]  eta: 0:02:38  lr: 0.000019  loss: 3.5999 (3.4294)  time: 0.8047  data: 0.0006  max mem: 19734
Epoch: [17]  [1070/1251]  eta: 0:02:30  lr: 0.000019  loss: 3.7223 (3.4324)  time: 0.8065  data: 0.0006  max mem: 19734
Epoch: [17]  [1080/1251]  eta: 0:02:22  lr: 0.000019  loss: 3.6995 (3.4339)  time: 0.8096  data: 0.0004  max mem: 19734
Epoch: [17]  [1090/1251]  eta: 0:02:13  lr: 0.000019  loss: 3.3710 (3.4327)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [17]  [1100/1251]  eta: 0:02:05  lr: 0.000019  loss: 3.3606 (3.4343)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [17]  [1110/1251]  eta: 0:01:57  lr: 0.000019  loss: 3.6892 (3.4346)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [17]  [1120/1251]  eta: 0:01:48  lr: 0.000019  loss: 3.7198 (3.4361)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [17]  [1130/1251]  eta: 0:01:40  lr: 0.000019  loss: 3.6978 (3.4370)  time: 0.8115  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5115, ratio_loss=0.0055, pruning_loss=0.1433, mse_loss=0.7100
Epoch: [17]  [1140/1251]  eta: 0:01:32  lr: 0.000019  loss: 3.6904 (3.4367)  time: 0.8152  data: 0.0005  max mem: 19734
Epoch: [17]  [1150/1251]  eta: 0:01:23  lr: 0.000019  loss: 3.7045 (3.4386)  time: 0.8058  data: 0.0004  max mem: 19734
Epoch: [17]  [1160/1251]  eta: 0:01:15  lr: 0.000019  loss: 3.5839 (3.4374)  time: 0.8135  data: 0.0004  max mem: 19734
Epoch: [17]  [1170/1251]  eta: 0:01:07  lr: 0.000019  loss: 3.6119 (3.4390)  time: 0.8189  data: 0.0004  max mem: 19734
Epoch: [17]  [1180/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.6368 (3.4410)  time: 0.8282  data: 0.0004  max mem: 19734
Epoch: [17]  [1190/1251]  eta: 0:00:50  lr: 0.000019  loss: 3.6131 (3.4411)  time: 0.8214  data: 0.0009  max mem: 19734
Epoch: [17]  [1200/1251]  eta: 0:00:42  lr: 0.000019  loss: 3.6206 (3.4427)  time: 0.7959  data: 0.0008  max mem: 19734
Epoch: [17]  [1210/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.6340 (3.4416)  time: 0.7934  data: 0.0002  max mem: 19734
Epoch: [17]  [1220/1251]  eta: 0:00:25  lr: 0.000019  loss: 3.5601 (3.4420)  time: 0.7934  data: 0.0001  max mem: 19734
Epoch: [17]  [1230/1251]  eta: 0:00:17  lr: 0.000019  loss: 3.2934 (3.4409)  time: 0.7995  data: 0.0001  max mem: 19734
loss info: cls_loss=3.4639, ratio_loss=0.0057, pruning_loss=0.1449, mse_loss=0.7168
Epoch: [17]  [1240/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.5432 (3.4426)  time: 0.7997  data: 0.0001  max mem: 19734
Epoch: [17]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.7632 (3.4458)  time: 0.7940  data: 0.0002  max mem: 19734
Epoch: [17] Total time: 0:17:16 (0.8286 s / it)
Averaged stats: lr: 0.000019  loss: 3.7632 (3.4454)
Test:  [  0/261]  eta: 1:40:27  loss: 0.7350 (0.7350)  acc1: 82.2917 (82.2917)  acc5: 95.8333 (95.8333)  time: 23.0935  data: 22.9456  max mem: 19734
Test:  [ 10/261]  eta: 0:10:26  loss: 0.7350 (0.7463)  acc1: 83.3333 (83.5227)  acc5: 96.3542 (96.2121)  time: 2.4968  data: 2.0950  max mem: 19734
Test:  [ 20/261]  eta: 0:05:35  loss: 0.9077 (0.9130)  acc1: 79.6875 (78.7698)  acc5: 93.7500 (94.5189)  time: 0.3061  data: 0.0097  max mem: 19734
Test:  [ 30/261]  eta: 0:03:50  loss: 0.8117 (0.8330)  acc1: 82.2917 (81.6028)  acc5: 94.2708 (95.0605)  time: 0.1717  data: 0.0107  max mem: 19734
Test:  [ 40/261]  eta: 0:03:52  loss: 0.6422 (0.8034)  acc1: 86.4583 (82.6601)  acc5: 95.8333 (95.3252)  time: 0.6999  data: 0.4463  max mem: 19734
Test:  [ 50/261]  eta: 0:03:12  loss: 0.9237 (0.8608)  acc1: 78.1250 (80.8619)  acc5: 94.2708 (95.0163)  time: 0.7820  data: 0.4478  max mem: 19734
Test:  [ 60/261]  eta: 0:02:45  loss: 1.0003 (0.8729)  acc1: 75.5208 (80.4389)  acc5: 94.2708 (95.0649)  time: 0.3508  data: 0.0134  max mem: 19734
Test:  [ 70/261]  eta: 0:02:30  loss: 0.9695 (0.8742)  acc1: 75.5208 (79.9516)  acc5: 95.8333 (95.2465)  time: 0.4633  data: 0.1789  max mem: 19734
Test:  [ 80/261]  eta: 0:02:15  loss: 0.8465 (0.8767)  acc1: 78.1250 (80.1247)  acc5: 96.3542 (95.3447)  time: 0.5131  data: 0.1806  max mem: 19734
Test:  [ 90/261]  eta: 0:02:02  loss: 0.8414 (0.8638)  acc1: 82.8125 (80.5403)  acc5: 96.3542 (95.4613)  time: 0.4652  data: 0.0165  max mem: 19734
Test:  [100/261]  eta: 0:01:51  loss: 0.8414 (0.8678)  acc1: 83.8542 (80.4765)  acc5: 95.8333 (95.4827)  time: 0.4787  data: 0.0554  max mem: 19734
Test:  [110/261]  eta: 0:01:41  loss: 0.8920 (0.8897)  acc1: 77.0833 (79.9972)  acc5: 94.2708 (95.1952)  time: 0.4747  data: 0.0571  max mem: 19734
Test:  [120/261]  eta: 0:01:29  loss: 1.2210 (0.9276)  acc1: 71.3542 (79.1322)  acc5: 90.6250 (94.7099)  time: 0.3422  data: 0.0152  max mem: 19734
Test:  [130/261]  eta: 0:01:25  loss: 1.3942 (0.9738)  acc1: 70.3125 (78.1767)  acc5: 88.5417 (94.1039)  time: 0.5508  data: 0.3151  max mem: 19734
Test:  [140/261]  eta: 0:01:17  loss: 1.3159 (1.0015)  acc1: 68.2292 (77.4712)  acc5: 89.5833 (93.8423)  time: 0.6442  data: 0.3178  max mem: 19734
Test:  [150/261]  eta: 0:01:09  loss: 1.2636 (1.0076)  acc1: 72.3958 (77.4386)  acc5: 91.1458 (93.7052)  time: 0.4174  data: 0.0174  max mem: 19734
Test:  [160/261]  eta: 0:01:00  loss: 1.0620 (1.0272)  acc1: 76.5625 (77.0801)  acc5: 91.6667 (93.4200)  time: 0.3701  data: 0.0170  max mem: 19734
Test:  [170/261]  eta: 0:00:52  loss: 1.3115 (1.0583)  acc1: 64.0625 (76.2914)  acc5: 90.1042 (93.0647)  time: 0.2737  data: 0.0100  max mem: 19734
Test:  [180/261]  eta: 0:00:45  loss: 1.4787 (1.0762)  acc1: 62.5000 (75.8460)  acc5: 87.5000 (92.8666)  time: 0.2107  data: 0.0063  max mem: 19734
Test:  [190/261]  eta: 0:00:38  loss: 1.4306 (1.0900)  acc1: 67.7083 (75.5808)  acc5: 90.1042 (92.6892)  time: 0.1658  data: 0.0057  max mem: 19734
Test:  [200/261]  eta: 0:00:32  loss: 1.3951 (1.1061)  acc1: 72.3958 (75.2617)  acc5: 89.0625 (92.4311)  time: 0.3409  data: 0.1455  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.3951 (1.1201)  acc1: 70.3125 (74.9654)  acc5: 87.5000 (92.2221)  time: 0.4504  data: 0.1461  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4562 (1.1384)  acc1: 67.1875 (74.5004)  acc5: 87.5000 (92.0225)  time: 0.4931  data: 0.2266  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4825 (1.1488)  acc1: 67.1875 (74.2402)  acc5: 88.5417 (91.9147)  time: 0.3773  data: 0.2239  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3228 (1.1573)  acc1: 69.2708 (74.0469)  acc5: 90.6250 (91.8590)  time: 0.1205  data: 0.0003  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1209 (1.1511)  acc1: 75.0000 (74.1866)  acc5: 93.2292 (91.9613)  time: 0.1170  data: 0.0003  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9876 (1.1508)  acc1: 76.5625 (74.2080)  acc5: 95.3125 (92.0120)  time: 0.1119  data: 0.0001  max mem: 19734
Test: Total time: 0:02:03 (0.4715 s / it)
* Acc@1 74.208 Acc@5 92.012 loss 1.151
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.24%
Epoch: [18]  [   0/1251]  eta: 6:52:10  lr: 0.000019  loss: 3.2792 (3.2792)  time: 19.7689  data: 9.3970  max mem: 19734
Epoch: [18]  [  10/1251]  eta: 0:53:31  lr: 0.000019  loss: 3.5222 (3.4669)  time: 2.5881  data: 0.8552  max mem: 19734
Epoch: [18]  [  20/1251]  eta: 0:35:46  lr: 0.000019  loss: 3.5222 (3.4419)  time: 0.8428  data: 0.0009  max mem: 19734
Epoch: [18]  [  30/1251]  eta: 0:29:22  lr: 0.000019  loss: 3.5018 (3.3911)  time: 0.8139  data: 0.0006  max mem: 19734
Epoch: [18]  [  40/1251]  eta: 0:25:58  lr: 0.000019  loss: 3.5018 (3.4256)  time: 0.8063  data: 0.0005  max mem: 19734
Epoch: [18]  [  50/1251]  eta: 0:23:50  lr: 0.000019  loss: 3.2039 (3.3237)  time: 0.8005  data: 0.0006  max mem: 19734
Epoch: [18]  [  60/1251]  eta: 0:22:31  lr: 0.000019  loss: 3.5980 (3.3779)  time: 0.8231  data: 0.0006  max mem: 19734
Epoch: [18]  [  70/1251]  eta: 0:21:32  lr: 0.000019  loss: 3.6813 (3.3685)  time: 0.8474  data: 0.0006  max mem: 19734
Epoch: [18]  [  80/1251]  eta: 0:20:38  lr: 0.000019  loss: 3.1447 (3.3600)  time: 0.8245  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4167, ratio_loss=0.0059, pruning_loss=0.1472, mse_loss=0.7116
Epoch: [18]  [  90/1251]  eta: 0:19:58  lr: 0.000019  loss: 3.5256 (3.3779)  time: 0.8109  data: 0.0006  max mem: 19734
Epoch: [18]  [ 100/1251]  eta: 0:19:21  lr: 0.000019  loss: 3.5256 (3.3720)  time: 0.8116  data: 0.0005  max mem: 19734
Epoch: [18]  [ 110/1251]  eta: 0:18:49  lr: 0.000019  loss: 3.6850 (3.4001)  time: 0.7993  data: 0.0005  max mem: 19734
Epoch: [18]  [ 120/1251]  eta: 0:18:21  lr: 0.000019  loss: 3.6850 (3.4130)  time: 0.7976  data: 0.0007  max mem: 19734
Epoch: [18]  [ 130/1251]  eta: 0:17:57  lr: 0.000019  loss: 3.4452 (3.4142)  time: 0.7986  data: 0.0006  max mem: 19734
Epoch: [18]  [ 140/1251]  eta: 0:17:34  lr: 0.000019  loss: 3.6573 (3.4331)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [18]  [ 150/1251]  eta: 0:17:14  lr: 0.000019  loss: 3.6724 (3.4321)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [18]  [ 160/1251]  eta: 0:16:56  lr: 0.000019  loss: 3.4082 (3.4177)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [18]  [ 170/1251]  eta: 0:16:41  lr: 0.000019  loss: 3.1159 (3.3893)  time: 0.8248  data: 0.0005  max mem: 19734
Epoch: [18]  [ 180/1251]  eta: 0:16:24  lr: 0.000019  loss: 3.0351 (3.3816)  time: 0.8225  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3769, ratio_loss=0.0056, pruning_loss=0.1467, mse_loss=0.7116
Epoch: [18]  [ 190/1251]  eta: 0:16:09  lr: 0.000019  loss: 3.1681 (3.3784)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [18]  [ 200/1251]  eta: 0:15:56  lr: 0.000019  loss: 3.5512 (3.3867)  time: 0.8227  data: 0.0005  max mem: 19734
Epoch: [18]  [ 210/1251]  eta: 0:15:44  lr: 0.000019  loss: 3.5512 (3.3853)  time: 0.8495  data: 0.0005  max mem: 19734
Epoch: [18]  [ 220/1251]  eta: 0:15:31  lr: 0.000019  loss: 3.4501 (3.3920)  time: 0.8347  data: 0.0005  max mem: 19734
Epoch: [18]  [ 230/1251]  eta: 0:15:17  lr: 0.000019  loss: 3.5826 (3.3914)  time: 0.8055  data: 0.0005  max mem: 19734
Epoch: [18]  [ 240/1251]  eta: 0:15:05  lr: 0.000019  loss: 3.6947 (3.3975)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [18]  [ 250/1251]  eta: 0:14:52  lr: 0.000019  loss: 3.7372 (3.4056)  time: 0.8088  data: 0.0005  max mem: 19734
Epoch: [18]  [ 260/1251]  eta: 0:14:40  lr: 0.000019  loss: 3.5609 (3.4007)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [18]  [ 270/1251]  eta: 0:14:28  lr: 0.000019  loss: 3.5645 (3.4037)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [18]  [ 280/1251]  eta: 0:14:16  lr: 0.000019  loss: 3.5771 (3.4072)  time: 0.8036  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4341, ratio_loss=0.0054, pruning_loss=0.1463, mse_loss=0.6970
Epoch: [18]  [ 290/1251]  eta: 0:14:05  lr: 0.000019  loss: 3.6499 (3.4172)  time: 0.8030  data: 0.0006  max mem: 19734
Epoch: [18]  [ 300/1251]  eta: 0:13:54  lr: 0.000019  loss: 3.5995 (3.4136)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [18]  [ 310/1251]  eta: 0:13:43  lr: 0.000019  loss: 3.3302 (3.4067)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [18]  [ 320/1251]  eta: 0:13:33  lr: 0.000019  loss: 3.1160 (3.4065)  time: 0.8207  data: 0.0005  max mem: 19734
Epoch: [18]  [ 330/1251]  eta: 0:13:22  lr: 0.000019  loss: 3.6147 (3.4121)  time: 0.8132  data: 0.0005  max mem: 19734
Epoch: [18]  [ 340/1251]  eta: 0:13:12  lr: 0.000019  loss: 3.5159 (3.4106)  time: 0.8072  data: 0.0005  max mem: 19734
Epoch: [18]  [ 350/1251]  eta: 0:13:02  lr: 0.000019  loss: 3.5014 (3.4118)  time: 0.8097  data: 0.0005  max mem: 19734
Epoch: [18]  [ 360/1251]  eta: 0:12:53  lr: 0.000019  loss: 3.2019 (3.3988)  time: 0.8339  data: 0.0005  max mem: 19734
Epoch: [18]  [ 370/1251]  eta: 0:12:43  lr: 0.000019  loss: 3.2144 (3.4009)  time: 0.8319  data: 0.0005  max mem: 19734
Epoch: [18]  [ 380/1251]  eta: 0:12:33  lr: 0.000019  loss: 3.5877 (3.4002)  time: 0.8119  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3571, ratio_loss=0.0057, pruning_loss=0.1486, mse_loss=0.7044
Epoch: [18]  [ 390/1251]  eta: 0:12:23  lr: 0.000019  loss: 3.5865 (3.4013)  time: 0.8113  data: 0.0006  max mem: 19734
Epoch: [18]  [ 400/1251]  eta: 0:12:13  lr: 0.000019  loss: 3.6288 (3.4093)  time: 0.8064  data: 0.0006  max mem: 19734
Epoch: [18]  [ 410/1251]  eta: 0:12:03  lr: 0.000019  loss: 3.5983 (3.4077)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [18]  [ 420/1251]  eta: 0:11:53  lr: 0.000019  loss: 3.5066 (3.4111)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [18]  [ 430/1251]  eta: 0:11:44  lr: 0.000019  loss: 3.5129 (3.4105)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [18]  [ 440/1251]  eta: 0:11:34  lr: 0.000019  loss: 3.5129 (3.4122)  time: 0.8026  data: 0.0004  max mem: 19734
Epoch: [18]  [ 450/1251]  eta: 0:11:25  lr: 0.000019  loss: 3.6087 (3.4109)  time: 0.8026  data: 0.0004  max mem: 19734
Epoch: [18]  [ 460/1251]  eta: 0:11:16  lr: 0.000019  loss: 3.4170 (3.4089)  time: 0.8103  data: 0.0004  max mem: 19734
Epoch: [18]  [ 470/1251]  eta: 0:11:06  lr: 0.000019  loss: 3.4824 (3.4127)  time: 0.8209  data: 0.0004  max mem: 19734
Epoch: [18]  [ 480/1251]  eta: 0:10:57  lr: 0.000019  loss: 3.5110 (3.4099)  time: 0.8132  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4255, ratio_loss=0.0060, pruning_loss=0.1474, mse_loss=0.7291
Epoch: [18]  [ 490/1251]  eta: 0:10:48  lr: 0.000019  loss: 3.5778 (3.4153)  time: 0.8098  data: 0.0005  max mem: 19734
Epoch: [18]  [ 500/1251]  eta: 0:10:39  lr: 0.000019  loss: 3.7456 (3.4196)  time: 0.8239  data: 0.0004  max mem: 19734
Epoch: [18]  [ 510/1251]  eta: 0:10:30  lr: 0.000019  loss: 3.5933 (3.4181)  time: 0.8300  data: 0.0004  max mem: 19734
Epoch: [18]  [ 520/1251]  eta: 0:10:21  lr: 0.000019  loss: 3.2776 (3.4158)  time: 0.8167  data: 0.0004  max mem: 19734
Epoch: [18]  [ 530/1251]  eta: 0:10:12  lr: 0.000019  loss: 3.1461 (3.4100)  time: 0.8105  data: 0.0004  max mem: 19734
Epoch: [18]  [ 540/1251]  eta: 0:10:03  lr: 0.000019  loss: 3.1461 (3.4078)  time: 0.8100  data: 0.0004  max mem: 19734
Epoch: [18]  [ 550/1251]  eta: 0:09:54  lr: 0.000019  loss: 3.5538 (3.4100)  time: 0.8034  data: 0.0004  max mem: 19734
Epoch: [18]  [ 560/1251]  eta: 0:09:45  lr: 0.000019  loss: 3.7745 (3.4098)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [18]  [ 570/1251]  eta: 0:09:36  lr: 0.000019  loss: 3.5935 (3.4116)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [18]  [ 580/1251]  eta: 0:09:27  lr: 0.000019  loss: 3.5935 (3.4114)  time: 0.8015  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4006, ratio_loss=0.0059, pruning_loss=0.1471, mse_loss=0.7021
Epoch: [18]  [ 590/1251]  eta: 0:09:18  lr: 0.000019  loss: 3.6574 (3.4145)  time: 0.8038  data: 0.0004  max mem: 19734
Epoch: [18]  [ 600/1251]  eta: 0:09:09  lr: 0.000019  loss: 3.5660 (3.4106)  time: 0.8170  data: 0.0005  max mem: 19734
Epoch: [18]  [ 610/1251]  eta: 0:09:01  lr: 0.000019  loss: 3.3141 (3.4111)  time: 0.8243  data: 0.0005  max mem: 19734
Epoch: [18]  [ 620/1251]  eta: 0:08:52  lr: 0.000019  loss: 3.3141 (3.4111)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [18]  [ 630/1251]  eta: 0:08:43  lr: 0.000019  loss: 3.6725 (3.4126)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [18]  [ 640/1251]  eta: 0:08:34  lr: 0.000019  loss: 3.6158 (3.4143)  time: 0.8085  data: 0.0005  max mem: 19734
Epoch: [18]  [ 650/1251]  eta: 0:08:26  lr: 0.000019  loss: 3.4978 (3.4104)  time: 0.8345  data: 0.0005  max mem: 19734
Epoch: [18]  [ 660/1251]  eta: 0:08:17  lr: 0.000019  loss: 3.3004 (3.4099)  time: 0.8309  data: 0.0006  max mem: 19734
Epoch: [18]  [ 670/1251]  eta: 0:08:08  lr: 0.000019  loss: 3.4050 (3.4068)  time: 0.8095  data: 0.0006  max mem: 19734
Epoch: [18]  [ 680/1251]  eta: 0:08:00  lr: 0.000019  loss: 3.6360 (3.4102)  time: 0.8143  data: 0.0008  max mem: 19734
loss info: cls_loss=3.3626, ratio_loss=0.0057, pruning_loss=0.1469, mse_loss=0.7055
Epoch: [18]  [ 690/1251]  eta: 0:07:51  lr: 0.000019  loss: 3.6988 (3.4091)  time: 0.8110  data: 0.0008  max mem: 19734
Epoch: [18]  [ 700/1251]  eta: 0:07:42  lr: 0.000019  loss: 3.5570 (3.4104)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [18]  [ 710/1251]  eta: 0:07:34  lr: 0.000019  loss: 3.5641 (3.4127)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [18]  [ 720/1251]  eta: 0:07:25  lr: 0.000019  loss: 3.5667 (3.4157)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [18]  [ 730/1251]  eta: 0:07:17  lr: 0.000019  loss: 3.4112 (3.4132)  time: 0.8061  data: 0.0005  max mem: 19734
Epoch: [18]  [ 740/1251]  eta: 0:07:08  lr: 0.000019  loss: 3.3216 (3.4139)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [18]  [ 750/1251]  eta: 0:06:59  lr: 0.000019  loss: 3.6149 (3.4173)  time: 0.8153  data: 0.0006  max mem: 19734
Epoch: [18]  [ 760/1251]  eta: 0:06:51  lr: 0.000019  loss: 3.6043 (3.4172)  time: 0.8214  data: 0.0005  max mem: 19734
Epoch: [18]  [ 770/1251]  eta: 0:06:42  lr: 0.000019  loss: 3.0090 (3.4117)  time: 0.8115  data: 0.0005  max mem: 19734
Epoch: [18]  [ 780/1251]  eta: 0:06:34  lr: 0.000019  loss: 3.3752 (3.4109)  time: 0.8072  data: 0.0009  max mem: 19734
loss info: cls_loss=3.4122, ratio_loss=0.0054, pruning_loss=0.1448, mse_loss=0.7114
Epoch: [18]  [ 790/1251]  eta: 0:06:25  lr: 0.000019  loss: 3.5565 (3.4134)  time: 0.8092  data: 0.0009  max mem: 19734
Epoch: [18]  [ 800/1251]  eta: 0:06:17  lr: 0.000019  loss: 3.8168 (3.4171)  time: 0.8303  data: 0.0006  max mem: 19734
Epoch: [18]  [ 810/1251]  eta: 0:06:08  lr: 0.000019  loss: 3.6739 (3.4169)  time: 0.8281  data: 0.0005  max mem: 19734
Epoch: [18]  [ 820/1251]  eta: 0:06:00  lr: 0.000019  loss: 3.5952 (3.4158)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [18]  [ 830/1251]  eta: 0:05:51  lr: 0.000019  loss: 3.6663 (3.4175)  time: 0.8171  data: 0.0006  max mem: 19734
Epoch: [18]  [ 840/1251]  eta: 0:05:43  lr: 0.000019  loss: 3.4432 (3.4167)  time: 0.8141  data: 0.0006  max mem: 19734
Epoch: [18]  [ 850/1251]  eta: 0:05:34  lr: 0.000019  loss: 3.4688 (3.4164)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [18]  [ 860/1251]  eta: 0:05:26  lr: 0.000019  loss: 3.5610 (3.4153)  time: 0.8045  data: 0.0006  max mem: 19734
Epoch: [18]  [ 870/1251]  eta: 0:05:17  lr: 0.000019  loss: 3.6468 (3.4141)  time: 0.8039  data: 0.0006  max mem: 19734
Epoch: [18]  [ 880/1251]  eta: 0:05:09  lr: 0.000019  loss: 3.4234 (3.4122)  time: 0.8025  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3848, ratio_loss=0.0059, pruning_loss=0.1456, mse_loss=0.6808
Epoch: [18]  [ 890/1251]  eta: 0:05:01  lr: 0.000019  loss: 3.4234 (3.4098)  time: 0.8142  data: 0.0004  max mem: 19734
Epoch: [18]  [ 900/1251]  eta: 0:04:52  lr: 0.000019  loss: 3.4533 (3.4087)  time: 0.8176  data: 0.0004  max mem: 19734
Epoch: [18]  [ 910/1251]  eta: 0:04:44  lr: 0.000019  loss: 3.5430 (3.4071)  time: 0.8080  data: 0.0004  max mem: 19734
Epoch: [18]  [ 920/1251]  eta: 0:04:35  lr: 0.000019  loss: 3.5430 (3.4071)  time: 0.8080  data: 0.0004  max mem: 19734
Epoch: [18]  [ 930/1251]  eta: 0:04:27  lr: 0.000019  loss: 3.5667 (3.4086)  time: 0.8116  data: 0.0005  max mem: 19734
Epoch: [18]  [ 940/1251]  eta: 0:04:19  lr: 0.000019  loss: 3.6964 (3.4094)  time: 0.8206  data: 0.0006  max mem: 19734
Epoch: [18]  [ 950/1251]  eta: 0:04:10  lr: 0.000019  loss: 3.5877 (3.4107)  time: 0.8363  data: 0.0005  max mem: 19734
Epoch: [18]  [ 960/1251]  eta: 0:04:02  lr: 0.000019  loss: 3.5597 (3.4115)  time: 0.8254  data: 0.0005  max mem: 19734
Epoch: [18]  [ 970/1251]  eta: 0:03:53  lr: 0.000019  loss: 3.5415 (3.4107)  time: 0.8101  data: 0.0005  max mem: 19734
Epoch: [18]  [ 980/1251]  eta: 0:03:45  lr: 0.000019  loss: 3.1650 (3.4081)  time: 0.8081  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3619, ratio_loss=0.0059, pruning_loss=0.1453, mse_loss=0.7291
Epoch: [18]  [ 990/1251]  eta: 0:03:37  lr: 0.000019  loss: 3.1650 (3.4086)  time: 0.8031  data: 0.0004  max mem: 19734
Epoch: [18]  [1000/1251]  eta: 0:03:28  lr: 0.000019  loss: 3.6064 (3.4098)  time: 0.8061  data: 0.0004  max mem: 19734
Epoch: [18]  [1010/1251]  eta: 0:03:20  lr: 0.000019  loss: 3.3526 (3.4085)  time: 0.8075  data: 0.0005  max mem: 19734
Epoch: [18]  [1020/1251]  eta: 0:03:12  lr: 0.000019  loss: 3.4553 (3.4088)  time: 0.8098  data: 0.0004  max mem: 19734
Epoch: [18]  [1030/1251]  eta: 0:03:03  lr: 0.000019  loss: 3.5883 (3.4090)  time: 0.8098  data: 0.0004  max mem: 19734
Epoch: [18]  [1040/1251]  eta: 0:02:55  lr: 0.000019  loss: 3.6188 (3.4104)  time: 0.8134  data: 0.0004  max mem: 19734
Epoch: [18]  [1050/1251]  eta: 0:02:47  lr: 0.000019  loss: 3.6642 (3.4122)  time: 0.8158  data: 0.0005  max mem: 19734
Epoch: [18]  [1060/1251]  eta: 0:02:38  lr: 0.000019  loss: 3.6202 (3.4134)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [18]  [1070/1251]  eta: 0:02:30  lr: 0.000019  loss: 3.4943 (3.4136)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [18]  [1080/1251]  eta: 0:02:21  lr: 0.000019  loss: 3.4823 (3.4152)  time: 0.8086  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4680, ratio_loss=0.0058, pruning_loss=0.1412, mse_loss=0.6701
Epoch: [18]  [1090/1251]  eta: 0:02:13  lr: 0.000019  loss: 3.8124 (3.4184)  time: 0.8354  data: 0.0005  max mem: 19734
Epoch: [18]  [1100/1251]  eta: 0:02:05  lr: 0.000019  loss: 3.8143 (3.4204)  time: 0.8288  data: 0.0005  max mem: 19734
Epoch: [18]  [1110/1251]  eta: 0:01:57  lr: 0.000019  loss: 3.5100 (3.4208)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [18]  [1120/1251]  eta: 0:01:48  lr: 0.000019  loss: 3.4965 (3.4209)  time: 0.8082  data: 0.0005  max mem: 19734
Epoch: [18]  [1130/1251]  eta: 0:01:40  lr: 0.000019  loss: 3.4139 (3.4194)  time: 0.8099  data: 0.0005  max mem: 19734
Epoch: [18]  [1140/1251]  eta: 0:01:32  lr: 0.000019  loss: 3.4636 (3.4188)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [18]  [1150/1251]  eta: 0:01:23  lr: 0.000019  loss: 3.5682 (3.4190)  time: 0.8099  data: 0.0005  max mem: 19734
Epoch: [18]  [1160/1251]  eta: 0:01:15  lr: 0.000019  loss: 3.6812 (3.4213)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [18]  [1170/1251]  eta: 0:01:07  lr: 0.000019  loss: 3.6812 (3.4209)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [18]  [1180/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.3440 (3.4215)  time: 0.8027  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4628, ratio_loss=0.0054, pruning_loss=0.1437, mse_loss=0.7086
Epoch: [18]  [1190/1251]  eta: 0:00:50  lr: 0.000019  loss: 3.4232 (3.4216)  time: 0.8082  data: 0.0008  max mem: 19734
Epoch: [18]  [1200/1251]  eta: 0:00:42  lr: 0.000019  loss: 3.6200 (3.4223)  time: 0.8111  data: 0.0006  max mem: 19734
Epoch: [18]  [1210/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.6000 (3.4206)  time: 0.8010  data: 0.0001  max mem: 19734
Epoch: [18]  [1220/1251]  eta: 0:00:25  lr: 0.000019  loss: 3.4224 (3.4215)  time: 0.7939  data: 0.0001  max mem: 19734
Epoch: [18]  [1230/1251]  eta: 0:00:17  lr: 0.000019  loss: 3.4555 (3.4217)  time: 0.7964  data: 0.0001  max mem: 19734
Epoch: [18]  [1240/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.4555 (3.4207)  time: 0.8167  data: 0.0001  max mem: 19734
Epoch: [18]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.5373 (3.4226)  time: 0.8134  data: 0.0001  max mem: 19734
Epoch: [18] Total time: 0:17:16 (0.8283 s / it)
Averaged stats: lr: 0.000019  loss: 3.5373 (3.4393)
Test:  [  0/261]  eta: 1:17:32  loss: 0.7320 (0.7320)  acc1: 82.8125 (82.8125)  acc5: 94.7917 (94.7917)  time: 17.8250  data: 17.6938  max mem: 19734
Test:  [ 10/261]  eta: 0:11:16  loss: 0.7320 (0.7391)  acc1: 85.4167 (84.0436)  acc5: 96.3542 (96.1174)  time: 2.6971  data: 2.5614  max mem: 19734
Test:  [ 20/261]  eta: 0:06:00  loss: 0.9574 (0.9055)  acc1: 79.1667 (79.4891)  acc5: 93.2292 (94.6181)  time: 0.6806  data: 0.5318  max mem: 19734
Test:  [ 30/261]  eta: 0:04:25  loss: 0.8210 (0.8279)  acc1: 82.8125 (81.8716)  acc5: 94.2708 (95.1613)  time: 0.3010  data: 0.0149  max mem: 19734
Test:  [ 40/261]  eta: 0:03:29  loss: 0.5987 (0.7989)  acc1: 85.9375 (82.8252)  acc5: 96.8750 (95.4014)  time: 0.3680  data: 0.0369  max mem: 19734
Test:  [ 50/261]  eta: 0:02:58  loss: 0.9077 (0.8596)  acc1: 78.6458 (81.0151)  acc5: 94.7917 (95.0163)  time: 0.3718  data: 0.0928  max mem: 19734
Test:  [ 60/261]  eta: 0:02:30  loss: 0.9732 (0.8716)  acc1: 75.5208 (80.5328)  acc5: 94.2708 (95.0137)  time: 0.3444  data: 0.0669  max mem: 19734
Test:  [ 70/261]  eta: 0:02:16  loss: 0.9417 (0.8742)  acc1: 76.0417 (80.0836)  acc5: 95.8333 (95.2318)  time: 0.3895  data: 0.0146  max mem: 19734
Test:  [ 80/261]  eta: 0:02:06  loss: 0.8627 (0.8762)  acc1: 81.2500 (80.1762)  acc5: 96.8750 (95.2996)  time: 0.5527  data: 0.1781  max mem: 19734
Test:  [ 90/261]  eta: 0:01:56  loss: 0.8211 (0.8612)  acc1: 83.8542 (80.5975)  acc5: 95.8333 (95.4212)  time: 0.5587  data: 0.1768  max mem: 19734
Test:  [100/261]  eta: 0:01:56  loss: 0.8075 (0.8629)  acc1: 84.3750 (80.5848)  acc5: 95.8333 (95.4672)  time: 0.8118  data: 0.3932  max mem: 19734
Test:  [110/261]  eta: 0:01:46  loss: 0.8541 (0.8854)  acc1: 77.0833 (80.0394)  acc5: 94.7917 (95.2093)  time: 0.8218  data: 0.3911  max mem: 19734
Test:  [120/261]  eta: 0:01:36  loss: 1.2550 (0.9248)  acc1: 70.3125 (79.0375)  acc5: 89.5833 (94.6927)  time: 0.5064  data: 0.0146  max mem: 19734
Test:  [130/261]  eta: 0:01:27  loss: 1.3658 (0.9693)  acc1: 68.2292 (78.1489)  acc5: 88.0208 (94.1357)  time: 0.4426  data: 0.0137  max mem: 19734
Test:  [140/261]  eta: 0:01:18  loss: 1.3526 (0.9954)  acc1: 68.2292 (77.4638)  acc5: 89.0625 (93.8534)  time: 0.4417  data: 0.0123  max mem: 19734
Test:  [150/261]  eta: 0:01:11  loss: 1.1920 (1.0019)  acc1: 72.9167 (77.4145)  acc5: 91.1458 (93.6914)  time: 0.4971  data: 0.0159  max mem: 19734
Test:  [160/261]  eta: 0:01:02  loss: 1.0521 (1.0209)  acc1: 77.6042 (77.0575)  acc5: 91.6667 (93.4136)  time: 0.4217  data: 0.0151  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.3459 (1.0516)  acc1: 63.5417 (76.2640)  acc5: 88.5417 (93.0860)  time: 0.3361  data: 0.0122  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: 1.5054 (1.0692)  acc1: 64.5833 (75.8546)  acc5: 88.5417 (92.9184)  time: 0.3125  data: 0.0147  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: 1.3842 (1.0835)  acc1: 66.6667 (75.5754)  acc5: 90.1042 (92.7492)  time: 0.2908  data: 0.0139  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: 1.3855 (1.0996)  acc1: 71.3542 (75.2280)  acc5: 89.0625 (92.4959)  time: 0.2271  data: 0.0086  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.3855 (1.1143)  acc1: 69.2708 (74.9383)  acc5: 87.5000 (92.2887)  time: 0.1736  data: 0.0485  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4779 (1.1336)  acc1: 65.6250 (74.4509)  acc5: 87.5000 (92.0697)  time: 0.1702  data: 0.0501  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.4699 (1.1426)  acc1: 66.1458 (74.2289)  acc5: 88.5417 (91.9801)  time: 0.1199  data: 0.0040  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3670 (1.1517)  acc1: 67.7083 (74.0361)  acc5: 91.1458 (91.9152)  time: 0.1153  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0943 (1.1459)  acc1: 76.5625 (74.1949)  acc5: 92.7083 (92.0111)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9739 (1.1445)  acc1: 76.5625 (74.2220)  acc5: 94.7917 (92.0840)  time: 0.1119  data: 0.0001  max mem: 19734
Test: Total time: 0:01:59 (0.4572 s / it)
* Acc@1 74.222 Acc@5 92.084 loss 1.144
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.24%
Epoch: [19]  [   0/1251]  eta: 6:01:27  lr: 0.000019  loss: 3.3247 (3.3247)  time: 17.3360  data: 14.5465  max mem: 19734
Epoch: [19]  [  10/1251]  eta: 0:51:50  lr: 0.000019  loss: 3.3247 (3.4028)  time: 2.5064  data: 1.3260  max mem: 19734
Epoch: [19]  [  20/1251]  eta: 0:34:43  lr: 0.000019  loss: 3.2105 (3.2687)  time: 0.9105  data: 0.0023  max mem: 19734
loss info: cls_loss=3.3692, ratio_loss=0.0056, pruning_loss=0.1468, mse_loss=0.6740
Epoch: [19]  [  30/1251]  eta: 0:28:34  lr: 0.000019  loss: 3.0454 (3.2881)  time: 0.7986  data: 0.0006  max mem: 19734
Epoch: [19]  [  40/1251]  eta: 0:25:21  lr: 0.000019  loss: 3.3738 (3.3024)  time: 0.7982  data: 0.0005  max mem: 19734
Epoch: [19]  [  50/1251]  eta: 0:23:21  lr: 0.000019  loss: 3.3973 (3.2765)  time: 0.7977  data: 0.0005  max mem: 19734
Epoch: [19]  [  60/1251]  eta: 0:21:59  lr: 0.000019  loss: 3.4313 (3.3102)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [19]  [  70/1251]  eta: 0:20:57  lr: 0.000019  loss: 3.6808 (3.3486)  time: 0.8057  data: 0.0004  max mem: 19734
Epoch: [19]  [  80/1251]  eta: 0:20:14  lr: 0.000019  loss: 3.5424 (3.3284)  time: 0.8214  data: 0.0005  max mem: 19734
Epoch: [19]  [  90/1251]  eta: 0:19:33  lr: 0.000019  loss: 3.3827 (3.3594)  time: 0.8176  data: 0.0006  max mem: 19734
Epoch: [19]  [ 100/1251]  eta: 0:18:58  lr: 0.000019  loss: 3.7531 (3.4042)  time: 0.7966  data: 0.0006  max mem: 19734
Epoch: [19]  [ 110/1251]  eta: 0:18:30  lr: 0.000019  loss: 3.6705 (3.3942)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [19]  [ 120/1251]  eta: 0:18:10  lr: 0.000019  loss: 3.5218 (3.4076)  time: 0.8365  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4229, ratio_loss=0.0054, pruning_loss=0.1461, mse_loss=0.6973
Epoch: [19]  [ 130/1251]  eta: 0:17:47  lr: 0.000019  loss: 3.6336 (3.4075)  time: 0.8355  data: 0.0006  max mem: 19734
Epoch: [19]  [ 140/1251]  eta: 0:17:27  lr: 0.000019  loss: 3.6336 (3.4152)  time: 0.8111  data: 0.0006  max mem: 19734
Epoch: [19]  [ 150/1251]  eta: 0:17:07  lr: 0.000019  loss: 3.6581 (3.4114)  time: 0.8117  data: 0.0005  max mem: 19734
Epoch: [19]  [ 160/1251]  eta: 0:16:49  lr: 0.000019  loss: 3.4889 (3.4139)  time: 0.8063  data: 0.0005  max mem: 19734
Epoch: [19]  [ 170/1251]  eta: 0:16:32  lr: 0.000019  loss: 3.5582 (3.4229)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [19]  [ 180/1251]  eta: 0:16:16  lr: 0.000019  loss: 3.5305 (3.4221)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [19]  [ 190/1251]  eta: 0:16:01  lr: 0.000019  loss: 3.5135 (3.4294)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [19]  [ 200/1251]  eta: 0:15:46  lr: 0.000019  loss: 3.6865 (3.4332)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [19]  [ 210/1251]  eta: 0:15:32  lr: 0.000019  loss: 3.6413 (3.4289)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [19]  [ 220/1251]  eta: 0:15:20  lr: 0.000019  loss: 3.3465 (3.4346)  time: 0.8086  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4463, ratio_loss=0.0056, pruning_loss=0.1457, mse_loss=0.6727
Epoch: [19]  [ 230/1251]  eta: 0:15:08  lr: 0.000019  loss: 3.6259 (3.4337)  time: 0.8183  data: 0.0005  max mem: 19734
Epoch: [19]  [ 240/1251]  eta: 0:14:55  lr: 0.000019  loss: 3.7080 (3.4483)  time: 0.8105  data: 0.0005  max mem: 19734
Epoch: [19]  [ 250/1251]  eta: 0:14:42  lr: 0.000019  loss: 3.6536 (3.4414)  time: 0.7979  data: 0.0005  max mem: 19734
Epoch: [19]  [ 260/1251]  eta: 0:14:31  lr: 0.000019  loss: 3.4367 (3.4382)  time: 0.8042  data: 0.0006  max mem: 19734
Epoch: [19]  [ 270/1251]  eta: 0:14:21  lr: 0.000019  loss: 3.1067 (3.4247)  time: 0.8309  data: 0.0006  max mem: 19734
Epoch: [19]  [ 280/1251]  eta: 0:14:10  lr: 0.000019  loss: 3.4550 (3.4265)  time: 0.8303  data: 0.0006  max mem: 19734
Epoch: [19]  [ 290/1251]  eta: 0:13:59  lr: 0.000019  loss: 3.5727 (3.4267)  time: 0.8165  data: 0.0006  max mem: 19734
Epoch: [19]  [ 300/1251]  eta: 0:13:49  lr: 0.000019  loss: 3.4376 (3.4230)  time: 0.8138  data: 0.0005  max mem: 19734
Epoch: [19]  [ 310/1251]  eta: 0:13:38  lr: 0.000019  loss: 3.4376 (3.4220)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [19]  [ 320/1251]  eta: 0:13:27  lr: 0.000019  loss: 3.5389 (3.4260)  time: 0.8049  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4098, ratio_loss=0.0055, pruning_loss=0.1436, mse_loss=0.6817
Epoch: [19]  [ 330/1251]  eta: 0:13:17  lr: 0.000019  loss: 3.5389 (3.4330)  time: 0.8052  data: 0.0004  max mem: 19734
Epoch: [19]  [ 340/1251]  eta: 0:13:06  lr: 0.000019  loss: 3.7653 (3.4352)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [19]  [ 350/1251]  eta: 0:12:56  lr: 0.000019  loss: 3.6307 (3.4318)  time: 0.8034  data: 0.0008  max mem: 19734
Epoch: [19]  [ 360/1251]  eta: 0:12:46  lr: 0.000019  loss: 3.4370 (3.4241)  time: 0.8039  data: 0.0009  max mem: 19734
Epoch: [19]  [ 370/1251]  eta: 0:12:37  lr: 0.000019  loss: 3.4767 (3.4303)  time: 0.8136  data: 0.0007  max mem: 19734
Epoch: [19]  [ 380/1251]  eta: 0:12:27  lr: 0.000019  loss: 3.6577 (3.4342)  time: 0.8168  data: 0.0005  max mem: 19734
Epoch: [19]  [ 390/1251]  eta: 0:12:17  lr: 0.000019  loss: 3.6502 (3.4407)  time: 0.8040  data: 0.0007  max mem: 19734
Epoch: [19]  [ 400/1251]  eta: 0:12:08  lr: 0.000019  loss: 3.6947 (3.4433)  time: 0.8051  data: 0.0007  max mem: 19734
Epoch: [19]  [ 410/1251]  eta: 0:11:58  lr: 0.000019  loss: 3.6337 (3.4400)  time: 0.8201  data: 0.0005  max mem: 19734
Epoch: [19]  [ 420/1251]  eta: 0:11:50  lr: 0.000019  loss: 3.5368 (3.4418)  time: 0.8332  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4575, ratio_loss=0.0055, pruning_loss=0.1445, mse_loss=0.6934
Epoch: [19]  [ 430/1251]  eta: 0:11:40  lr: 0.000019  loss: 3.5467 (3.4437)  time: 0.8205  data: 0.0007  max mem: 19734
Epoch: [19]  [ 440/1251]  eta: 0:11:31  lr: 0.000019  loss: 3.6659 (3.4460)  time: 0.8111  data: 0.0005  max mem: 19734
Epoch: [19]  [ 450/1251]  eta: 0:11:21  lr: 0.000019  loss: 3.6404 (3.4444)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [19]  [ 460/1251]  eta: 0:11:12  lr: 0.000019  loss: 3.5522 (3.4413)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [19]  [ 470/1251]  eta: 0:11:03  lr: 0.000019  loss: 3.1668 (3.4379)  time: 0.8097  data: 0.0009  max mem: 19734
Epoch: [19]  [ 480/1251]  eta: 0:10:54  lr: 0.000019  loss: 3.4880 (3.4402)  time: 0.8059  data: 0.0009  max mem: 19734
Epoch: [19]  [ 490/1251]  eta: 0:10:45  lr: 0.000019  loss: 3.4880 (3.4399)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [19]  [ 500/1251]  eta: 0:10:35  lr: 0.000019  loss: 3.4854 (3.4414)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [19]  [ 510/1251]  eta: 0:10:27  lr: 0.000019  loss: 3.6515 (3.4441)  time: 0.8106  data: 0.0005  max mem: 19734
Epoch: [19]  [ 520/1251]  eta: 0:10:18  lr: 0.000019  loss: 3.2715 (3.4376)  time: 0.8215  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3965, ratio_loss=0.0055, pruning_loss=0.1456, mse_loss=0.7169
Epoch: [19]  [ 530/1251]  eta: 0:10:09  lr: 0.000019  loss: 3.3103 (3.4389)  time: 0.8140  data: 0.0005  max mem: 19734
Epoch: [19]  [ 540/1251]  eta: 0:10:00  lr: 0.000019  loss: 3.6665 (3.4439)  time: 0.8078  data: 0.0004  max mem: 19734
Epoch: [19]  [ 550/1251]  eta: 0:09:51  lr: 0.000019  loss: 3.6665 (3.4446)  time: 0.8157  data: 0.0004  max mem: 19734
Epoch: [19]  [ 560/1251]  eta: 0:09:43  lr: 0.000019  loss: 3.4024 (3.4423)  time: 0.8386  data: 0.0004  max mem: 19734
Epoch: [19]  [ 570/1251]  eta: 0:09:34  lr: 0.000019  loss: 3.4404 (3.4429)  time: 0.8303  data: 0.0005  max mem: 19734
Epoch: [19]  [ 580/1251]  eta: 0:09:25  lr: 0.000019  loss: 3.5457 (3.4432)  time: 0.8016  data: 0.0006  max mem: 19734
Epoch: [19]  [ 590/1251]  eta: 0:09:16  lr: 0.000019  loss: 3.4412 (3.4401)  time: 0.8049  data: 0.0004  max mem: 19734
Epoch: [19]  [ 600/1251]  eta: 0:09:07  lr: 0.000019  loss: 3.2699 (3.4386)  time: 0.8095  data: 0.0004  max mem: 19734
Epoch: [19]  [ 610/1251]  eta: 0:08:59  lr: 0.000019  loss: 3.2352 (3.4366)  time: 0.8072  data: 0.0005  max mem: 19734
Epoch: [19]  [ 620/1251]  eta: 0:08:50  lr: 0.000019  loss: 3.1444 (3.4377)  time: 0.8050  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4137, ratio_loss=0.0055, pruning_loss=0.1439, mse_loss=0.6795
Epoch: [19]  [ 630/1251]  eta: 0:08:41  lr: 0.000019  loss: 3.3913 (3.4383)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [19]  [ 640/1251]  eta: 0:08:32  lr: 0.000019  loss: 3.6063 (3.4413)  time: 0.7995  data: 0.0005  max mem: 19734
Epoch: [19]  [ 650/1251]  eta: 0:08:23  lr: 0.000019  loss: 3.6789 (3.4404)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [19]  [ 660/1251]  eta: 0:08:15  lr: 0.000019  loss: 3.2946 (3.4345)  time: 0.8132  data: 0.0004  max mem: 19734
Epoch: [19]  [ 670/1251]  eta: 0:08:06  lr: 0.000019  loss: 3.3609 (3.4348)  time: 0.8182  data: 0.0004  max mem: 19734
Epoch: [19]  [ 680/1251]  eta: 0:07:58  lr: 0.000019  loss: 2.9058 (3.4276)  time: 0.8072  data: 0.0005  max mem: 19734
Epoch: [19]  [ 690/1251]  eta: 0:07:49  lr: 0.000019  loss: 2.9058 (3.4281)  time: 0.8030  data: 0.0004  max mem: 19734
Epoch: [19]  [ 700/1251]  eta: 0:07:41  lr: 0.000019  loss: 3.6336 (3.4292)  time: 0.8142  data: 0.0004  max mem: 19734
Epoch: [19]  [ 710/1251]  eta: 0:07:32  lr: 0.000019  loss: 3.6016 (3.4318)  time: 0.8376  data: 0.0004  max mem: 19734
Epoch: [19]  [ 720/1251]  eta: 0:07:24  lr: 0.000019  loss: 3.5441 (3.4320)  time: 0.8280  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3676, ratio_loss=0.0056, pruning_loss=0.1449, mse_loss=0.6849
Epoch: [19]  [ 730/1251]  eta: 0:07:15  lr: 0.000019  loss: 3.5079 (3.4316)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [19]  [ 740/1251]  eta: 0:07:07  lr: 0.000019  loss: 3.5161 (3.4313)  time: 0.8126  data: 0.0005  max mem: 19734
Epoch: [19]  [ 750/1251]  eta: 0:06:58  lr: 0.000019  loss: 3.5161 (3.4305)  time: 0.8094  data: 0.0006  max mem: 19734
Epoch: [19]  [ 760/1251]  eta: 0:06:49  lr: 0.000019  loss: 3.4658 (3.4283)  time: 0.8000  data: 0.0005  max mem: 19734
Epoch: [19]  [ 770/1251]  eta: 0:06:41  lr: 0.000019  loss: 3.6383 (3.4336)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [19]  [ 780/1251]  eta: 0:06:32  lr: 0.000019  loss: 3.7679 (3.4360)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [19]  [ 790/1251]  eta: 0:06:24  lr: 0.000019  loss: 3.3974 (3.4336)  time: 0.8000  data: 0.0006  max mem: 19734
Epoch: [19]  [ 800/1251]  eta: 0:06:15  lr: 0.000019  loss: 3.4568 (3.4337)  time: 0.7991  data: 0.0007  max mem: 19734
Epoch: [19]  [ 810/1251]  eta: 0:06:07  lr: 0.000019  loss: 3.6642 (3.4367)  time: 0.8151  data: 0.0006  max mem: 19734
Epoch: [19]  [ 820/1251]  eta: 0:05:58  lr: 0.000019  loss: 3.6488 (3.4359)  time: 0.8167  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4488, ratio_loss=0.0054, pruning_loss=0.1430, mse_loss=0.7073
Epoch: [19]  [ 830/1251]  eta: 0:05:50  lr: 0.000019  loss: 3.4825 (3.4362)  time: 0.8052  data: 0.0004  max mem: 19734
Epoch: [19]  [ 840/1251]  eta: 0:05:42  lr: 0.000019  loss: 3.4459 (3.4349)  time: 0.8070  data: 0.0004  max mem: 19734
Epoch: [19]  [ 850/1251]  eta: 0:05:33  lr: 0.000019  loss: 3.4363 (3.4342)  time: 0.8248  data: 0.0004  max mem: 19734
Epoch: [19]  [ 860/1251]  eta: 0:05:25  lr: 0.000019  loss: 3.5578 (3.4360)  time: 0.8272  data: 0.0004  max mem: 19734
Epoch: [19]  [ 870/1251]  eta: 0:05:16  lr: 0.000019  loss: 3.5578 (3.4350)  time: 0.8106  data: 0.0004  max mem: 19734
Epoch: [19]  [ 880/1251]  eta: 0:05:08  lr: 0.000019  loss: 3.3924 (3.4367)  time: 0.8055  data: 0.0005  max mem: 19734
Epoch: [19]  [ 890/1251]  eta: 0:05:00  lr: 0.000019  loss: 3.6818 (3.4388)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [19]  [ 900/1251]  eta: 0:04:51  lr: 0.000019  loss: 3.5975 (3.4395)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [19]  [ 910/1251]  eta: 0:04:43  lr: 0.000019  loss: 3.5054 (3.4391)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [19]  [ 920/1251]  eta: 0:04:34  lr: 0.000019  loss: 3.4232 (3.4386)  time: 0.8026  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4425, ratio_loss=0.0056, pruning_loss=0.1426, mse_loss=0.6686
Epoch: [19]  [ 930/1251]  eta: 0:04:26  lr: 0.000019  loss: 3.4232 (3.4392)  time: 0.8043  data: 0.0007  max mem: 19734
Epoch: [19]  [ 940/1251]  eta: 0:04:18  lr: 0.000019  loss: 3.4101 (3.4388)  time: 0.8030  data: 0.0009  max mem: 19734
Epoch: [19]  [ 950/1251]  eta: 0:04:09  lr: 0.000019  loss: 3.6593 (3.4405)  time: 0.8086  data: 0.0007  max mem: 19734
Epoch: [19]  [ 960/1251]  eta: 0:04:01  lr: 0.000019  loss: 3.7200 (3.4410)  time: 0.8147  data: 0.0005  max mem: 19734
Epoch: [19]  [ 970/1251]  eta: 0:03:52  lr: 0.000019  loss: 3.7713 (3.4450)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [19]  [ 980/1251]  eta: 0:03:44  lr: 0.000019  loss: 3.7713 (3.4477)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [19]  [ 990/1251]  eta: 0:03:36  lr: 0.000019  loss: 3.7076 (3.4492)  time: 0.8161  data: 0.0005  max mem: 19734
Epoch: [19]  [1000/1251]  eta: 0:03:28  lr: 0.000019  loss: 3.6017 (3.4501)  time: 0.8302  data: 0.0008  max mem: 19734
Epoch: [19]  [1010/1251]  eta: 0:03:19  lr: 0.000019  loss: 3.6825 (3.4511)  time: 0.8186  data: 0.0009  max mem: 19734
Epoch: [19]  [1020/1251]  eta: 0:03:11  lr: 0.000019  loss: 3.1405 (3.4458)  time: 0.8015  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4763, ratio_loss=0.0056, pruning_loss=0.1428, mse_loss=0.7041
Epoch: [19]  [1030/1251]  eta: 0:03:03  lr: 0.000019  loss: 3.0681 (3.4449)  time: 0.7990  data: 0.0004  max mem: 19734
Epoch: [19]  [1040/1251]  eta: 0:02:54  lr: 0.000019  loss: 3.5306 (3.4446)  time: 0.8043  data: 0.0004  max mem: 19734
Epoch: [19]  [1050/1251]  eta: 0:02:46  lr: 0.000019  loss: 3.6423 (3.4459)  time: 0.8052  data: 0.0004  max mem: 19734
Epoch: [19]  [1060/1251]  eta: 0:02:38  lr: 0.000019  loss: 3.3790 (3.4437)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [19]  [1070/1251]  eta: 0:02:29  lr: 0.000019  loss: 3.1297 (3.4431)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [19]  [1080/1251]  eta: 0:02:21  lr: 0.000019  loss: 3.5599 (3.4446)  time: 0.8040  data: 0.0004  max mem: 19734
Epoch: [19]  [1090/1251]  eta: 0:02:13  lr: 0.000019  loss: 3.6092 (3.4456)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [19]  [1100/1251]  eta: 0:02:04  lr: 0.000019  loss: 3.6092 (3.4465)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [19]  [1110/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.3733 (3.4447)  time: 0.8130  data: 0.0005  max mem: 19734
Epoch: [19]  [1120/1251]  eta: 0:01:48  lr: 0.000019  loss: 3.4759 (3.4457)  time: 0.8110  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4020, ratio_loss=0.0054, pruning_loss=0.1445, mse_loss=0.6866
Epoch: [19]  [1130/1251]  eta: 0:01:39  lr: 0.000019  loss: 3.6010 (3.4430)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [19]  [1140/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.3321 (3.4418)  time: 0.8269  data: 0.0005  max mem: 19734
Epoch: [19]  [1150/1251]  eta: 0:01:23  lr: 0.000019  loss: 3.3321 (3.4400)  time: 0.8440  data: 0.0005  max mem: 19734
Epoch: [19]  [1160/1251]  eta: 0:01:15  lr: 0.000019  loss: 3.3445 (3.4398)  time: 0.8211  data: 0.0004  max mem: 19734
Epoch: [19]  [1170/1251]  eta: 0:01:06  lr: 0.000019  loss: 3.6672 (3.4400)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [19]  [1180/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.6147 (3.4407)  time: 0.8065  data: 0.0006  max mem: 19734
Epoch: [19]  [1190/1251]  eta: 0:00:50  lr: 0.000019  loss: 3.5915 (3.4433)  time: 0.8068  data: 0.0008  max mem: 19734
Epoch: [19]  [1200/1251]  eta: 0:00:42  lr: 0.000019  loss: 3.6329 (3.4429)  time: 0.7969  data: 0.0006  max mem: 19734
Epoch: [19]  [1210/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.6913 (3.4465)  time: 0.7924  data: 0.0002  max mem: 19734
Epoch: [19]  [1220/1251]  eta: 0:00:25  lr: 0.000019  loss: 3.6628 (3.4446)  time: 0.7935  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4471, ratio_loss=0.0055, pruning_loss=0.1432, mse_loss=0.7140
Epoch: [19]  [1230/1251]  eta: 0:00:17  lr: 0.000019  loss: 3.5248 (3.4451)  time: 0.7937  data: 0.0001  max mem: 19734
Epoch: [19]  [1240/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.5527 (3.4454)  time: 0.7941  data: 0.0001  max mem: 19734
Epoch: [19]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.5919 (3.4453)  time: 0.8065  data: 0.0002  max mem: 19734
Epoch: [19] Total time: 0:17:12 (0.8254 s / it)
Averaged stats: lr: 0.000019  loss: 3.5919 (3.4370)
Test:  [  0/261]  eta: 2:35:08  loss: 0.7157 (0.7157)  acc1: 82.8125 (82.8125)  acc5: 95.8333 (95.8333)  time: 35.6662  data: 35.2180  max mem: 19734
Test:  [ 10/261]  eta: 0:18:52  loss: 0.7157 (0.7297)  acc1: 84.8958 (83.8542)  acc5: 96.8750 (96.4962)  time: 4.5116  data: 4.1103  max mem: 19734
Test:  [ 20/261]  eta: 0:10:36  loss: 0.9447 (0.9149)  acc1: 80.2083 (79.2163)  acc5: 93.7500 (94.5437)  time: 0.9907  data: 0.5087  max mem: 19734
Test:  [ 30/261]  eta: 0:07:38  loss: 0.8680 (0.8362)  acc1: 83.8542 (81.8716)  acc5: 94.2708 (95.0101)  time: 0.5944  data: 0.0161  max mem: 19734
Test:  [ 40/261]  eta: 0:05:48  loss: 0.5959 (0.8030)  acc1: 88.0208 (82.8379)  acc5: 96.8750 (95.3252)  time: 0.4568  data: 0.0135  max mem: 19734
Test:  [ 50/261]  eta: 0:04:45  loss: 0.9075 (0.8632)  acc1: 78.1250 (80.9641)  acc5: 94.2708 (94.8836)  time: 0.3796  data: 0.0102  max mem: 19734
Test:  [ 60/261]  eta: 0:04:01  loss: 1.0056 (0.8725)  acc1: 76.0417 (80.5755)  acc5: 93.7500 (94.8941)  time: 0.4266  data: 0.0112  max mem: 19734
Test:  [ 70/261]  eta: 0:03:24  loss: 0.9369 (0.8725)  acc1: 78.6458 (80.1276)  acc5: 95.8333 (95.0851)  time: 0.3531  data: 0.0144  max mem: 19734
Test:  [ 80/261]  eta: 0:02:57  loss: 0.8640 (0.8747)  acc1: 81.2500 (80.2019)  acc5: 96.8750 (95.1903)  time: 0.3046  data: 0.0167  max mem: 19734
Test:  [ 90/261]  eta: 0:02:37  loss: 0.8086 (0.8594)  acc1: 83.8542 (80.6033)  acc5: 96.3542 (95.2953)  time: 0.3748  data: 0.0111  max mem: 19734
Test:  [100/261]  eta: 0:02:30  loss: 0.7947 (0.8621)  acc1: 83.8542 (80.5074)  acc5: 95.3125 (95.3538)  time: 0.7671  data: 0.3989  max mem: 19734
Test:  [110/261]  eta: 0:02:12  loss: 0.8466 (0.8829)  acc1: 76.5625 (80.0300)  acc5: 94.7917 (95.0685)  time: 0.6698  data: 0.4047  max mem: 19734
Test:  [120/261]  eta: 0:01:56  loss: 1.2221 (0.9237)  acc1: 70.8333 (79.0418)  acc5: 90.1042 (94.5162)  time: 0.2767  data: 0.0145  max mem: 19734
Test:  [130/261]  eta: 0:01:44  loss: 1.3706 (0.9693)  acc1: 69.2708 (78.1091)  acc5: 87.5000 (93.9051)  time: 0.3481  data: 0.0170  max mem: 19734
Test:  [140/261]  eta: 0:01:35  loss: 1.3528 (0.9960)  acc1: 69.2708 (77.3788)  acc5: 89.0625 (93.6429)  time: 0.5408  data: 0.1497  max mem: 19734
Test:  [150/261]  eta: 0:01:23  loss: 1.2582 (1.0015)  acc1: 71.3542 (77.3489)  acc5: 91.1458 (93.4982)  time: 0.5089  data: 0.1514  max mem: 19734
Test:  [160/261]  eta: 0:01:12  loss: 1.1127 (1.0218)  acc1: 77.6042 (77.0219)  acc5: 91.6667 (93.2130)  time: 0.2534  data: 0.0220  max mem: 19734
Test:  [170/261]  eta: 0:01:02  loss: 1.3260 (1.0514)  acc1: 66.6667 (76.2214)  acc5: 88.0208 (92.8941)  time: 0.1817  data: 0.0277  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 1.4323 (1.0692)  acc1: 64.0625 (75.8201)  acc5: 88.0208 (92.7400)  time: 0.1565  data: 0.0229  max mem: 19734
Test:  [190/261]  eta: 0:00:44  loss: 1.3982 (1.0826)  acc1: 67.1875 (75.5863)  acc5: 90.6250 (92.5884)  time: 0.1419  data: 0.0255  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: 1.3955 (1.0985)  acc1: 71.3542 (75.2643)  acc5: 88.5417 (92.3611)  time: 0.1352  data: 0.0200  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.3778 (1.1119)  acc1: 69.2708 (75.0222)  acc5: 88.5417 (92.1776)  time: 0.1159  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.4050 (1.1304)  acc1: 67.1875 (74.5334)  acc5: 88.0208 (91.9754)  time: 0.1156  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4470 (1.1407)  acc1: 66.6667 (74.2695)  acc5: 88.5417 (91.8741)  time: 0.1153  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.3439 (1.1503)  acc1: 67.1875 (74.0232)  acc5: 90.1042 (91.7963)  time: 0.1155  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0656 (1.1429)  acc1: 74.4792 (74.1928)  acc5: 93.2292 (91.9219)  time: 0.1156  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9744 (1.1421)  acc1: 76.0417 (74.2140)  acc5: 95.3125 (91.9980)  time: 0.1122  data: 0.0001  max mem: 19734
Test: Total time: 0:02:09 (0.4945 s / it)
* Acc@1 74.214 Acc@5 91.998 loss 1.142
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.24%
Epoch: [20]  [   0/1251]  eta: 6:57:28  lr: 0.000018  loss: 3.1981 (3.1981)  time: 20.0228  data: 15.7658  max mem: 19734
Epoch: [20]  [  10/1251]  eta: 0:53:33  lr: 0.000018  loss: 3.5753 (3.5035)  time: 2.5898  data: 1.4676  max mem: 19734
Epoch: [20]  [  20/1251]  eta: 0:35:37  lr: 0.000018  loss: 3.6497 (3.5705)  time: 0.8219  data: 0.0192  max mem: 19734
Epoch: [20]  [  30/1251]  eta: 0:29:27  lr: 0.000018  loss: 3.5595 (3.5194)  time: 0.8199  data: 0.0006  max mem: 19734
Epoch: [20]  [  40/1251]  eta: 0:26:16  lr: 0.000018  loss: 3.3002 (3.4494)  time: 0.8460  data: 0.0005  max mem: 19734
Epoch: [20]  [  50/1251]  eta: 0:24:03  lr: 0.000018  loss: 3.1347 (3.4208)  time: 0.8204  data: 0.0004  max mem: 19734
Epoch: [20]  [  60/1251]  eta: 0:22:32  lr: 0.000018  loss: 3.3315 (3.4207)  time: 0.7947  data: 0.0005  max mem: 19734
Epoch: [20]  [  70/1251]  eta: 0:21:25  lr: 0.000018  loss: 3.4431 (3.4280)  time: 0.7980  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4115, ratio_loss=0.0053, pruning_loss=0.1436, mse_loss=0.6760
Epoch: [20]  [  80/1251]  eta: 0:20:32  lr: 0.000018  loss: 3.4431 (3.4282)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [20]  [  90/1251]  eta: 0:19:50  lr: 0.000018  loss: 3.4344 (3.4058)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [20]  [ 100/1251]  eta: 0:19:15  lr: 0.000018  loss: 2.9664 (3.3681)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [20]  [ 110/1251]  eta: 0:18:44  lr: 0.000018  loss: 3.2616 (3.3717)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [20]  [ 120/1251]  eta: 0:18:17  lr: 0.000018  loss: 3.3812 (3.3752)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [20]  [ 130/1251]  eta: 0:17:55  lr: 0.000018  loss: 3.3975 (3.3731)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [20]  [ 140/1251]  eta: 0:17:34  lr: 0.000018  loss: 3.4020 (3.3728)  time: 0.8212  data: 0.0005  max mem: 19734
Epoch: [20]  [ 150/1251]  eta: 0:17:14  lr: 0.000018  loss: 3.5604 (3.3848)  time: 0.8122  data: 0.0005  max mem: 19734
Epoch: [20]  [ 160/1251]  eta: 0:16:55  lr: 0.000018  loss: 3.5969 (3.3894)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [20]  [ 170/1251]  eta: 0:16:40  lr: 0.000018  loss: 3.4582 (3.3707)  time: 0.8193  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3283, ratio_loss=0.0056, pruning_loss=0.1452, mse_loss=0.6955
Epoch: [20]  [ 180/1251]  eta: 0:16:25  lr: 0.000018  loss: 3.5414 (3.3854)  time: 0.8294  data: 0.0005  max mem: 19734
Epoch: [20]  [ 190/1251]  eta: 0:16:11  lr: 0.000018  loss: 3.6623 (3.3974)  time: 0.8277  data: 0.0005  max mem: 19734
Epoch: [20]  [ 200/1251]  eta: 0:15:55  lr: 0.000018  loss: 3.7000 (3.4171)  time: 0.8154  data: 0.0005  max mem: 19734
Epoch: [20]  [ 210/1251]  eta: 0:15:42  lr: 0.000018  loss: 3.7296 (3.4256)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [20]  [ 220/1251]  eta: 0:15:28  lr: 0.000018  loss: 3.5697 (3.4275)  time: 0.8123  data: 0.0006  max mem: 19734
Epoch: [20]  [ 230/1251]  eta: 0:15:15  lr: 0.000018  loss: 3.5525 (3.4313)  time: 0.8063  data: 0.0006  max mem: 19734
Epoch: [20]  [ 240/1251]  eta: 0:15:02  lr: 0.000018  loss: 3.5022 (3.4272)  time: 0.8003  data: 0.0004  max mem: 19734
Epoch: [20]  [ 250/1251]  eta: 0:14:49  lr: 0.000018  loss: 3.4731 (3.4264)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [20]  [ 260/1251]  eta: 0:14:37  lr: 0.000018  loss: 3.4358 (3.4268)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [20]  [ 270/1251]  eta: 0:14:25  lr: 0.000018  loss: 3.4433 (3.4283)  time: 0.7991  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5076, ratio_loss=0.0054, pruning_loss=0.1418, mse_loss=0.6804
Epoch: [20]  [ 280/1251]  eta: 0:14:15  lr: 0.000018  loss: 3.6496 (3.4335)  time: 0.8216  data: 0.0005  max mem: 19734
Epoch: [20]  [ 290/1251]  eta: 0:14:04  lr: 0.000018  loss: 3.5933 (3.4318)  time: 0.8258  data: 0.0005  max mem: 19734
Epoch: [20]  [ 300/1251]  eta: 0:13:53  lr: 0.000018  loss: 3.4200 (3.4312)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [20]  [ 310/1251]  eta: 0:13:42  lr: 0.000018  loss: 3.4493 (3.4308)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [20]  [ 320/1251]  eta: 0:13:32  lr: 0.000018  loss: 3.4776 (3.4357)  time: 0.8198  data: 0.0006  max mem: 19734
Epoch: [20]  [ 330/1251]  eta: 0:13:23  lr: 0.000018  loss: 3.5228 (3.4338)  time: 0.8422  data: 0.0005  max mem: 19734
Epoch: [20]  [ 340/1251]  eta: 0:13:13  lr: 0.000018  loss: 3.5879 (3.4358)  time: 0.8395  data: 0.0005  max mem: 19734
Epoch: [20]  [ 350/1251]  eta: 0:13:02  lr: 0.000018  loss: 3.6347 (3.4368)  time: 0.8166  data: 0.0007  max mem: 19734
Epoch: [20]  [ 360/1251]  eta: 0:12:52  lr: 0.000018  loss: 3.4793 (3.4295)  time: 0.8103  data: 0.0009  max mem: 19734
Epoch: [20]  [ 370/1251]  eta: 0:12:42  lr: 0.000018  loss: 3.3016 (3.4284)  time: 0.8069  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3751, ratio_loss=0.0055, pruning_loss=0.1439, mse_loss=0.7285
Epoch: [20]  [ 380/1251]  eta: 0:12:32  lr: 0.000018  loss: 3.3025 (3.4261)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [20]  [ 390/1251]  eta: 0:12:22  lr: 0.000018  loss: 3.2929 (3.4216)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [20]  [ 400/1251]  eta: 0:12:12  lr: 0.000018  loss: 3.2929 (3.4194)  time: 0.7988  data: 0.0005  max mem: 19734
Epoch: [20]  [ 410/1251]  eta: 0:12:02  lr: 0.000018  loss: 3.3955 (3.4171)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [20]  [ 420/1251]  eta: 0:11:52  lr: 0.000018  loss: 3.4492 (3.4228)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [20]  [ 430/1251]  eta: 0:11:43  lr: 0.000018  loss: 3.7378 (3.4309)  time: 0.8154  data: 0.0005  max mem: 19734
Epoch: [20]  [ 440/1251]  eta: 0:11:34  lr: 0.000018  loss: 3.5459 (3.4236)  time: 0.8175  data: 0.0005  max mem: 19734
Epoch: [20]  [ 450/1251]  eta: 0:11:24  lr: 0.000018  loss: 2.8928 (3.4176)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [20]  [ 460/1251]  eta: 0:11:15  lr: 0.000018  loss: 3.4621 (3.4195)  time: 0.8174  data: 0.0005  max mem: 19734
Epoch: [20]  [ 470/1251]  eta: 0:11:06  lr: 0.000018  loss: 3.6447 (3.4188)  time: 0.8243  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3643, ratio_loss=0.0056, pruning_loss=0.1450, mse_loss=0.6710
Epoch: [20]  [ 480/1251]  eta: 0:10:57  lr: 0.000018  loss: 3.2638 (3.4141)  time: 0.8175  data: 0.0006  max mem: 19734
Epoch: [20]  [ 490/1251]  eta: 0:10:48  lr: 0.000018  loss: 3.2638 (3.4112)  time: 0.8211  data: 0.0006  max mem: 19734
Epoch: [20]  [ 500/1251]  eta: 0:10:39  lr: 0.000018  loss: 3.3362 (3.4074)  time: 0.8122  data: 0.0006  max mem: 19734
Epoch: [20]  [ 510/1251]  eta: 0:10:29  lr: 0.000018  loss: 3.1502 (3.3999)  time: 0.8043  data: 0.0006  max mem: 19734
Epoch: [20]  [ 520/1251]  eta: 0:10:20  lr: 0.000018  loss: 3.1502 (3.3966)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [20]  [ 530/1251]  eta: 0:10:11  lr: 0.000018  loss: 3.4368 (3.3983)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [20]  [ 540/1251]  eta: 0:10:02  lr: 0.000018  loss: 3.4368 (3.3938)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [20]  [ 550/1251]  eta: 0:09:53  lr: 0.000018  loss: 3.4134 (3.3967)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [20]  [ 560/1251]  eta: 0:09:44  lr: 0.000018  loss: 3.4580 (3.3991)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [20]  [ 570/1251]  eta: 0:09:36  lr: 0.000018  loss: 3.6061 (3.4033)  time: 0.8185  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3099, ratio_loss=0.0052, pruning_loss=0.1478, mse_loss=0.7207
Epoch: [20]  [ 580/1251]  eta: 0:09:27  lr: 0.000018  loss: 3.5564 (3.4034)  time: 0.8172  data: 0.0004  max mem: 19734
Epoch: [20]  [ 590/1251]  eta: 0:09:18  lr: 0.000018  loss: 3.5245 (3.4046)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [20]  [ 600/1251]  eta: 0:09:09  lr: 0.000018  loss: 3.6878 (3.4096)  time: 0.7995  data: 0.0005  max mem: 19734
Epoch: [20]  [ 610/1251]  eta: 0:09:00  lr: 0.000018  loss: 3.7202 (3.4111)  time: 0.8214  data: 0.0005  max mem: 19734
Epoch: [20]  [ 620/1251]  eta: 0:08:52  lr: 0.000018  loss: 3.5736 (3.4102)  time: 0.8459  data: 0.0005  max mem: 19734
Epoch: [20]  [ 630/1251]  eta: 0:08:43  lr: 0.000018  loss: 3.5261 (3.4083)  time: 0.8329  data: 0.0005  max mem: 19734
Epoch: [20]  [ 640/1251]  eta: 0:08:34  lr: 0.000018  loss: 3.5435 (3.4092)  time: 0.8097  data: 0.0004  max mem: 19734
Epoch: [20]  [ 650/1251]  eta: 0:08:26  lr: 0.000018  loss: 3.5435 (3.4070)  time: 0.8028  data: 0.0004  max mem: 19734
Epoch: [20]  [ 660/1251]  eta: 0:08:17  lr: 0.000018  loss: 3.3917 (3.4058)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [20]  [ 670/1251]  eta: 0:08:08  lr: 0.000018  loss: 3.2485 (3.4033)  time: 0.8039  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3695, ratio_loss=0.0054, pruning_loss=0.1461, mse_loss=0.6674
Epoch: [20]  [ 680/1251]  eta: 0:07:59  lr: 0.000018  loss: 3.1115 (3.4009)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [20]  [ 690/1251]  eta: 0:07:51  lr: 0.000018  loss: 3.4682 (3.4052)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [20]  [ 700/1251]  eta: 0:07:42  lr: 0.000018  loss: 3.8499 (3.4085)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [20]  [ 710/1251]  eta: 0:07:33  lr: 0.000018  loss: 3.7306 (3.4073)  time: 0.8014  data: 0.0004  max mem: 19734
Epoch: [20]  [ 720/1251]  eta: 0:07:25  lr: 0.000018  loss: 3.6728 (3.4105)  time: 0.8140  data: 0.0004  max mem: 19734
Epoch: [20]  [ 730/1251]  eta: 0:07:16  lr: 0.000018  loss: 3.5445 (3.4092)  time: 0.8169  data: 0.0004  max mem: 19734
Epoch: [20]  [ 740/1251]  eta: 0:07:08  lr: 0.000018  loss: 3.6528 (3.4123)  time: 0.8081  data: 0.0004  max mem: 19734
Epoch: [20]  [ 750/1251]  eta: 0:06:59  lr: 0.000018  loss: 3.3491 (3.4069)  time: 0.8193  data: 0.0004  max mem: 19734
Epoch: [20]  [ 760/1251]  eta: 0:06:51  lr: 0.000018  loss: 3.1775 (3.4055)  time: 0.8229  data: 0.0004  max mem: 19734
Epoch: [20]  [ 770/1251]  eta: 0:06:42  lr: 0.000018  loss: 3.2702 (3.4066)  time: 0.8319  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4329, ratio_loss=0.0055, pruning_loss=0.1445, mse_loss=0.6521
Epoch: [20]  [ 780/1251]  eta: 0:06:34  lr: 0.000018  loss: 3.4731 (3.4080)  time: 0.8339  data: 0.0004  max mem: 19734
Epoch: [20]  [ 790/1251]  eta: 0:06:25  lr: 0.000018  loss: 3.4987 (3.4101)  time: 0.8124  data: 0.0004  max mem: 19734
Epoch: [20]  [ 800/1251]  eta: 0:06:17  lr: 0.000018  loss: 3.4987 (3.4105)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [20]  [ 810/1251]  eta: 0:06:08  lr: 0.000018  loss: 3.5485 (3.4113)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [20]  [ 820/1251]  eta: 0:06:00  lr: 0.000018  loss: 3.5485 (3.4123)  time: 0.8068  data: 0.0005  max mem: 19734
Epoch: [20]  [ 830/1251]  eta: 0:05:51  lr: 0.000018  loss: 3.7112 (3.4160)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [20]  [ 840/1251]  eta: 0:05:43  lr: 0.000018  loss: 3.7837 (3.4176)  time: 0.8075  data: 0.0004  max mem: 19734
Epoch: [20]  [ 850/1251]  eta: 0:05:34  lr: 0.000018  loss: 3.7713 (3.4201)  time: 0.8049  data: 0.0004  max mem: 19734
Epoch: [20]  [ 860/1251]  eta: 0:05:26  lr: 0.000018  loss: 3.4095 (3.4161)  time: 0.8122  data: 0.0004  max mem: 19734
Epoch: [20]  [ 870/1251]  eta: 0:05:17  lr: 0.000018  loss: 3.1746 (3.4118)  time: 0.8190  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4292, ratio_loss=0.0055, pruning_loss=0.1443, mse_loss=0.6837
Epoch: [20]  [ 880/1251]  eta: 0:05:09  lr: 0.000018  loss: 3.2487 (3.4120)  time: 0.8091  data: 0.0009  max mem: 19734
Epoch: [20]  [ 890/1251]  eta: 0:05:00  lr: 0.000018  loss: 3.2806 (3.4082)  time: 0.8011  data: 0.0006  max mem: 19734
Epoch: [20]  [ 900/1251]  eta: 0:04:52  lr: 0.000018  loss: 3.3069 (3.4094)  time: 0.8088  data: 0.0005  max mem: 19734
Epoch: [20]  [ 910/1251]  eta: 0:04:44  lr: 0.000018  loss: 3.5747 (3.4108)  time: 0.8284  data: 0.0005  max mem: 19734
Epoch: [20]  [ 920/1251]  eta: 0:04:35  lr: 0.000018  loss: 3.6279 (3.4103)  time: 0.8329  data: 0.0006  max mem: 19734
Epoch: [20]  [ 930/1251]  eta: 0:04:27  lr: 0.000018  loss: 3.2428 (3.4077)  time: 0.8240  data: 0.0006  max mem: 19734
Epoch: [20]  [ 940/1251]  eta: 0:04:19  lr: 0.000018  loss: 3.2712 (3.4070)  time: 0.8149  data: 0.0005  max mem: 19734
Epoch: [20]  [ 950/1251]  eta: 0:04:10  lr: 0.000018  loss: 3.4907 (3.4067)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [20]  [ 960/1251]  eta: 0:04:02  lr: 0.000018  loss: 3.4727 (3.4056)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [20]  [ 970/1251]  eta: 0:03:53  lr: 0.000018  loss: 3.3574 (3.4036)  time: 0.8007  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2922, ratio_loss=0.0055, pruning_loss=0.1466, mse_loss=0.7012
Epoch: [20]  [ 980/1251]  eta: 0:03:45  lr: 0.000018  loss: 3.3533 (3.4020)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [20]  [ 990/1251]  eta: 0:03:37  lr: 0.000018  loss: 3.4093 (3.4025)  time: 0.8034  data: 0.0004  max mem: 19734
Epoch: [20]  [1000/1251]  eta: 0:03:28  lr: 0.000018  loss: 3.4572 (3.4020)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [20]  [1010/1251]  eta: 0:03:20  lr: 0.000018  loss: 3.4572 (3.4026)  time: 0.8220  data: 0.0005  max mem: 19734
Epoch: [20]  [1020/1251]  eta: 0:03:11  lr: 0.000018  loss: 3.4724 (3.4026)  time: 0.8213  data: 0.0005  max mem: 19734
Epoch: [20]  [1030/1251]  eta: 0:03:03  lr: 0.000018  loss: 3.2636 (3.4007)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [20]  [1040/1251]  eta: 0:02:55  lr: 0.000018  loss: 3.2635 (3.4002)  time: 0.7985  data: 0.0005  max mem: 19734
Epoch: [20]  [1050/1251]  eta: 0:02:46  lr: 0.000018  loss: 3.3918 (3.3986)  time: 0.8193  data: 0.0004  max mem: 19734
Epoch: [20]  [1060/1251]  eta: 0:02:38  lr: 0.000018  loss: 3.3918 (3.3978)  time: 0.8335  data: 0.0005  max mem: 19734
Epoch: [20]  [1070/1251]  eta: 0:02:30  lr: 0.000018  loss: 3.5258 (3.4009)  time: 0.8152  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3584, ratio_loss=0.0057, pruning_loss=0.1442, mse_loss=0.6706
Epoch: [20]  [1080/1251]  eta: 0:02:21  lr: 0.000018  loss: 3.6781 (3.4009)  time: 0.8181  data: 0.0005  max mem: 19734
Epoch: [20]  [1090/1251]  eta: 0:02:13  lr: 0.000018  loss: 3.6143 (3.4026)  time: 0.8180  data: 0.0005  max mem: 19734
Epoch: [20]  [1100/1251]  eta: 0:02:05  lr: 0.000018  loss: 3.3928 (3.4007)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [20]  [1110/1251]  eta: 0:01:56  lr: 0.000018  loss: 3.2745 (3.4001)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [20]  [1120/1251]  eta: 0:01:48  lr: 0.000018  loss: 3.5526 (3.4016)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [20]  [1130/1251]  eta: 0:01:40  lr: 0.000018  loss: 3.5526 (3.4012)  time: 0.8059  data: 0.0004  max mem: 19734
Epoch: [20]  [1140/1251]  eta: 0:01:32  lr: 0.000018  loss: 3.3438 (3.4000)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [20]  [1150/1251]  eta: 0:01:23  lr: 0.000018  loss: 3.6319 (3.4017)  time: 0.8058  data: 0.0004  max mem: 19734
Epoch: [20]  [1160/1251]  eta: 0:01:15  lr: 0.000018  loss: 3.6243 (3.4029)  time: 0.8188  data: 0.0004  max mem: 19734
Epoch: [20]  [1170/1251]  eta: 0:01:07  lr: 0.000018  loss: 3.5349 (3.4031)  time: 0.8172  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4223, ratio_loss=0.0057, pruning_loss=0.1442, mse_loss=0.6883
Epoch: [20]  [1180/1251]  eta: 0:00:58  lr: 0.000018  loss: 3.5366 (3.4041)  time: 0.8043  data: 0.0006  max mem: 19734
Epoch: [20]  [1190/1251]  eta: 0:00:50  lr: 0.000018  loss: 3.3685 (3.4026)  time: 0.8054  data: 0.0011  max mem: 19734
Epoch: [20]  [1200/1251]  eta: 0:00:42  lr: 0.000018  loss: 3.3172 (3.4036)  time: 0.8149  data: 0.0009  max mem: 19734
Epoch: [20]  [1210/1251]  eta: 0:00:33  lr: 0.000018  loss: 3.5521 (3.4026)  time: 0.8191  data: 0.0002  max mem: 19734
Epoch: [20]  [1220/1251]  eta: 0:00:25  lr: 0.000018  loss: 3.5687 (3.4026)  time: 0.8035  data: 0.0001  max mem: 19734
Epoch: [20]  [1230/1251]  eta: 0:00:17  lr: 0.000018  loss: 3.6300 (3.4038)  time: 0.7974  data: 0.0001  max mem: 19734
Epoch: [20]  [1240/1251]  eta: 0:00:09  lr: 0.000018  loss: 3.6907 (3.4056)  time: 0.7968  data: 0.0001  max mem: 19734
Epoch: [20]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.6640 (3.4063)  time: 0.7930  data: 0.0001  max mem: 19734
Epoch: [20] Total time: 0:17:15 (0.8278 s / it)
Averaged stats: lr: 0.000018  loss: 3.6640 (3.4343)
Test:  [  0/261]  eta: 1:17:03  loss: 0.7662 (0.7662)  acc1: 81.2500 (81.2500)  acc5: 95.3125 (95.3125)  time: 17.7152  data: 17.5397  max mem: 19734
Test:  [ 10/261]  eta: 0:14:01  loss: 0.7181 (0.7410)  acc1: 84.8958 (83.7595)  acc5: 96.8750 (96.4489)  time: 3.3513  data: 3.0918  max mem: 19734
Test:  [ 20/261]  eta: 0:07:40  loss: 0.9376 (0.9110)  acc1: 79.1667 (78.7946)  acc5: 93.7500 (94.9157)  time: 1.1208  data: 0.8317  max mem: 19734
Test:  [ 30/261]  eta: 0:05:45  loss: 0.8512 (0.8277)  acc1: 83.8542 (81.6700)  acc5: 94.2708 (95.3797)  time: 0.4751  data: 0.0209  max mem: 19734
Test:  [ 40/261]  eta: 0:05:08  loss: 0.5885 (0.7953)  acc1: 87.5000 (82.6728)  acc5: 97.3958 (95.6936)  time: 0.8596  data: 0.3224  max mem: 19734
Test:  [ 50/261]  eta: 0:04:09  loss: 0.9208 (0.8553)  acc1: 76.0417 (80.7700)  acc5: 94.2708 (95.1797)  time: 0.6942  data: 0.3173  max mem: 19734
Test:  [ 60/261]  eta: 0:03:29  loss: 0.9706 (0.8670)  acc1: 75.5208 (80.3962)  acc5: 93.7500 (95.1332)  time: 0.3148  data: 0.0163  max mem: 19734
Test:  [ 70/261]  eta: 0:03:08  loss: 0.9405 (0.8714)  acc1: 77.6042 (79.9296)  acc5: 95.3125 (95.2832)  time: 0.4937  data: 0.1270  max mem: 19734
Test:  [ 80/261]  eta: 0:02:43  loss: 0.8301 (0.8726)  acc1: 80.7292 (80.0926)  acc5: 96.3542 (95.3961)  time: 0.4793  data: 0.1262  max mem: 19734
Test:  [ 90/261]  eta: 0:02:21  loss: 0.8188 (0.8575)  acc1: 83.3333 (80.4659)  acc5: 96.3542 (95.4899)  time: 0.2520  data: 0.0133  max mem: 19734
Test:  [100/261]  eta: 0:02:11  loss: 0.8162 (0.8613)  acc1: 83.3333 (80.3888)  acc5: 95.3125 (95.5188)  time: 0.4488  data: 0.2639  max mem: 19734
Test:  [110/261]  eta: 0:01:53  loss: 0.8610 (0.8829)  acc1: 77.6042 (79.9362)  acc5: 94.2708 (95.2374)  time: 0.4236  data: 0.2627  max mem: 19734
Test:  [120/261]  eta: 0:01:41  loss: 1.2509 (0.9223)  acc1: 71.3542 (78.9773)  acc5: 90.1042 (94.7228)  time: 0.2377  data: 0.0081  max mem: 19734
Test:  [130/261]  eta: 0:01:30  loss: 1.3695 (0.9674)  acc1: 68.2292 (78.0375)  acc5: 87.5000 (94.1357)  time: 0.3651  data: 0.0855  max mem: 19734
Test:  [140/261]  eta: 0:01:26  loss: 1.3437 (0.9936)  acc1: 68.2292 (77.3862)  acc5: 88.5417 (93.8571)  time: 0.6906  data: 0.4730  max mem: 19734
Test:  [150/261]  eta: 0:01:16  loss: 1.2038 (0.9987)  acc1: 73.4375 (77.3696)  acc5: 91.6667 (93.6914)  time: 0.6722  data: 0.4414  max mem: 19734
Test:  [160/261]  eta: 0:01:08  loss: 1.0308 (1.0187)  acc1: 77.0833 (77.0316)  acc5: 92.1875 (93.4071)  time: 0.4499  data: 0.1740  max mem: 19734
Test:  [170/261]  eta: 0:00:59  loss: 1.3097 (1.0503)  acc1: 64.0625 (76.2001)  acc5: 88.0208 (93.0616)  time: 0.3666  data: 0.1283  max mem: 19734
Test:  [180/261]  eta: 0:00:50  loss: 1.4816 (1.0674)  acc1: 64.0625 (75.8230)  acc5: 88.5417 (92.9040)  time: 0.1833  data: 0.0055  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: 1.4149 (1.0811)  acc1: 68.7500 (75.5863)  acc5: 90.6250 (92.7247)  time: 0.3009  data: 0.1586  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: 1.3850 (1.0969)  acc1: 70.8333 (75.2617)  acc5: 88.5417 (92.4881)  time: 0.2745  data: 0.1577  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.3850 (1.1103)  acc1: 69.2708 (75.0123)  acc5: 87.5000 (92.2566)  time: 0.1158  data: 0.0005  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4482 (1.1299)  acc1: 67.1875 (74.5357)  acc5: 87.5000 (92.0249)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4482 (1.1394)  acc1: 66.6667 (74.2988)  acc5: 88.0208 (91.9124)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3015 (1.1476)  acc1: 68.7500 (74.1269)  acc5: 90.6250 (91.8504)  time: 0.1150  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0153 (1.1400)  acc1: 76.0417 (74.3090)  acc5: 92.7083 (91.9758)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9230 (1.1397)  acc1: 77.6042 (74.3240)  acc5: 95.3125 (92.0360)  time: 0.1118  data: 0.0001  max mem: 19734
Test: Total time: 0:02:05 (0.4827 s / it)
* Acc@1 74.324 Acc@5 92.036 loss 1.140
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.32%
Epoch: [21]  [   0/1251]  eta: 4:49:35  lr: 0.000018  loss: 3.0595 (3.0595)  time: 13.8893  data: 13.1253  max mem: 19734
Epoch: [21]  [  10/1251]  eta: 0:43:11  lr: 0.000018  loss: 3.6011 (3.6050)  time: 2.0878  data: 1.1937  max mem: 19734
Epoch: [21]  [  20/1251]  eta: 0:30:10  lr: 0.000018  loss: 3.6003 (3.5529)  time: 0.8499  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4355, ratio_loss=0.0055, pruning_loss=0.1400, mse_loss=0.6516
Epoch: [21]  [  30/1251]  eta: 0:25:30  lr: 0.000018  loss: 3.5223 (3.5151)  time: 0.7950  data: 0.0005  max mem: 19734
Epoch: [21]  [  40/1251]  eta: 0:23:01  lr: 0.000018  loss: 3.2535 (3.4609)  time: 0.7949  data: 0.0005  max mem: 19734
Epoch: [21]  [  50/1251]  eta: 0:21:33  lr: 0.000018  loss: 3.2535 (3.4658)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [21]  [  60/1251]  eta: 0:20:33  lr: 0.000018  loss: 3.5924 (3.4523)  time: 0.8201  data: 0.0006  max mem: 19734
Epoch: [21]  [  70/1251]  eta: 0:19:44  lr: 0.000018  loss: 3.7298 (3.4834)  time: 0.8149  data: 0.0006  max mem: 19734
Epoch: [21]  [  80/1251]  eta: 0:19:07  lr: 0.000018  loss: 3.5237 (3.4694)  time: 0.8103  data: 0.0006  max mem: 19734
Epoch: [21]  [  90/1251]  eta: 0:18:41  lr: 0.000018  loss: 3.4480 (3.4785)  time: 0.8330  data: 0.0005  max mem: 19734
Epoch: [21]  [ 100/1251]  eta: 0:18:15  lr: 0.000018  loss: 3.8074 (3.4867)  time: 0.8373  data: 0.0007  max mem: 19734
Epoch: [21]  [ 110/1251]  eta: 0:17:50  lr: 0.000018  loss: 3.6987 (3.4949)  time: 0.8132  data: 0.0007  max mem: 19734
Epoch: [21]  [ 120/1251]  eta: 0:17:28  lr: 0.000018  loss: 3.5110 (3.4957)  time: 0.8009  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4778, ratio_loss=0.0056, pruning_loss=0.1401, mse_loss=0.6680
Epoch: [21]  [ 130/1251]  eta: 0:17:08  lr: 0.000018  loss: 3.4652 (3.4914)  time: 0.8004  data: 0.0006  max mem: 19734
Epoch: [21]  [ 140/1251]  eta: 0:16:49  lr: 0.000018  loss: 3.4652 (3.4770)  time: 0.8005  data: 0.0006  max mem: 19734
Epoch: [21]  [ 150/1251]  eta: 0:16:32  lr: 0.000018  loss: 3.6200 (3.4896)  time: 0.8002  data: 0.0006  max mem: 19734
Epoch: [21]  [ 160/1251]  eta: 0:16:16  lr: 0.000018  loss: 3.6808 (3.4869)  time: 0.7989  data: 0.0006  max mem: 19734
Epoch: [21]  [ 170/1251]  eta: 0:16:02  lr: 0.000018  loss: 3.5945 (3.4846)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [21]  [ 180/1251]  eta: 0:15:48  lr: 0.000018  loss: 3.5495 (3.4872)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [21]  [ 190/1251]  eta: 0:15:36  lr: 0.000018  loss: 3.5585 (3.4858)  time: 0.8184  data: 0.0005  max mem: 19734
Epoch: [21]  [ 200/1251]  eta: 0:15:23  lr: 0.000018  loss: 3.5041 (3.4802)  time: 0.8172  data: 0.0005  max mem: 19734
Epoch: [21]  [ 210/1251]  eta: 0:15:11  lr: 0.000018  loss: 3.2486 (3.4646)  time: 0.8105  data: 0.0006  max mem: 19734
Epoch: [21]  [ 220/1251]  eta: 0:14:59  lr: 0.000018  loss: 3.2691 (3.4645)  time: 0.8107  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3953, ratio_loss=0.0057, pruning_loss=0.1433, mse_loss=0.6675
Epoch: [21]  [ 230/1251]  eta: 0:14:49  lr: 0.000018  loss: 3.5472 (3.4636)  time: 0.8204  data: 0.0005  max mem: 19734
Epoch: [21]  [ 240/1251]  eta: 0:14:39  lr: 0.000018  loss: 3.5744 (3.4575)  time: 0.8383  data: 0.0005  max mem: 19734
Epoch: [21]  [ 250/1251]  eta: 0:14:27  lr: 0.000018  loss: 3.5669 (3.4554)  time: 0.8212  data: 0.0005  max mem: 19734
Epoch: [21]  [ 260/1251]  eta: 0:14:16  lr: 0.000018  loss: 3.5017 (3.4549)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [21]  [ 270/1251]  eta: 0:14:05  lr: 0.000018  loss: 3.6127 (3.4676)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [21]  [ 280/1251]  eta: 0:13:55  lr: 0.000018  loss: 3.7109 (3.4685)  time: 0.8039  data: 0.0005  max mem: 19734
Epoch: [21]  [ 290/1251]  eta: 0:13:45  lr: 0.000018  loss: 3.4451 (3.4665)  time: 0.8065  data: 0.0006  max mem: 19734
Epoch: [21]  [ 300/1251]  eta: 0:13:35  lr: 0.000018  loss: 3.6636 (3.4712)  time: 0.8130  data: 0.0005  max mem: 19734
Epoch: [21]  [ 310/1251]  eta: 0:13:25  lr: 0.000018  loss: 3.6277 (3.4706)  time: 0.8137  data: 0.0004  max mem: 19734
Epoch: [21]  [ 320/1251]  eta: 0:13:15  lr: 0.000018  loss: 3.5244 (3.4675)  time: 0.8092  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4526, ratio_loss=0.0054, pruning_loss=0.1405, mse_loss=0.6858
Epoch: [21]  [ 330/1251]  eta: 0:13:05  lr: 0.000018  loss: 3.5418 (3.4637)  time: 0.8045  data: 0.0004  max mem: 19734
Epoch: [21]  [ 340/1251]  eta: 0:12:56  lr: 0.000018  loss: 3.3918 (3.4618)  time: 0.8193  data: 0.0004  max mem: 19734
Epoch: [21]  [ 350/1251]  eta: 0:12:46  lr: 0.000018  loss: 3.1942 (3.4558)  time: 0.8267  data: 0.0004  max mem: 19734
Epoch: [21]  [ 360/1251]  eta: 0:12:37  lr: 0.000018  loss: 3.1942 (3.4544)  time: 0.8099  data: 0.0004  max mem: 19734
Epoch: [21]  [ 370/1251]  eta: 0:12:27  lr: 0.000018  loss: 3.2192 (3.4448)  time: 0.8039  data: 0.0004  max mem: 19734
Epoch: [21]  [ 380/1251]  eta: 0:12:18  lr: 0.000018  loss: 3.5109 (3.4513)  time: 0.8231  data: 0.0005  max mem: 19734
Epoch: [21]  [ 390/1251]  eta: 0:12:10  lr: 0.000018  loss: 3.7651 (3.4604)  time: 0.8420  data: 0.0005  max mem: 19734
Epoch: [21]  [ 400/1251]  eta: 0:12:00  lr: 0.000018  loss: 3.8980 (3.4648)  time: 0.8230  data: 0.0005  max mem: 19734
Epoch: [21]  [ 410/1251]  eta: 0:11:51  lr: 0.000018  loss: 3.5461 (3.4685)  time: 0.8043  data: 0.0006  max mem: 19734
Epoch: [21]  [ 420/1251]  eta: 0:11:42  lr: 0.000018  loss: 3.4954 (3.4680)  time: 0.8075  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4491, ratio_loss=0.0053, pruning_loss=0.1426, mse_loss=0.6597
Epoch: [21]  [ 430/1251]  eta: 0:11:33  lr: 0.000018  loss: 3.4301 (3.4659)  time: 0.8043  data: 0.0006  max mem: 19734
Epoch: [21]  [ 440/1251]  eta: 0:11:23  lr: 0.000018  loss: 3.5663 (3.4683)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [21]  [ 450/1251]  eta: 0:11:14  lr: 0.000018  loss: 3.5901 (3.4707)  time: 0.8020  data: 0.0004  max mem: 19734
Epoch: [21]  [ 460/1251]  eta: 0:11:05  lr: 0.000018  loss: 3.5702 (3.4691)  time: 0.8014  data: 0.0004  max mem: 19734
Epoch: [21]  [ 470/1251]  eta: 0:10:56  lr: 0.000018  loss: 3.3681 (3.4669)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [21]  [ 480/1251]  eta: 0:10:47  lr: 0.000018  loss: 3.3681 (3.4673)  time: 0.8064  data: 0.0006  max mem: 19734
Epoch: [21]  [ 490/1251]  eta: 0:10:39  lr: 0.000018  loss: 3.1534 (3.4585)  time: 0.8236  data: 0.0006  max mem: 19734
Epoch: [21]  [ 500/1251]  eta: 0:10:30  lr: 0.000018  loss: 3.2601 (3.4605)  time: 0.8294  data: 0.0005  max mem: 19734
Epoch: [21]  [ 510/1251]  eta: 0:10:21  lr: 0.000018  loss: 3.5997 (3.4608)  time: 0.8076  data: 0.0005  max mem: 19734
Epoch: [21]  [ 520/1251]  eta: 0:10:12  lr: 0.000018  loss: 3.5679 (3.4596)  time: 0.8060  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4214, ratio_loss=0.0055, pruning_loss=0.1406, mse_loss=0.6772
Epoch: [21]  [ 530/1251]  eta: 0:10:04  lr: 0.000018  loss: 3.5865 (3.4630)  time: 0.8264  data: 0.0004  max mem: 19734
Epoch: [21]  [ 540/1251]  eta: 0:09:55  lr: 0.000018  loss: 3.5865 (3.4668)  time: 0.8254  data: 0.0005  max mem: 19734
Epoch: [21]  [ 550/1251]  eta: 0:09:46  lr: 0.000018  loss: 3.6200 (3.4658)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [21]  [ 560/1251]  eta: 0:09:37  lr: 0.000018  loss: 3.6317 (3.4674)  time: 0.7975  data: 0.0005  max mem: 19734
Epoch: [21]  [ 570/1251]  eta: 0:09:29  lr: 0.000018  loss: 3.5071 (3.4626)  time: 0.7985  data: 0.0005  max mem: 19734
Epoch: [21]  [ 580/1251]  eta: 0:09:20  lr: 0.000018  loss: 3.4192 (3.4648)  time: 0.8000  data: 0.0006  max mem: 19734
Epoch: [21]  [ 590/1251]  eta: 0:09:11  lr: 0.000018  loss: 3.6317 (3.4691)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [21]  [ 600/1251]  eta: 0:09:02  lr: 0.000018  loss: 3.7187 (3.4702)  time: 0.7965  data: 0.0005  max mem: 19734
Epoch: [21]  [ 610/1251]  eta: 0:08:54  lr: 0.000018  loss: 3.6496 (3.4720)  time: 0.7964  data: 0.0005  max mem: 19734
Epoch: [21]  [ 620/1251]  eta: 0:08:45  lr: 0.000018  loss: 3.5506 (3.4712)  time: 0.8002  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5043, ratio_loss=0.0052, pruning_loss=0.1422, mse_loss=0.6755
Epoch: [21]  [ 630/1251]  eta: 0:08:36  lr: 0.000018  loss: 3.5546 (3.4720)  time: 0.8054  data: 0.0004  max mem: 19734
Epoch: [21]  [ 640/1251]  eta: 0:08:28  lr: 0.000018  loss: 3.5546 (3.4700)  time: 0.8158  data: 0.0005  max mem: 19734
Epoch: [21]  [ 650/1251]  eta: 0:08:19  lr: 0.000018  loss: 3.5331 (3.4703)  time: 0.8135  data: 0.0005  max mem: 19734
Epoch: [21]  [ 660/1251]  eta: 0:08:11  lr: 0.000018  loss: 3.5331 (3.4672)  time: 0.8017  data: 0.0008  max mem: 19734
Epoch: [21]  [ 670/1251]  eta: 0:08:02  lr: 0.000018  loss: 3.2965 (3.4660)  time: 0.8120  data: 0.0008  max mem: 19734
Epoch: [21]  [ 680/1251]  eta: 0:07:54  lr: 0.000018  loss: 3.5439 (3.4674)  time: 0.8317  data: 0.0005  max mem: 19734
Epoch: [21]  [ 690/1251]  eta: 0:07:46  lr: 0.000018  loss: 3.7378 (3.4693)  time: 0.8185  data: 0.0006  max mem: 19734
Epoch: [21]  [ 700/1251]  eta: 0:07:37  lr: 0.000018  loss: 3.6220 (3.4671)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [21]  [ 710/1251]  eta: 0:07:29  lr: 0.000018  loss: 3.4408 (3.4654)  time: 0.8037  data: 0.0004  max mem: 19734
Epoch: [21]  [ 720/1251]  eta: 0:07:20  lr: 0.000018  loss: 3.3350 (3.4639)  time: 0.7981  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4095, ratio_loss=0.0053, pruning_loss=0.1402, mse_loss=0.6607
Epoch: [21]  [ 730/1251]  eta: 0:07:12  lr: 0.000018  loss: 3.4002 (3.4640)  time: 0.8004  data: 0.0006  max mem: 19734
Epoch: [21]  [ 740/1251]  eta: 0:07:03  lr: 0.000018  loss: 3.3500 (3.4615)  time: 0.8021  data: 0.0007  max mem: 19734
Epoch: [21]  [ 750/1251]  eta: 0:06:55  lr: 0.000018  loss: 3.3500 (3.4609)  time: 0.7986  data: 0.0006  max mem: 19734
Epoch: [21]  [ 760/1251]  eta: 0:06:46  lr: 0.000018  loss: 3.5753 (3.4613)  time: 0.7963  data: 0.0004  max mem: 19734
Epoch: [21]  [ 770/1251]  eta: 0:06:38  lr: 0.000018  loss: 3.5753 (3.4600)  time: 0.7973  data: 0.0004  max mem: 19734
Epoch: [21]  [ 780/1251]  eta: 0:06:29  lr: 0.000018  loss: 3.5719 (3.4636)  time: 0.8159  data: 0.0004  max mem: 19734
Epoch: [21]  [ 790/1251]  eta: 0:06:21  lr: 0.000018  loss: 3.5382 (3.4614)  time: 0.8262  data: 0.0004  max mem: 19734
Epoch: [21]  [ 800/1251]  eta: 0:06:13  lr: 0.000018  loss: 3.3041 (3.4600)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [21]  [ 810/1251]  eta: 0:06:04  lr: 0.000018  loss: 3.5866 (3.4594)  time: 0.7957  data: 0.0005  max mem: 19734
Epoch: [21]  [ 820/1251]  eta: 0:05:56  lr: 0.000018  loss: 3.6363 (3.4646)  time: 0.8253  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4467, ratio_loss=0.0054, pruning_loss=0.1416, mse_loss=0.7019
Epoch: [21]  [ 830/1251]  eta: 0:05:48  lr: 0.000018  loss: 3.7354 (3.4656)  time: 0.8356  data: 0.0004  max mem: 19734
Epoch: [21]  [ 840/1251]  eta: 0:05:39  lr: 0.000018  loss: 3.5790 (3.4658)  time: 0.8062  data: 0.0004  max mem: 19734
Epoch: [21]  [ 850/1251]  eta: 0:05:31  lr: 0.000018  loss: 3.4845 (3.4665)  time: 0.7970  data: 0.0004  max mem: 19734
Epoch: [21]  [ 860/1251]  eta: 0:05:22  lr: 0.000018  loss: 3.4845 (3.4669)  time: 0.7976  data: 0.0005  max mem: 19734
Epoch: [21]  [ 870/1251]  eta: 0:05:14  lr: 0.000018  loss: 3.5130 (3.4669)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [21]  [ 880/1251]  eta: 0:05:06  lr: 0.000018  loss: 3.6235 (3.4679)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [21]  [ 890/1251]  eta: 0:04:57  lr: 0.000018  loss: 3.6235 (3.4704)  time: 0.7979  data: 0.0005  max mem: 19734
Epoch: [21]  [ 900/1251]  eta: 0:04:49  lr: 0.000018  loss: 3.5094 (3.4693)  time: 0.7975  data: 0.0005  max mem: 19734
Epoch: [21]  [ 910/1251]  eta: 0:04:41  lr: 0.000018  loss: 3.4993 (3.4688)  time: 0.7980  data: 0.0004  max mem: 19734
Epoch: [21]  [ 920/1251]  eta: 0:04:32  lr: 0.000018  loss: 3.4583 (3.4666)  time: 0.8055  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4569, ratio_loss=0.0053, pruning_loss=0.1407, mse_loss=0.6588
Epoch: [21]  [ 930/1251]  eta: 0:04:24  lr: 0.000018  loss: 3.4672 (3.4687)  time: 0.8176  data: 0.0005  max mem: 19734
Epoch: [21]  [ 940/1251]  eta: 0:04:16  lr: 0.000018  loss: 3.4577 (3.4649)  time: 0.8131  data: 0.0005  max mem: 19734
Epoch: [21]  [ 950/1251]  eta: 0:04:07  lr: 0.000018  loss: 3.2474 (3.4631)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [21]  [ 960/1251]  eta: 0:03:59  lr: 0.000018  loss: 3.2040 (3.4597)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [21]  [ 970/1251]  eta: 0:03:51  lr: 0.000018  loss: 3.2461 (3.4593)  time: 0.8242  data: 0.0005  max mem: 19734
Epoch: [21]  [ 980/1251]  eta: 0:03:43  lr: 0.000018  loss: 3.4601 (3.4579)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [21]  [ 990/1251]  eta: 0:03:34  lr: 0.000018  loss: 3.6500 (3.4608)  time: 0.7981  data: 0.0004  max mem: 19734
Epoch: [21]  [1000/1251]  eta: 0:03:26  lr: 0.000018  loss: 3.6668 (3.4626)  time: 0.7983  data: 0.0004  max mem: 19734
Epoch: [21]  [1010/1251]  eta: 0:03:18  lr: 0.000018  loss: 3.6536 (3.4620)  time: 0.7970  data: 0.0004  max mem: 19734
Epoch: [21]  [1020/1251]  eta: 0:03:10  lr: 0.000018  loss: 3.6888 (3.4635)  time: 0.7989  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3775, ratio_loss=0.0052, pruning_loss=0.1443, mse_loss=0.6753
Epoch: [21]  [1030/1251]  eta: 0:03:01  lr: 0.000018  loss: 3.4145 (3.4607)  time: 0.7983  data: 0.0005  max mem: 19734
Epoch: [21]  [1040/1251]  eta: 0:02:53  lr: 0.000018  loss: 3.2203 (3.4584)  time: 0.7964  data: 0.0005  max mem: 19734
Epoch: [21]  [1050/1251]  eta: 0:02:45  lr: 0.000018  loss: 3.4362 (3.4593)  time: 0.7977  data: 0.0004  max mem: 19734
Epoch: [21]  [1060/1251]  eta: 0:02:36  lr: 0.000018  loss: 3.7045 (3.4601)  time: 0.7971  data: 0.0004  max mem: 19734
Epoch: [21]  [1070/1251]  eta: 0:02:28  lr: 0.000018  loss: 3.2414 (3.4559)  time: 0.8145  data: 0.0004  max mem: 19734
Epoch: [21]  [1080/1251]  eta: 0:02:20  lr: 0.000018  loss: 3.2360 (3.4569)  time: 0.8163  data: 0.0004  max mem: 19734
Epoch: [21]  [1090/1251]  eta: 0:02:12  lr: 0.000018  loss: 3.7451 (3.4578)  time: 0.8079  data: 0.0004  max mem: 19734
Epoch: [21]  [1100/1251]  eta: 0:02:04  lr: 0.000018  loss: 3.3728 (3.4559)  time: 0.8080  data: 0.0004  max mem: 19734
Epoch: [21]  [1110/1251]  eta: 0:01:55  lr: 0.000018  loss: 3.1747 (3.4561)  time: 0.8135  data: 0.0004  max mem: 19734
Epoch: [21]  [1120/1251]  eta: 0:01:47  lr: 0.000018  loss: 3.5713 (3.4576)  time: 0.8288  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3993, ratio_loss=0.0052, pruning_loss=0.1440, mse_loss=0.6769
Epoch: [21]  [1130/1251]  eta: 0:01:39  lr: 0.000018  loss: 3.6584 (3.4579)  time: 0.8149  data: 0.0005  max mem: 19734
Epoch: [21]  [1140/1251]  eta: 0:01:31  lr: 0.000018  loss: 3.6584 (3.4576)  time: 0.8011  data: 0.0004  max mem: 19734
Epoch: [21]  [1150/1251]  eta: 0:01:22  lr: 0.000018  loss: 3.6393 (3.4559)  time: 0.8042  data: 0.0004  max mem: 19734
Epoch: [21]  [1160/1251]  eta: 0:01:14  lr: 0.000018  loss: 3.4628 (3.4544)  time: 0.8013  data: 0.0006  max mem: 19734
Epoch: [21]  [1170/1251]  eta: 0:01:06  lr: 0.000018  loss: 3.4794 (3.4554)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [21]  [1180/1251]  eta: 0:00:58  lr: 0.000018  loss: 3.4794 (3.4545)  time: 0.8029  data: 0.0004  max mem: 19734
Epoch: [21]  [1190/1251]  eta: 0:00:50  lr: 0.000018  loss: 3.2705 (3.4540)  time: 0.8019  data: 0.0007  max mem: 19734
Epoch: [21]  [1200/1251]  eta: 0:00:41  lr: 0.000018  loss: 3.2705 (3.4523)  time: 0.7994  data: 0.0006  max mem: 19734
Epoch: [21]  [1210/1251]  eta: 0:00:33  lr: 0.000018  loss: 3.6169 (3.4537)  time: 0.8018  data: 0.0001  max mem: 19734
Epoch: [21]  [1220/1251]  eta: 0:00:25  lr: 0.000018  loss: 3.7818 (3.4539)  time: 0.8099  data: 0.0002  max mem: 19734
loss info: cls_loss=3.3964, ratio_loss=0.0054, pruning_loss=0.1438, mse_loss=0.6777
Epoch: [21]  [1230/1251]  eta: 0:00:17  lr: 0.000018  loss: 3.4541 (3.4536)  time: 0.8039  data: 0.0001  max mem: 19734
Epoch: [21]  [1240/1251]  eta: 0:00:09  lr: 0.000018  loss: 3.4297 (3.4531)  time: 0.7990  data: 0.0001  max mem: 19734
Epoch: [21]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.3822 (3.4512)  time: 0.8024  data: 0.0002  max mem: 19734
Epoch: [21] Total time: 0:17:06 (0.8203 s / it)
Averaged stats: lr: 0.000018  loss: 3.3822 (3.4289)
Test:  [  0/261]  eta: 2:11:56  loss: 0.7220 (0.7220)  acc1: 83.3333 (83.3333)  acc5: 95.3125 (95.3125)  time: 30.3297  data: 30.1794  max mem: 19734
Test:  [ 10/261]  eta: 0:13:50  loss: 0.7220 (0.7443)  acc1: 83.3333 (83.3807)  acc5: 96.3542 (95.8333)  time: 3.3090  data: 3.0439  max mem: 19734
Test:  [ 20/261]  eta: 0:08:01  loss: 0.9234 (0.9045)  acc1: 79.6875 (79.1915)  acc5: 93.7500 (94.3948)  time: 0.5831  data: 0.2984  max mem: 19734
Test:  [ 30/261]  eta: 0:05:40  loss: 0.8056 (0.8197)  acc1: 82.2917 (81.9220)  acc5: 95.3125 (95.1109)  time: 0.4642  data: 0.1426  max mem: 19734
Test:  [ 40/261]  eta: 0:05:11  loss: 0.5831 (0.7911)  acc1: 87.5000 (82.7998)  acc5: 96.8750 (95.3252)  time: 0.7891  data: 0.4957  max mem: 19734
Test:  [ 50/261]  eta: 0:04:12  loss: 0.9128 (0.8488)  acc1: 78.1250 (81.1683)  acc5: 94.7917 (94.9346)  time: 0.7668  data: 0.4927  max mem: 19734
Test:  [ 60/261]  eta: 0:03:27  loss: 0.9644 (0.8607)  acc1: 76.5625 (80.7292)  acc5: 94.2708 (94.9368)  time: 0.2548  data: 0.0130  max mem: 19734
Test:  [ 70/261]  eta: 0:03:13  loss: 0.9632 (0.8612)  acc1: 76.5625 (80.2523)  acc5: 95.8333 (95.1878)  time: 0.5415  data: 0.2947  max mem: 19734
Test:  [ 80/261]  eta: 0:02:52  loss: 0.8345 (0.8647)  acc1: 78.1250 (80.3755)  acc5: 96.8750 (95.2804)  time: 0.7188  data: 0.3152  max mem: 19734
Test:  [ 90/261]  eta: 0:02:30  loss: 0.7981 (0.8491)  acc1: 83.8542 (80.8265)  acc5: 96.8750 (95.4270)  time: 0.4121  data: 0.0378  max mem: 19734
Test:  [100/261]  eta: 0:02:19  loss: 0.7981 (0.8518)  acc1: 83.3333 (80.8065)  acc5: 95.8333 (95.4827)  time: 0.5084  data: 0.2520  max mem: 19734
Test:  [110/261]  eta: 0:02:06  loss: 0.8586 (0.8755)  acc1: 78.1250 (80.2787)  acc5: 94.2708 (95.1670)  time: 0.6276  data: 0.2464  max mem: 19734
Test:  [120/261]  eta: 0:01:53  loss: 1.2234 (0.9175)  acc1: 72.3958 (79.2786)  acc5: 90.1042 (94.6410)  time: 0.4796  data: 0.0108  max mem: 19734
Test:  [130/261]  eta: 0:01:39  loss: 1.4426 (0.9621)  acc1: 66.1458 (78.3119)  acc5: 86.9792 (94.0840)  time: 0.3210  data: 0.0131  max mem: 19734
Test:  [140/261]  eta: 0:01:30  loss: 1.3479 (0.9890)  acc1: 67.1875 (77.6559)  acc5: 89.5833 (93.7906)  time: 0.3956  data: 0.1770  max mem: 19734
Test:  [150/261]  eta: 0:01:18  loss: 1.2591 (0.9955)  acc1: 73.4375 (77.6352)  acc5: 90.1042 (93.6155)  time: 0.4156  data: 0.1800  max mem: 19734
Test:  [160/261]  eta: 0:01:09  loss: 0.9752 (1.0150)  acc1: 78.6458 (77.2936)  acc5: 91.6667 (93.3359)  time: 0.2868  data: 0.0568  max mem: 19734
Test:  [170/261]  eta: 0:01:00  loss: 1.3002 (1.0459)  acc1: 65.1042 (76.5260)  acc5: 88.0208 (93.0068)  time: 0.3040  data: 0.0978  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 1.4580 (1.0632)  acc1: 64.5833 (76.1222)  acc5: 88.0208 (92.8378)  time: 0.3923  data: 0.1960  max mem: 19734
Test:  [190/261]  eta: 0:00:44  loss: 1.3997 (1.0765)  acc1: 67.1875 (75.8644)  acc5: 90.6250 (92.6783)  time: 0.3466  data: 0.1504  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 1.3669 (1.0932)  acc1: 71.3542 (75.5079)  acc5: 89.0625 (92.4388)  time: 0.1741  data: 0.0099  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.4045 (1.1075)  acc1: 68.7500 (75.2098)  acc5: 88.0208 (92.2369)  time: 0.1837  data: 0.0509  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.4711 (1.1270)  acc1: 67.1875 (74.6913)  acc5: 88.0208 (92.0272)  time: 0.1615  data: 0.0468  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4695 (1.1371)  acc1: 64.5833 (74.4656)  acc5: 88.5417 (91.9282)  time: 0.1148  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.3334 (1.1466)  acc1: 70.3125 (74.2674)  acc5: 90.6250 (91.8482)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0400 (1.1391)  acc1: 76.0417 (74.4460)  acc5: 93.7500 (91.9696)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9474 (1.1389)  acc1: 76.5625 (74.4620)  acc5: 95.3125 (92.0300)  time: 0.1117  data: 0.0001  max mem: 19734
Test: Total time: 0:02:10 (0.4985 s / it)
* Acc@1 74.462 Acc@5 92.030 loss 1.139
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.46%
Epoch: [22]  [   0/1251]  eta: 6:44:14  lr: 0.000018  loss: 3.9437 (3.9437)  time: 19.3880  data: 18.5855  max mem: 19734
Epoch: [22]  [  10/1251]  eta: 0:52:27  lr: 0.000018  loss: 3.6554 (3.6036)  time: 2.5367  data: 1.6901  max mem: 19734
Epoch: [22]  [  20/1251]  eta: 0:35:06  lr: 0.000018  loss: 3.8016 (3.6411)  time: 0.8275  data: 0.0006  max mem: 19734
Epoch: [22]  [  30/1251]  eta: 0:28:52  lr: 0.000018  loss: 3.7629 (3.6152)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [22]  [  40/1251]  eta: 0:25:35  lr: 0.000018  loss: 3.7501 (3.5858)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [22]  [  50/1251]  eta: 0:23:33  lr: 0.000018  loss: 3.4836 (3.5169)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [22]  [  60/1251]  eta: 0:22:08  lr: 0.000018  loss: 3.4073 (3.4831)  time: 0.8033  data: 0.0004  max mem: 19734
Epoch: [22]  [  70/1251]  eta: 0:21:09  lr: 0.000018  loss: 3.3878 (3.4733)  time: 0.8152  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3836, ratio_loss=0.0054, pruning_loss=0.1430, mse_loss=0.6596
Epoch: [22]  [  80/1251]  eta: 0:20:19  lr: 0.000018  loss: 3.3878 (3.4637)  time: 0.8149  data: 0.0006  max mem: 19734
Epoch: [22]  [  90/1251]  eta: 0:19:39  lr: 0.000018  loss: 3.6436 (3.4616)  time: 0.8065  data: 0.0006  max mem: 19734
Epoch: [22]  [ 100/1251]  eta: 0:19:06  lr: 0.000018  loss: 3.2666 (3.4268)  time: 0.8115  data: 0.0005  max mem: 19734
Epoch: [22]  [ 110/1251]  eta: 0:18:38  lr: 0.000018  loss: 3.2666 (3.4414)  time: 0.8196  data: 0.0006  max mem: 19734
Epoch: [22]  [ 120/1251]  eta: 0:18:11  lr: 0.000018  loss: 3.5762 (3.4321)  time: 0.8128  data: 0.0006  max mem: 19734
Epoch: [22]  [ 130/1251]  eta: 0:17:48  lr: 0.000018  loss: 3.3405 (3.4247)  time: 0.8006  data: 0.0004  max mem: 19734
Epoch: [22]  [ 140/1251]  eta: 0:17:28  lr: 0.000018  loss: 3.7156 (3.4424)  time: 0.8098  data: 0.0005  max mem: 19734
Epoch: [22]  [ 150/1251]  eta: 0:17:11  lr: 0.000018  loss: 3.6935 (3.4391)  time: 0.8301  data: 0.0005  max mem: 19734
Epoch: [22]  [ 160/1251]  eta: 0:16:53  lr: 0.000018  loss: 3.4222 (3.4208)  time: 0.8299  data: 0.0005  max mem: 19734
Epoch: [22]  [ 170/1251]  eta: 0:16:36  lr: 0.000018  loss: 3.4399 (3.4265)  time: 0.8113  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3967, ratio_loss=0.0054, pruning_loss=0.1425, mse_loss=0.6683
Epoch: [22]  [ 180/1251]  eta: 0:16:20  lr: 0.000018  loss: 3.5510 (3.4291)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [22]  [ 190/1251]  eta: 0:16:05  lr: 0.000018  loss: 3.4850 (3.4184)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [22]  [ 200/1251]  eta: 0:15:50  lr: 0.000018  loss: 3.3749 (3.4233)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [22]  [ 210/1251]  eta: 0:15:37  lr: 0.000018  loss: 3.4684 (3.4186)  time: 0.8167  data: 0.0005  max mem: 19734
Epoch: [22]  [ 220/1251]  eta: 0:15:24  lr: 0.000018  loss: 3.2846 (3.4078)  time: 0.8167  data: 0.0006  max mem: 19734
Epoch: [22]  [ 230/1251]  eta: 0:15:11  lr: 0.000018  loss: 3.2846 (3.4058)  time: 0.8050  data: 0.0006  max mem: 19734
Epoch: [22]  [ 240/1251]  eta: 0:14:59  lr: 0.000018  loss: 3.5112 (3.4070)  time: 0.8181  data: 0.0005  max mem: 19734
Epoch: [22]  [ 250/1251]  eta: 0:14:47  lr: 0.000018  loss: 3.5126 (3.4089)  time: 0.8177  data: 0.0004  max mem: 19734
Epoch: [22]  [ 260/1251]  eta: 0:14:35  lr: 0.000018  loss: 3.5295 (3.4041)  time: 0.8085  data: 0.0005  max mem: 19734
Epoch: [22]  [ 270/1251]  eta: 0:14:23  lr: 0.000018  loss: 3.1676 (3.4006)  time: 0.8071  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3525, ratio_loss=0.0054, pruning_loss=0.1418, mse_loss=0.6639
Epoch: [22]  [ 280/1251]  eta: 0:14:13  lr: 0.000018  loss: 3.5743 (3.4105)  time: 0.8135  data: 0.0005  max mem: 19734
Epoch: [22]  [ 290/1251]  eta: 0:14:02  lr: 0.000018  loss: 3.6701 (3.4111)  time: 0.8252  data: 0.0005  max mem: 19734
Epoch: [22]  [ 300/1251]  eta: 0:13:52  lr: 0.000018  loss: 3.6682 (3.4147)  time: 0.8292  data: 0.0005  max mem: 19734
Epoch: [22]  [ 310/1251]  eta: 0:13:41  lr: 0.000018  loss: 3.5274 (3.4156)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [22]  [ 320/1251]  eta: 0:13:30  lr: 0.000018  loss: 3.6432 (3.4230)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [22]  [ 330/1251]  eta: 0:13:20  lr: 0.000018  loss: 3.7110 (3.4306)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [22]  [ 340/1251]  eta: 0:13:09  lr: 0.000018  loss: 3.6423 (3.4332)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [22]  [ 350/1251]  eta: 0:12:59  lr: 0.000018  loss: 3.7285 (3.4377)  time: 0.8003  data: 0.0004  max mem: 19734
Epoch: [22]  [ 360/1251]  eta: 0:12:49  lr: 0.000018  loss: 3.7723 (3.4407)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [22]  [ 370/1251]  eta: 0:12:39  lr: 0.000018  loss: 3.5088 (3.4380)  time: 0.8097  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4776, ratio_loss=0.0051, pruning_loss=0.1401, mse_loss=0.6779
Epoch: [22]  [ 380/1251]  eta: 0:12:29  lr: 0.000018  loss: 3.3559 (3.4323)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [22]  [ 390/1251]  eta: 0:12:20  lr: 0.000018  loss: 3.1444 (3.4261)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [22]  [ 400/1251]  eta: 0:12:10  lr: 0.000018  loss: 3.3565 (3.4279)  time: 0.8224  data: 0.0005  max mem: 19734
Epoch: [22]  [ 410/1251]  eta: 0:12:01  lr: 0.000018  loss: 3.4478 (3.4286)  time: 0.8137  data: 0.0005  max mem: 19734
Epoch: [22]  [ 420/1251]  eta: 0:11:51  lr: 0.000018  loss: 3.3514 (3.4216)  time: 0.8136  data: 0.0006  max mem: 19734
Epoch: [22]  [ 430/1251]  eta: 0:11:42  lr: 0.000018  loss: 3.3514 (3.4240)  time: 0.8134  data: 0.0005  max mem: 19734
Epoch: [22]  [ 440/1251]  eta: 0:11:33  lr: 0.000018  loss: 3.5315 (3.4236)  time: 0.8359  data: 0.0005  max mem: 19734
Epoch: [22]  [ 450/1251]  eta: 0:11:24  lr: 0.000018  loss: 3.5800 (3.4285)  time: 0.8413  data: 0.0005  max mem: 19734
Epoch: [22]  [ 460/1251]  eta: 0:11:15  lr: 0.000018  loss: 3.5800 (3.4247)  time: 0.8097  data: 0.0005  max mem: 19734
Epoch: [22]  [ 470/1251]  eta: 0:11:05  lr: 0.000018  loss: 3.2939 (3.4249)  time: 0.8046  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3799, ratio_loss=0.0056, pruning_loss=0.1420, mse_loss=0.6728
Epoch: [22]  [ 480/1251]  eta: 0:10:56  lr: 0.000018  loss: 3.5226 (3.4268)  time: 0.8061  data: 0.0005  max mem: 19734
Epoch: [22]  [ 490/1251]  eta: 0:10:47  lr: 0.000018  loss: 3.5659 (3.4236)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [22]  [ 500/1251]  eta: 0:10:38  lr: 0.000018  loss: 3.3463 (3.4241)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [22]  [ 510/1251]  eta: 0:10:29  lr: 0.000018  loss: 3.3463 (3.4223)  time: 0.8112  data: 0.0005  max mem: 19734
Epoch: [22]  [ 520/1251]  eta: 0:10:20  lr: 0.000018  loss: 3.5093 (3.4251)  time: 0.8130  data: 0.0004  max mem: 19734
Epoch: [22]  [ 530/1251]  eta: 0:10:11  lr: 0.000018  loss: 3.5093 (3.4234)  time: 0.8085  data: 0.0005  max mem: 19734
Epoch: [22]  [ 540/1251]  eta: 0:10:02  lr: 0.000018  loss: 3.2959 (3.4214)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [22]  [ 550/1251]  eta: 0:09:53  lr: 0.000018  loss: 3.2959 (3.4189)  time: 0.8139  data: 0.0005  max mem: 19734
Epoch: [22]  [ 560/1251]  eta: 0:09:44  lr: 0.000018  loss: 3.6336 (3.4237)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [22]  [ 570/1251]  eta: 0:09:35  lr: 0.000018  loss: 3.6749 (3.4284)  time: 0.8067  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3955, ratio_loss=0.0053, pruning_loss=0.1428, mse_loss=0.6585
Epoch: [22]  [ 580/1251]  eta: 0:09:26  lr: 0.000018  loss: 3.5565 (3.4210)  time: 0.8150  data: 0.0005  max mem: 19734
Epoch: [22]  [ 590/1251]  eta: 0:09:18  lr: 0.000018  loss: 3.5130 (3.4253)  time: 0.8280  data: 0.0005  max mem: 19734
Epoch: [22]  [ 600/1251]  eta: 0:09:09  lr: 0.000018  loss: 3.5187 (3.4237)  time: 0.8194  data: 0.0005  max mem: 19734
Epoch: [22]  [ 610/1251]  eta: 0:09:00  lr: 0.000018  loss: 3.2239 (3.4192)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [22]  [ 620/1251]  eta: 0:08:51  lr: 0.000018  loss: 3.2239 (3.4181)  time: 0.8047  data: 0.0006  max mem: 19734
Epoch: [22]  [ 630/1251]  eta: 0:08:42  lr: 0.000018  loss: 3.5811 (3.4239)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [22]  [ 640/1251]  eta: 0:08:33  lr: 0.000018  loss: 3.6732 (3.4206)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [22]  [ 650/1251]  eta: 0:08:25  lr: 0.000018  loss: 3.5936 (3.4232)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [22]  [ 660/1251]  eta: 0:08:16  lr: 0.000018  loss: 3.5045 (3.4240)  time: 0.8039  data: 0.0004  max mem: 19734
Epoch: [22]  [ 670/1251]  eta: 0:08:07  lr: 0.000018  loss: 3.3376 (3.4206)  time: 0.8078  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3864, ratio_loss=0.0055, pruning_loss=0.1430, mse_loss=0.6537
Epoch: [22]  [ 680/1251]  eta: 0:07:59  lr: 0.000018  loss: 3.3401 (3.4233)  time: 0.8166  data: 0.0005  max mem: 19734
Epoch: [22]  [ 690/1251]  eta: 0:07:50  lr: 0.000018  loss: 3.5915 (3.4215)  time: 0.8216  data: 0.0004  max mem: 19734
Epoch: [22]  [ 700/1251]  eta: 0:07:42  lr: 0.000018  loss: 3.2315 (3.4209)  time: 0.8078  data: 0.0004  max mem: 19734
Epoch: [22]  [ 710/1251]  eta: 0:07:33  lr: 0.000018  loss: 3.6128 (3.4235)  time: 0.7988  data: 0.0005  max mem: 19734
Epoch: [22]  [ 720/1251]  eta: 0:07:24  lr: 0.000018  loss: 3.3669 (3.4185)  time: 0.8072  data: 0.0006  max mem: 19734
Epoch: [22]  [ 730/1251]  eta: 0:07:16  lr: 0.000018  loss: 3.3845 (3.4215)  time: 0.8205  data: 0.0006  max mem: 19734
Epoch: [22]  [ 740/1251]  eta: 0:07:08  lr: 0.000018  loss: 3.7486 (3.4256)  time: 0.8402  data: 0.0005  max mem: 19734
Epoch: [22]  [ 750/1251]  eta: 0:06:59  lr: 0.000018  loss: 3.5699 (3.4255)  time: 0.8277  data: 0.0005  max mem: 19734
Epoch: [22]  [ 760/1251]  eta: 0:06:50  lr: 0.000018  loss: 3.4538 (3.4253)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [22]  [ 770/1251]  eta: 0:06:42  lr: 0.000018  loss: 3.4752 (3.4258)  time: 0.7994  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4188, ratio_loss=0.0053, pruning_loss=0.1419, mse_loss=0.6714
Epoch: [22]  [ 780/1251]  eta: 0:06:33  lr: 0.000018  loss: 3.4655 (3.4245)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [22]  [ 790/1251]  eta: 0:06:25  lr: 0.000018  loss: 3.5022 (3.4259)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [22]  [ 800/1251]  eta: 0:06:16  lr: 0.000018  loss: 3.5022 (3.4244)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [22]  [ 810/1251]  eta: 0:06:08  lr: 0.000018  loss: 3.2669 (3.4226)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [22]  [ 820/1251]  eta: 0:05:59  lr: 0.000018  loss: 3.3341 (3.4212)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [22]  [ 830/1251]  eta: 0:05:51  lr: 0.000018  loss: 3.3304 (3.4184)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [22]  [ 840/1251]  eta: 0:05:42  lr: 0.000018  loss: 3.4084 (3.4211)  time: 0.8171  data: 0.0004  max mem: 19734
Epoch: [22]  [ 850/1251]  eta: 0:05:34  lr: 0.000018  loss: 3.6526 (3.4212)  time: 0.8064  data: 0.0004  max mem: 19734
Epoch: [22]  [ 860/1251]  eta: 0:05:25  lr: 0.000018  loss: 3.3198 (3.4201)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [22]  [ 870/1251]  eta: 0:05:17  lr: 0.000018  loss: 3.2745 (3.4181)  time: 0.8073  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3500, ratio_loss=0.0052, pruning_loss=0.1418, mse_loss=0.6743
Epoch: [22]  [ 880/1251]  eta: 0:05:09  lr: 0.000018  loss: 3.4561 (3.4175)  time: 0.8223  data: 0.0004  max mem: 19734
Epoch: [22]  [ 890/1251]  eta: 0:05:00  lr: 0.000018  loss: 3.4827 (3.4168)  time: 0.8366  data: 0.0008  max mem: 19734
Epoch: [22]  [ 900/1251]  eta: 0:04:52  lr: 0.000018  loss: 3.4827 (3.4165)  time: 0.8151  data: 0.0007  max mem: 19734
Epoch: [22]  [ 910/1251]  eta: 0:04:43  lr: 0.000018  loss: 3.6400 (3.4160)  time: 0.8008  data: 0.0004  max mem: 19734
Epoch: [22]  [ 920/1251]  eta: 0:04:35  lr: 0.000018  loss: 3.4220 (3.4147)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [22]  [ 930/1251]  eta: 0:04:26  lr: 0.000018  loss: 3.2522 (3.4140)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [22]  [ 940/1251]  eta: 0:04:18  lr: 0.000018  loss: 3.5111 (3.4150)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [22]  [ 950/1251]  eta: 0:04:10  lr: 0.000018  loss: 3.6325 (3.4152)  time: 0.8105  data: 0.0005  max mem: 19734
Epoch: [22]  [ 960/1251]  eta: 0:04:01  lr: 0.000018  loss: 3.4052 (3.4139)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [22]  [ 970/1251]  eta: 0:03:53  lr: 0.000018  loss: 3.4052 (3.4140)  time: 0.7982  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3537, ratio_loss=0.0052, pruning_loss=0.1433, mse_loss=0.7143
Epoch: [22]  [ 980/1251]  eta: 0:03:44  lr: 0.000018  loss: 3.6235 (3.4145)  time: 0.8074  data: 0.0004  max mem: 19734
Epoch: [22]  [ 990/1251]  eta: 0:03:36  lr: 0.000018  loss: 3.6235 (3.4167)  time: 0.8157  data: 0.0004  max mem: 19734
Epoch: [22]  [1000/1251]  eta: 0:03:28  lr: 0.000018  loss: 3.4956 (3.4164)  time: 0.8105  data: 0.0004  max mem: 19734
Epoch: [22]  [1010/1251]  eta: 0:03:19  lr: 0.000018  loss: 3.4406 (3.4159)  time: 0.8131  data: 0.0004  max mem: 19734
Epoch: [22]  [1020/1251]  eta: 0:03:11  lr: 0.000018  loss: 3.4963 (3.4170)  time: 0.8349  data: 0.0006  max mem: 19734
Epoch: [22]  [1030/1251]  eta: 0:03:03  lr: 0.000018  loss: 3.5115 (3.4163)  time: 0.8385  data: 0.0006  max mem: 19734
Epoch: [22]  [1040/1251]  eta: 0:02:55  lr: 0.000018  loss: 3.5445 (3.4180)  time: 0.8169  data: 0.0004  max mem: 19734
Epoch: [22]  [1050/1251]  eta: 0:02:46  lr: 0.000018  loss: 3.5816 (3.4160)  time: 0.8024  data: 0.0004  max mem: 19734
Epoch: [22]  [1060/1251]  eta: 0:02:38  lr: 0.000018  loss: 3.4744 (3.4169)  time: 0.7993  data: 0.0005  max mem: 19734
Epoch: [22]  [1070/1251]  eta: 0:02:30  lr: 0.000018  loss: 3.5157 (3.4174)  time: 0.7995  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4469, ratio_loss=0.0056, pruning_loss=0.1396, mse_loss=0.6514
Epoch: [22]  [1080/1251]  eta: 0:02:21  lr: 0.000018  loss: 3.6393 (3.4177)  time: 0.8020  data: 0.0004  max mem: 19734
Epoch: [22]  [1090/1251]  eta: 0:02:13  lr: 0.000018  loss: 3.6817 (3.4194)  time: 0.8024  data: 0.0004  max mem: 19734
Epoch: [22]  [1100/1251]  eta: 0:02:05  lr: 0.000018  loss: 3.7504 (3.4225)  time: 0.8046  data: 0.0004  max mem: 19734
Epoch: [22]  [1110/1251]  eta: 0:01:56  lr: 0.000018  loss: 3.7504 (3.4225)  time: 0.8053  data: 0.0004  max mem: 19734
Epoch: [22]  [1120/1251]  eta: 0:01:48  lr: 0.000018  loss: 3.5561 (3.4227)  time: 0.8052  data: 0.0004  max mem: 19734
Epoch: [22]  [1130/1251]  eta: 0:01:40  lr: 0.000018  loss: 3.5561 (3.4228)  time: 0.8133  data: 0.0004  max mem: 19734
Epoch: [22]  [1140/1251]  eta: 0:01:31  lr: 0.000018  loss: 3.4727 (3.4244)  time: 0.8103  data: 0.0004  max mem: 19734
Epoch: [22]  [1150/1251]  eta: 0:01:23  lr: 0.000018  loss: 3.3911 (3.4229)  time: 0.8067  data: 0.0005  max mem: 19734
Epoch: [22]  [1160/1251]  eta: 0:01:15  lr: 0.000018  loss: 3.3213 (3.4237)  time: 0.8062  data: 0.0004  max mem: 19734
Epoch: [22]  [1170/1251]  eta: 0:01:07  lr: 0.000018  loss: 3.5329 (3.4245)  time: 0.8172  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4902, ratio_loss=0.0057, pruning_loss=0.1402, mse_loss=0.6313
Epoch: [22]  [1180/1251]  eta: 0:00:58  lr: 0.000018  loss: 3.5746 (3.4260)  time: 0.8310  data: 0.0005  max mem: 19734
Epoch: [22]  [1190/1251]  eta: 0:00:50  lr: 0.000018  loss: 3.6127 (3.4259)  time: 0.8137  data: 0.0009  max mem: 19734
Epoch: [22]  [1200/1251]  eta: 0:00:42  lr: 0.000018  loss: 3.6127 (3.4270)  time: 0.7958  data: 0.0008  max mem: 19734
Epoch: [22]  [1210/1251]  eta: 0:00:33  lr: 0.000018  loss: 3.7155 (3.4289)  time: 0.7924  data: 0.0002  max mem: 19734
Epoch: [22]  [1220/1251]  eta: 0:00:25  lr: 0.000018  loss: 3.7155 (3.4280)  time: 0.7920  data: 0.0002  max mem: 19734
Epoch: [22]  [1230/1251]  eta: 0:00:17  lr: 0.000018  loss: 3.4270 (3.4291)  time: 0.7928  data: 0.0002  max mem: 19734
Epoch: [22]  [1240/1251]  eta: 0:00:09  lr: 0.000018  loss: 3.5233 (3.4283)  time: 0.7917  data: 0.0001  max mem: 19734
Epoch: [22]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.4211 (3.4259)  time: 0.7981  data: 0.0001  max mem: 19734
Epoch: [22] Total time: 0:17:13 (0.8264 s / it)
Averaged stats: lr: 0.000018  loss: 3.4211 (3.4278)
Test:  [  0/261]  eta: 2:21:01  loss: 0.7336 (0.7336)  acc1: 85.4167 (85.4167)  acc5: 95.8333 (95.8333)  time: 32.4209  data: 32.2502  max mem: 19734
Test:  [ 10/261]  eta: 0:13:10  loss: 0.7123 (0.7398)  acc1: 85.4167 (83.4280)  acc5: 97.3958 (96.4489)  time: 3.1478  data: 2.9547  max mem: 19734
Test:  [ 20/261]  eta: 0:07:02  loss: 0.9261 (0.9056)  acc1: 79.1667 (78.8442)  acc5: 93.7500 (94.7669)  time: 0.2194  data: 0.0199  max mem: 19734
Test:  [ 30/261]  eta: 0:05:06  loss: 0.8208 (0.8185)  acc1: 82.2917 (81.6196)  acc5: 94.2708 (95.2789)  time: 0.3256  data: 0.0114  max mem: 19734
Test:  [ 40/261]  eta: 0:04:47  loss: 0.5692 (0.7838)  acc1: 86.9792 (82.7490)  acc5: 96.3542 (95.5031)  time: 0.8249  data: 0.3566  max mem: 19734
Test:  [ 50/261]  eta: 0:04:00  loss: 0.8962 (0.8458)  acc1: 77.6042 (80.9845)  acc5: 94.2708 (95.0061)  time: 0.8503  data: 0.3620  max mem: 19734
Test:  [ 60/261]  eta: 0:03:22  loss: 0.9758 (0.8562)  acc1: 76.0417 (80.5072)  acc5: 94.2708 (95.0051)  time: 0.4067  data: 0.0193  max mem: 19734
Test:  [ 70/261]  eta: 0:02:53  loss: 0.9307 (0.8575)  acc1: 76.5625 (80.0690)  acc5: 95.3125 (95.1951)  time: 0.3204  data: 0.0213  max mem: 19734
Test:  [ 80/261]  eta: 0:02:31  loss: 0.8371 (0.8590)  acc1: 79.6875 (80.2212)  acc5: 96.8750 (95.2996)  time: 0.3160  data: 0.0208  max mem: 19734
Test:  [ 90/261]  eta: 0:02:13  loss: 0.8265 (0.8459)  acc1: 83.3333 (80.6147)  acc5: 95.8333 (95.4041)  time: 0.3272  data: 0.0154  max mem: 19734
Test:  [100/261]  eta: 0:02:02  loss: 0.8302 (0.8491)  acc1: 83.3333 (80.5745)  acc5: 95.8333 (95.4775)  time: 0.4537  data: 0.1249  max mem: 19734
Test:  [110/261]  eta: 0:01:47  loss: 0.8641 (0.8726)  acc1: 78.6458 (80.0253)  acc5: 94.7917 (95.1905)  time: 0.3957  data: 0.1285  max mem: 19734
Test:  [120/261]  eta: 0:01:34  loss: 1.2103 (0.9132)  acc1: 70.3125 (79.0375)  acc5: 90.1042 (94.6539)  time: 0.2227  data: 0.0139  max mem: 19734
Test:  [130/261]  eta: 0:01:29  loss: 1.3703 (0.9581)  acc1: 69.2708 (78.1767)  acc5: 87.5000 (94.0760)  time: 0.5475  data: 0.3547  max mem: 19734
Test:  [140/261]  eta: 0:01:19  loss: 1.2961 (0.9840)  acc1: 69.7917 (77.5525)  acc5: 89.0625 (93.8165)  time: 0.5487  data: 0.3598  max mem: 19734
Test:  [150/261]  eta: 0:01:09  loss: 1.2123 (0.9886)  acc1: 72.9167 (77.5421)  acc5: 91.6667 (93.6672)  time: 0.2299  data: 0.0151  max mem: 19734
Test:  [160/261]  eta: 0:01:01  loss: 1.0472 (1.0094)  acc1: 77.0833 (77.1836)  acc5: 92.7083 (93.3553)  time: 0.2764  data: 0.0153  max mem: 19734
Test:  [170/261]  eta: 0:00:52  loss: 1.3247 (1.0407)  acc1: 64.5833 (76.4102)  acc5: 88.5417 (93.0464)  time: 0.2458  data: 0.0133  max mem: 19734
Test:  [180/261]  eta: 0:00:45  loss: 1.4472 (1.0581)  acc1: 64.5833 (76.0215)  acc5: 88.5417 (92.8839)  time: 0.1903  data: 0.0372  max mem: 19734
Test:  [190/261]  eta: 0:00:38  loss: 1.4378 (1.0731)  acc1: 66.1458 (75.7363)  acc5: 90.1042 (92.7165)  time: 0.2259  data: 0.0374  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.3563 (1.0906)  acc1: 70.3125 (75.3861)  acc5: 89.0625 (92.4622)  time: 0.5751  data: 0.3784  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.3563 (1.1035)  acc1: 70.3125 (75.1506)  acc5: 89.0625 (92.2665)  time: 0.5602  data: 0.3748  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4193 (1.1230)  acc1: 67.7083 (74.6701)  acc5: 88.0208 (92.0320)  time: 0.1632  data: 0.0007  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.4259 (1.1329)  acc1: 65.1042 (74.3935)  acc5: 88.0208 (91.9327)  time: 0.1159  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3317 (1.1421)  acc1: 67.1875 (74.1831)  acc5: 90.6250 (91.8547)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0983 (1.1355)  acc1: 76.0417 (74.3484)  acc5: 92.7083 (91.9592)  time: 0.1146  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9781 (1.1351)  acc1: 76.5625 (74.3620)  acc5: 95.3125 (92.0260)  time: 0.1112  data: 0.0001  max mem: 19734
Test: Total time: 0:02:01 (0.4645 s / it)
* Acc@1 74.362 Acc@5 92.026 loss 1.135
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.46%
Epoch: [23]  [   0/1251]  eta: 6:08:57  lr: 0.000018  loss: 2.4545 (2.4545)  time: 17.6957  data: 14.3238  max mem: 19734
Epoch: [23]  [  10/1251]  eta: 0:56:15  lr: 0.000018  loss: 3.4228 (3.3164)  time: 2.7201  data: 1.3032  max mem: 19734
Epoch: [23]  [  20/1251]  eta: 0:37:14  lr: 0.000018  loss: 3.5636 (3.4262)  time: 1.0213  data: 0.0009  max mem: 19734
loss info: cls_loss=3.3875, ratio_loss=0.0054, pruning_loss=0.1428, mse_loss=0.6274
Epoch: [23]  [  30/1251]  eta: 0:30:15  lr: 0.000018  loss: 3.4198 (3.3971)  time: 0.8084  data: 0.0006  max mem: 19734
Epoch: [23]  [  40/1251]  eta: 0:26:37  lr: 0.000018  loss: 3.4198 (3.3740)  time: 0.7988  data: 0.0005  max mem: 19734
Epoch: [23]  [  50/1251]  eta: 0:24:25  lr: 0.000018  loss: 3.5253 (3.3796)  time: 0.8067  data: 0.0006  max mem: 19734
Epoch: [23]  [  60/1251]  eta: 0:22:57  lr: 0.000018  loss: 3.5571 (3.3825)  time: 0.8215  data: 0.0006  max mem: 19734
Epoch: [23]  [  70/1251]  eta: 0:21:50  lr: 0.000018  loss: 3.4405 (3.3872)  time: 0.8276  data: 0.0005  max mem: 19734
Epoch: [23]  [  80/1251]  eta: 0:20:55  lr: 0.000018  loss: 3.4194 (3.3925)  time: 0.8148  data: 0.0006  max mem: 19734
Epoch: [23]  [  90/1251]  eta: 0:20:10  lr: 0.000018  loss: 3.6925 (3.4264)  time: 0.8042  data: 0.0006  max mem: 19734
Epoch: [23]  [ 100/1251]  eta: 0:19:33  lr: 0.000018  loss: 3.8132 (3.4601)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [23]  [ 110/1251]  eta: 0:19:01  lr: 0.000018  loss: 3.7439 (3.4629)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [23]  [ 120/1251]  eta: 0:18:32  lr: 0.000018  loss: 3.3236 (3.4340)  time: 0.8068  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4185, ratio_loss=0.0055, pruning_loss=0.1413, mse_loss=0.6442
Epoch: [23]  [ 130/1251]  eta: 0:18:07  lr: 0.000018  loss: 3.3236 (3.4243)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [23]  [ 140/1251]  eta: 0:17:44  lr: 0.000018  loss: 3.3706 (3.4110)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [23]  [ 150/1251]  eta: 0:17:23  lr: 0.000018  loss: 3.2594 (3.3967)  time: 0.8029  data: 0.0006  max mem: 19734
Epoch: [23]  [ 160/1251]  eta: 0:17:05  lr: 0.000018  loss: 3.3419 (3.3942)  time: 0.8117  data: 0.0006  max mem: 19734
Epoch: [23]  [ 170/1251]  eta: 0:16:48  lr: 0.000018  loss: 3.2262 (3.3900)  time: 0.8205  data: 0.0005  max mem: 19734
Epoch: [23]  [ 180/1251]  eta: 0:16:31  lr: 0.000018  loss: 3.0948 (3.3764)  time: 0.8127  data: 0.0004  max mem: 19734
Epoch: [23]  [ 190/1251]  eta: 0:16:16  lr: 0.000018  loss: 3.4453 (3.3914)  time: 0.8114  data: 0.0006  max mem: 19734
Epoch: [23]  [ 200/1251]  eta: 0:16:02  lr: 0.000018  loss: 3.5234 (3.3968)  time: 0.8245  data: 0.0006  max mem: 19734
Epoch: [23]  [ 210/1251]  eta: 0:15:48  lr: 0.000018  loss: 3.4817 (3.3992)  time: 0.8276  data: 0.0006  max mem: 19734
Epoch: [23]  [ 220/1251]  eta: 0:15:34  lr: 0.000018  loss: 3.4423 (3.3967)  time: 0.8140  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3335, ratio_loss=0.0053, pruning_loss=0.1447, mse_loss=0.6918
Epoch: [23]  [ 230/1251]  eta: 0:15:21  lr: 0.000018  loss: 3.5887 (3.3983)  time: 0.8036  data: 0.0006  max mem: 19734
Epoch: [23]  [ 240/1251]  eta: 0:15:07  lr: 0.000018  loss: 3.5292 (3.3937)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [23]  [ 250/1251]  eta: 0:14:55  lr: 0.000018  loss: 3.4703 (3.3931)  time: 0.8094  data: 0.0005  max mem: 19734
Epoch: [23]  [ 260/1251]  eta: 0:14:43  lr: 0.000018  loss: 3.5254 (3.3931)  time: 0.8072  data: 0.0005  max mem: 19734
Epoch: [23]  [ 270/1251]  eta: 0:14:31  lr: 0.000018  loss: 3.5297 (3.3921)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [23]  [ 280/1251]  eta: 0:14:19  lr: 0.000018  loss: 3.5170 (3.3936)  time: 0.8029  data: 0.0006  max mem: 19734
Epoch: [23]  [ 290/1251]  eta: 0:14:07  lr: 0.000018  loss: 3.5149 (3.3969)  time: 0.8036  data: 0.0006  max mem: 19734
Epoch: [23]  [ 300/1251]  eta: 0:13:57  lr: 0.000018  loss: 3.3838 (3.3901)  time: 0.8160  data: 0.0007  max mem: 19734
Epoch: [23]  [ 310/1251]  eta: 0:13:46  lr: 0.000018  loss: 3.4569 (3.4004)  time: 0.8284  data: 0.0006  max mem: 19734
Epoch: [23]  [ 320/1251]  eta: 0:13:35  lr: 0.000018  loss: 3.5728 (3.4002)  time: 0.8169  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3952, ratio_loss=0.0052, pruning_loss=0.1396, mse_loss=0.6761
Epoch: [23]  [ 330/1251]  eta: 0:13:25  lr: 0.000018  loss: 3.4698 (3.3998)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [23]  [ 340/1251]  eta: 0:13:14  lr: 0.000018  loss: 3.4976 (3.4019)  time: 0.8114  data: 0.0005  max mem: 19734
Epoch: [23]  [ 350/1251]  eta: 0:13:05  lr: 0.000018  loss: 3.5023 (3.3973)  time: 0.8259  data: 0.0005  max mem: 19734
Epoch: [23]  [ 360/1251]  eta: 0:12:55  lr: 0.000018  loss: 3.3135 (3.3934)  time: 0.8275  data: 0.0006  max mem: 19734
Epoch: [23]  [ 370/1251]  eta: 0:12:44  lr: 0.000018  loss: 3.5007 (3.3975)  time: 0.8124  data: 0.0005  max mem: 19734
Epoch: [23]  [ 380/1251]  eta: 0:12:34  lr: 0.000018  loss: 3.6421 (3.4008)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [23]  [ 390/1251]  eta: 0:12:24  lr: 0.000018  loss: 3.4871 (3.4034)  time: 0.8019  data: 0.0006  max mem: 19734
Epoch: [23]  [ 400/1251]  eta: 0:12:14  lr: 0.000018  loss: 3.4469 (3.4032)  time: 0.8084  data: 0.0007  max mem: 19734
Epoch: [23]  [ 410/1251]  eta: 0:12:05  lr: 0.000018  loss: 3.3591 (3.4015)  time: 0.8096  data: 0.0006  max mem: 19734
Epoch: [23]  [ 420/1251]  eta: 0:11:55  lr: 0.000018  loss: 3.2441 (3.3966)  time: 0.8084  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3737, ratio_loss=0.0059, pruning_loss=0.1416, mse_loss=0.6463
Epoch: [23]  [ 430/1251]  eta: 0:11:45  lr: 0.000018  loss: 3.4548 (3.4001)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [23]  [ 440/1251]  eta: 0:11:35  lr: 0.000018  loss: 3.5312 (3.3972)  time: 0.8011  data: 0.0006  max mem: 19734
Epoch: [23]  [ 450/1251]  eta: 0:11:26  lr: 0.000018  loss: 3.4021 (3.3960)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [23]  [ 460/1251]  eta: 0:11:17  lr: 0.000018  loss: 3.4397 (3.3977)  time: 0.8177  data: 0.0005  max mem: 19734
Epoch: [23]  [ 470/1251]  eta: 0:11:08  lr: 0.000018  loss: 3.4397 (3.3946)  time: 0.8129  data: 0.0005  max mem: 19734
Epoch: [23]  [ 480/1251]  eta: 0:10:58  lr: 0.000018  loss: 3.6511 (3.4005)  time: 0.8097  data: 0.0004  max mem: 19734
Epoch: [23]  [ 490/1251]  eta: 0:10:49  lr: 0.000018  loss: 3.5501 (3.3999)  time: 0.8080  data: 0.0004  max mem: 19734
Epoch: [23]  [ 500/1251]  eta: 0:10:40  lr: 0.000018  loss: 3.4993 (3.4023)  time: 0.8237  data: 0.0005  max mem: 19734
Epoch: [23]  [ 510/1251]  eta: 0:10:31  lr: 0.000018  loss: 3.6279 (3.4083)  time: 0.8330  data: 0.0005  max mem: 19734
Epoch: [23]  [ 520/1251]  eta: 0:10:22  lr: 0.000018  loss: 3.7134 (3.4162)  time: 0.8098  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4604, ratio_loss=0.0053, pruning_loss=0.1400, mse_loss=0.6779
Epoch: [23]  [ 530/1251]  eta: 0:10:13  lr: 0.000018  loss: 3.7282 (3.4175)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [23]  [ 540/1251]  eta: 0:10:04  lr: 0.000018  loss: 3.4052 (3.4181)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [23]  [ 550/1251]  eta: 0:09:55  lr: 0.000018  loss: 3.4052 (3.4170)  time: 0.8099  data: 0.0005  max mem: 19734
Epoch: [23]  [ 560/1251]  eta: 0:09:46  lr: 0.000018  loss: 3.5054 (3.4191)  time: 0.8117  data: 0.0005  max mem: 19734
Epoch: [23]  [ 570/1251]  eta: 0:09:37  lr: 0.000018  loss: 3.8135 (3.4269)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [23]  [ 580/1251]  eta: 0:09:28  lr: 0.000018  loss: 3.9086 (3.4302)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [23]  [ 590/1251]  eta: 0:09:19  lr: 0.000018  loss: 3.5279 (3.4282)  time: 0.8083  data: 0.0004  max mem: 19734
Epoch: [23]  [ 600/1251]  eta: 0:09:10  lr: 0.000018  loss: 3.3884 (3.4277)  time: 0.8087  data: 0.0004  max mem: 19734
Epoch: [23]  [ 610/1251]  eta: 0:09:01  lr: 0.000018  loss: 3.2580 (3.4220)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [23]  [ 620/1251]  eta: 0:08:52  lr: 0.000018  loss: 3.4313 (3.4254)  time: 0.8103  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4549, ratio_loss=0.0051, pruning_loss=0.1389, mse_loss=0.6393
Epoch: [23]  [ 630/1251]  eta: 0:08:44  lr: 0.000018  loss: 3.6739 (3.4260)  time: 0.8100  data: 0.0005  max mem: 19734
Epoch: [23]  [ 640/1251]  eta: 0:08:35  lr: 0.000018  loss: 3.6167 (3.4273)  time: 0.8180  data: 0.0005  max mem: 19734
Epoch: [23]  [ 650/1251]  eta: 0:08:27  lr: 0.000018  loss: 3.5409 (3.4263)  time: 0.8375  data: 0.0007  max mem: 19734
Epoch: [23]  [ 660/1251]  eta: 0:08:18  lr: 0.000018  loss: 3.4599 (3.4249)  time: 0.8287  data: 0.0006  max mem: 19734
Epoch: [23]  [ 670/1251]  eta: 0:08:09  lr: 0.000018  loss: 3.5475 (3.4241)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [23]  [ 680/1251]  eta: 0:08:00  lr: 0.000018  loss: 3.3670 (3.4225)  time: 0.8071  data: 0.0005  max mem: 19734
Epoch: [23]  [ 690/1251]  eta: 0:07:52  lr: 0.000018  loss: 3.3930 (3.4247)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [23]  [ 700/1251]  eta: 0:07:43  lr: 0.000018  loss: 3.4876 (3.4233)  time: 0.8059  data: 0.0004  max mem: 19734
Epoch: [23]  [ 710/1251]  eta: 0:07:34  lr: 0.000018  loss: 3.6657 (3.4289)  time: 0.8060  data: 0.0004  max mem: 19734
Epoch: [23]  [ 720/1251]  eta: 0:07:26  lr: 0.000018  loss: 3.8108 (3.4303)  time: 0.8023  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4557, ratio_loss=0.0053, pruning_loss=0.1406, mse_loss=0.6714
Epoch: [23]  [ 730/1251]  eta: 0:07:17  lr: 0.000018  loss: 3.5716 (3.4312)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [23]  [ 740/1251]  eta: 0:07:08  lr: 0.000018  loss: 3.5938 (3.4348)  time: 0.8103  data: 0.0005  max mem: 19734
Epoch: [23]  [ 750/1251]  eta: 0:07:00  lr: 0.000018  loss: 3.5586 (3.4306)  time: 0.8167  data: 0.0006  max mem: 19734
Epoch: [23]  [ 760/1251]  eta: 0:06:51  lr: 0.000018  loss: 3.1377 (3.4278)  time: 0.8101  data: 0.0005  max mem: 19734
Epoch: [23]  [ 770/1251]  eta: 0:06:43  lr: 0.000018  loss: 3.4786 (3.4299)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [23]  [ 780/1251]  eta: 0:06:34  lr: 0.000018  loss: 3.5194 (3.4275)  time: 0.8176  data: 0.0006  max mem: 19734
Epoch: [23]  [ 790/1251]  eta: 0:06:26  lr: 0.000018  loss: 3.5194 (3.4307)  time: 0.8294  data: 0.0005  max mem: 19734
Epoch: [23]  [ 800/1251]  eta: 0:06:17  lr: 0.000018  loss: 3.6200 (3.4315)  time: 0.8299  data: 0.0005  max mem: 19734
Epoch: [23]  [ 810/1251]  eta: 0:06:09  lr: 0.000018  loss: 3.5252 (3.4316)  time: 0.8173  data: 0.0005  max mem: 19734
Epoch: [23]  [ 820/1251]  eta: 0:06:00  lr: 0.000018  loss: 3.3480 (3.4281)  time: 0.8030  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3905, ratio_loss=0.0050, pruning_loss=0.1420, mse_loss=0.6772
Epoch: [23]  [ 830/1251]  eta: 0:05:52  lr: 0.000018  loss: 3.3068 (3.4276)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [23]  [ 840/1251]  eta: 0:05:43  lr: 0.000018  loss: 3.3446 (3.4247)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [23]  [ 850/1251]  eta: 0:05:35  lr: 0.000018  loss: 3.1618 (3.4236)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [23]  [ 860/1251]  eta: 0:05:26  lr: 0.000018  loss: 3.3130 (3.4235)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [23]  [ 870/1251]  eta: 0:05:18  lr: 0.000018  loss: 3.6905 (3.4259)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [23]  [ 880/1251]  eta: 0:05:09  lr: 0.000018  loss: 3.6493 (3.4269)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [23]  [ 890/1251]  eta: 0:05:01  lr: 0.000018  loss: 3.4781 (3.4260)  time: 0.8122  data: 0.0005  max mem: 19734
Epoch: [23]  [ 900/1251]  eta: 0:04:52  lr: 0.000018  loss: 3.5915 (3.4274)  time: 0.8203  data: 0.0004  max mem: 19734
Epoch: [23]  [ 910/1251]  eta: 0:04:44  lr: 0.000018  loss: 3.5959 (3.4296)  time: 0.8104  data: 0.0004  max mem: 19734
Epoch: [23]  [ 920/1251]  eta: 0:04:35  lr: 0.000018  loss: 3.6564 (3.4309)  time: 0.8094  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4136, ratio_loss=0.0051, pruning_loss=0.1411, mse_loss=0.6676
Epoch: [23]  [ 930/1251]  eta: 0:04:27  lr: 0.000018  loss: 3.6649 (3.4315)  time: 0.8206  data: 0.0005  max mem: 19734
Epoch: [23]  [ 940/1251]  eta: 0:04:19  lr: 0.000018  loss: 3.3874 (3.4300)  time: 0.8309  data: 0.0006  max mem: 19734
Epoch: [23]  [ 950/1251]  eta: 0:04:10  lr: 0.000018  loss: 3.5472 (3.4320)  time: 0.8290  data: 0.0006  max mem: 19734
Epoch: [23]  [ 960/1251]  eta: 0:04:02  lr: 0.000018  loss: 3.4861 (3.4299)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [23]  [ 970/1251]  eta: 0:03:54  lr: 0.000018  loss: 3.2324 (3.4290)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [23]  [ 980/1251]  eta: 0:03:45  lr: 0.000018  loss: 3.4156 (3.4298)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [23]  [ 990/1251]  eta: 0:03:37  lr: 0.000018  loss: 3.4320 (3.4272)  time: 0.8128  data: 0.0005  max mem: 19734
Epoch: [23]  [1000/1251]  eta: 0:03:28  lr: 0.000018  loss: 3.4320 (3.4269)  time: 0.8137  data: 0.0005  max mem: 19734
Epoch: [23]  [1010/1251]  eta: 0:03:20  lr: 0.000018  loss: 3.3322 (3.4250)  time: 0.8030  data: 0.0005  max mem: 19734
Epoch: [23]  [1020/1251]  eta: 0:03:12  lr: 0.000018  loss: 3.2673 (3.4250)  time: 0.8014  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3678, ratio_loss=0.0050, pruning_loss=0.1407, mse_loss=0.6687
Epoch: [23]  [1030/1251]  eta: 0:03:03  lr: 0.000018  loss: 3.4270 (3.4267)  time: 0.8093  data: 0.0004  max mem: 19734
Epoch: [23]  [1040/1251]  eta: 0:02:55  lr: 0.000018  loss: 3.7234 (3.4287)  time: 0.8101  data: 0.0005  max mem: 19734
Epoch: [23]  [1050/1251]  eta: 0:02:47  lr: 0.000018  loss: 3.6458 (3.4292)  time: 0.8142  data: 0.0004  max mem: 19734
Epoch: [23]  [1060/1251]  eta: 0:02:38  lr: 0.000018  loss: 3.6458 (3.4295)  time: 0.8144  data: 0.0004  max mem: 19734
Epoch: [23]  [1070/1251]  eta: 0:02:30  lr: 0.000018  loss: 3.5022 (3.4299)  time: 0.8105  data: 0.0005  max mem: 19734
Epoch: [23]  [1080/1251]  eta: 0:02:22  lr: 0.000018  loss: 3.5416 (3.4302)  time: 0.8275  data: 0.0005  max mem: 19734
Epoch: [23]  [1090/1251]  eta: 0:02:13  lr: 0.000018  loss: 3.4622 (3.4295)  time: 0.8333  data: 0.0005  max mem: 19734
Epoch: [23]  [1100/1251]  eta: 0:02:05  lr: 0.000018  loss: 3.4673 (3.4313)  time: 0.8161  data: 0.0005  max mem: 19734
Epoch: [23]  [1110/1251]  eta: 0:01:57  lr: 0.000018  loss: 3.5673 (3.4292)  time: 0.8012  data: 0.0004  max mem: 19734
Epoch: [23]  [1120/1251]  eta: 0:01:48  lr: 0.000018  loss: 2.9910 (3.4274)  time: 0.8002  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4237, ratio_loss=0.0053, pruning_loss=0.1413, mse_loss=0.6496
Epoch: [23]  [1130/1251]  eta: 0:01:40  lr: 0.000018  loss: 3.4775 (3.4280)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [23]  [1140/1251]  eta: 0:01:32  lr: 0.000018  loss: 3.4775 (3.4276)  time: 0.8152  data: 0.0005  max mem: 19734
Epoch: [23]  [1150/1251]  eta: 0:01:23  lr: 0.000018  loss: 3.4459 (3.4265)  time: 0.8132  data: 0.0005  max mem: 19734
Epoch: [23]  [1160/1251]  eta: 0:01:15  lr: 0.000018  loss: 3.5606 (3.4267)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [23]  [1170/1251]  eta: 0:01:07  lr: 0.000018  loss: 3.6397 (3.4274)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [23]  [1180/1251]  eta: 0:00:58  lr: 0.000018  loss: 3.6397 (3.4293)  time: 0.8046  data: 0.0006  max mem: 19734
Epoch: [23]  [1190/1251]  eta: 0:00:50  lr: 0.000018  loss: 3.6684 (3.4301)  time: 0.8149  data: 0.0013  max mem: 19734
Epoch: [23]  [1200/1251]  eta: 0:00:42  lr: 0.000018  loss: 3.7388 (3.4306)  time: 0.8047  data: 0.0011  max mem: 19734
Epoch: [23]  [1210/1251]  eta: 0:00:33  lr: 0.000018  loss: 3.6039 (3.4285)  time: 0.7932  data: 0.0001  max mem: 19734
Epoch: [23]  [1220/1251]  eta: 0:00:25  lr: 0.000018  loss: 3.6356 (3.4307)  time: 0.8022  data: 0.0001  max mem: 19734
loss info: cls_loss=3.4420, ratio_loss=0.0052, pruning_loss=0.1390, mse_loss=0.6717
Epoch: [23]  [1230/1251]  eta: 0:00:17  lr: 0.000018  loss: 3.6356 (3.4307)  time: 0.8172  data: 0.0001  max mem: 19734
Epoch: [23]  [1240/1251]  eta: 0:00:09  lr: 0.000018  loss: 3.4522 (3.4308)  time: 0.8139  data: 0.0001  max mem: 19734
Epoch: [23]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.5164 (3.4304)  time: 0.7971  data: 0.0001  max mem: 19734
Epoch: [23] Total time: 0:17:16 (0.8285 s / it)
Averaged stats: lr: 0.000018  loss: 3.5164 (3.4300)
Test:  [  0/261]  eta: 2:03:09  loss: 0.7148 (0.7148)  acc1: 83.8542 (83.8542)  acc5: 96.8750 (96.8750)  time: 28.3126  data: 28.1322  max mem: 19734
Test:  [ 10/261]  eta: 0:13:06  loss: 0.7148 (0.7291)  acc1: 83.8542 (83.9489)  acc5: 96.8750 (96.2595)  time: 3.1333  data: 2.9121  max mem: 19734
Test:  [ 20/261]  eta: 0:07:40  loss: 0.9278 (0.8978)  acc1: 78.6458 (79.2659)  acc5: 93.7500 (94.6677)  time: 0.5891  data: 0.2062  max mem: 19734
Test:  [ 30/261]  eta: 0:05:30  loss: 0.8022 (0.8156)  acc1: 83.3333 (82.1573)  acc5: 94.2708 (95.1781)  time: 0.4936  data: 0.0228  max mem: 19734
Test:  [ 40/261]  eta: 0:04:20  loss: 0.5766 (0.7825)  acc1: 87.5000 (83.0539)  acc5: 96.8750 (95.4903)  time: 0.4157  data: 0.1042  max mem: 19734
Test:  [ 50/261]  eta: 0:03:34  loss: 0.9184 (0.8492)  acc1: 78.6458 (81.1581)  acc5: 94.2708 (94.9653)  time: 0.3810  data: 0.0980  max mem: 19734
Test:  [ 60/261]  eta: 0:03:06  loss: 0.9736 (0.8592)  acc1: 76.0417 (80.7462)  acc5: 93.7500 (95.0051)  time: 0.4153  data: 0.0170  max mem: 19734
Test:  [ 70/261]  eta: 0:02:39  loss: 0.9508 (0.8612)  acc1: 76.5625 (80.2083)  acc5: 95.3125 (95.2245)  time: 0.3686  data: 0.0151  max mem: 19734
Test:  [ 80/261]  eta: 0:02:24  loss: 0.8686 (0.8637)  acc1: 79.1667 (80.3498)  acc5: 96.3542 (95.3125)  time: 0.3949  data: 0.1719  max mem: 19734
Test:  [ 90/261]  eta: 0:02:08  loss: 0.8060 (0.8511)  acc1: 83.3333 (80.6662)  acc5: 95.8333 (95.4384)  time: 0.4468  data: 0.2539  max mem: 19734
Test:  [100/261]  eta: 0:01:59  loss: 0.8196 (0.8560)  acc1: 83.3333 (80.5951)  acc5: 95.8333 (95.4672)  time: 0.5277  data: 0.3517  max mem: 19734
Test:  [110/261]  eta: 0:01:54  loss: 0.8919 (0.8801)  acc1: 75.5208 (80.0488)  acc5: 94.2708 (95.1858)  time: 0.7989  data: 0.5026  max mem: 19734
Test:  [120/261]  eta: 0:01:46  loss: 1.2186 (0.9190)  acc1: 70.8333 (79.1322)  acc5: 90.1042 (94.7142)  time: 0.8091  data: 0.3568  max mem: 19734
Test:  [130/261]  eta: 0:01:35  loss: 1.3662 (0.9642)  acc1: 68.2292 (78.2045)  acc5: 88.5417 (94.1635)  time: 0.5479  data: 0.1248  max mem: 19734
Test:  [140/261]  eta: 0:01:24  loss: 1.3125 (0.9914)  acc1: 68.7500 (77.5414)  acc5: 88.5417 (93.8830)  time: 0.3519  data: 0.0137  max mem: 19734
Test:  [150/261]  eta: 0:01:17  loss: 1.2715 (0.9973)  acc1: 72.3958 (77.4938)  acc5: 91.1458 (93.7224)  time: 0.5260  data: 0.2135  max mem: 19734
Test:  [160/261]  eta: 0:01:08  loss: 1.0003 (1.0161)  acc1: 78.6458 (77.1966)  acc5: 92.1875 (93.4297)  time: 0.5207  data: 0.2109  max mem: 19734
Test:  [170/261]  eta: 0:00:59  loss: 1.3047 (1.0465)  acc1: 66.6667 (76.4498)  acc5: 88.0208 (93.0952)  time: 0.3076  data: 0.0140  max mem: 19734
Test:  [180/261]  eta: 0:00:50  loss: 1.4660 (1.0647)  acc1: 64.5833 (76.0043)  acc5: 87.5000 (92.9098)  time: 0.2545  data: 0.0120  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.3743 (1.0780)  acc1: 66.6667 (75.7608)  acc5: 90.1042 (92.7411)  time: 0.1636  data: 0.0082  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.3720 (1.0935)  acc1: 72.3958 (75.4690)  acc5: 89.5833 (92.5036)  time: 0.1248  data: 0.0047  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.3622 (1.1078)  acc1: 69.2708 (75.1901)  acc5: 88.0208 (92.2739)  time: 0.1197  data: 0.0010  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.3928 (1.1266)  acc1: 67.1875 (74.7408)  acc5: 87.5000 (92.0437)  time: 0.1157  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4345 (1.1367)  acc1: 66.1458 (74.4792)  acc5: 89.5833 (91.9530)  time: 0.1153  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3534 (1.1445)  acc1: 68.7500 (74.2955)  acc5: 91.1458 (91.9130)  time: 0.1146  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0797 (1.1379)  acc1: 73.9583 (74.4273)  acc5: 93.2292 (92.0236)  time: 0.1145  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9208 (1.1377)  acc1: 77.6042 (74.4260)  acc5: 95.3125 (92.0840)  time: 0.1116  data: 0.0001  max mem: 19734
Test: Total time: 0:02:03 (0.4738 s / it)
* Acc@1 74.426 Acc@5 92.084 loss 1.138
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.46%
Epoch: [24]  [   0/1251]  eta: 5:42:38  lr: 0.000018  loss: 3.7560 (3.7560)  time: 16.4340  data: 13.4841  max mem: 19734
Epoch: [24]  [  10/1251]  eta: 0:52:47  lr: 0.000018  loss: 3.5104 (3.3669)  time: 2.5527  data: 1.2273  max mem: 19734
Epoch: [24]  [  20/1251]  eta: 0:35:15  lr: 0.000018  loss: 3.3585 (3.3671)  time: 0.9825  data: 0.0010  max mem: 19734
Epoch: [24]  [  30/1251]  eta: 0:28:56  lr: 0.000018  loss: 3.5798 (3.4118)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [24]  [  40/1251]  eta: 0:25:38  lr: 0.000018  loss: 3.4416 (3.4115)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [24]  [  50/1251]  eta: 0:23:34  lr: 0.000018  loss: 3.5034 (3.4432)  time: 0.7981  data: 0.0006  max mem: 19734
Epoch: [24]  [  60/1251]  eta: 0:22:08  lr: 0.000018  loss: 3.5034 (3.3917)  time: 0.7970  data: 0.0006  max mem: 19734
Epoch: [24]  [  70/1251]  eta: 0:21:06  lr: 0.000018  loss: 3.5113 (3.3864)  time: 0.8039  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4008, ratio_loss=0.0053, pruning_loss=0.1401, mse_loss=0.6258
Epoch: [24]  [  80/1251]  eta: 0:20:19  lr: 0.000018  loss: 3.7051 (3.4325)  time: 0.8163  data: 0.0005  max mem: 19734
Epoch: [24]  [  90/1251]  eta: 0:19:38  lr: 0.000018  loss: 3.8959 (3.4475)  time: 0.8132  data: 0.0005  max mem: 19734
Epoch: [24]  [ 100/1251]  eta: 0:19:06  lr: 0.000018  loss: 3.7647 (3.4594)  time: 0.8139  data: 0.0005  max mem: 19734
Epoch: [24]  [ 110/1251]  eta: 0:18:38  lr: 0.000018  loss: 3.7647 (3.4862)  time: 0.8215  data: 0.0005  max mem: 19734
Epoch: [24]  [ 120/1251]  eta: 0:18:14  lr: 0.000018  loss: 3.7118 (3.4744)  time: 0.8250  data: 0.0005  max mem: 19734
Epoch: [24]  [ 130/1251]  eta: 0:17:50  lr: 0.000018  loss: 3.6296 (3.4901)  time: 0.8151  data: 0.0006  max mem: 19734
Epoch: [24]  [ 140/1251]  eta: 0:17:29  lr: 0.000018  loss: 3.5891 (3.4774)  time: 0.8052  data: 0.0007  max mem: 19734
Epoch: [24]  [ 150/1251]  eta: 0:17:11  lr: 0.000018  loss: 3.5902 (3.4898)  time: 0.8161  data: 0.0007  max mem: 19734
Epoch: [24]  [ 160/1251]  eta: 0:16:53  lr: 0.000018  loss: 3.4551 (3.4766)  time: 0.8128  data: 0.0008  max mem: 19734
Epoch: [24]  [ 170/1251]  eta: 0:16:35  lr: 0.000018  loss: 3.3854 (3.4812)  time: 0.8048  data: 0.0007  max mem: 19734
loss info: cls_loss=3.5108, ratio_loss=0.0054, pruning_loss=0.1373, mse_loss=0.6487
Epoch: [24]  [ 180/1251]  eta: 0:16:19  lr: 0.000018  loss: 3.4622 (3.4863)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [24]  [ 190/1251]  eta: 0:16:04  lr: 0.000018  loss: 3.4370 (3.4812)  time: 0.8039  data: 0.0005  max mem: 19734
Epoch: [24]  [ 200/1251]  eta: 0:15:49  lr: 0.000018  loss: 3.1069 (3.4621)  time: 0.8048  data: 0.0004  max mem: 19734
Epoch: [24]  [ 210/1251]  eta: 0:15:36  lr: 0.000018  loss: 3.2066 (3.4519)  time: 0.8100  data: 0.0004  max mem: 19734
Epoch: [24]  [ 220/1251]  eta: 0:15:23  lr: 0.000018  loss: 3.4714 (3.4518)  time: 0.8147  data: 0.0006  max mem: 19734
Epoch: [24]  [ 230/1251]  eta: 0:15:10  lr: 0.000018  loss: 3.7038 (3.4630)  time: 0.8062  data: 0.0005  max mem: 19734
Epoch: [24]  [ 240/1251]  eta: 0:14:57  lr: 0.000018  loss: 3.7038 (3.4638)  time: 0.7993  data: 0.0004  max mem: 19734
Epoch: [24]  [ 250/1251]  eta: 0:14:45  lr: 0.000018  loss: 3.4592 (3.4592)  time: 0.8033  data: 0.0004  max mem: 19734
Epoch: [24]  [ 260/1251]  eta: 0:14:35  lr: 0.000018  loss: 3.4309 (3.4532)  time: 0.8283  data: 0.0005  max mem: 19734
Epoch: [24]  [ 270/1251]  eta: 0:14:24  lr: 0.000018  loss: 3.3315 (3.4470)  time: 0.8339  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3775, ratio_loss=0.0052, pruning_loss=0.1395, mse_loss=0.6410
Epoch: [24]  [ 280/1251]  eta: 0:14:12  lr: 0.000018  loss: 3.4935 (3.4486)  time: 0.8087  data: 0.0004  max mem: 19734
Epoch: [24]  [ 290/1251]  eta: 0:14:00  lr: 0.000018  loss: 3.5832 (3.4493)  time: 0.7980  data: 0.0005  max mem: 19734
Epoch: [24]  [ 300/1251]  eta: 0:13:50  lr: 0.000018  loss: 3.5839 (3.4506)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [24]  [ 310/1251]  eta: 0:13:39  lr: 0.000018  loss: 3.4462 (3.4487)  time: 0.8137  data: 0.0005  max mem: 19734
Epoch: [24]  [ 320/1251]  eta: 0:13:29  lr: 0.000018  loss: 3.4395 (3.4489)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [24]  [ 330/1251]  eta: 0:13:18  lr: 0.000018  loss: 3.3884 (3.4438)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [24]  [ 340/1251]  eta: 0:13:08  lr: 0.000018  loss: 3.3435 (3.4481)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [24]  [ 350/1251]  eta: 0:12:58  lr: 0.000018  loss: 3.3435 (3.4449)  time: 0.8055  data: 0.0005  max mem: 19734
Epoch: [24]  [ 360/1251]  eta: 0:12:48  lr: 0.000018  loss: 3.3294 (3.4446)  time: 0.8165  data: 0.0005  max mem: 19734
Epoch: [24]  [ 370/1251]  eta: 0:12:38  lr: 0.000018  loss: 3.5004 (3.4435)  time: 0.8236  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3864, ratio_loss=0.0054, pruning_loss=0.1391, mse_loss=0.6295
Epoch: [24]  [ 380/1251]  eta: 0:12:28  lr: 0.000018  loss: 3.3941 (3.4366)  time: 0.8110  data: 0.0004  max mem: 19734
Epoch: [24]  [ 390/1251]  eta: 0:12:19  lr: 0.000018  loss: 3.1766 (3.4310)  time: 0.8113  data: 0.0004  max mem: 19734
Epoch: [24]  [ 400/1251]  eta: 0:12:10  lr: 0.000018  loss: 3.3683 (3.4356)  time: 0.8192  data: 0.0005  max mem: 19734
Epoch: [24]  [ 410/1251]  eta: 0:12:00  lr: 0.000018  loss: 3.6139 (3.4376)  time: 0.8195  data: 0.0006  max mem: 19734
Epoch: [24]  [ 420/1251]  eta: 0:11:51  lr: 0.000018  loss: 3.6557 (3.4406)  time: 0.8210  data: 0.0005  max mem: 19734
Epoch: [24]  [ 430/1251]  eta: 0:11:41  lr: 0.000018  loss: 3.5142 (3.4341)  time: 0.8140  data: 0.0005  max mem: 19734
Epoch: [24]  [ 440/1251]  eta: 0:11:32  lr: 0.000018  loss: 3.5085 (3.4363)  time: 0.8073  data: 0.0004  max mem: 19734
Epoch: [24]  [ 450/1251]  eta: 0:11:23  lr: 0.000018  loss: 3.6454 (3.4379)  time: 0.8145  data: 0.0004  max mem: 19734
Epoch: [24]  [ 460/1251]  eta: 0:11:13  lr: 0.000018  loss: 3.6454 (3.4422)  time: 0.8109  data: 0.0005  max mem: 19734
Epoch: [24]  [ 470/1251]  eta: 0:11:04  lr: 0.000018  loss: 3.6181 (3.4422)  time: 0.8020  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4292, ratio_loss=0.0055, pruning_loss=0.1385, mse_loss=0.6472
Epoch: [24]  [ 480/1251]  eta: 0:10:55  lr: 0.000018  loss: 3.4427 (3.4411)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [24]  [ 490/1251]  eta: 0:10:46  lr: 0.000018  loss: 3.1322 (3.4348)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [24]  [ 500/1251]  eta: 0:10:37  lr: 0.000018  loss: 3.0569 (3.4288)  time: 0.8082  data: 0.0005  max mem: 19734
Epoch: [24]  [ 510/1251]  eta: 0:10:28  lr: 0.000018  loss: 3.0385 (3.4231)  time: 0.8141  data: 0.0005  max mem: 19734
Epoch: [24]  [ 520/1251]  eta: 0:10:18  lr: 0.000018  loss: 3.0385 (3.4239)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [24]  [ 530/1251]  eta: 0:10:09  lr: 0.000018  loss: 3.6210 (3.4248)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [24]  [ 540/1251]  eta: 0:10:01  lr: 0.000018  loss: 3.2900 (3.4209)  time: 0.8144  data: 0.0005  max mem: 19734
Epoch: [24]  [ 550/1251]  eta: 0:09:52  lr: 0.000018  loss: 3.1711 (3.4158)  time: 0.8330  data: 0.0005  max mem: 19734
Epoch: [24]  [ 560/1251]  eta: 0:09:43  lr: 0.000018  loss: 3.3559 (3.4183)  time: 0.8328  data: 0.0004  max mem: 19734
Epoch: [24]  [ 570/1251]  eta: 0:09:34  lr: 0.000018  loss: 3.5836 (3.4196)  time: 0.8116  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2996, ratio_loss=0.0050, pruning_loss=0.1423, mse_loss=0.6640
Epoch: [24]  [ 580/1251]  eta: 0:09:25  lr: 0.000018  loss: 3.5288 (3.4192)  time: 0.7988  data: 0.0004  max mem: 19734
Epoch: [24]  [ 590/1251]  eta: 0:09:17  lr: 0.000018  loss: 3.5288 (3.4198)  time: 0.8114  data: 0.0005  max mem: 19734
Epoch: [24]  [ 600/1251]  eta: 0:09:08  lr: 0.000018  loss: 3.4815 (3.4213)  time: 0.8142  data: 0.0005  max mem: 19734
Epoch: [24]  [ 610/1251]  eta: 0:08:59  lr: 0.000018  loss: 3.4673 (3.4198)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [24]  [ 620/1251]  eta: 0:08:50  lr: 0.000018  loss: 3.4953 (3.4201)  time: 0.8063  data: 0.0005  max mem: 19734
Epoch: [24]  [ 630/1251]  eta: 0:08:42  lr: 0.000018  loss: 3.5896 (3.4225)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [24]  [ 640/1251]  eta: 0:08:33  lr: 0.000018  loss: 3.6380 (3.4268)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [24]  [ 650/1251]  eta: 0:08:24  lr: 0.000018  loss: 3.6120 (3.4240)  time: 0.8039  data: 0.0004  max mem: 19734
Epoch: [24]  [ 660/1251]  eta: 0:08:16  lr: 0.000018  loss: 3.1555 (3.4197)  time: 0.8092  data: 0.0005  max mem: 19734
Epoch: [24]  [ 670/1251]  eta: 0:08:07  lr: 0.000018  loss: 3.1555 (3.4183)  time: 0.8064  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3762, ratio_loss=0.0052, pruning_loss=0.1409, mse_loss=0.6612
Epoch: [24]  [ 680/1251]  eta: 0:07:58  lr: 0.000018  loss: 3.4224 (3.4183)  time: 0.7981  data: 0.0005  max mem: 19734
Epoch: [24]  [ 690/1251]  eta: 0:07:49  lr: 0.000018  loss: 3.5264 (3.4187)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [24]  [ 700/1251]  eta: 0:07:41  lr: 0.000018  loss: 3.5309 (3.4190)  time: 0.8252  data: 0.0005  max mem: 19734
Epoch: [24]  [ 710/1251]  eta: 0:07:33  lr: 0.000018  loss: 3.5309 (3.4201)  time: 0.8262  data: 0.0005  max mem: 19734
Epoch: [24]  [ 720/1251]  eta: 0:07:24  lr: 0.000018  loss: 3.5245 (3.4184)  time: 0.8098  data: 0.0005  max mem: 19734
Epoch: [24]  [ 730/1251]  eta: 0:07:15  lr: 0.000018  loss: 3.5691 (3.4219)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [24]  [ 740/1251]  eta: 0:07:07  lr: 0.000018  loss: 3.8367 (3.4226)  time: 0.8122  data: 0.0005  max mem: 19734
Epoch: [24]  [ 750/1251]  eta: 0:06:58  lr: 0.000018  loss: 3.7336 (3.4264)  time: 0.8105  data: 0.0004  max mem: 19734
Epoch: [24]  [ 760/1251]  eta: 0:06:50  lr: 0.000018  loss: 3.6564 (3.4272)  time: 0.7989  data: 0.0004  max mem: 19734
Epoch: [24]  [ 770/1251]  eta: 0:06:41  lr: 0.000018  loss: 3.5518 (3.4298)  time: 0.8004  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4936, ratio_loss=0.0050, pruning_loss=0.1380, mse_loss=0.6871
Epoch: [24]  [ 780/1251]  eta: 0:06:33  lr: 0.000018  loss: 3.5332 (3.4310)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [24]  [ 790/1251]  eta: 0:06:24  lr: 0.000018  loss: 3.7489 (3.4360)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [24]  [ 800/1251]  eta: 0:06:16  lr: 0.000018  loss: 3.7527 (3.4396)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [24]  [ 810/1251]  eta: 0:06:07  lr: 0.000018  loss: 3.7257 (3.4406)  time: 0.8081  data: 0.0004  max mem: 19734
Epoch: [24]  [ 820/1251]  eta: 0:05:59  lr: 0.000018  loss: 3.6430 (3.4411)  time: 0.8110  data: 0.0004  max mem: 19734
Epoch: [24]  [ 830/1251]  eta: 0:05:50  lr: 0.000018  loss: 3.4973 (3.4422)  time: 0.8079  data: 0.0004  max mem: 19734
Epoch: [24]  [ 840/1251]  eta: 0:05:42  lr: 0.000018  loss: 3.6720 (3.4451)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [24]  [ 850/1251]  eta: 0:05:33  lr: 0.000018  loss: 3.6488 (3.4462)  time: 0.8180  data: 0.0004  max mem: 19734
Epoch: [24]  [ 860/1251]  eta: 0:05:25  lr: 0.000018  loss: 3.5508 (3.4466)  time: 0.8293  data: 0.0005  max mem: 19734
Epoch: [24]  [ 870/1251]  eta: 0:05:17  lr: 0.000018  loss: 3.4089 (3.4441)  time: 0.8101  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5252, ratio_loss=0.0051, pruning_loss=0.1365, mse_loss=0.6609
Epoch: [24]  [ 880/1251]  eta: 0:05:08  lr: 0.000018  loss: 3.3454 (3.4428)  time: 0.8009  data: 0.0004  max mem: 19734
Epoch: [24]  [ 890/1251]  eta: 0:05:00  lr: 0.000018  loss: 3.5764 (3.4445)  time: 0.8048  data: 0.0004  max mem: 19734
Epoch: [24]  [ 900/1251]  eta: 0:04:51  lr: 0.000018  loss: 3.6382 (3.4430)  time: 0.8032  data: 0.0004  max mem: 19734
Epoch: [24]  [ 910/1251]  eta: 0:04:43  lr: 0.000018  loss: 3.7210 (3.4450)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [24]  [ 920/1251]  eta: 0:04:34  lr: 0.000018  loss: 3.5045 (3.4413)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [24]  [ 930/1251]  eta: 0:04:26  lr: 0.000018  loss: 3.3239 (3.4391)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [24]  [ 940/1251]  eta: 0:04:18  lr: 0.000018  loss: 3.5404 (3.4405)  time: 0.8092  data: 0.0004  max mem: 19734
Epoch: [24]  [ 950/1251]  eta: 0:04:09  lr: 0.000018  loss: 3.5284 (3.4392)  time: 0.8169  data: 0.0004  max mem: 19734
Epoch: [24]  [ 960/1251]  eta: 0:04:01  lr: 0.000018  loss: 3.3465 (3.4383)  time: 0.8133  data: 0.0004  max mem: 19734
Epoch: [24]  [ 970/1251]  eta: 0:03:53  lr: 0.000018  loss: 3.3652 (3.4372)  time: 0.8049  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3660, ratio_loss=0.0053, pruning_loss=0.1404, mse_loss=0.6489
Epoch: [24]  [ 980/1251]  eta: 0:03:44  lr: 0.000018  loss: 3.4913 (3.4357)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [24]  [ 990/1251]  eta: 0:03:36  lr: 0.000018  loss: 3.5479 (3.4375)  time: 0.8291  data: 0.0004  max mem: 19734
Epoch: [24]  [1000/1251]  eta: 0:03:28  lr: 0.000018  loss: 3.5358 (3.4359)  time: 0.8325  data: 0.0005  max mem: 19734
Epoch: [24]  [1010/1251]  eta: 0:03:19  lr: 0.000018  loss: 3.3943 (3.4373)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [24]  [1020/1251]  eta: 0:03:11  lr: 0.000018  loss: 3.5977 (3.4394)  time: 0.7984  data: 0.0005  max mem: 19734
Epoch: [24]  [1030/1251]  eta: 0:03:03  lr: 0.000018  loss: 3.7134 (3.4414)  time: 0.8000  data: 0.0004  max mem: 19734
Epoch: [24]  [1040/1251]  eta: 0:02:54  lr: 0.000018  loss: 3.6783 (3.4428)  time: 0.8085  data: 0.0004  max mem: 19734
Epoch: [24]  [1050/1251]  eta: 0:02:46  lr: 0.000018  loss: 3.6543 (3.4436)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [24]  [1060/1251]  eta: 0:02:38  lr: 0.000018  loss: 3.7212 (3.4466)  time: 0.7982  data: 0.0005  max mem: 19734
Epoch: [24]  [1070/1251]  eta: 0:02:29  lr: 0.000018  loss: 3.5541 (3.4469)  time: 0.7995  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5279, ratio_loss=0.0050, pruning_loss=0.1370, mse_loss=0.6436
Epoch: [24]  [1080/1251]  eta: 0:02:21  lr: 0.000018  loss: 3.5349 (3.4490)  time: 0.8019  data: 0.0006  max mem: 19734
Epoch: [24]  [1090/1251]  eta: 0:02:13  lr: 0.000018  loss: 3.5460 (3.4497)  time: 0.8063  data: 0.0006  max mem: 19734
Epoch: [24]  [1100/1251]  eta: 0:02:04  lr: 0.000018  loss: 3.5742 (3.4498)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [24]  [1110/1251]  eta: 0:01:56  lr: 0.000018  loss: 3.6561 (3.4525)  time: 0.8103  data: 0.0005  max mem: 19734
Epoch: [24]  [1120/1251]  eta: 0:01:48  lr: 0.000018  loss: 3.7072 (3.4533)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [24]  [1130/1251]  eta: 0:01:40  lr: 0.000018  loss: 3.6760 (3.4534)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [24]  [1140/1251]  eta: 0:01:31  lr: 0.000018  loss: 3.5503 (3.4521)  time: 0.8142  data: 0.0004  max mem: 19734
Epoch: [24]  [1150/1251]  eta: 0:01:23  lr: 0.000018  loss: 3.5032 (3.4522)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [24]  [1160/1251]  eta: 0:01:15  lr: 0.000018  loss: 3.5032 (3.4513)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [24]  [1170/1251]  eta: 0:01:06  lr: 0.000018  loss: 3.1407 (3.4493)  time: 0.7987  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4433, ratio_loss=0.0053, pruning_loss=0.1380, mse_loss=0.6614
Epoch: [24]  [1180/1251]  eta: 0:00:58  lr: 0.000018  loss: 3.1407 (3.4484)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [24]  [1190/1251]  eta: 0:00:50  lr: 0.000018  loss: 3.4588 (3.4476)  time: 0.8077  data: 0.0013  max mem: 19734
Epoch: [24]  [1200/1251]  eta: 0:00:42  lr: 0.000018  loss: 3.3162 (3.4461)  time: 0.8103  data: 0.0011  max mem: 19734
Epoch: [24]  [1210/1251]  eta: 0:00:33  lr: 0.000018  loss: 3.4533 (3.4468)  time: 0.7998  data: 0.0001  max mem: 19734
Epoch: [24]  [1220/1251]  eta: 0:00:25  lr: 0.000018  loss: 3.6690 (3.4478)  time: 0.7934  data: 0.0002  max mem: 19734
Epoch: [24]  [1230/1251]  eta: 0:00:17  lr: 0.000018  loss: 3.6690 (3.4483)  time: 0.7941  data: 0.0002  max mem: 19734
Epoch: [24]  [1240/1251]  eta: 0:00:09  lr: 0.000018  loss: 3.6609 (3.4493)  time: 0.7943  data: 0.0002  max mem: 19734
Epoch: [24]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.5865 (3.4484)  time: 0.7966  data: 0.0002  max mem: 19734
Epoch: [24] Total time: 0:17:12 (0.8252 s / it)
Averaged stats: lr: 0.000018  loss: 3.5865 (3.4325)
Test:  [  0/261]  eta: 1:42:23  loss: 0.7014 (0.7014)  acc1: 83.8542 (83.8542)  acc5: 96.3542 (96.3542)  time: 23.5380  data: 23.0608  max mem: 19734
Test:  [ 10/261]  eta: 0:14:12  loss: 0.7014 (0.7221)  acc1: 83.8542 (83.6174)  acc5: 96.3542 (96.3068)  time: 3.3965  data: 2.8540  max mem: 19734
Test:  [ 20/261]  eta: 0:07:53  loss: 0.9378 (0.9015)  acc1: 78.6458 (78.9683)  acc5: 94.7917 (94.7917)  time: 0.8873  data: 0.4238  max mem: 19734
Test:  [ 30/261]  eta: 0:05:24  loss: 0.7771 (0.8206)  acc1: 83.8542 (81.8548)  acc5: 94.7917 (95.3293)  time: 0.3081  data: 0.0182  max mem: 19734
Test:  [ 40/261]  eta: 0:04:17  loss: 0.5937 (0.7909)  acc1: 88.0208 (82.7363)  acc5: 96.8750 (95.5920)  time: 0.3272  data: 0.1112  max mem: 19734
Test:  [ 50/261]  eta: 0:03:25  loss: 0.9379 (0.8562)  acc1: 76.5625 (80.8109)  acc5: 93.7500 (95.1491)  time: 0.3114  data: 0.1029  max mem: 19734
Test:  [ 60/261]  eta: 0:02:49  loss: 1.0085 (0.8673)  acc1: 75.0000 (80.4047)  acc5: 93.7500 (95.0820)  time: 0.1793  data: 0.0085  max mem: 19734
Test:  [ 70/261]  eta: 0:02:29  loss: 0.9593 (0.8668)  acc1: 75.5208 (79.8929)  acc5: 95.3125 (95.2685)  time: 0.2894  data: 0.0692  max mem: 19734
Test:  [ 80/261]  eta: 0:02:11  loss: 0.8726 (0.8680)  acc1: 80.2083 (80.0733)  acc5: 96.8750 (95.3511)  time: 0.3733  data: 0.0769  max mem: 19734
Test:  [ 90/261]  eta: 0:02:01  loss: 0.7910 (0.8537)  acc1: 82.2917 (80.5117)  acc5: 95.8333 (95.4270)  time: 0.4641  data: 0.1360  max mem: 19734
Test:  [100/261]  eta: 0:02:03  loss: 0.7990 (0.8579)  acc1: 84.8958 (80.5126)  acc5: 95.3125 (95.4878)  time: 0.9233  data: 0.5626  max mem: 19734
Test:  [110/261]  eta: 0:01:50  loss: 0.8879 (0.8817)  acc1: 77.6042 (79.9925)  acc5: 94.7917 (95.1858)  time: 0.8259  data: 0.4506  max mem: 19734
Test:  [120/261]  eta: 0:01:39  loss: 1.2682 (0.9218)  acc1: 69.7917 (79.0160)  acc5: 90.6250 (94.6970)  time: 0.3902  data: 0.0256  max mem: 19734
Test:  [130/261]  eta: 0:01:28  loss: 1.4125 (0.9665)  acc1: 65.6250 (78.0972)  acc5: 86.9792 (94.1237)  time: 0.3605  data: 0.0258  max mem: 19734
Test:  [140/261]  eta: 0:01:18  loss: 1.3266 (0.9922)  acc1: 69.2708 (77.4232)  acc5: 88.5417 (93.8645)  time: 0.3293  data: 0.0181  max mem: 19734
Test:  [150/261]  eta: 0:01:09  loss: 1.2349 (0.9984)  acc1: 72.3958 (77.3627)  acc5: 91.1458 (93.6845)  time: 0.3114  data: 0.0150  max mem: 19734
Test:  [160/261]  eta: 0:01:02  loss: 0.9928 (1.0194)  acc1: 77.6042 (76.9798)  acc5: 91.6667 (93.3877)  time: 0.3714  data: 0.0155  max mem: 19734
Test:  [170/261]  eta: 0:00:57  loss: 1.2872 (1.0510)  acc1: 65.6250 (76.2244)  acc5: 88.5417 (93.0190)  time: 0.6215  data: 0.2285  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: 1.4580 (1.0680)  acc1: 64.5833 (75.7769)  acc5: 89.0625 (92.8580)  time: 0.6034  data: 0.2312  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.3587 (1.0819)  acc1: 68.2292 (75.5508)  acc5: 90.6250 (92.6947)  time: 0.3865  data: 0.0166  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.3506 (1.0982)  acc1: 70.8333 (75.2436)  acc5: 89.5833 (92.4518)  time: 0.3634  data: 0.0492  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.3506 (1.1119)  acc1: 70.8333 (75.0222)  acc5: 88.5417 (92.2739)  time: 0.2371  data: 0.0430  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4626 (1.1322)  acc1: 67.7083 (74.5098)  acc5: 88.5417 (92.0320)  time: 0.1163  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4626 (1.1419)  acc1: 65.6250 (74.2537)  acc5: 89.0625 (91.9417)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3376 (1.1509)  acc1: 68.2292 (74.0448)  acc5: 91.1458 (91.8871)  time: 0.1146  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0745 (1.1429)  acc1: 75.5208 (74.2281)  acc5: 94.2708 (92.0174)  time: 0.1141  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9277 (1.1431)  acc1: 77.0833 (74.2500)  acc5: 95.3125 (92.0640)  time: 0.1111  data: 0.0001  max mem: 19734
Test: Total time: 0:02:05 (0.4815 s / it)
* Acc@1 74.250 Acc@5 92.064 loss 1.143
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.46%
Epoch: [25]  [   0/1251]  eta: 6:46:19  lr: 0.000017  loss: 4.1752 (4.1752)  time: 19.4877  data: 12.1317  max mem: 19734
Epoch: [25]  [  10/1251]  eta: 0:57:08  lr: 0.000017  loss: 3.6619 (3.6337)  time: 2.7628  data: 1.1069  max mem: 19734
Epoch: [25]  [  20/1251]  eta: 0:37:48  lr: 0.000017  loss: 3.4723 (3.4455)  time: 0.9605  data: 0.0027  max mem: 19734
loss info: cls_loss=3.3908, ratio_loss=0.0055, pruning_loss=0.1409, mse_loss=0.6854
Epoch: [25]  [  30/1251]  eta: 0:30:43  lr: 0.000017  loss: 3.0534 (3.3504)  time: 0.8211  data: 0.0008  max mem: 19734
Epoch: [25]  [  40/1251]  eta: 0:26:58  lr: 0.000017  loss: 3.3208 (3.3767)  time: 0.8057  data: 0.0007  max mem: 19734
Epoch: [25]  [  50/1251]  eta: 0:24:43  lr: 0.000017  loss: 3.3138 (3.3292)  time: 0.8091  data: 0.0006  max mem: 19734
Epoch: [25]  [  60/1251]  eta: 0:23:05  lr: 0.000017  loss: 3.6830 (3.4069)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [25]  [  70/1251]  eta: 0:21:53  lr: 0.000017  loss: 3.6844 (3.4094)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [25]  [  80/1251]  eta: 0:20:57  lr: 0.000017  loss: 3.5148 (3.3968)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [25]  [  90/1251]  eta: 0:20:12  lr: 0.000017  loss: 3.6613 (3.4315)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [25]  [ 100/1251]  eta: 0:19:34  lr: 0.000017  loss: 3.6158 (3.4416)  time: 0.8036  data: 0.0004  max mem: 19734
Epoch: [25]  [ 110/1251]  eta: 0:19:01  lr: 0.000017  loss: 3.5888 (3.4338)  time: 0.8021  data: 0.0004  max mem: 19734
Epoch: [25]  [ 120/1251]  eta: 0:18:34  lr: 0.000017  loss: 3.6242 (3.4350)  time: 0.8083  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4295, ratio_loss=0.0052, pruning_loss=0.1395, mse_loss=0.6490
Epoch: [25]  [ 130/1251]  eta: 0:18:09  lr: 0.000017  loss: 3.6242 (3.4307)  time: 0.8159  data: 0.0004  max mem: 19734
Epoch: [25]  [ 140/1251]  eta: 0:17:46  lr: 0.000017  loss: 3.3942 (3.4261)  time: 0.8064  data: 0.0004  max mem: 19734
Epoch: [25]  [ 150/1251]  eta: 0:17:25  lr: 0.000017  loss: 3.4418 (3.4380)  time: 0.8010  data: 0.0004  max mem: 19734
Epoch: [25]  [ 160/1251]  eta: 0:17:07  lr: 0.000017  loss: 3.7175 (3.4467)  time: 0.8129  data: 0.0004  max mem: 19734
Epoch: [25]  [ 170/1251]  eta: 0:16:51  lr: 0.000017  loss: 3.6028 (3.4374)  time: 0.8319  data: 0.0004  max mem: 19734
Epoch: [25]  [ 180/1251]  eta: 0:16:35  lr: 0.000017  loss: 3.4887 (3.4344)  time: 0.8310  data: 0.0004  max mem: 19734
Epoch: [25]  [ 190/1251]  eta: 0:16:19  lr: 0.000017  loss: 3.5929 (3.4384)  time: 0.8111  data: 0.0005  max mem: 19734
Epoch: [25]  [ 200/1251]  eta: 0:16:04  lr: 0.000017  loss: 3.6208 (3.4445)  time: 0.8138  data: 0.0006  max mem: 19734
Epoch: [25]  [ 210/1251]  eta: 0:15:50  lr: 0.000017  loss: 3.5292 (3.4400)  time: 0.8208  data: 0.0006  max mem: 19734
Epoch: [25]  [ 220/1251]  eta: 0:15:36  lr: 0.000017  loss: 3.6401 (3.4379)  time: 0.8121  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4438, ratio_loss=0.0050, pruning_loss=0.1389, mse_loss=0.6377
Epoch: [25]  [ 230/1251]  eta: 0:15:22  lr: 0.000017  loss: 3.7063 (3.4457)  time: 0.8046  data: 0.0004  max mem: 19734
Epoch: [25]  [ 240/1251]  eta: 0:15:09  lr: 0.000017  loss: 3.6136 (3.4481)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [25]  [ 250/1251]  eta: 0:14:56  lr: 0.000017  loss: 3.4695 (3.4534)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [25]  [ 260/1251]  eta: 0:14:44  lr: 0.000017  loss: 3.5165 (3.4545)  time: 0.8104  data: 0.0005  max mem: 19734
Epoch: [25]  [ 270/1251]  eta: 0:14:33  lr: 0.000017  loss: 3.4436 (3.4382)  time: 0.8226  data: 0.0005  max mem: 19734
Epoch: [25]  [ 280/1251]  eta: 0:14:21  lr: 0.000017  loss: 2.8919 (3.4333)  time: 0.8149  data: 0.0007  max mem: 19734
Epoch: [25]  [ 290/1251]  eta: 0:14:09  lr: 0.000017  loss: 3.4256 (3.4295)  time: 0.8061  data: 0.0007  max mem: 19734
Epoch: [25]  [ 300/1251]  eta: 0:13:58  lr: 0.000017  loss: 3.5997 (3.4365)  time: 0.8068  data: 0.0006  max mem: 19734
Epoch: [25]  [ 310/1251]  eta: 0:13:48  lr: 0.000017  loss: 3.6079 (3.4390)  time: 0.8226  data: 0.0005  max mem: 19734
Epoch: [25]  [ 320/1251]  eta: 0:13:38  lr: 0.000017  loss: 3.4862 (3.4436)  time: 0.8388  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4149, ratio_loss=0.0054, pruning_loss=0.1390, mse_loss=0.6375
Epoch: [25]  [ 330/1251]  eta: 0:13:27  lr: 0.000017  loss: 3.3242 (3.4371)  time: 0.8299  data: 0.0005  max mem: 19734
Epoch: [25]  [ 340/1251]  eta: 0:13:17  lr: 0.000017  loss: 3.3242 (3.4406)  time: 0.8125  data: 0.0005  max mem: 19734
Epoch: [25]  [ 350/1251]  eta: 0:13:07  lr: 0.000017  loss: 3.3490 (3.4375)  time: 0.8112  data: 0.0005  max mem: 19734
Epoch: [25]  [ 360/1251]  eta: 0:12:56  lr: 0.000017  loss: 3.1736 (3.4345)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [25]  [ 370/1251]  eta: 0:12:46  lr: 0.000017  loss: 3.5207 (3.4346)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [25]  [ 380/1251]  eta: 0:12:36  lr: 0.000017  loss: 3.5207 (3.4359)  time: 0.8055  data: 0.0005  max mem: 19734
Epoch: [25]  [ 390/1251]  eta: 0:12:26  lr: 0.000017  loss: 3.5338 (3.4370)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [25]  [ 400/1251]  eta: 0:12:16  lr: 0.000017  loss: 3.6543 (3.4400)  time: 0.8062  data: 0.0006  max mem: 19734
Epoch: [25]  [ 410/1251]  eta: 0:12:06  lr: 0.000017  loss: 3.6494 (3.4414)  time: 0.8109  data: 0.0006  max mem: 19734
Epoch: [25]  [ 420/1251]  eta: 0:11:56  lr: 0.000017  loss: 3.5296 (3.4368)  time: 0.8165  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4175, ratio_loss=0.0053, pruning_loss=0.1377, mse_loss=0.6373
Epoch: [25]  [ 430/1251]  eta: 0:11:47  lr: 0.000017  loss: 3.5296 (3.4412)  time: 0.8094  data: 0.0005  max mem: 19734
Epoch: [25]  [ 440/1251]  eta: 0:11:37  lr: 0.000017  loss: 3.6007 (3.4411)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [25]  [ 450/1251]  eta: 0:11:27  lr: 0.000017  loss: 3.4328 (3.4404)  time: 0.8085  data: 0.0005  max mem: 19734
Epoch: [25]  [ 460/1251]  eta: 0:11:18  lr: 0.000017  loss: 3.1837 (3.4327)  time: 0.8245  data: 0.0007  max mem: 19734
Epoch: [25]  [ 470/1251]  eta: 0:11:09  lr: 0.000017  loss: 3.3406 (3.4342)  time: 0.8342  data: 0.0007  max mem: 19734
Epoch: [25]  [ 480/1251]  eta: 0:11:00  lr: 0.000017  loss: 3.5453 (3.4342)  time: 0.8244  data: 0.0006  max mem: 19734
Epoch: [25]  [ 490/1251]  eta: 0:10:51  lr: 0.000017  loss: 3.5726 (3.4371)  time: 0.8136  data: 0.0006  max mem: 19734
Epoch: [25]  [ 500/1251]  eta: 0:10:42  lr: 0.000017  loss: 3.7549 (3.4420)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [25]  [ 510/1251]  eta: 0:10:32  lr: 0.000017  loss: 3.7701 (3.4447)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [25]  [ 520/1251]  eta: 0:10:23  lr: 0.000017  loss: 3.6148 (3.4485)  time: 0.8025  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4537, ratio_loss=0.0057, pruning_loss=0.1364, mse_loss=0.6386
Epoch: [25]  [ 530/1251]  eta: 0:10:14  lr: 0.000017  loss: 3.5788 (3.4498)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [25]  [ 540/1251]  eta: 0:10:05  lr: 0.000017  loss: 3.5024 (3.4504)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [25]  [ 550/1251]  eta: 0:09:56  lr: 0.000017  loss: 3.5024 (3.4466)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [25]  [ 560/1251]  eta: 0:09:47  lr: 0.000017  loss: 3.4538 (3.4485)  time: 0.8209  data: 0.0005  max mem: 19734
Epoch: [25]  [ 570/1251]  eta: 0:09:38  lr: 0.000017  loss: 3.4526 (3.4468)  time: 0.8120  data: 0.0005  max mem: 19734
Epoch: [25]  [ 580/1251]  eta: 0:09:29  lr: 0.000017  loss: 3.2536 (3.4423)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [25]  [ 590/1251]  eta: 0:09:20  lr: 0.000017  loss: 3.7333 (3.4453)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [25]  [ 600/1251]  eta: 0:09:11  lr: 0.000017  loss: 3.7333 (3.4460)  time: 0.8164  data: 0.0005  max mem: 19734
Epoch: [25]  [ 610/1251]  eta: 0:09:03  lr: 0.000017  loss: 3.3836 (3.4441)  time: 0.8423  data: 0.0005  max mem: 19734
Epoch: [25]  [ 620/1251]  eta: 0:08:54  lr: 0.000017  loss: 3.1778 (3.4404)  time: 0.8408  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3879, ratio_loss=0.0056, pruning_loss=0.1408, mse_loss=0.6507
Epoch: [25]  [ 630/1251]  eta: 0:08:45  lr: 0.000017  loss: 3.2922 (3.4423)  time: 0.8104  data: 0.0005  max mem: 19734
Epoch: [25]  [ 640/1251]  eta: 0:08:36  lr: 0.000017  loss: 3.6025 (3.4445)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [25]  [ 650/1251]  eta: 0:08:27  lr: 0.000017  loss: 3.6025 (3.4465)  time: 0.8100  data: 0.0005  max mem: 19734
Epoch: [25]  [ 660/1251]  eta: 0:08:19  lr: 0.000017  loss: 3.5019 (3.4470)  time: 0.8001  data: 0.0007  max mem: 19734
Epoch: [25]  [ 670/1251]  eta: 0:08:10  lr: 0.000017  loss: 3.5401 (3.4485)  time: 0.8003  data: 0.0008  max mem: 19734
Epoch: [25]  [ 680/1251]  eta: 0:08:01  lr: 0.000017  loss: 3.6689 (3.4490)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [25]  [ 690/1251]  eta: 0:07:52  lr: 0.000017  loss: 3.6225 (3.4456)  time: 0.8013  data: 0.0006  max mem: 19734
Epoch: [25]  [ 700/1251]  eta: 0:07:44  lr: 0.000017  loss: 3.4166 (3.4443)  time: 0.8108  data: 0.0007  max mem: 19734
Epoch: [25]  [ 710/1251]  eta: 0:07:35  lr: 0.000017  loss: 3.2208 (3.4404)  time: 0.8240  data: 0.0005  max mem: 19734
Epoch: [25]  [ 720/1251]  eta: 0:07:26  lr: 0.000017  loss: 3.3421 (3.4410)  time: 0.8150  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4122, ratio_loss=0.0056, pruning_loss=0.1395, mse_loss=0.6299
Epoch: [25]  [ 730/1251]  eta: 0:07:18  lr: 0.000017  loss: 3.4578 (3.4399)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [25]  [ 740/1251]  eta: 0:07:09  lr: 0.000017  loss: 3.4784 (3.4384)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [25]  [ 750/1251]  eta: 0:07:01  lr: 0.000017  loss: 3.5213 (3.4388)  time: 0.8268  data: 0.0005  max mem: 19734
Epoch: [25]  [ 760/1251]  eta: 0:06:52  lr: 0.000017  loss: 3.5112 (3.4373)  time: 0.8348  data: 0.0005  max mem: 19734
Epoch: [25]  [ 770/1251]  eta: 0:06:44  lr: 0.000017  loss: 3.1507 (3.4348)  time: 0.8154  data: 0.0005  max mem: 19734
Epoch: [25]  [ 780/1251]  eta: 0:06:35  lr: 0.000017  loss: 3.1658 (3.4324)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [25]  [ 790/1251]  eta: 0:06:26  lr: 0.000017  loss: 3.3217 (3.4341)  time: 0.8089  data: 0.0004  max mem: 19734
Epoch: [25]  [ 800/1251]  eta: 0:06:18  lr: 0.000017  loss: 3.5740 (3.4343)  time: 0.8114  data: 0.0005  max mem: 19734
Epoch: [25]  [ 810/1251]  eta: 0:06:09  lr: 0.000017  loss: 3.5211 (3.4350)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [25]  [ 820/1251]  eta: 0:06:01  lr: 0.000017  loss: 3.4310 (3.4334)  time: 0.8025  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3732, ratio_loss=0.0053, pruning_loss=0.1385, mse_loss=0.6314
Epoch: [25]  [ 830/1251]  eta: 0:05:52  lr: 0.000017  loss: 3.5302 (3.4366)  time: 0.8029  data: 0.0006  max mem: 19734
Epoch: [25]  [ 840/1251]  eta: 0:05:44  lr: 0.000017  loss: 3.5792 (3.4369)  time: 0.8026  data: 0.0006  max mem: 19734
Epoch: [25]  [ 850/1251]  eta: 0:05:35  lr: 0.000017  loss: 3.5792 (3.4385)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [25]  [ 860/1251]  eta: 0:05:27  lr: 0.000017  loss: 3.5512 (3.4380)  time: 0.8183  data: 0.0004  max mem: 19734
Epoch: [25]  [ 870/1251]  eta: 0:05:18  lr: 0.000017  loss: 3.5512 (3.4396)  time: 0.8109  data: 0.0006  max mem: 19734
Epoch: [25]  [ 880/1251]  eta: 0:05:10  lr: 0.000017  loss: 3.6834 (3.4417)  time: 0.8012  data: 0.0006  max mem: 19734
Epoch: [25]  [ 890/1251]  eta: 0:05:01  lr: 0.000017  loss: 3.6826 (3.4422)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [25]  [ 900/1251]  eta: 0:04:53  lr: 0.000017  loss: 3.6443 (3.4432)  time: 0.8214  data: 0.0005  max mem: 19734
Epoch: [25]  [ 910/1251]  eta: 0:04:44  lr: 0.000017  loss: 3.7129 (3.4454)  time: 0.8234  data: 0.0005  max mem: 19734
Epoch: [25]  [ 920/1251]  eta: 0:04:36  lr: 0.000017  loss: 3.6628 (3.4464)  time: 0.8102  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5161, ratio_loss=0.0050, pruning_loss=0.1360, mse_loss=0.6394
Epoch: [25]  [ 930/1251]  eta: 0:04:27  lr: 0.000017  loss: 3.6628 (3.4461)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [25]  [ 940/1251]  eta: 0:04:19  lr: 0.000017  loss: 3.6481 (3.4482)  time: 0.8048  data: 0.0006  max mem: 19734
Epoch: [25]  [ 950/1251]  eta: 0:04:11  lr: 0.000017  loss: 3.5860 (3.4477)  time: 0.8069  data: 0.0006  max mem: 19734
Epoch: [25]  [ 960/1251]  eta: 0:04:02  lr: 0.000017  loss: 3.4227 (3.4468)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [25]  [ 970/1251]  eta: 0:03:54  lr: 0.000017  loss: 3.5238 (3.4491)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [25]  [ 980/1251]  eta: 0:03:45  lr: 0.000017  loss: 3.5228 (3.4484)  time: 0.8031  data: 0.0006  max mem: 19734
Epoch: [25]  [ 990/1251]  eta: 0:03:37  lr: 0.000017  loss: 3.4854 (3.4484)  time: 0.8118  data: 0.0005  max mem: 19734
Epoch: [25]  [1000/1251]  eta: 0:03:29  lr: 0.000017  loss: 3.5137 (3.4472)  time: 0.8226  data: 0.0005  max mem: 19734
Epoch: [25]  [1010/1251]  eta: 0:03:20  lr: 0.000017  loss: 3.5137 (3.4466)  time: 0.8126  data: 0.0005  max mem: 19734
Epoch: [25]  [1020/1251]  eta: 0:03:12  lr: 0.000017  loss: 3.5315 (3.4451)  time: 0.8022  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4331, ratio_loss=0.0050, pruning_loss=0.1378, mse_loss=0.6298
Epoch: [25]  [1030/1251]  eta: 0:03:03  lr: 0.000017  loss: 3.5536 (3.4469)  time: 0.8110  data: 0.0005  max mem: 19734
Epoch: [25]  [1040/1251]  eta: 0:02:55  lr: 0.000017  loss: 3.6587 (3.4484)  time: 0.8118  data: 0.0005  max mem: 19734
Epoch: [25]  [1050/1251]  eta: 0:02:47  lr: 0.000017  loss: 3.5610 (3.4487)  time: 0.8341  data: 0.0005  max mem: 19734
Epoch: [25]  [1060/1251]  eta: 0:02:38  lr: 0.000017  loss: 3.5739 (3.4504)  time: 0.8357  data: 0.0005  max mem: 19734
Epoch: [25]  [1070/1251]  eta: 0:02:30  lr: 0.000017  loss: 3.6510 (3.4513)  time: 0.8084  data: 0.0007  max mem: 19734
Epoch: [25]  [1080/1251]  eta: 0:02:22  lr: 0.000017  loss: 3.5458 (3.4502)  time: 0.8189  data: 0.0006  max mem: 19734
Epoch: [25]  [1090/1251]  eta: 0:02:13  lr: 0.000017  loss: 3.2803 (3.4490)  time: 0.8166  data: 0.0005  max mem: 19734
Epoch: [25]  [1100/1251]  eta: 0:02:05  lr: 0.000017  loss: 3.2486 (3.4480)  time: 0.8054  data: 0.0006  max mem: 19734
Epoch: [25]  [1110/1251]  eta: 0:01:57  lr: 0.000017  loss: 3.4425 (3.4489)  time: 0.8061  data: 0.0006  max mem: 19734
Epoch: [25]  [1120/1251]  eta: 0:01:48  lr: 0.000017  loss: 3.5578 (3.4490)  time: 0.8092  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4560, ratio_loss=0.0050, pruning_loss=0.1363, mse_loss=0.6175
Epoch: [25]  [1130/1251]  eta: 0:01:40  lr: 0.000017  loss: 3.6358 (3.4492)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [25]  [1140/1251]  eta: 0:01:32  lr: 0.000017  loss: 3.5964 (3.4477)  time: 0.8100  data: 0.0008  max mem: 19734
Epoch: [25]  [1150/1251]  eta: 0:01:23  lr: 0.000017  loss: 3.6313 (3.4480)  time: 0.8186  data: 0.0008  max mem: 19734
Epoch: [25]  [1160/1251]  eta: 0:01:15  lr: 0.000017  loss: 3.6313 (3.4469)  time: 0.8145  data: 0.0005  max mem: 19734
Epoch: [25]  [1170/1251]  eta: 0:01:07  lr: 0.000017  loss: 3.6395 (3.4497)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [25]  [1180/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.5935 (3.4469)  time: 0.8148  data: 0.0004  max mem: 19734
Epoch: [25]  [1190/1251]  eta: 0:00:50  lr: 0.000017  loss: 3.2063 (3.4473)  time: 0.8279  data: 0.0007  max mem: 19734
Epoch: [25]  [1200/1251]  eta: 0:00:42  lr: 0.000017  loss: 3.6840 (3.4479)  time: 0.8227  data: 0.0006  max mem: 19734
Epoch: [25]  [1210/1251]  eta: 0:00:34  lr: 0.000017  loss: 3.4651 (3.4464)  time: 0.8046  data: 0.0001  max mem: 19734
Epoch: [25]  [1220/1251]  eta: 0:00:25  lr: 0.000017  loss: 3.2809 (3.4447)  time: 0.7939  data: 0.0001  max mem: 19734
loss info: cls_loss=3.3740, ratio_loss=0.0053, pruning_loss=0.1394, mse_loss=0.6112
Epoch: [25]  [1230/1251]  eta: 0:00:17  lr: 0.000017  loss: 3.2809 (3.4444)  time: 0.7989  data: 0.0001  max mem: 19734
Epoch: [25]  [1240/1251]  eta: 0:00:09  lr: 0.000017  loss: 3.1248 (3.4423)  time: 0.8001  data: 0.0001  max mem: 19734
Epoch: [25]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.3395 (3.4428)  time: 0.7935  data: 0.0001  max mem: 19734
Epoch: [25] Total time: 0:17:17 (0.8294 s / it)
Averaged stats: lr: 0.000017  loss: 3.3395 (3.4290)
Test:  [  0/261]  eta: 1:33:33  loss: 0.7400 (0.7400)  acc1: 81.2500 (81.2500)  acc5: 95.3125 (95.3125)  time: 21.5093  data: 21.3340  max mem: 19734
Test:  [ 10/261]  eta: 0:14:13  loss: 0.7400 (0.7234)  acc1: 83.8542 (83.5701)  acc5: 96.3542 (96.1174)  time: 3.3996  data: 3.1344  max mem: 19734
Test:  [ 20/261]  eta: 0:07:35  loss: 0.8659 (0.8979)  acc1: 79.6875 (78.9931)  acc5: 93.7500 (94.6429)  time: 0.9104  data: 0.6692  max mem: 19734
Test:  [ 30/261]  eta: 0:05:33  loss: 0.7801 (0.8150)  acc1: 83.8542 (82.0733)  acc5: 95.3125 (95.2117)  time: 0.3665  data: 0.0180  max mem: 19734
Test:  [ 40/261]  eta: 0:05:13  loss: 0.5834 (0.7843)  acc1: 88.0208 (83.0031)  acc5: 97.3958 (95.5793)  time: 0.9185  data: 0.5154  max mem: 19734
Test:  [ 50/261]  eta: 0:04:14  loss: 0.9340 (0.8513)  acc1: 77.0833 (81.1070)  acc5: 94.7917 (95.1185)  time: 0.8359  data: 0.5184  max mem: 19734
Test:  [ 60/261]  eta: 0:03:34  loss: 0.9733 (0.8575)  acc1: 75.5208 (80.7462)  acc5: 94.7917 (95.1332)  time: 0.3462  data: 0.0149  max mem: 19734
Test:  [ 70/261]  eta: 0:03:04  loss: 0.9312 (0.8583)  acc1: 76.5625 (80.3477)  acc5: 95.8333 (95.2832)  time: 0.3580  data: 0.0139  max mem: 19734
Test:  [ 80/261]  eta: 0:02:37  loss: 0.8259 (0.8597)  acc1: 80.7292 (80.5170)  acc5: 96.8750 (95.4218)  time: 0.2684  data: 0.0158  max mem: 19734
Test:  [ 90/261]  eta: 0:02:18  loss: 0.8100 (0.8478)  acc1: 83.3333 (80.8551)  acc5: 95.8333 (95.4728)  time: 0.2634  data: 0.0132  max mem: 19734
Test:  [100/261]  eta: 0:02:14  loss: 0.7988 (0.8502)  acc1: 83.3333 (80.8117)  acc5: 95.3125 (95.5446)  time: 0.6864  data: 0.3993  max mem: 19734
Test:  [110/261]  eta: 0:01:58  loss: 0.8432 (0.8747)  acc1: 78.6458 (80.2459)  acc5: 94.7917 (95.2046)  time: 0.6800  data: 0.4026  max mem: 19734
Test:  [120/261]  eta: 0:01:47  loss: 1.2918 (0.9161)  acc1: 69.7917 (79.3001)  acc5: 88.5417 (94.6281)  time: 0.3925  data: 0.0183  max mem: 19734
Test:  [130/261]  eta: 0:01:42  loss: 1.4034 (0.9621)  acc1: 67.1875 (78.3238)  acc5: 86.9792 (94.0442)  time: 0.7385  data: 0.4001  max mem: 19734
Test:  [140/261]  eta: 0:01:31  loss: 1.3525 (0.9886)  acc1: 69.2708 (77.6633)  acc5: 89.0625 (93.8091)  time: 0.7560  data: 0.3978  max mem: 19734
Test:  [150/261]  eta: 0:01:22  loss: 1.1972 (0.9943)  acc1: 71.8750 (77.6214)  acc5: 91.1458 (93.6707)  time: 0.4685  data: 0.0203  max mem: 19734
Test:  [160/261]  eta: 0:01:12  loss: 1.0279 (1.0132)  acc1: 77.6042 (77.2807)  acc5: 92.1875 (93.3909)  time: 0.4193  data: 0.0183  max mem: 19734
Test:  [170/261]  eta: 0:01:03  loss: 1.3335 (1.0444)  acc1: 64.5833 (76.4924)  acc5: 88.5417 (93.0373)  time: 0.3412  data: 0.0122  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 1.4799 (1.0627)  acc1: 64.5833 (76.0762)  acc5: 88.0208 (92.8724)  time: 0.2163  data: 0.0127  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: 1.4064 (1.0765)  acc1: 66.1458 (75.8017)  acc5: 91.1458 (92.7002)  time: 0.1475  data: 0.0080  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 1.3751 (1.0918)  acc1: 70.8333 (75.4379)  acc5: 88.0208 (92.4725)  time: 0.1453  data: 0.0038  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.3463 (1.1055)  acc1: 69.2708 (75.1703)  acc5: 88.0208 (92.2764)  time: 0.1300  data: 0.0015  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.3819 (1.1257)  acc1: 66.6667 (74.6536)  acc5: 89.0625 (92.0320)  time: 0.1219  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4360 (1.1360)  acc1: 65.6250 (74.3800)  acc5: 89.0625 (91.9282)  time: 0.1145  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.3179 (1.1445)  acc1: 67.7083 (74.1766)  acc5: 90.6250 (91.8806)  time: 0.1144  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0494 (1.1374)  acc1: 73.9583 (74.3401)  acc5: 93.7500 (91.9924)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9384 (1.1372)  acc1: 74.4792 (74.3420)  acc5: 95.3125 (92.0500)  time: 0.1114  data: 0.0001  max mem: 19734
Test: Total time: 0:02:10 (0.4981 s / it)
* Acc@1 74.342 Acc@5 92.050 loss 1.137
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.46%
Epoch: [26]  [   0/1251]  eta: 5:52:44  lr: 0.000017  loss: 3.9022 (3.9022)  time: 16.9180  data: 5.3661  max mem: 19734
Epoch: [26]  [  10/1251]  eta: 0:52:45  lr: 0.000017  loss: 3.4815 (3.3751)  time: 2.5509  data: 0.5728  max mem: 19734
Epoch: [26]  [  20/1251]  eta: 0:35:23  lr: 0.000017  loss: 3.4767 (3.3740)  time: 0.9650  data: 0.0471  max mem: 19734
Epoch: [26]  [  30/1251]  eta: 0:29:08  lr: 0.000017  loss: 3.3418 (3.3189)  time: 0.8164  data: 0.0006  max mem: 19734
Epoch: [26]  [  40/1251]  eta: 0:25:46  lr: 0.000017  loss: 3.6133 (3.4025)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [26]  [  50/1251]  eta: 0:23:40  lr: 0.000017  loss: 3.6914 (3.4201)  time: 0.7975  data: 0.0004  max mem: 19734
Epoch: [26]  [  60/1251]  eta: 0:22:13  lr: 0.000017  loss: 3.4487 (3.4334)  time: 0.7979  data: 0.0004  max mem: 19734
Epoch: [26]  [  70/1251]  eta: 0:21:13  lr: 0.000017  loss: 3.6418 (3.4354)  time: 0.8097  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3913, ratio_loss=0.0052, pruning_loss=0.1383, mse_loss=0.6837
Epoch: [26]  [  80/1251]  eta: 0:20:26  lr: 0.000017  loss: 3.5345 (3.4346)  time: 0.8261  data: 0.0005  max mem: 19734
Epoch: [26]  [  90/1251]  eta: 0:19:47  lr: 0.000017  loss: 3.4252 (3.4340)  time: 0.8267  data: 0.0006  max mem: 19734
Epoch: [26]  [ 100/1251]  eta: 0:19:12  lr: 0.000017  loss: 3.3716 (3.4180)  time: 0.8160  data: 0.0005  max mem: 19734
Epoch: [26]  [ 110/1251]  eta: 0:18:41  lr: 0.000017  loss: 3.6087 (3.4391)  time: 0.8031  data: 0.0004  max mem: 19734
Epoch: [26]  [ 120/1251]  eta: 0:18:15  lr: 0.000017  loss: 3.8177 (3.4503)  time: 0.7997  data: 0.0004  max mem: 19734
Epoch: [26]  [ 130/1251]  eta: 0:17:51  lr: 0.000017  loss: 3.6609 (3.4483)  time: 0.8040  data: 0.0004  max mem: 19734
Epoch: [26]  [ 140/1251]  eta: 0:17:29  lr: 0.000017  loss: 3.5943 (3.4401)  time: 0.8044  data: 0.0006  max mem: 19734
Epoch: [26]  [ 150/1251]  eta: 0:17:09  lr: 0.000017  loss: 3.4331 (3.4357)  time: 0.8023  data: 0.0009  max mem: 19734
Epoch: [26]  [ 160/1251]  eta: 0:16:52  lr: 0.000017  loss: 3.4808 (3.4371)  time: 0.8119  data: 0.0008  max mem: 19734
Epoch: [26]  [ 170/1251]  eta: 0:16:35  lr: 0.000017  loss: 3.4808 (3.4308)  time: 0.8121  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3957, ratio_loss=0.0051, pruning_loss=0.1383, mse_loss=0.6300
Epoch: [26]  [ 180/1251]  eta: 0:16:20  lr: 0.000017  loss: 3.1233 (3.4118)  time: 0.8103  data: 0.0005  max mem: 19734
Epoch: [26]  [ 190/1251]  eta: 0:16:04  lr: 0.000017  loss: 3.4866 (3.4274)  time: 0.8101  data: 0.0005  max mem: 19734
Epoch: [26]  [ 200/1251]  eta: 0:15:50  lr: 0.000017  loss: 3.6087 (3.4355)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [26]  [ 210/1251]  eta: 0:15:35  lr: 0.000017  loss: 3.5994 (3.4467)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [26]  [ 220/1251]  eta: 0:15:24  lr: 0.000017  loss: 3.5423 (3.4400)  time: 0.8196  data: 0.0006  max mem: 19734
Epoch: [26]  [ 230/1251]  eta: 0:15:12  lr: 0.000017  loss: 3.2749 (3.4367)  time: 0.8411  data: 0.0005  max mem: 19734
Epoch: [26]  [ 240/1251]  eta: 0:15:00  lr: 0.000017  loss: 3.4218 (3.4368)  time: 0.8252  data: 0.0007  max mem: 19734
Epoch: [26]  [ 250/1251]  eta: 0:14:48  lr: 0.000017  loss: 3.5941 (3.4435)  time: 0.8162  data: 0.0007  max mem: 19734
Epoch: [26]  [ 260/1251]  eta: 0:14:36  lr: 0.000017  loss: 3.6286 (3.4497)  time: 0.8132  data: 0.0005  max mem: 19734
Epoch: [26]  [ 270/1251]  eta: 0:14:25  lr: 0.000017  loss: 3.5865 (3.4520)  time: 0.8060  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4771, ratio_loss=0.0049, pruning_loss=0.1363, mse_loss=0.6569
Epoch: [26]  [ 280/1251]  eta: 0:14:13  lr: 0.000017  loss: 3.4417 (3.4428)  time: 0.8067  data: 0.0006  max mem: 19734
Epoch: [26]  [ 290/1251]  eta: 0:14:02  lr: 0.000017  loss: 3.5126 (3.4476)  time: 0.8020  data: 0.0006  max mem: 19734
Epoch: [26]  [ 300/1251]  eta: 0:13:51  lr: 0.000017  loss: 3.5586 (3.4570)  time: 0.8023  data: 0.0007  max mem: 19734
Epoch: [26]  [ 310/1251]  eta: 0:13:40  lr: 0.000017  loss: 3.7636 (3.4600)  time: 0.8143  data: 0.0006  max mem: 19734
Epoch: [26]  [ 320/1251]  eta: 0:13:30  lr: 0.000017  loss: 3.5175 (3.4556)  time: 0.8201  data: 0.0006  max mem: 19734
Epoch: [26]  [ 330/1251]  eta: 0:13:20  lr: 0.000017  loss: 3.6938 (3.4665)  time: 0.8112  data: 0.0005  max mem: 19734
Epoch: [26]  [ 340/1251]  eta: 0:13:09  lr: 0.000017  loss: 3.7737 (3.4681)  time: 0.8069  data: 0.0004  max mem: 19734
Epoch: [26]  [ 350/1251]  eta: 0:12:59  lr: 0.000017  loss: 3.5905 (3.4610)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [26]  [ 360/1251]  eta: 0:12:49  lr: 0.000017  loss: 3.3356 (3.4586)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [26]  [ 370/1251]  eta: 0:12:39  lr: 0.000017  loss: 3.4300 (3.4530)  time: 0.8147  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4567, ratio_loss=0.0052, pruning_loss=0.1369, mse_loss=0.6242
Epoch: [26]  [ 380/1251]  eta: 0:12:29  lr: 0.000017  loss: 3.4905 (3.4509)  time: 0.8181  data: 0.0004  max mem: 19734
Epoch: [26]  [ 390/1251]  eta: 0:12:20  lr: 0.000017  loss: 3.5302 (3.4537)  time: 0.8151  data: 0.0005  max mem: 19734
Epoch: [26]  [ 400/1251]  eta: 0:12:10  lr: 0.000017  loss: 3.5106 (3.4511)  time: 0.8093  data: 0.0005  max mem: 19734
Epoch: [26]  [ 410/1251]  eta: 0:12:00  lr: 0.000017  loss: 3.5106 (3.4527)  time: 0.8055  data: 0.0004  max mem: 19734
Epoch: [26]  [ 420/1251]  eta: 0:11:51  lr: 0.000017  loss: 3.6292 (3.4515)  time: 0.8079  data: 0.0005  max mem: 19734
Epoch: [26]  [ 430/1251]  eta: 0:11:41  lr: 0.000017  loss: 3.2879 (3.4474)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [26]  [ 440/1251]  eta: 0:11:32  lr: 0.000017  loss: 3.2879 (3.4448)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [26]  [ 450/1251]  eta: 0:11:22  lr: 0.000017  loss: 3.4044 (3.4450)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [26]  [ 460/1251]  eta: 0:11:13  lr: 0.000017  loss: 3.2876 (3.4344)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [26]  [ 470/1251]  eta: 0:11:04  lr: 0.000017  loss: 3.3377 (3.4341)  time: 0.8178  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3193, ratio_loss=0.0050, pruning_loss=0.1381, mse_loss=0.6169
Epoch: [26]  [ 480/1251]  eta: 0:10:55  lr: 0.000017  loss: 3.4627 (3.4351)  time: 0.8111  data: 0.0006  max mem: 19734
Epoch: [26]  [ 490/1251]  eta: 0:10:46  lr: 0.000017  loss: 3.5926 (3.4355)  time: 0.8046  data: 0.0006  max mem: 19734
Epoch: [26]  [ 500/1251]  eta: 0:10:36  lr: 0.000017  loss: 3.7105 (3.4391)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [26]  [ 510/1251]  eta: 0:10:28  lr: 0.000017  loss: 3.6848 (3.4391)  time: 0.8116  data: 0.0005  max mem: 19734
Epoch: [26]  [ 520/1251]  eta: 0:10:19  lr: 0.000017  loss: 3.6289 (3.4423)  time: 0.8266  data: 0.0005  max mem: 19734
Epoch: [26]  [ 530/1251]  eta: 0:10:10  lr: 0.000017  loss: 3.6038 (3.4455)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [26]  [ 540/1251]  eta: 0:10:01  lr: 0.000017  loss: 3.6038 (3.4473)  time: 0.8197  data: 0.0005  max mem: 19734
Epoch: [26]  [ 550/1251]  eta: 0:09:52  lr: 0.000017  loss: 3.5759 (3.4443)  time: 0.8191  data: 0.0005  max mem: 19734
Epoch: [26]  [ 560/1251]  eta: 0:09:43  lr: 0.000017  loss: 3.5397 (3.4460)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [26]  [ 570/1251]  eta: 0:09:34  lr: 0.000017  loss: 3.4906 (3.4453)  time: 0.8035  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4957, ratio_loss=0.0057, pruning_loss=0.1357, mse_loss=0.6181
Epoch: [26]  [ 580/1251]  eta: 0:09:25  lr: 0.000017  loss: 3.3766 (3.4439)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [26]  [ 590/1251]  eta: 0:09:17  lr: 0.000017  loss: 3.6418 (3.4482)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [26]  [ 600/1251]  eta: 0:09:08  lr: 0.000017  loss: 3.6733 (3.4497)  time: 0.8125  data: 0.0005  max mem: 19734
Epoch: [26]  [ 610/1251]  eta: 0:08:59  lr: 0.000017  loss: 3.5730 (3.4492)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [26]  [ 620/1251]  eta: 0:08:50  lr: 0.000017  loss: 3.3068 (3.4431)  time: 0.8103  data: 0.0007  max mem: 19734
Epoch: [26]  [ 630/1251]  eta: 0:08:42  lr: 0.000017  loss: 3.3110 (3.4419)  time: 0.8103  data: 0.0007  max mem: 19734
Epoch: [26]  [ 640/1251]  eta: 0:08:33  lr: 0.000017  loss: 3.5042 (3.4434)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [26]  [ 650/1251]  eta: 0:08:24  lr: 0.000017  loss: 3.5024 (3.4414)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [26]  [ 660/1251]  eta: 0:08:16  lr: 0.000017  loss: 3.4386 (3.4401)  time: 0.8218  data: 0.0005  max mem: 19734
Epoch: [26]  [ 670/1251]  eta: 0:08:07  lr: 0.000017  loss: 3.5045 (3.4402)  time: 0.8378  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3874, ratio_loss=0.0051, pruning_loss=0.1374, mse_loss=0.6472
Epoch: [26]  [ 680/1251]  eta: 0:07:59  lr: 0.000017  loss: 3.5039 (3.4414)  time: 0.8197  data: 0.0005  max mem: 19734
Epoch: [26]  [ 690/1251]  eta: 0:07:50  lr: 0.000017  loss: 3.4587 (3.4387)  time: 0.8103  data: 0.0006  max mem: 19734
Epoch: [26]  [ 700/1251]  eta: 0:07:41  lr: 0.000017  loss: 3.3651 (3.4376)  time: 0.8082  data: 0.0005  max mem: 19734
Epoch: [26]  [ 710/1251]  eta: 0:07:33  lr: 0.000017  loss: 3.4415 (3.4388)  time: 0.7980  data: 0.0005  max mem: 19734
Epoch: [26]  [ 720/1251]  eta: 0:07:24  lr: 0.000017  loss: 3.7420 (3.4422)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [26]  [ 730/1251]  eta: 0:07:15  lr: 0.000017  loss: 3.5202 (3.4384)  time: 0.8051  data: 0.0006  max mem: 19734
Epoch: [26]  [ 740/1251]  eta: 0:07:07  lr: 0.000017  loss: 3.2488 (3.4401)  time: 0.8044  data: 0.0006  max mem: 19734
Epoch: [26]  [ 750/1251]  eta: 0:06:58  lr: 0.000017  loss: 3.5072 (3.4391)  time: 0.8086  data: 0.0004  max mem: 19734
Epoch: [26]  [ 760/1251]  eta: 0:06:50  lr: 0.000017  loss: 3.6254 (3.4419)  time: 0.8160  data: 0.0004  max mem: 19734
Epoch: [26]  [ 770/1251]  eta: 0:06:41  lr: 0.000017  loss: 3.4969 (3.4408)  time: 0.8089  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4296, ratio_loss=0.0051, pruning_loss=0.1368, mse_loss=0.6181
Epoch: [26]  [ 780/1251]  eta: 0:06:33  lr: 0.000017  loss: 3.4969 (3.4426)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [26]  [ 790/1251]  eta: 0:06:24  lr: 0.000017  loss: 3.5402 (3.4431)  time: 0.8009  data: 0.0006  max mem: 19734
Epoch: [26]  [ 800/1251]  eta: 0:06:16  lr: 0.000017  loss: 3.5402 (3.4447)  time: 0.8073  data: 0.0007  max mem: 19734
Epoch: [26]  [ 810/1251]  eta: 0:06:07  lr: 0.000017  loss: 3.7333 (3.4469)  time: 0.8162  data: 0.0007  max mem: 19734
Epoch: [26]  [ 820/1251]  eta: 0:05:59  lr: 0.000017  loss: 3.5757 (3.4461)  time: 0.8327  data: 0.0005  max mem: 19734
Epoch: [26]  [ 830/1251]  eta: 0:05:50  lr: 0.000017  loss: 3.2879 (3.4477)  time: 0.8241  data: 0.0005  max mem: 19734
Epoch: [26]  [ 840/1251]  eta: 0:05:42  lr: 0.000017  loss: 3.3659 (3.4438)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [26]  [ 850/1251]  eta: 0:05:34  lr: 0.000017  loss: 3.3659 (3.4449)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [26]  [ 860/1251]  eta: 0:05:25  lr: 0.000017  loss: 3.5979 (3.4465)  time: 0.7986  data: 0.0004  max mem: 19734
Epoch: [26]  [ 870/1251]  eta: 0:05:17  lr: 0.000017  loss: 3.5003 (3.4446)  time: 0.7972  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4556, ratio_loss=0.0052, pruning_loss=0.1371, mse_loss=0.6317
Epoch: [26]  [ 880/1251]  eta: 0:05:08  lr: 0.000017  loss: 3.1988 (3.4421)  time: 0.7973  data: 0.0004  max mem: 19734
Epoch: [26]  [ 890/1251]  eta: 0:05:00  lr: 0.000017  loss: 3.1988 (3.4387)  time: 0.7984  data: 0.0004  max mem: 19734
Epoch: [26]  [ 900/1251]  eta: 0:04:51  lr: 0.000017  loss: 3.3848 (3.4386)  time: 0.8058  data: 0.0004  max mem: 19734
Epoch: [26]  [ 910/1251]  eta: 0:04:43  lr: 0.000017  loss: 3.3402 (3.4346)  time: 0.8147  data: 0.0004  max mem: 19734
Epoch: [26]  [ 920/1251]  eta: 0:04:34  lr: 0.000017  loss: 3.4060 (3.4355)  time: 0.8073  data: 0.0004  max mem: 19734
Epoch: [26]  [ 930/1251]  eta: 0:04:26  lr: 0.000017  loss: 3.4532 (3.4351)  time: 0.7991  data: 0.0005  max mem: 19734
Epoch: [26]  [ 940/1251]  eta: 0:04:18  lr: 0.000017  loss: 3.4696 (3.4367)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [26]  [ 950/1251]  eta: 0:04:09  lr: 0.000017  loss: 3.5934 (3.4407)  time: 0.8121  data: 0.0004  max mem: 19734
Epoch: [26]  [ 960/1251]  eta: 0:04:01  lr: 0.000017  loss: 3.6404 (3.4410)  time: 0.8294  data: 0.0003  max mem: 19734
Epoch: [26]  [ 970/1251]  eta: 0:03:53  lr: 0.000017  loss: 3.5363 (3.4407)  time: 0.8225  data: 0.0003  max mem: 19734
loss info: cls_loss=3.3921, ratio_loss=0.0052, pruning_loss=0.1374, mse_loss=0.6265
Epoch: [26]  [ 980/1251]  eta: 0:03:44  lr: 0.000017  loss: 3.6736 (3.4426)  time: 0.8059  data: 0.0004  max mem: 19734
Epoch: [26]  [ 990/1251]  eta: 0:03:36  lr: 0.000017  loss: 3.7038 (3.4445)  time: 0.8027  data: 0.0004  max mem: 19734
Epoch: [26]  [1000/1251]  eta: 0:03:28  lr: 0.000017  loss: 3.6497 (3.4448)  time: 0.8039  data: 0.0004  max mem: 19734
Epoch: [26]  [1010/1251]  eta: 0:03:19  lr: 0.000017  loss: 3.4904 (3.4457)  time: 0.7995  data: 0.0004  max mem: 19734
Epoch: [26]  [1020/1251]  eta: 0:03:11  lr: 0.000017  loss: 3.6140 (3.4468)  time: 0.7967  data: 0.0004  max mem: 19734
Epoch: [26]  [1030/1251]  eta: 0:03:03  lr: 0.000017  loss: 3.5523 (3.4459)  time: 0.8004  data: 0.0004  max mem: 19734
Epoch: [26]  [1040/1251]  eta: 0:02:54  lr: 0.000017  loss: 3.5523 (3.4464)  time: 0.8100  data: 0.0004  max mem: 19734
Epoch: [26]  [1050/1251]  eta: 0:02:46  lr: 0.000017  loss: 3.5658 (3.4470)  time: 0.8084  data: 0.0004  max mem: 19734
Epoch: [26]  [1060/1251]  eta: 0:02:38  lr: 0.000017  loss: 3.4571 (3.4465)  time: 0.8118  data: 0.0004  max mem: 19734
Epoch: [26]  [1070/1251]  eta: 0:02:29  lr: 0.000017  loss: 3.4737 (3.4476)  time: 0.8119  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4837, ratio_loss=0.0051, pruning_loss=0.1340, mse_loss=0.6381
Epoch: [26]  [1080/1251]  eta: 0:02:21  lr: 0.000017  loss: 3.5998 (3.4481)  time: 0.8019  data: 0.0004  max mem: 19734
Epoch: [26]  [1090/1251]  eta: 0:02:13  lr: 0.000017  loss: 3.7170 (3.4507)  time: 0.8007  data: 0.0004  max mem: 19734
Epoch: [26]  [1100/1251]  eta: 0:02:04  lr: 0.000017  loss: 3.6698 (3.4501)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [26]  [1110/1251]  eta: 0:01:56  lr: 0.000017  loss: 3.5851 (3.4494)  time: 0.8343  data: 0.0005  max mem: 19734
Epoch: [26]  [1120/1251]  eta: 0:01:48  lr: 0.000017  loss: 3.5663 (3.4464)  time: 0.8249  data: 0.0006  max mem: 19734
Epoch: [26]  [1130/1251]  eta: 0:01:40  lr: 0.000017  loss: 3.1169 (3.4448)  time: 0.8104  data: 0.0005  max mem: 19734
Epoch: [26]  [1140/1251]  eta: 0:01:31  lr: 0.000017  loss: 3.3682 (3.4435)  time: 0.8120  data: 0.0005  max mem: 19734
Epoch: [26]  [1150/1251]  eta: 0:01:23  lr: 0.000017  loss: 3.2706 (3.4425)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [26]  [1160/1251]  eta: 0:01:15  lr: 0.000017  loss: 3.2975 (3.4419)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [26]  [1170/1251]  eta: 0:01:06  lr: 0.000017  loss: 3.4073 (3.4417)  time: 0.8020  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3498, ratio_loss=0.0051, pruning_loss=0.1380, mse_loss=0.6299
Epoch: [26]  [1180/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.4073 (3.4405)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [26]  [1190/1251]  eta: 0:00:50  lr: 0.000017  loss: 3.4130 (3.4401)  time: 0.8093  data: 0.0009  max mem: 19734
Epoch: [26]  [1200/1251]  eta: 0:00:42  lr: 0.000017  loss: 3.3963 (3.4365)  time: 0.8052  data: 0.0007  max mem: 19734
Epoch: [26]  [1210/1251]  eta: 0:00:33  lr: 0.000017  loss: 3.4203 (3.4366)  time: 0.8029  data: 0.0001  max mem: 19734
Epoch: [26]  [1220/1251]  eta: 0:00:25  lr: 0.000017  loss: 3.6614 (3.4392)  time: 0.8022  data: 0.0001  max mem: 19734
Epoch: [26]  [1230/1251]  eta: 0:00:17  lr: 0.000017  loss: 3.5516 (3.4370)  time: 0.7940  data: 0.0001  max mem: 19734
Epoch: [26]  [1240/1251]  eta: 0:00:09  lr: 0.000017  loss: 3.1146 (3.4351)  time: 0.8002  data: 0.0001  max mem: 19734
Epoch: [26]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.4188 (3.4351)  time: 0.8003  data: 0.0002  max mem: 19734
Epoch: [26] Total time: 0:17:12 (0.8253 s / it)
Averaged stats: lr: 0.000017  loss: 3.4188 (3.4254)
Test:  [  0/261]  eta: 1:10:22  loss: 0.7266 (0.7266)  acc1: 81.7708 (81.7708)  acc5: 94.7917 (94.7917)  time: 16.1784  data: 16.0406  max mem: 19734
Test:  [ 10/261]  eta: 0:11:18  loss: 0.7266 (0.7380)  acc1: 83.8542 (83.4280)  acc5: 96.3542 (96.0227)  time: 2.7051  data: 2.2964  max mem: 19734
Test:  [ 20/261]  eta: 0:06:03  loss: 0.9141 (0.9126)  acc1: 78.6458 (78.4970)  acc5: 93.2292 (94.4444)  time: 0.7766  data: 0.4704  max mem: 19734
Test:  [ 30/261]  eta: 0:04:11  loss: 0.8517 (0.8326)  acc1: 82.8125 (81.5020)  acc5: 93.7500 (95.0269)  time: 0.2029  data: 0.0159  max mem: 19734
Test:  [ 40/261]  eta: 0:04:04  loss: 0.6174 (0.7957)  acc1: 88.0208 (82.5457)  acc5: 96.8750 (95.4141)  time: 0.6834  data: 0.4005  max mem: 19734
Test:  [ 50/261]  eta: 0:03:23  loss: 0.9270 (0.8563)  acc1: 77.0833 (80.6577)  acc5: 94.2708 (94.9857)  time: 0.7644  data: 0.4039  max mem: 19734
Test:  [ 60/261]  eta: 0:02:57  loss: 0.9859 (0.8664)  acc1: 75.0000 (80.1913)  acc5: 94.2708 (94.9880)  time: 0.4276  data: 0.0205  max mem: 19734
Test:  [ 70/261]  eta: 0:02:36  loss: 0.9222 (0.8656)  acc1: 77.6042 (79.8122)  acc5: 95.8333 (95.1878)  time: 0.4453  data: 0.0160  max mem: 19734
Test:  [ 80/261]  eta: 0:02:25  loss: 0.8296 (0.8672)  acc1: 80.2083 (79.9961)  acc5: 96.8750 (95.3189)  time: 0.5637  data: 0.0162  max mem: 19734
Test:  [ 90/261]  eta: 0:02:08  loss: 0.8142 (0.8539)  acc1: 83.3333 (80.4029)  acc5: 95.8333 (95.4327)  time: 0.5255  data: 0.0382  max mem: 19734
Test:  [100/261]  eta: 0:01:56  loss: 0.7966 (0.8555)  acc1: 84.3750 (80.4352)  acc5: 95.8333 (95.5239)  time: 0.3877  data: 0.0790  max mem: 19734
Test:  [110/261]  eta: 0:01:44  loss: 0.8393 (0.8783)  acc1: 77.6042 (79.9831)  acc5: 94.7917 (95.1858)  time: 0.4152  data: 0.0636  max mem: 19734
Test:  [120/261]  eta: 0:01:39  loss: 1.2336 (0.9190)  acc1: 70.3125 (78.9773)  acc5: 89.0625 (94.6711)  time: 0.6336  data: 0.3365  max mem: 19734
Test:  [130/261]  eta: 0:01:28  loss: 1.3484 (0.9651)  acc1: 66.1458 (78.0177)  acc5: 86.9792 (94.0522)  time: 0.6006  data: 0.3323  max mem: 19734
Test:  [140/261]  eta: 0:01:19  loss: 1.3682 (0.9915)  acc1: 68.7500 (77.3899)  acc5: 88.5417 (93.7980)  time: 0.3792  data: 0.0168  max mem: 19734
Test:  [150/261]  eta: 0:01:13  loss: 1.2354 (0.9976)  acc1: 71.3542 (77.3179)  acc5: 91.1458 (93.6500)  time: 0.5741  data: 0.1899  max mem: 19734
Test:  [160/261]  eta: 0:01:05  loss: 1.0651 (1.0166)  acc1: 76.0417 (76.9410)  acc5: 91.6667 (93.3844)  time: 0.5596  data: 0.1899  max mem: 19734
Test:  [170/261]  eta: 0:00:58  loss: 1.2816 (1.0456)  acc1: 65.6250 (76.2275)  acc5: 88.0208 (93.0251)  time: 0.4450  data: 0.1229  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: 1.4006 (1.0622)  acc1: 65.1042 (75.8028)  acc5: 89.5833 (92.8695)  time: 0.3650  data: 0.1253  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.3604 (1.0748)  acc1: 67.1875 (75.5672)  acc5: 91.1458 (92.7165)  time: 0.2326  data: 0.0150  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.3431 (1.0891)  acc1: 71.3542 (75.2954)  acc5: 88.5417 (92.4725)  time: 0.1940  data: 0.0065  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.3579 (1.1025)  acc1: 70.3125 (75.0617)  acc5: 88.5417 (92.2788)  time: 0.1764  data: 0.0174  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4152 (1.1206)  acc1: 67.7083 (74.5782)  acc5: 88.5417 (92.0838)  time: 0.1494  data: 0.0146  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4475 (1.1300)  acc1: 64.5833 (74.3326)  acc5: 89.0625 (91.9643)  time: 0.1166  data: 0.0003  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3383 (1.1389)  acc1: 66.6667 (74.1053)  acc5: 90.6250 (91.9044)  time: 0.1155  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0778 (1.1316)  acc1: 76.0417 (74.2737)  acc5: 93.7500 (92.0319)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.8930 (1.1318)  acc1: 76.5625 (74.2840)  acc5: 95.3125 (92.0920)  time: 0.1118  data: 0.0001  max mem: 19734
Test: Total time: 0:02:03 (0.4728 s / it)
* Acc@1 74.284 Acc@5 92.092 loss 1.132
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.46%
Epoch: [27]  [   0/1251]  eta: 6:31:31  lr: 0.000017  loss: 3.7252 (3.7252)  time: 18.7779  data: 10.1448  max mem: 19734
Epoch: [27]  [  10/1251]  eta: 0:56:32  lr: 0.000017  loss: 3.2986 (3.1281)  time: 2.7337  data: 0.9237  max mem: 19734
Epoch: [27]  [  20/1251]  eta: 0:37:11  lr: 0.000017  loss: 3.3435 (3.2619)  time: 0.9642  data: 0.0011  max mem: 19734
loss info: cls_loss=3.3165, ratio_loss=0.0050, pruning_loss=0.1403, mse_loss=0.6425
Epoch: [27]  [  30/1251]  eta: 0:30:14  lr: 0.000017  loss: 3.6954 (3.4313)  time: 0.7995  data: 0.0007  max mem: 19734
Epoch: [27]  [  40/1251]  eta: 0:26:36  lr: 0.000017  loss: 3.6076 (3.4244)  time: 0.7997  data: 0.0006  max mem: 19734
Epoch: [27]  [  50/1251]  eta: 0:24:20  lr: 0.000017  loss: 3.4512 (3.4178)  time: 0.7979  data: 0.0005  max mem: 19734
Epoch: [27]  [  60/1251]  eta: 0:22:46  lr: 0.000017  loss: 3.6764 (3.4239)  time: 0.7976  data: 0.0006  max mem: 19734
Epoch: [27]  [  70/1251]  eta: 0:21:37  lr: 0.000017  loss: 3.5173 (3.4272)  time: 0.8003  data: 0.0005  max mem: 19734
Epoch: [27]  [  80/1251]  eta: 0:20:45  lr: 0.000017  loss: 3.5543 (3.4542)  time: 0.8079  data: 0.0005  max mem: 19734
Epoch: [27]  [  90/1251]  eta: 0:20:01  lr: 0.000017  loss: 3.7087 (3.4822)  time: 0.8090  data: 0.0007  max mem: 19734
Epoch: [27]  [ 100/1251]  eta: 0:19:26  lr: 0.000017  loss: 3.7205 (3.4967)  time: 0.8111  data: 0.0007  max mem: 19734
Epoch: [27]  [ 110/1251]  eta: 0:18:54  lr: 0.000017  loss: 3.4931 (3.4906)  time: 0.8098  data: 0.0006  max mem: 19734
Epoch: [27]  [ 120/1251]  eta: 0:18:26  lr: 0.000017  loss: 3.4931 (3.4914)  time: 0.7989  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4898, ratio_loss=0.0050, pruning_loss=0.1363, mse_loss=0.6463
Epoch: [27]  [ 130/1251]  eta: 0:18:03  lr: 0.000017  loss: 3.3545 (3.4607)  time: 0.8121  data: 0.0005  max mem: 19734
Epoch: [27]  [ 140/1251]  eta: 0:17:43  lr: 0.000017  loss: 3.2702 (3.4508)  time: 0.8286  data: 0.0006  max mem: 19734
Epoch: [27]  [ 150/1251]  eta: 0:17:24  lr: 0.000017  loss: 3.3320 (3.4447)  time: 0.8307  data: 0.0006  max mem: 19734
Epoch: [27]  [ 160/1251]  eta: 0:17:05  lr: 0.000017  loss: 3.3320 (3.4460)  time: 0.8182  data: 0.0006  max mem: 19734
Epoch: [27]  [ 170/1251]  eta: 0:16:47  lr: 0.000017  loss: 3.2259 (3.4361)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [27]  [ 180/1251]  eta: 0:16:30  lr: 0.000017  loss: 2.9627 (3.4093)  time: 0.8063  data: 0.0006  max mem: 19734
Epoch: [27]  [ 190/1251]  eta: 0:16:14  lr: 0.000017  loss: 2.9136 (3.4091)  time: 0.8029  data: 0.0006  max mem: 19734
Epoch: [27]  [ 200/1251]  eta: 0:15:59  lr: 0.000017  loss: 3.5569 (3.4177)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [27]  [ 210/1251]  eta: 0:15:44  lr: 0.000017  loss: 3.5515 (3.4088)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [27]  [ 220/1251]  eta: 0:15:31  lr: 0.000017  loss: 3.2498 (3.3984)  time: 0.8093  data: 0.0006  max mem: 19734
loss info: cls_loss=3.2879, ratio_loss=0.0049, pruning_loss=0.1400, mse_loss=0.6387
Epoch: [27]  [ 230/1251]  eta: 0:15:17  lr: 0.000017  loss: 3.4310 (3.4038)  time: 0.8095  data: 0.0006  max mem: 19734
Epoch: [27]  [ 240/1251]  eta: 0:15:05  lr: 0.000017  loss: 3.5819 (3.3997)  time: 0.8112  data: 0.0006  max mem: 19734
Epoch: [27]  [ 250/1251]  eta: 0:14:52  lr: 0.000017  loss: 3.3844 (3.3997)  time: 0.8112  data: 0.0006  max mem: 19734
Epoch: [27]  [ 260/1251]  eta: 0:14:40  lr: 0.000017  loss: 3.3543 (3.4008)  time: 0.8050  data: 0.0006  max mem: 19734
Epoch: [27]  [ 270/1251]  eta: 0:14:29  lr: 0.000017  loss: 3.4922 (3.3988)  time: 0.8124  data: 0.0006  max mem: 19734
Epoch: [27]  [ 280/1251]  eta: 0:14:17  lr: 0.000017  loss: 3.4371 (3.3940)  time: 0.8115  data: 0.0005  max mem: 19734
Epoch: [27]  [ 290/1251]  eta: 0:14:07  lr: 0.000017  loss: 3.4237 (3.4020)  time: 0.8211  data: 0.0005  max mem: 19734
Epoch: [27]  [ 300/1251]  eta: 0:13:56  lr: 0.000017  loss: 3.4237 (3.4045)  time: 0.8313  data: 0.0005  max mem: 19734
Epoch: [27]  [ 310/1251]  eta: 0:13:45  lr: 0.000017  loss: 3.2781 (3.3977)  time: 0.8116  data: 0.0005  max mem: 19734
Epoch: [27]  [ 320/1251]  eta: 0:13:34  lr: 0.000017  loss: 3.4920 (3.4027)  time: 0.8026  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3824, ratio_loss=0.0051, pruning_loss=0.1374, mse_loss=0.6507
Epoch: [27]  [ 330/1251]  eta: 0:13:23  lr: 0.000017  loss: 3.5337 (3.3955)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [27]  [ 340/1251]  eta: 0:13:13  lr: 0.000017  loss: 3.2801 (3.3933)  time: 0.8075  data: 0.0006  max mem: 19734
Epoch: [27]  [ 350/1251]  eta: 0:13:03  lr: 0.000017  loss: 3.3451 (3.3909)  time: 0.8127  data: 0.0006  max mem: 19734
Epoch: [27]  [ 360/1251]  eta: 0:12:53  lr: 0.000017  loss: 3.5903 (3.3973)  time: 0.8093  data: 0.0006  max mem: 19734
Epoch: [27]  [ 370/1251]  eta: 0:12:43  lr: 0.000017  loss: 3.4953 (3.3918)  time: 0.8106  data: 0.0006  max mem: 19734
Epoch: [27]  [ 380/1251]  eta: 0:12:32  lr: 0.000017  loss: 3.4840 (3.3949)  time: 0.8089  data: 0.0007  max mem: 19734
Epoch: [27]  [ 390/1251]  eta: 0:12:23  lr: 0.000017  loss: 3.6957 (3.3993)  time: 0.8151  data: 0.0007  max mem: 19734
Epoch: [27]  [ 400/1251]  eta: 0:12:13  lr: 0.000017  loss: 3.2264 (3.3885)  time: 0.8168  data: 0.0007  max mem: 19734
Epoch: [27]  [ 410/1251]  eta: 0:12:03  lr: 0.000017  loss: 3.5688 (3.3971)  time: 0.8056  data: 0.0007  max mem: 19734
Epoch: [27]  [ 420/1251]  eta: 0:11:54  lr: 0.000017  loss: 3.5688 (3.3904)  time: 0.8195  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3185, ratio_loss=0.0050, pruning_loss=0.1405, mse_loss=0.6475
Epoch: [27]  [ 430/1251]  eta: 0:11:45  lr: 0.000017  loss: 3.1956 (3.3886)  time: 0.8221  data: 0.0006  max mem: 19734
Epoch: [27]  [ 440/1251]  eta: 0:11:36  lr: 0.000017  loss: 3.1956 (3.3880)  time: 0.8281  data: 0.0005  max mem: 19734
Epoch: [27]  [ 450/1251]  eta: 0:11:26  lr: 0.000017  loss: 3.1423 (3.3777)  time: 0.8269  data: 0.0005  max mem: 19734
Epoch: [27]  [ 460/1251]  eta: 0:11:17  lr: 0.000017  loss: 3.3920 (3.3823)  time: 0.8055  data: 0.0006  max mem: 19734
Epoch: [27]  [ 470/1251]  eta: 0:11:07  lr: 0.000017  loss: 3.6464 (3.3816)  time: 0.8037  data: 0.0007  max mem: 19734
Epoch: [27]  [ 480/1251]  eta: 0:10:58  lr: 0.000017  loss: 3.5432 (3.3813)  time: 0.8072  data: 0.0007  max mem: 19734
Epoch: [27]  [ 490/1251]  eta: 0:10:49  lr: 0.000017  loss: 3.3375 (3.3794)  time: 0.8104  data: 0.0007  max mem: 19734
Epoch: [27]  [ 500/1251]  eta: 0:10:40  lr: 0.000017  loss: 3.6062 (3.3833)  time: 0.8076  data: 0.0006  max mem: 19734
Epoch: [27]  [ 510/1251]  eta: 0:10:30  lr: 0.000017  loss: 3.6185 (3.3851)  time: 0.8023  data: 0.0006  max mem: 19734
Epoch: [27]  [ 520/1251]  eta: 0:10:22  lr: 0.000017  loss: 3.5602 (3.3854)  time: 0.8135  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3655, ratio_loss=0.0049, pruning_loss=0.1360, mse_loss=0.6374
Epoch: [27]  [ 530/1251]  eta: 0:10:12  lr: 0.000017  loss: 3.5836 (3.3899)  time: 0.8139  data: 0.0006  max mem: 19734
Epoch: [27]  [ 540/1251]  eta: 0:10:03  lr: 0.000017  loss: 3.5765 (3.3888)  time: 0.8118  data: 0.0005  max mem: 19734
Epoch: [27]  [ 550/1251]  eta: 0:09:54  lr: 0.000017  loss: 3.5922 (3.3931)  time: 0.8137  data: 0.0006  max mem: 19734
Epoch: [27]  [ 560/1251]  eta: 0:09:46  lr: 0.000017  loss: 3.6870 (3.3918)  time: 0.8106  data: 0.0006  max mem: 19734
Epoch: [27]  [ 570/1251]  eta: 0:09:37  lr: 0.000017  loss: 3.2665 (3.3857)  time: 0.8163  data: 0.0006  max mem: 19734
Epoch: [27]  [ 580/1251]  eta: 0:09:28  lr: 0.000017  loss: 3.3392 (3.3897)  time: 0.8179  data: 0.0006  max mem: 19734
Epoch: [27]  [ 590/1251]  eta: 0:09:19  lr: 0.000017  loss: 3.7836 (3.3943)  time: 0.8187  data: 0.0005  max mem: 19734
Epoch: [27]  [ 600/1251]  eta: 0:09:10  lr: 0.000017  loss: 3.7836 (3.4007)  time: 0.8110  data: 0.0011  max mem: 19734
Epoch: [27]  [ 610/1251]  eta: 0:09:01  lr: 0.000017  loss: 3.6693 (3.4019)  time: 0.8043  data: 0.0012  max mem: 19734
Epoch: [27]  [ 620/1251]  eta: 0:08:52  lr: 0.000017  loss: 3.4059 (3.4021)  time: 0.8080  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4632, ratio_loss=0.0053, pruning_loss=0.1350, mse_loss=0.6290
Epoch: [27]  [ 630/1251]  eta: 0:08:44  lr: 0.000017  loss: 3.5357 (3.4054)  time: 0.8090  data: 0.0006  max mem: 19734
Epoch: [27]  [ 640/1251]  eta: 0:08:35  lr: 0.000017  loss: 3.5357 (3.4038)  time: 0.8044  data: 0.0006  max mem: 19734
Epoch: [27]  [ 650/1251]  eta: 0:08:26  lr: 0.000017  loss: 3.1834 (3.4008)  time: 0.8004  data: 0.0006  max mem: 19734
Epoch: [27]  [ 660/1251]  eta: 0:08:17  lr: 0.000017  loss: 3.3211 (3.3990)  time: 0.8097  data: 0.0006  max mem: 19734
Epoch: [27]  [ 670/1251]  eta: 0:08:09  lr: 0.000017  loss: 3.4784 (3.4001)  time: 0.8124  data: 0.0008  max mem: 19734
Epoch: [27]  [ 680/1251]  eta: 0:08:00  lr: 0.000017  loss: 3.5000 (3.4001)  time: 0.8105  data: 0.0008  max mem: 19734
Epoch: [27]  [ 690/1251]  eta: 0:07:51  lr: 0.000017  loss: 3.1382 (3.3981)  time: 0.8105  data: 0.0006  max mem: 19734
Epoch: [27]  [ 700/1251]  eta: 0:07:43  lr: 0.000017  loss: 3.4804 (3.3977)  time: 0.8063  data: 0.0006  max mem: 19734
Epoch: [27]  [ 710/1251]  eta: 0:07:34  lr: 0.000017  loss: 3.5394 (3.4020)  time: 0.8125  data: 0.0006  max mem: 19734
Epoch: [27]  [ 720/1251]  eta: 0:07:25  lr: 0.000017  loss: 3.6160 (3.4019)  time: 0.8115  data: 0.0009  max mem: 19734
loss info: cls_loss=3.3863, ratio_loss=0.0049, pruning_loss=0.1375, mse_loss=0.6436
Epoch: [27]  [ 730/1251]  eta: 0:07:17  lr: 0.000017  loss: 3.5367 (3.4035)  time: 0.8389  data: 0.0009  max mem: 19734
Epoch: [27]  [ 740/1251]  eta: 0:07:09  lr: 0.000017  loss: 3.5367 (3.4022)  time: 0.8385  data: 0.0008  max mem: 19734
Epoch: [27]  [ 750/1251]  eta: 0:07:00  lr: 0.000017  loss: 3.3815 (3.4022)  time: 0.8055  data: 0.0007  max mem: 19734
Epoch: [27]  [ 760/1251]  eta: 0:06:51  lr: 0.000017  loss: 3.3815 (3.4006)  time: 0.8040  data: 0.0006  max mem: 19734
Epoch: [27]  [ 770/1251]  eta: 0:06:43  lr: 0.000017  loss: 3.3871 (3.3997)  time: 0.8036  data: 0.0006  max mem: 19734
Epoch: [27]  [ 780/1251]  eta: 0:06:34  lr: 0.000017  loss: 3.4506 (3.4001)  time: 0.8078  data: 0.0006  max mem: 19734
Epoch: [27]  [ 790/1251]  eta: 0:06:26  lr: 0.000017  loss: 3.5347 (3.4006)  time: 0.8074  data: 0.0005  max mem: 19734
Epoch: [27]  [ 800/1251]  eta: 0:06:17  lr: 0.000017  loss: 3.5805 (3.4024)  time: 0.8058  data: 0.0006  max mem: 19734
Epoch: [27]  [ 810/1251]  eta: 0:06:09  lr: 0.000017  loss: 3.5707 (3.4028)  time: 0.8140  data: 0.0006  max mem: 19734
Epoch: [27]  [ 820/1251]  eta: 0:06:00  lr: 0.000017  loss: 3.3628 (3.3987)  time: 0.8118  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3610, ratio_loss=0.0050, pruning_loss=0.1374, mse_loss=0.6392
Epoch: [27]  [ 830/1251]  eta: 0:05:52  lr: 0.000017  loss: 3.3052 (3.3988)  time: 0.8149  data: 0.0005  max mem: 19734
Epoch: [27]  [ 840/1251]  eta: 0:05:43  lr: 0.000017  loss: 3.4043 (3.3981)  time: 0.8146  data: 0.0006  max mem: 19734
Epoch: [27]  [ 850/1251]  eta: 0:05:35  lr: 0.000017  loss: 3.4043 (3.3973)  time: 0.8044  data: 0.0006  max mem: 19734
Epoch: [27]  [ 860/1251]  eta: 0:05:26  lr: 0.000017  loss: 3.4927 (3.3969)  time: 0.8186  data: 0.0006  max mem: 19734
Epoch: [27]  [ 870/1251]  eta: 0:05:18  lr: 0.000017  loss: 3.3172 (3.3943)  time: 0.8211  data: 0.0005  max mem: 19734
Epoch: [27]  [ 880/1251]  eta: 0:05:09  lr: 0.000017  loss: 3.1064 (3.3925)  time: 0.8234  data: 0.0005  max mem: 19734
Epoch: [27]  [ 890/1251]  eta: 0:05:01  lr: 0.000017  loss: 3.3854 (3.3939)  time: 0.8236  data: 0.0005  max mem: 19734
Epoch: [27]  [ 900/1251]  eta: 0:04:52  lr: 0.000017  loss: 3.4750 (3.3930)  time: 0.8103  data: 0.0006  max mem: 19734
Epoch: [27]  [ 910/1251]  eta: 0:04:44  lr: 0.000017  loss: 3.6019 (3.3952)  time: 0.8062  data: 0.0008  max mem: 19734
Epoch: [27]  [ 920/1251]  eta: 0:04:36  lr: 0.000017  loss: 3.6767 (3.3990)  time: 0.8037  data: 0.0008  max mem: 19734
loss info: cls_loss=3.3781, ratio_loss=0.0049, pruning_loss=0.1379, mse_loss=0.6605
Epoch: [27]  [ 930/1251]  eta: 0:04:27  lr: 0.000017  loss: 3.7766 (3.4035)  time: 0.8039  data: 0.0006  max mem: 19734
Epoch: [27]  [ 940/1251]  eta: 0:04:19  lr: 0.000017  loss: 3.7345 (3.4057)  time: 0.8039  data: 0.0011  max mem: 19734
Epoch: [27]  [ 950/1251]  eta: 0:04:10  lr: 0.000017  loss: 3.6315 (3.4059)  time: 0.8097  data: 0.0011  max mem: 19734
Epoch: [27]  [ 960/1251]  eta: 0:04:02  lr: 0.000017  loss: 3.4499 (3.4060)  time: 0.8079  data: 0.0007  max mem: 19734
Epoch: [27]  [ 970/1251]  eta: 0:03:53  lr: 0.000017  loss: 3.4948 (3.4064)  time: 0.7984  data: 0.0005  max mem: 19734
Epoch: [27]  [ 980/1251]  eta: 0:03:45  lr: 0.000017  loss: 3.5648 (3.4078)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [27]  [ 990/1251]  eta: 0:03:37  lr: 0.000017  loss: 3.8017 (3.4108)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [27]  [1000/1251]  eta: 0:03:28  lr: 0.000017  loss: 3.7711 (3.4127)  time: 0.8134  data: 0.0005  max mem: 19734
Epoch: [27]  [1010/1251]  eta: 0:03:20  lr: 0.000017  loss: 3.5088 (3.4115)  time: 0.8143  data: 0.0004  max mem: 19734
Epoch: [27]  [1020/1251]  eta: 0:03:12  lr: 0.000017  loss: 3.3040 (3.4103)  time: 0.8151  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4757, ratio_loss=0.0052, pruning_loss=0.1361, mse_loss=0.6317
Epoch: [27]  [1030/1251]  eta: 0:03:03  lr: 0.000017  loss: 3.3414 (3.4084)  time: 0.8209  data: 0.0006  max mem: 19734
Epoch: [27]  [1040/1251]  eta: 0:02:55  lr: 0.000017  loss: 3.5345 (3.4100)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [27]  [1050/1251]  eta: 0:02:46  lr: 0.000017  loss: 3.3946 (3.4099)  time: 0.7988  data: 0.0006  max mem: 19734
Epoch: [27]  [1060/1251]  eta: 0:02:38  lr: 0.000017  loss: 3.5133 (3.4128)  time: 0.7983  data: 0.0007  max mem: 19734
Epoch: [27]  [1070/1251]  eta: 0:02:30  lr: 0.000017  loss: 3.6498 (3.4116)  time: 0.7992  data: 0.0006  max mem: 19734
Epoch: [27]  [1080/1251]  eta: 0:02:21  lr: 0.000017  loss: 3.5458 (3.4135)  time: 0.7991  data: 0.0005  max mem: 19734
Epoch: [27]  [1090/1251]  eta: 0:02:13  lr: 0.000017  loss: 3.6113 (3.4143)  time: 0.7984  data: 0.0005  max mem: 19734
Epoch: [27]  [1100/1251]  eta: 0:02:05  lr: 0.000017  loss: 3.5854 (3.4157)  time: 0.8070  data: 0.0006  max mem: 19734
Epoch: [27]  [1110/1251]  eta: 0:01:56  lr: 0.000017  loss: 3.5618 (3.4140)  time: 0.8070  data: 0.0007  max mem: 19734
Epoch: [27]  [1120/1251]  eta: 0:01:48  lr: 0.000017  loss: 3.2343 (3.4124)  time: 0.8082  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4218, ratio_loss=0.0052, pruning_loss=0.1369, mse_loss=0.6359
Epoch: [27]  [1130/1251]  eta: 0:01:40  lr: 0.000017  loss: 3.4568 (3.4128)  time: 0.8092  data: 0.0006  max mem: 19734
Epoch: [27]  [1140/1251]  eta: 0:01:31  lr: 0.000017  loss: 3.4937 (3.4132)  time: 0.8005  data: 0.0007  max mem: 19734
Epoch: [27]  [1150/1251]  eta: 0:01:23  lr: 0.000017  loss: 3.4990 (3.4139)  time: 0.8146  data: 0.0006  max mem: 19734
Epoch: [27]  [1160/1251]  eta: 0:01:15  lr: 0.000017  loss: 3.4211 (3.4137)  time: 0.8136  data: 0.0005  max mem: 19734
Epoch: [27]  [1170/1251]  eta: 0:01:07  lr: 0.000017  loss: 3.5362 (3.4154)  time: 0.8231  data: 0.0005  max mem: 19734
Epoch: [27]  [1180/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.6169 (3.4169)  time: 0.8226  data: 0.0005  max mem: 19734
Epoch: [27]  [1190/1251]  eta: 0:00:50  lr: 0.000017  loss: 3.6459 (3.4189)  time: 0.7973  data: 0.0009  max mem: 19734
Epoch: [27]  [1200/1251]  eta: 0:00:42  lr: 0.000017  loss: 3.5445 (3.4187)  time: 0.7937  data: 0.0007  max mem: 19734
Epoch: [27]  [1210/1251]  eta: 0:00:33  lr: 0.000017  loss: 3.2205 (3.4152)  time: 0.7902  data: 0.0001  max mem: 19734
Epoch: [27]  [1220/1251]  eta: 0:00:25  lr: 0.000017  loss: 3.2587 (3.4154)  time: 0.7901  data: 0.0001  max mem: 19734
loss info: cls_loss=3.4445, ratio_loss=0.0052, pruning_loss=0.1352, mse_loss=0.6275
Epoch: [27]  [1230/1251]  eta: 0:00:17  lr: 0.000017  loss: 3.6804 (3.4164)  time: 0.7915  data: 0.0001  max mem: 19734
Epoch: [27]  [1240/1251]  eta: 0:00:09  lr: 0.000017  loss: 3.6365 (3.4150)  time: 0.7977  data: 0.0001  max mem: 19734
Epoch: [27]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.1322 (3.4139)  time: 0.7961  data: 0.0001  max mem: 19734
Epoch: [27] Total time: 0:17:15 (0.8273 s / it)
Averaged stats: lr: 0.000017  loss: 3.1322 (3.4205)
Test:  [  0/261]  eta: 1:42:57  loss: 0.7597 (0.7597)  acc1: 82.8125 (82.8125)  acc5: 95.8333 (95.8333)  time: 23.6699  data: 23.5231  max mem: 19734
Test:  [ 10/261]  eta: 0:11:53  loss: 0.7459 (0.7373)  acc1: 84.8958 (83.7121)  acc5: 95.8333 (96.0227)  time: 2.8410  data: 2.6991  max mem: 19734
Test:  [ 20/261]  eta: 0:06:24  loss: 0.9673 (0.8979)  acc1: 80.2083 (79.3155)  acc5: 94.2708 (94.7421)  time: 0.4911  data: 0.3119  max mem: 19734
Test:  [ 30/261]  eta: 0:04:25  loss: 0.8029 (0.8140)  acc1: 83.8542 (82.2245)  acc5: 94.2708 (95.2117)  time: 0.2162  data: 0.0072  max mem: 19734
Test:  [ 40/261]  eta: 0:04:29  loss: 0.5699 (0.7813)  acc1: 88.0208 (83.1428)  acc5: 96.8750 (95.5539)  time: 0.8251  data: 0.5987  max mem: 19734
Test:  [ 50/261]  eta: 0:03:41  loss: 0.9262 (0.8450)  acc1: 78.6458 (81.2704)  acc5: 94.2708 (95.0878)  time: 0.9037  data: 0.6054  max mem: 19734
Test:  [ 60/261]  eta: 0:03:09  loss: 0.9864 (0.8565)  acc1: 75.5208 (80.6352)  acc5: 94.2708 (95.1161)  time: 0.3742  data: 0.0169  max mem: 19734
Test:  [ 70/261]  eta: 0:02:45  loss: 0.9633 (0.8581)  acc1: 77.6042 (80.2303)  acc5: 95.8333 (95.2758)  time: 0.3897  data: 0.0907  max mem: 19734
Test:  [ 80/261]  eta: 0:02:35  loss: 0.8310 (0.8588)  acc1: 79.1667 (80.3691)  acc5: 96.3542 (95.3704)  time: 0.6148  data: 0.3756  max mem: 19734
Test:  [ 90/261]  eta: 0:02:16  loss: 0.8041 (0.8469)  acc1: 82.2917 (80.6834)  acc5: 96.3542 (95.4556)  time: 0.5713  data: 0.2994  max mem: 19734
Test:  [100/261]  eta: 0:02:11  loss: 0.8071 (0.8501)  acc1: 83.3333 (80.6054)  acc5: 95.8333 (95.4775)  time: 0.6454  data: 0.2327  max mem: 19734
Test:  [110/261]  eta: 0:01:58  loss: 0.8632 (0.8734)  acc1: 75.5208 (80.0160)  acc5: 93.7500 (95.1624)  time: 0.6951  data: 0.2308  max mem: 19734
Test:  [120/261]  eta: 0:01:43  loss: 1.2269 (0.9137)  acc1: 69.7917 (79.0375)  acc5: 89.0625 (94.6367)  time: 0.3139  data: 0.0153  max mem: 19734
Test:  [130/261]  eta: 0:01:32  loss: 1.3781 (0.9574)  acc1: 69.2708 (78.1528)  acc5: 88.0208 (94.0840)  time: 0.3071  data: 0.0238  max mem: 19734
Test:  [140/261]  eta: 0:01:24  loss: 1.3139 (0.9837)  acc1: 69.2708 (77.4601)  acc5: 89.5833 (93.8423)  time: 0.4839  data: 0.1982  max mem: 19734
Test:  [150/261]  eta: 0:01:15  loss: 1.2105 (0.9892)  acc1: 71.8750 (77.4179)  acc5: 91.1458 (93.7086)  time: 0.5047  data: 0.1915  max mem: 19734
Test:  [160/261]  eta: 0:01:06  loss: 1.0057 (1.0077)  acc1: 79.1667 (77.1027)  acc5: 92.1875 (93.4200)  time: 0.3707  data: 0.0172  max mem: 19734
Test:  [170/261]  eta: 0:00:59  loss: 1.3359 (1.0385)  acc1: 65.6250 (76.3615)  acc5: 88.5417 (93.0830)  time: 0.4259  data: 0.1193  max mem: 19734
Test:  [180/261]  eta: 0:00:50  loss: 1.4460 (1.0555)  acc1: 65.6250 (75.9669)  acc5: 88.5417 (92.9126)  time: 0.3647  data: 0.1152  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.3940 (1.0690)  acc1: 66.6667 (75.7226)  acc5: 90.1042 (92.7438)  time: 0.2056  data: 0.0100  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.3931 (1.0854)  acc1: 71.3542 (75.4042)  acc5: 89.5833 (92.5114)  time: 0.2132  data: 0.0098  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.3790 (1.0992)  acc1: 70.3125 (75.1530)  acc5: 86.9792 (92.2887)  time: 0.2115  data: 0.0609  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4003 (1.1183)  acc1: 66.6667 (74.6512)  acc5: 88.0208 (92.1144)  time: 0.1702  data: 0.0557  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4325 (1.1272)  acc1: 66.1458 (74.4183)  acc5: 89.5833 (92.0297)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3018 (1.1353)  acc1: 68.7500 (74.2436)  acc5: 91.1458 (91.9908)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0381 (1.1279)  acc1: 75.0000 (74.4169)  acc5: 93.7500 (92.1004)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9340 (1.1285)  acc1: 77.0833 (74.3940)  acc5: 95.3125 (92.1520)  time: 0.1116  data: 0.0001  max mem: 19734
Test: Total time: 0:02:05 (0.4824 s / it)
* Acc@1 74.394 Acc@5 92.152 loss 1.129
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.46%
Epoch: [28]  [   0/1251]  eta: 5:46:37  lr: 0.000017  loss: 4.1513 (4.1513)  time: 16.6248  data: 12.5559  max mem: 19734
Epoch: [28]  [  10/1251]  eta: 0:56:46  lr: 0.000017  loss: 3.8040 (3.6200)  time: 2.7448  data: 1.1436  max mem: 19734
Epoch: [28]  [  20/1251]  eta: 0:37:18  lr: 0.000017  loss: 3.3776 (3.3521)  time: 1.0779  data: 0.0015  max mem: 19734
Epoch: [28]  [  30/1251]  eta: 0:30:19  lr: 0.000017  loss: 3.1548 (3.3631)  time: 0.7995  data: 0.0005  max mem: 19734
Epoch: [28]  [  40/1251]  eta: 0:26:48  lr: 0.000017  loss: 3.1548 (3.3730)  time: 0.8141  data: 0.0005  max mem: 19734
Epoch: [28]  [  50/1251]  eta: 0:24:30  lr: 0.000017  loss: 3.6120 (3.3316)  time: 0.8127  data: 0.0005  max mem: 19734
Epoch: [28]  [  60/1251]  eta: 0:23:02  lr: 0.000017  loss: 3.5195 (3.3789)  time: 0.8165  data: 0.0005  max mem: 19734
Epoch: [28]  [  70/1251]  eta: 0:21:50  lr: 0.000017  loss: 3.5195 (3.3863)  time: 0.8188  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3341, ratio_loss=0.0050, pruning_loss=0.1381, mse_loss=0.6421
Epoch: [28]  [  80/1251]  eta: 0:20:54  lr: 0.000017  loss: 3.4223 (3.3592)  time: 0.8007  data: 0.0006  max mem: 19734
Epoch: [28]  [  90/1251]  eta: 0:20:09  lr: 0.000017  loss: 3.4948 (3.4001)  time: 0.8012  data: 0.0006  max mem: 19734
Epoch: [28]  [ 100/1251]  eta: 0:19:32  lr: 0.000017  loss: 3.7129 (3.4247)  time: 0.8024  data: 0.0006  max mem: 19734
Epoch: [28]  [ 110/1251]  eta: 0:18:59  lr: 0.000017  loss: 3.7658 (3.4542)  time: 0.8023  data: 0.0006  max mem: 19734
Epoch: [28]  [ 120/1251]  eta: 0:18:31  lr: 0.000017  loss: 3.6849 (3.4506)  time: 0.8047  data: 0.0004  max mem: 19734
Epoch: [28]  [ 130/1251]  eta: 0:18:07  lr: 0.000017  loss: 3.6849 (3.4630)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [28]  [ 140/1251]  eta: 0:17:44  lr: 0.000017  loss: 3.6967 (3.4726)  time: 0.8094  data: 0.0006  max mem: 19734
Epoch: [28]  [ 150/1251]  eta: 0:17:23  lr: 0.000017  loss: 3.6048 (3.4714)  time: 0.8055  data: 0.0007  max mem: 19734
Epoch: [28]  [ 160/1251]  eta: 0:17:05  lr: 0.000017  loss: 3.7487 (3.4918)  time: 0.8112  data: 0.0007  max mem: 19734
Epoch: [28]  [ 170/1251]  eta: 0:16:47  lr: 0.000017  loss: 3.6062 (3.4939)  time: 0.8090  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5638, ratio_loss=0.0051, pruning_loss=0.1326, mse_loss=0.6154
Epoch: [28]  [ 180/1251]  eta: 0:16:30  lr: 0.000017  loss: 3.6062 (3.5031)  time: 0.8041  data: 0.0006  max mem: 19734
Epoch: [28]  [ 190/1251]  eta: 0:16:16  lr: 0.000017  loss: 3.7541 (3.5006)  time: 0.8215  data: 0.0005  max mem: 19734
Epoch: [28]  [ 200/1251]  eta: 0:16:02  lr: 0.000017  loss: 3.4279 (3.4973)  time: 0.8315  data: 0.0005  max mem: 19734
Epoch: [28]  [ 210/1251]  eta: 0:15:49  lr: 0.000017  loss: 3.3262 (3.4901)  time: 0.8296  data: 0.0005  max mem: 19734
Epoch: [28]  [ 220/1251]  eta: 0:15:35  lr: 0.000017  loss: 3.6059 (3.5072)  time: 0.8208  data: 0.0005  max mem: 19734
Epoch: [28]  [ 230/1251]  eta: 0:15:21  lr: 0.000017  loss: 3.8365 (3.5156)  time: 0.8091  data: 0.0006  max mem: 19734
Epoch: [28]  [ 240/1251]  eta: 0:15:08  lr: 0.000017  loss: 3.7404 (3.5121)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [28]  [ 250/1251]  eta: 0:14:55  lr: 0.000017  loss: 3.4487 (3.4997)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [28]  [ 260/1251]  eta: 0:14:43  lr: 0.000017  loss: 3.0688 (3.4839)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [28]  [ 270/1251]  eta: 0:14:31  lr: 0.000017  loss: 3.1562 (3.4818)  time: 0.8081  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4315, ratio_loss=0.0049, pruning_loss=0.1380, mse_loss=0.6383
Epoch: [28]  [ 280/1251]  eta: 0:14:19  lr: 0.000017  loss: 3.3565 (3.4819)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [28]  [ 290/1251]  eta: 0:14:08  lr: 0.000017  loss: 3.3066 (3.4752)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [28]  [ 300/1251]  eta: 0:13:56  lr: 0.000017  loss: 3.3902 (3.4747)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [28]  [ 310/1251]  eta: 0:13:46  lr: 0.000017  loss: 3.5514 (3.4731)  time: 0.8125  data: 0.0005  max mem: 19734
Epoch: [28]  [ 320/1251]  eta: 0:13:35  lr: 0.000017  loss: 3.3469 (3.4690)  time: 0.8125  data: 0.0005  max mem: 19734
Epoch: [28]  [ 330/1251]  eta: 0:13:25  lr: 0.000017  loss: 3.0935 (3.4573)  time: 0.8132  data: 0.0004  max mem: 19734
Epoch: [28]  [ 340/1251]  eta: 0:13:14  lr: 0.000017  loss: 3.3124 (3.4594)  time: 0.8215  data: 0.0004  max mem: 19734
Epoch: [28]  [ 350/1251]  eta: 0:13:05  lr: 0.000017  loss: 3.2874 (3.4484)  time: 0.8290  data: 0.0004  max mem: 19734
Epoch: [28]  [ 360/1251]  eta: 0:12:55  lr: 0.000017  loss: 3.2874 (3.4486)  time: 0.8276  data: 0.0004  max mem: 19734
Epoch: [28]  [ 370/1251]  eta: 0:12:44  lr: 0.000017  loss: 3.0891 (3.4357)  time: 0.8095  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2990, ratio_loss=0.0050, pruning_loss=0.1409, mse_loss=0.6318
Epoch: [28]  [ 380/1251]  eta: 0:12:34  lr: 0.000017  loss: 3.0354 (3.4296)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [28]  [ 390/1251]  eta: 0:12:24  lr: 0.000017  loss: 3.3162 (3.4302)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [28]  [ 400/1251]  eta: 0:12:14  lr: 0.000017  loss: 3.4007 (3.4322)  time: 0.7995  data: 0.0005  max mem: 19734
Epoch: [28]  [ 410/1251]  eta: 0:12:04  lr: 0.000017  loss: 3.5131 (3.4344)  time: 0.8004  data: 0.0004  max mem: 19734
Epoch: [28]  [ 420/1251]  eta: 0:11:55  lr: 0.000017  loss: 3.4542 (3.4329)  time: 0.8090  data: 0.0004  max mem: 19734
Epoch: [28]  [ 430/1251]  eta: 0:11:45  lr: 0.000017  loss: 3.3856 (3.4302)  time: 0.8106  data: 0.0004  max mem: 19734
Epoch: [28]  [ 440/1251]  eta: 0:11:35  lr: 0.000017  loss: 3.4962 (3.4335)  time: 0.8030  data: 0.0004  max mem: 19734
Epoch: [28]  [ 450/1251]  eta: 0:11:26  lr: 0.000017  loss: 3.6196 (3.4371)  time: 0.8135  data: 0.0004  max mem: 19734
Epoch: [28]  [ 460/1251]  eta: 0:11:17  lr: 0.000017  loss: 3.6165 (3.4369)  time: 0.8155  data: 0.0004  max mem: 19734
Epoch: [28]  [ 470/1251]  eta: 0:11:07  lr: 0.000017  loss: 3.5462 (3.4365)  time: 0.8063  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4189, ratio_loss=0.0049, pruning_loss=0.1361, mse_loss=0.6326
Epoch: [28]  [ 480/1251]  eta: 0:10:59  lr: 0.000017  loss: 3.5205 (3.4336)  time: 0.8254  data: 0.0005  max mem: 19734
Epoch: [28]  [ 490/1251]  eta: 0:10:49  lr: 0.000017  loss: 3.5327 (3.4370)  time: 0.8235  data: 0.0004  max mem: 19734
Epoch: [28]  [ 500/1251]  eta: 0:10:41  lr: 0.000017  loss: 3.5337 (3.4339)  time: 0.8223  data: 0.0004  max mem: 19734
Epoch: [28]  [ 510/1251]  eta: 0:10:31  lr: 0.000017  loss: 3.2044 (3.4274)  time: 0.8203  data: 0.0005  max mem: 19734
Epoch: [28]  [ 520/1251]  eta: 0:10:22  lr: 0.000017  loss: 3.4381 (3.4288)  time: 0.7991  data: 0.0005  max mem: 19734
Epoch: [28]  [ 530/1251]  eta: 0:10:13  lr: 0.000017  loss: 3.6162 (3.4285)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [28]  [ 540/1251]  eta: 0:10:04  lr: 0.000017  loss: 3.7289 (3.4314)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [28]  [ 550/1251]  eta: 0:09:55  lr: 0.000017  loss: 3.6488 (3.4336)  time: 0.8033  data: 0.0004  max mem: 19734
Epoch: [28]  [ 560/1251]  eta: 0:09:46  lr: 0.000017  loss: 3.6965 (3.4340)  time: 0.8077  data: 0.0004  max mem: 19734
Epoch: [28]  [ 570/1251]  eta: 0:09:37  lr: 0.000017  loss: 3.6614 (3.4365)  time: 0.8077  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4200, ratio_loss=0.0052, pruning_loss=0.1367, mse_loss=0.6125
Epoch: [28]  [ 580/1251]  eta: 0:09:28  lr: 0.000017  loss: 3.6781 (3.4423)  time: 0.8016  data: 0.0006  max mem: 19734
Epoch: [28]  [ 590/1251]  eta: 0:09:19  lr: 0.000017  loss: 3.6781 (3.4410)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [28]  [ 600/1251]  eta: 0:09:10  lr: 0.000017  loss: 3.7306 (3.4466)  time: 0.8139  data: 0.0005  max mem: 19734
Epoch: [28]  [ 610/1251]  eta: 0:09:01  lr: 0.000017  loss: 3.6708 (3.4451)  time: 0.8144  data: 0.0005  max mem: 19734
Epoch: [28]  [ 620/1251]  eta: 0:08:52  lr: 0.000017  loss: 3.2505 (3.4378)  time: 0.8009  data: 0.0004  max mem: 19734
Epoch: [28]  [ 630/1251]  eta: 0:08:44  lr: 0.000017  loss: 3.5649 (3.4406)  time: 0.8184  data: 0.0004  max mem: 19734
Epoch: [28]  [ 640/1251]  eta: 0:08:35  lr: 0.000017  loss: 3.5649 (3.4355)  time: 0.8346  data: 0.0004  max mem: 19734
Epoch: [28]  [ 650/1251]  eta: 0:08:27  lr: 0.000017  loss: 3.0964 (3.4336)  time: 0.8332  data: 0.0007  max mem: 19734
Epoch: [28]  [ 660/1251]  eta: 0:08:18  lr: 0.000017  loss: 3.5691 (3.4304)  time: 0.8205  data: 0.0007  max mem: 19734
Epoch: [28]  [ 670/1251]  eta: 0:08:09  lr: 0.000017  loss: 3.5691 (3.4315)  time: 0.8046  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3818, ratio_loss=0.0048, pruning_loss=0.1370, mse_loss=0.6537
Epoch: [28]  [ 680/1251]  eta: 0:08:00  lr: 0.000017  loss: 3.6139 (3.4303)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [28]  [ 690/1251]  eta: 0:07:51  lr: 0.000017  loss: 3.0275 (3.4237)  time: 0.8025  data: 0.0009  max mem: 19734
Epoch: [28]  [ 700/1251]  eta: 0:07:43  lr: 0.000017  loss: 3.3059 (3.4253)  time: 0.8019  data: 0.0009  max mem: 19734
Epoch: [28]  [ 710/1251]  eta: 0:07:34  lr: 0.000017  loss: 3.4246 (3.4226)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [28]  [ 720/1251]  eta: 0:07:25  lr: 0.000017  loss: 3.6127 (3.4254)  time: 0.8061  data: 0.0005  max mem: 19734
Epoch: [28]  [ 730/1251]  eta: 0:07:17  lr: 0.000017  loss: 3.7666 (3.4294)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [28]  [ 740/1251]  eta: 0:07:08  lr: 0.000017  loss: 3.4518 (3.4219)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [28]  [ 750/1251]  eta: 0:07:00  lr: 0.000017  loss: 2.9688 (3.4205)  time: 0.8102  data: 0.0005  max mem: 19734
Epoch: [28]  [ 760/1251]  eta: 0:06:51  lr: 0.000017  loss: 3.2318 (3.4203)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [28]  [ 770/1251]  eta: 0:06:42  lr: 0.000017  loss: 3.4997 (3.4235)  time: 0.8114  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3502, ratio_loss=0.0051, pruning_loss=0.1374, mse_loss=0.6195
Epoch: [28]  [ 780/1251]  eta: 0:06:34  lr: 0.000017  loss: 3.7796 (3.4234)  time: 0.8200  data: 0.0005  max mem: 19734
Epoch: [28]  [ 790/1251]  eta: 0:06:26  lr: 0.000017  loss: 3.2485 (3.4222)  time: 0.8273  data: 0.0005  max mem: 19734
Epoch: [28]  [ 800/1251]  eta: 0:06:17  lr: 0.000017  loss: 3.4470 (3.4235)  time: 0.8227  data: 0.0006  max mem: 19734
Epoch: [28]  [ 810/1251]  eta: 0:06:09  lr: 0.000017  loss: 3.5117 (3.4240)  time: 0.8048  data: 0.0006  max mem: 19734
Epoch: [28]  [ 820/1251]  eta: 0:06:00  lr: 0.000017  loss: 3.5130 (3.4242)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [28]  [ 830/1251]  eta: 0:05:51  lr: 0.000017  loss: 3.2711 (3.4198)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [28]  [ 840/1251]  eta: 0:05:43  lr: 0.000017  loss: 3.2711 (3.4172)  time: 0.8006  data: 0.0004  max mem: 19734
Epoch: [28]  [ 850/1251]  eta: 0:05:34  lr: 0.000017  loss: 3.3733 (3.4171)  time: 0.8015  data: 0.0004  max mem: 19734
Epoch: [28]  [ 860/1251]  eta: 0:05:26  lr: 0.000017  loss: 3.3518 (3.4128)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [28]  [ 870/1251]  eta: 0:05:17  lr: 0.000017  loss: 3.0176 (3.4080)  time: 0.8095  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2645, ratio_loss=0.0052, pruning_loss=0.1381, mse_loss=0.6350
Epoch: [28]  [ 880/1251]  eta: 0:05:09  lr: 0.000017  loss: 3.4648 (3.4102)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [28]  [ 890/1251]  eta: 0:05:01  lr: 0.000017  loss: 3.6709 (3.4087)  time: 0.8109  data: 0.0005  max mem: 19734
Epoch: [28]  [ 900/1251]  eta: 0:04:52  lr: 0.000017  loss: 3.4789 (3.4101)  time: 0.8117  data: 0.0005  max mem: 19734
Epoch: [28]  [ 910/1251]  eta: 0:04:44  lr: 0.000017  loss: 3.5130 (3.4096)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [28]  [ 920/1251]  eta: 0:04:35  lr: 0.000017  loss: 3.4147 (3.4105)  time: 0.8180  data: 0.0006  max mem: 19734
Epoch: [28]  [ 930/1251]  eta: 0:04:27  lr: 0.000017  loss: 3.5997 (3.4129)  time: 0.8225  data: 0.0005  max mem: 19734
Epoch: [28]  [ 940/1251]  eta: 0:04:19  lr: 0.000017  loss: 3.5997 (3.4119)  time: 0.8246  data: 0.0005  max mem: 19734
Epoch: [28]  [ 950/1251]  eta: 0:04:10  lr: 0.000017  loss: 3.0514 (3.4081)  time: 0.8202  data: 0.0005  max mem: 19734
Epoch: [28]  [ 960/1251]  eta: 0:04:02  lr: 0.000017  loss: 3.0796 (3.4069)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [28]  [ 970/1251]  eta: 0:03:53  lr: 0.000017  loss: 3.4653 (3.4053)  time: 0.8045  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3608, ratio_loss=0.0049, pruning_loss=0.1370, mse_loss=0.6315
Epoch: [28]  [ 980/1251]  eta: 0:03:45  lr: 0.000017  loss: 3.4653 (3.4039)  time: 0.8013  data: 0.0006  max mem: 19734
Epoch: [28]  [ 990/1251]  eta: 0:03:37  lr: 0.000017  loss: 3.4669 (3.4042)  time: 0.8009  data: 0.0006  max mem: 19734
Epoch: [28]  [1000/1251]  eta: 0:03:28  lr: 0.000017  loss: 3.4826 (3.4042)  time: 0.8095  data: 0.0006  max mem: 19734
Epoch: [28]  [1010/1251]  eta: 0:03:20  lr: 0.000017  loss: 3.5824 (3.4048)  time: 0.8139  data: 0.0007  max mem: 19734
Epoch: [28]  [1020/1251]  eta: 0:03:11  lr: 0.000017  loss: 3.7536 (3.4082)  time: 0.8069  data: 0.0007  max mem: 19734
Epoch: [28]  [1030/1251]  eta: 0:03:03  lr: 0.000017  loss: 3.4520 (3.4065)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [28]  [1040/1251]  eta: 0:02:55  lr: 0.000017  loss: 3.4520 (3.4079)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [28]  [1050/1251]  eta: 0:02:46  lr: 0.000017  loss: 3.4845 (3.4060)  time: 0.8076  data: 0.0005  max mem: 19734
Epoch: [28]  [1060/1251]  eta: 0:02:38  lr: 0.000017  loss: 3.2614 (3.4054)  time: 0.8192  data: 0.0005  max mem: 19734
Epoch: [28]  [1070/1251]  eta: 0:02:30  lr: 0.000017  loss: 3.3491 (3.4049)  time: 0.8271  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3755, ratio_loss=0.0048, pruning_loss=0.1352, mse_loss=0.6260
Epoch: [28]  [1080/1251]  eta: 0:02:21  lr: 0.000017  loss: 3.1992 (3.4048)  time: 0.8226  data: 0.0005  max mem: 19734
Epoch: [28]  [1090/1251]  eta: 0:02:13  lr: 0.000017  loss: 3.2302 (3.4028)  time: 0.8216  data: 0.0005  max mem: 19734
Epoch: [28]  [1100/1251]  eta: 0:02:05  lr: 0.000017  loss: 3.2828 (3.4026)  time: 0.8082  data: 0.0006  max mem: 19734
Epoch: [28]  [1110/1251]  eta: 0:01:56  lr: 0.000017  loss: 3.4076 (3.4020)  time: 0.8017  data: 0.0006  max mem: 19734
Epoch: [28]  [1120/1251]  eta: 0:01:48  lr: 0.000017  loss: 3.4682 (3.4014)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [28]  [1130/1251]  eta: 0:01:40  lr: 0.000017  loss: 3.5658 (3.4030)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [28]  [1140/1251]  eta: 0:01:31  lr: 0.000017  loss: 3.5658 (3.4031)  time: 0.8054  data: 0.0006  max mem: 19734
Epoch: [28]  [1150/1251]  eta: 0:01:23  lr: 0.000017  loss: 3.4393 (3.4035)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [28]  [1160/1251]  eta: 0:01:15  lr: 0.000017  loss: 3.3993 (3.4033)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [28]  [1170/1251]  eta: 0:01:07  lr: 0.000017  loss: 3.5657 (3.4035)  time: 0.8012  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3683, ratio_loss=0.0052, pruning_loss=0.1346, mse_loss=0.5985
Epoch: [28]  [1180/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.5718 (3.4037)  time: 0.8085  data: 0.0005  max mem: 19734
Epoch: [28]  [1190/1251]  eta: 0:00:50  lr: 0.000017  loss: 3.5144 (3.4046)  time: 0.8100  data: 0.0015  max mem: 19734
Epoch: [28]  [1200/1251]  eta: 0:00:42  lr: 0.000017  loss: 3.4606 (3.4038)  time: 0.7994  data: 0.0013  max mem: 19734
Epoch: [28]  [1210/1251]  eta: 0:00:33  lr: 0.000017  loss: 3.3136 (3.4033)  time: 0.7993  data: 0.0002  max mem: 19734
Epoch: [28]  [1220/1251]  eta: 0:00:25  lr: 0.000017  loss: 3.3483 (3.4040)  time: 0.8059  data: 0.0002  max mem: 19734
Epoch: [28]  [1230/1251]  eta: 0:00:17  lr: 0.000017  loss: 3.6396 (3.4046)  time: 0.8158  data: 0.0002  max mem: 19734
Epoch: [28]  [1240/1251]  eta: 0:00:09  lr: 0.000017  loss: 3.5083 (3.4055)  time: 0.8096  data: 0.0002  max mem: 19734
Epoch: [28]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.6343 (3.4078)  time: 0.7928  data: 0.0002  max mem: 19734
Epoch: [28] Total time: 0:17:15 (0.8276 s / it)
Averaged stats: lr: 0.000017  loss: 3.6343 (3.4231)
Test:  [  0/261]  eta: 2:19:29  loss: 0.7176 (0.7176)  acc1: 83.3333 (83.3333)  acc5: 96.3542 (96.3542)  time: 32.0688  data: 31.9056  max mem: 19734
Test:  [ 10/261]  eta: 0:17:20  loss: 0.7176 (0.7258)  acc1: 83.8542 (84.1856)  acc5: 96.3542 (96.3542)  time: 4.1453  data: 3.9620  max mem: 19734
Test:  [ 20/261]  eta: 0:09:00  loss: 0.9141 (0.8964)  acc1: 80.7292 (79.1667)  acc5: 93.7500 (94.7917)  time: 0.7502  data: 0.5859  max mem: 19734
Test:  [ 30/261]  eta: 0:06:01  loss: 0.8253 (0.8154)  acc1: 82.2917 (82.0733)  acc5: 94.2708 (95.2789)  time: 0.1473  data: 0.0085  max mem: 19734
Test:  [ 40/261]  eta: 0:05:43  loss: 0.5754 (0.7850)  acc1: 88.5417 (83.0666)  acc5: 96.8750 (95.5920)  time: 0.8343  data: 0.6700  max mem: 19734
Test:  [ 50/261]  eta: 0:04:33  loss: 0.9246 (0.8477)  acc1: 79.1667 (81.2398)  acc5: 94.7917 (95.1287)  time: 0.8808  data: 0.6702  max mem: 19734
Test:  [ 60/261]  eta: 0:03:51  loss: 0.9843 (0.8571)  acc1: 76.0417 (80.7377)  acc5: 94.2708 (95.1759)  time: 0.3242  data: 0.0201  max mem: 19734
Test:  [ 70/261]  eta: 0:03:31  loss: 0.9239 (0.8579)  acc1: 77.6042 (80.2964)  acc5: 95.8333 (95.3418)  time: 0.6265  data: 0.3440  max mem: 19734
Test:  [ 80/261]  eta: 0:02:59  loss: 0.8430 (0.8614)  acc1: 78.6458 (80.4012)  acc5: 96.3542 (95.4218)  time: 0.5100  data: 0.3352  max mem: 19734
Test:  [ 90/261]  eta: 0:02:37  loss: 0.8107 (0.8471)  acc1: 82.2917 (80.7979)  acc5: 96.3542 (95.5243)  time: 0.2494  data: 0.0085  max mem: 19734
Test:  [100/261]  eta: 0:02:35  loss: 0.7949 (0.8505)  acc1: 82.2917 (80.7446)  acc5: 95.3125 (95.5600)  time: 0.8427  data: 0.5858  max mem: 19734
Test:  [110/261]  eta: 0:02:14  loss: 0.8802 (0.8733)  acc1: 76.5625 (80.2506)  acc5: 94.7917 (95.2890)  time: 0.7700  data: 0.5889  max mem: 19734
Test:  [120/261]  eta: 0:01:57  loss: 1.2263 (0.9135)  acc1: 71.3542 (79.2269)  acc5: 90.1042 (94.7228)  time: 0.1836  data: 0.0116  max mem: 19734
Test:  [130/261]  eta: 0:01:47  loss: 1.3428 (0.9588)  acc1: 68.2292 (78.3238)  acc5: 87.5000 (94.1039)  time: 0.4115  data: 0.2301  max mem: 19734
Test:  [140/261]  eta: 0:01:34  loss: 1.3384 (0.9837)  acc1: 69.2708 (77.6780)  acc5: 90.1042 (93.8645)  time: 0.4621  data: 0.2622  max mem: 19734
Test:  [150/261]  eta: 0:01:22  loss: 1.1927 (0.9889)  acc1: 71.3542 (77.6525)  acc5: 91.6667 (93.7259)  time: 0.2575  data: 0.0404  max mem: 19734
Test:  [160/261]  eta: 0:01:11  loss: 1.0140 (1.0082)  acc1: 77.6042 (77.3195)  acc5: 92.1875 (93.4394)  time: 0.2001  data: 0.0093  max mem: 19734
Test:  [170/261]  eta: 0:01:03  loss: 1.2868 (1.0381)  acc1: 65.6250 (76.5351)  acc5: 86.9792 (93.0982)  time: 0.3136  data: 0.1522  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 1.4380 (1.0550)  acc1: 65.1042 (76.1338)  acc5: 88.5417 (92.9299)  time: 0.2846  data: 0.1469  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: 1.4132 (1.0680)  acc1: 67.7083 (75.8617)  acc5: 90.6250 (92.7547)  time: 0.1164  data: 0.0005  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 1.3947 (1.0845)  acc1: 70.8333 (75.5234)  acc5: 89.0625 (92.5321)  time: 0.1157  data: 0.0005  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.3947 (1.0985)  acc1: 69.7917 (75.2345)  acc5: 89.0625 (92.3405)  time: 0.1153  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.4234 (1.1183)  acc1: 67.1875 (74.7384)  acc5: 88.0208 (92.1239)  time: 0.1146  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4234 (1.1283)  acc1: 67.1875 (74.5152)  acc5: 89.0625 (92.0207)  time: 0.1146  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.3585 (1.1378)  acc1: 68.2292 (74.2998)  acc5: 91.1458 (91.9476)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0652 (1.1303)  acc1: 75.5208 (74.4667)  acc5: 92.7083 (92.0713)  time: 0.1150  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9396 (1.1296)  acc1: 77.6042 (74.4740)  acc5: 95.3125 (92.1380)  time: 0.1118  data: 0.0001  max mem: 19734
Test: Total time: 0:02:09 (0.4956 s / it)
* Acc@1 74.474 Acc@5 92.138 loss 1.130
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.47%
Epoch: [29]  [   0/1251]  eta: 5:19:49  lr: 0.000017  loss: 3.7402 (3.7402)  time: 15.3397  data: 6.7270  max mem: 19734
Epoch: [29]  [  10/1251]  eta: 0:52:09  lr: 0.000017  loss: 3.4682 (3.5690)  time: 2.5216  data: 0.8070  max mem: 19734
loss info: cls_loss=3.4469, ratio_loss=0.0051, pruning_loss=0.1345, mse_loss=0.6598
Epoch: [29]  [  20/1251]  eta: 0:34:57  lr: 0.000017  loss: 3.3622 (3.4283)  time: 1.0218  data: 0.1078  max mem: 19734
Epoch: [29]  [  30/1251]  eta: 0:28:48  lr: 0.000017  loss: 2.9577 (3.4033)  time: 0.8078  data: 0.0007  max mem: 19734
Epoch: [29]  [  40/1251]  eta: 0:25:30  lr: 0.000017  loss: 3.0065 (3.3830)  time: 0.8027  data: 0.0006  max mem: 19734
Epoch: [29]  [  50/1251]  eta: 0:23:28  lr: 0.000017  loss: 3.0065 (3.3834)  time: 0.7956  data: 0.0004  max mem: 19734
Epoch: [29]  [  60/1251]  eta: 0:22:07  lr: 0.000017  loss: 3.0026 (3.3288)  time: 0.8085  data: 0.0006  max mem: 19734
Epoch: [29]  [  70/1251]  eta: 0:21:05  lr: 0.000017  loss: 3.0026 (3.3194)  time: 0.8145  data: 0.0006  max mem: 19734
Epoch: [29]  [  80/1251]  eta: 0:20:15  lr: 0.000017  loss: 3.5419 (3.3510)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [29]  [  90/1251]  eta: 0:19:35  lr: 0.000017  loss: 3.6723 (3.3683)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [29]  [ 100/1251]  eta: 0:19:03  lr: 0.000017  loss: 3.6519 (3.3734)  time: 0.8121  data: 0.0005  max mem: 19734
Epoch: [29]  [ 110/1251]  eta: 0:18:34  lr: 0.000017  loss: 3.3273 (3.3543)  time: 0.8128  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3497, ratio_loss=0.0052, pruning_loss=0.1377, mse_loss=0.6163
Epoch: [29]  [ 120/1251]  eta: 0:18:11  lr: 0.000017  loss: 3.4933 (3.3801)  time: 0.8219  data: 0.0004  max mem: 19734
Epoch: [29]  [ 130/1251]  eta: 0:17:47  lr: 0.000017  loss: 3.6989 (3.3930)  time: 0.8171  data: 0.0005  max mem: 19734
Epoch: [29]  [ 140/1251]  eta: 0:17:26  lr: 0.000017  loss: 3.6989 (3.3755)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [29]  [ 150/1251]  eta: 0:17:06  lr: 0.000017  loss: 3.2510 (3.3725)  time: 0.8025  data: 0.0004  max mem: 19734
Epoch: [29]  [ 160/1251]  eta: 0:16:48  lr: 0.000017  loss: 3.6693 (3.3979)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [29]  [ 170/1251]  eta: 0:16:32  lr: 0.000017  loss: 3.6826 (3.3963)  time: 0.8095  data: 0.0006  max mem: 19734
Epoch: [29]  [ 180/1251]  eta: 0:16:16  lr: 0.000017  loss: 3.3895 (3.3988)  time: 0.8092  data: 0.0005  max mem: 19734
Epoch: [29]  [ 190/1251]  eta: 0:16:01  lr: 0.000017  loss: 3.2508 (3.3896)  time: 0.8028  data: 0.0004  max mem: 19734
Epoch: [29]  [ 200/1251]  eta: 0:15:47  lr: 0.000017  loss: 3.1782 (3.3756)  time: 0.8128  data: 0.0004  max mem: 19734
Epoch: [29]  [ 210/1251]  eta: 0:15:33  lr: 0.000017  loss: 3.3494 (3.3780)  time: 0.8113  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3729, ratio_loss=0.0053, pruning_loss=0.1355, mse_loss=0.6274
Epoch: [29]  [ 220/1251]  eta: 0:15:21  lr: 0.000017  loss: 3.7085 (3.3860)  time: 0.8095  data: 0.0006  max mem: 19734
Epoch: [29]  [ 230/1251]  eta: 0:15:08  lr: 0.000017  loss: 3.6385 (3.3938)  time: 0.8094  data: 0.0005  max mem: 19734
Epoch: [29]  [ 240/1251]  eta: 0:14:56  lr: 0.000017  loss: 3.6385 (3.4005)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [29]  [ 250/1251]  eta: 0:14:44  lr: 0.000017  loss: 3.4604 (3.4028)  time: 0.8206  data: 0.0004  max mem: 19734
Epoch: [29]  [ 260/1251]  eta: 0:14:34  lr: 0.000017  loss: 3.3740 (3.3970)  time: 0.8309  data: 0.0004  max mem: 19734
Epoch: [29]  [ 270/1251]  eta: 0:14:22  lr: 0.000017  loss: 3.4436 (3.4042)  time: 0.8183  data: 0.0004  max mem: 19734
Epoch: [29]  [ 280/1251]  eta: 0:14:10  lr: 0.000017  loss: 3.4474 (3.4016)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [29]  [ 290/1251]  eta: 0:13:59  lr: 0.000017  loss: 3.2722 (3.3986)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [29]  [ 300/1251]  eta: 0:13:48  lr: 0.000017  loss: 3.3187 (3.3968)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [29]  [ 310/1251]  eta: 0:13:37  lr: 0.000017  loss: 3.6738 (3.4045)  time: 0.8026  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4327, ratio_loss=0.0051, pruning_loss=0.1346, mse_loss=0.6194
Epoch: [29]  [ 320/1251]  eta: 0:13:27  lr: 0.000017  loss: 3.6738 (3.4068)  time: 0.8118  data: 0.0004  max mem: 19734
Epoch: [29]  [ 330/1251]  eta: 0:13:17  lr: 0.000017  loss: 3.5672 (3.4100)  time: 0.8134  data: 0.0005  max mem: 19734
Epoch: [29]  [ 340/1251]  eta: 0:13:07  lr: 0.000017  loss: 3.5672 (3.4073)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [29]  [ 350/1251]  eta: 0:12:57  lr: 0.000017  loss: 3.3750 (3.4020)  time: 0.8079  data: 0.0005  max mem: 19734
Epoch: [29]  [ 360/1251]  eta: 0:12:47  lr: 0.000017  loss: 3.4145 (3.4041)  time: 0.8156  data: 0.0006  max mem: 19734
Epoch: [29]  [ 370/1251]  eta: 0:12:37  lr: 0.000017  loss: 3.4824 (3.4027)  time: 0.8094  data: 0.0006  max mem: 19734
Epoch: [29]  [ 380/1251]  eta: 0:12:27  lr: 0.000017  loss: 3.5884 (3.4087)  time: 0.8022  data: 0.0006  max mem: 19734
Epoch: [29]  [ 390/1251]  eta: 0:12:18  lr: 0.000017  loss: 3.5884 (3.4103)  time: 0.8243  data: 0.0006  max mem: 19734
Epoch: [29]  [ 400/1251]  eta: 0:12:09  lr: 0.000017  loss: 3.3840 (3.4044)  time: 0.8305  data: 0.0006  max mem: 19734
Epoch: [29]  [ 410/1251]  eta: 0:12:00  lr: 0.000017  loss: 3.3840 (3.4048)  time: 0.8294  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3596, ratio_loss=0.0051, pruning_loss=0.1351, mse_loss=0.6279
Epoch: [29]  [ 420/1251]  eta: 0:11:50  lr: 0.000017  loss: 3.3533 (3.4003)  time: 0.8281  data: 0.0006  max mem: 19734
Epoch: [29]  [ 430/1251]  eta: 0:11:41  lr: 0.000017  loss: 3.3769 (3.4021)  time: 0.8082  data: 0.0006  max mem: 19734
Epoch: [29]  [ 440/1251]  eta: 0:11:31  lr: 0.000017  loss: 3.4353 (3.3984)  time: 0.8000  data: 0.0005  max mem: 19734
Epoch: [29]  [ 450/1251]  eta: 0:11:22  lr: 0.000017  loss: 3.4336 (3.3985)  time: 0.7979  data: 0.0004  max mem: 19734
Epoch: [29]  [ 460/1251]  eta: 0:11:13  lr: 0.000017  loss: 3.4081 (3.3948)  time: 0.8069  data: 0.0004  max mem: 19734
Epoch: [29]  [ 470/1251]  eta: 0:11:03  lr: 0.000017  loss: 3.4733 (3.3965)  time: 0.8100  data: 0.0004  max mem: 19734
Epoch: [29]  [ 480/1251]  eta: 0:10:54  lr: 0.000017  loss: 3.6567 (3.4055)  time: 0.8028  data: 0.0004  max mem: 19734
Epoch: [29]  [ 490/1251]  eta: 0:10:45  lr: 0.000017  loss: 3.7559 (3.4078)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [29]  [ 500/1251]  eta: 0:10:36  lr: 0.000017  loss: 3.5543 (3.4064)  time: 0.8123  data: 0.0005  max mem: 19734
Epoch: [29]  [ 510/1251]  eta: 0:10:27  lr: 0.000017  loss: 3.4985 (3.4039)  time: 0.8203  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3812, ratio_loss=0.0050, pruning_loss=0.1338, mse_loss=0.6090
Epoch: [29]  [ 520/1251]  eta: 0:10:18  lr: 0.000017  loss: 3.3029 (3.4005)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [29]  [ 530/1251]  eta: 0:10:09  lr: 0.000017  loss: 3.2827 (3.3977)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [29]  [ 540/1251]  eta: 0:10:00  lr: 0.000017  loss: 3.2827 (3.3936)  time: 0.8150  data: 0.0004  max mem: 19734
Epoch: [29]  [ 550/1251]  eta: 0:09:52  lr: 0.000017  loss: 3.5971 (3.3964)  time: 0.8224  data: 0.0005  max mem: 19734
Epoch: [29]  [ 560/1251]  eta: 0:09:43  lr: 0.000017  loss: 3.5676 (3.3949)  time: 0.8267  data: 0.0004  max mem: 19734
Epoch: [29]  [ 570/1251]  eta: 0:09:34  lr: 0.000017  loss: 3.4826 (3.3976)  time: 0.8117  data: 0.0004  max mem: 19734
Epoch: [29]  [ 580/1251]  eta: 0:09:25  lr: 0.000017  loss: 3.6822 (3.3964)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [29]  [ 590/1251]  eta: 0:09:16  lr: 0.000017  loss: 3.4592 (3.3989)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [29]  [ 600/1251]  eta: 0:09:07  lr: 0.000017  loss: 3.4768 (3.3994)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [29]  [ 610/1251]  eta: 0:08:58  lr: 0.000017  loss: 3.5615 (3.4036)  time: 0.8054  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3853, ratio_loss=0.0050, pruning_loss=0.1343, mse_loss=0.6157
Epoch: [29]  [ 620/1251]  eta: 0:08:50  lr: 0.000017  loss: 3.5563 (3.4012)  time: 0.8075  data: 0.0005  max mem: 19734
Epoch: [29]  [ 630/1251]  eta: 0:08:41  lr: 0.000017  loss: 3.4311 (3.3999)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [29]  [ 640/1251]  eta: 0:08:32  lr: 0.000017  loss: 3.4481 (3.4018)  time: 0.7975  data: 0.0005  max mem: 19734
Epoch: [29]  [ 650/1251]  eta: 0:08:23  lr: 0.000017  loss: 3.4104 (3.4022)  time: 0.8069  data: 0.0006  max mem: 19734
Epoch: [29]  [ 660/1251]  eta: 0:08:15  lr: 0.000017  loss: 3.4104 (3.4039)  time: 0.8220  data: 0.0006  max mem: 19734
Epoch: [29]  [ 670/1251]  eta: 0:08:06  lr: 0.000017  loss: 3.3364 (3.4008)  time: 0.8137  data: 0.0006  max mem: 19734
Epoch: [29]  [ 680/1251]  eta: 0:07:58  lr: 0.000017  loss: 3.4691 (3.4037)  time: 0.8069  data: 0.0006  max mem: 19734
Epoch: [29]  [ 690/1251]  eta: 0:07:49  lr: 0.000017  loss: 3.6271 (3.4059)  time: 0.8213  data: 0.0006  max mem: 19734
Epoch: [29]  [ 700/1251]  eta: 0:07:41  lr: 0.000017  loss: 3.5380 (3.4076)  time: 0.8326  data: 0.0005  max mem: 19734
Epoch: [29]  [ 710/1251]  eta: 0:07:32  lr: 0.000017  loss: 3.5380 (3.4107)  time: 0.8211  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4544, ratio_loss=0.0050, pruning_loss=0.1326, mse_loss=0.6207
Epoch: [29]  [ 720/1251]  eta: 0:07:24  lr: 0.000017  loss: 3.5143 (3.4113)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [29]  [ 730/1251]  eta: 0:07:15  lr: 0.000017  loss: 3.4676 (3.4099)  time: 0.8022  data: 0.0006  max mem: 19734
Epoch: [29]  [ 740/1251]  eta: 0:07:06  lr: 0.000017  loss: 3.4110 (3.4091)  time: 0.8052  data: 0.0006  max mem: 19734
Epoch: [29]  [ 750/1251]  eta: 0:06:58  lr: 0.000017  loss: 3.4110 (3.4074)  time: 0.8050  data: 0.0006  max mem: 19734
Epoch: [29]  [ 760/1251]  eta: 0:06:49  lr: 0.000017  loss: 3.5102 (3.4080)  time: 0.8113  data: 0.0006  max mem: 19734
Epoch: [29]  [ 770/1251]  eta: 0:06:41  lr: 0.000017  loss: 3.4314 (3.4051)  time: 0.8083  data: 0.0005  max mem: 19734
Epoch: [29]  [ 780/1251]  eta: 0:06:32  lr: 0.000017  loss: 3.4403 (3.4067)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [29]  [ 790/1251]  eta: 0:06:24  lr: 0.000017  loss: 3.5471 (3.4053)  time: 0.7986  data: 0.0006  max mem: 19734
Epoch: [29]  [ 800/1251]  eta: 0:06:15  lr: 0.000017  loss: 3.5590 (3.4068)  time: 0.8102  data: 0.0006  max mem: 19734
Epoch: [29]  [ 810/1251]  eta: 0:06:07  lr: 0.000017  loss: 3.4352 (3.4033)  time: 0.8130  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3261, ratio_loss=0.0048, pruning_loss=0.1348, mse_loss=0.6424
Epoch: [29]  [ 820/1251]  eta: 0:05:58  lr: 0.000017  loss: 3.3698 (3.4034)  time: 0.7998  data: 0.0006  max mem: 19734
Epoch: [29]  [ 830/1251]  eta: 0:05:50  lr: 0.000017  loss: 3.3698 (3.4030)  time: 0.8108  data: 0.0006  max mem: 19734
Epoch: [29]  [ 840/1251]  eta: 0:05:42  lr: 0.000017  loss: 3.3167 (3.4034)  time: 0.8266  data: 0.0005  max mem: 19734
Epoch: [29]  [ 850/1251]  eta: 0:05:33  lr: 0.000017  loss: 3.3637 (3.4026)  time: 0.8281  data: 0.0004  max mem: 19734
Epoch: [29]  [ 860/1251]  eta: 0:05:25  lr: 0.000017  loss: 3.4497 (3.4038)  time: 0.8144  data: 0.0004  max mem: 19734
Epoch: [29]  [ 870/1251]  eta: 0:05:16  lr: 0.000017  loss: 3.5400 (3.4037)  time: 0.7988  data: 0.0005  max mem: 19734
Epoch: [29]  [ 880/1251]  eta: 0:05:08  lr: 0.000017  loss: 3.5101 (3.4032)  time: 0.7997  data: 0.0004  max mem: 19734
Epoch: [29]  [ 890/1251]  eta: 0:04:59  lr: 0.000017  loss: 3.5101 (3.4030)  time: 0.8023  data: 0.0006  max mem: 19734
Epoch: [29]  [ 900/1251]  eta: 0:04:51  lr: 0.000017  loss: 3.5528 (3.4064)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [29]  [ 910/1251]  eta: 0:04:43  lr: 0.000017  loss: 3.5211 (3.4043)  time: 0.8056  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3655, ratio_loss=0.0050, pruning_loss=0.1335, mse_loss=0.5885
Epoch: [29]  [ 920/1251]  eta: 0:04:34  lr: 0.000017  loss: 3.2673 (3.4014)  time: 0.7982  data: 0.0005  max mem: 19734
Epoch: [29]  [ 930/1251]  eta: 0:04:26  lr: 0.000017  loss: 3.1512 (3.3985)  time: 0.7977  data: 0.0006  max mem: 19734
Epoch: [29]  [ 940/1251]  eta: 0:04:17  lr: 0.000017  loss: 3.3047 (3.4003)  time: 0.8040  data: 0.0006  max mem: 19734
Epoch: [29]  [ 950/1251]  eta: 0:04:09  lr: 0.000017  loss: 3.5993 (3.3997)  time: 0.8099  data: 0.0005  max mem: 19734
Epoch: [29]  [ 960/1251]  eta: 0:04:01  lr: 0.000017  loss: 3.5512 (3.4016)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [29]  [ 970/1251]  eta: 0:03:52  lr: 0.000017  loss: 3.4181 (3.4010)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [29]  [ 980/1251]  eta: 0:03:44  lr: 0.000017  loss: 3.2205 (3.4004)  time: 0.8153  data: 0.0005  max mem: 19734
Epoch: [29]  [ 990/1251]  eta: 0:03:36  lr: 0.000017  loss: 3.3085 (3.4010)  time: 0.8208  data: 0.0006  max mem: 19734
Epoch: [29]  [1000/1251]  eta: 0:03:27  lr: 0.000017  loss: 3.5792 (3.4022)  time: 0.8164  data: 0.0005  max mem: 19734
Epoch: [29]  [1010/1251]  eta: 0:03:19  lr: 0.000017  loss: 3.6860 (3.4041)  time: 0.8069  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4004, ratio_loss=0.0050, pruning_loss=0.1360, mse_loss=0.6171
Epoch: [29]  [1020/1251]  eta: 0:03:11  lr: 0.000017  loss: 3.5590 (3.4033)  time: 0.7993  data: 0.0005  max mem: 19734
Epoch: [29]  [1030/1251]  eta: 0:03:02  lr: 0.000017  loss: 3.4926 (3.4051)  time: 0.7987  data: 0.0004  max mem: 19734
Epoch: [29]  [1040/1251]  eta: 0:02:54  lr: 0.000017  loss: 3.5512 (3.4056)  time: 0.7996  data: 0.0004  max mem: 19734
Epoch: [29]  [1050/1251]  eta: 0:02:46  lr: 0.000017  loss: 3.5512 (3.4089)  time: 0.8084  data: 0.0005  max mem: 19734
Epoch: [29]  [1060/1251]  eta: 0:02:37  lr: 0.000017  loss: 3.5494 (3.4070)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [29]  [1070/1251]  eta: 0:02:29  lr: 0.000017  loss: 3.5863 (3.4091)  time: 0.8022  data: 0.0006  max mem: 19734
Epoch: [29]  [1080/1251]  eta: 0:02:21  lr: 0.000017  loss: 3.6616 (3.4106)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [29]  [1090/1251]  eta: 0:02:13  lr: 0.000017  loss: 3.5621 (3.4094)  time: 0.8130  data: 0.0007  max mem: 19734
Epoch: [29]  [1100/1251]  eta: 0:02:04  lr: 0.000017  loss: 3.0580 (3.4079)  time: 0.8124  data: 0.0007  max mem: 19734
Epoch: [29]  [1110/1251]  eta: 0:01:56  lr: 0.000017  loss: 3.5338 (3.4097)  time: 0.7978  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4594, ratio_loss=0.0051, pruning_loss=0.1326, mse_loss=0.6189
Epoch: [29]  [1120/1251]  eta: 0:01:48  lr: 0.000017  loss: 3.5995 (3.4100)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [29]  [1130/1251]  eta: 0:01:39  lr: 0.000017  loss: 3.3637 (3.4095)  time: 0.8128  data: 0.0005  max mem: 19734
Epoch: [29]  [1140/1251]  eta: 0:01:31  lr: 0.000017  loss: 3.4035 (3.4113)  time: 0.8154  data: 0.0004  max mem: 19734
Epoch: [29]  [1150/1251]  eta: 0:01:23  lr: 0.000017  loss: 3.5228 (3.4111)  time: 0.8069  data: 0.0004  max mem: 19734
Epoch: [29]  [1160/1251]  eta: 0:01:15  lr: 0.000017  loss: 3.3612 (3.4099)  time: 0.7990  data: 0.0004  max mem: 19734
Epoch: [29]  [1170/1251]  eta: 0:01:06  lr: 0.000017  loss: 3.2541 (3.4094)  time: 0.8004  data: 0.0004  max mem: 19734
Epoch: [29]  [1180/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.4047 (3.4092)  time: 0.8012  data: 0.0006  max mem: 19734
Epoch: [29]  [1190/1251]  eta: 0:00:50  lr: 0.000017  loss: 3.5140 (3.4093)  time: 0.8065  data: 0.0011  max mem: 19734
Epoch: [29]  [1200/1251]  eta: 0:00:42  lr: 0.000017  loss: 3.4534 (3.4081)  time: 0.8027  data: 0.0009  max mem: 19734
Epoch: [29]  [1210/1251]  eta: 0:00:33  lr: 0.000017  loss: 3.3704 (3.4078)  time: 0.7932  data: 0.0002  max mem: 19734
loss info: cls_loss=3.3367, ratio_loss=0.0051, pruning_loss=0.1355, mse_loss=0.6146
Epoch: [29]  [1220/1251]  eta: 0:00:25  lr: 0.000017  loss: 3.1428 (3.4057)  time: 0.7915  data: 0.0001  max mem: 19734
Epoch: [29]  [1230/1251]  eta: 0:00:17  lr: 0.000017  loss: 3.1427 (3.4051)  time: 0.7913  data: 0.0001  max mem: 19734
Epoch: [29]  [1240/1251]  eta: 0:00:09  lr: 0.000017  loss: 3.3823 (3.4048)  time: 0.8052  data: 0.0002  max mem: 19734
Epoch: [29]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.3823 (3.4044)  time: 0.8064  data: 0.0002  max mem: 19734
Epoch: [29] Total time: 0:17:11 (0.8242 s / it)
Averaged stats: lr: 0.000017  loss: 3.3823 (3.4157)
Test:  [  0/261]  eta: 1:47:32  loss: 0.7295 (0.7295)  acc1: 83.8542 (83.8542)  acc5: 95.8333 (95.8333)  time: 24.7231  data: 24.1576  max mem: 19734
Test:  [ 10/261]  eta: 0:12:04  loss: 0.7295 (0.7447)  acc1: 83.8542 (83.9962)  acc5: 96.3542 (96.1174)  time: 2.8861  data: 2.6319  max mem: 19734
Test:  [ 20/261]  eta: 0:06:33  loss: 0.9202 (0.9091)  acc1: 79.6875 (79.1915)  acc5: 94.2708 (94.7421)  time: 0.4764  data: 0.2441  max mem: 19734
Test:  [ 30/261]  eta: 0:04:52  loss: 0.8283 (0.8228)  acc1: 82.2917 (81.9388)  acc5: 94.7917 (95.2957)  time: 0.3766  data: 0.0121  max mem: 19734
Test:  [ 40/261]  eta: 0:04:18  loss: 0.5764 (0.7905)  acc1: 86.9792 (82.8379)  acc5: 96.3542 (95.5285)  time: 0.6879  data: 0.2361  max mem: 19734
Test:  [ 50/261]  eta: 0:03:45  loss: 0.9522 (0.8558)  acc1: 77.6042 (80.8721)  acc5: 94.2708 (95.1083)  time: 0.7668  data: 0.2356  max mem: 19734
Test:  [ 60/261]  eta: 0:03:14  loss: 0.9657 (0.8652)  acc1: 76.0417 (80.5243)  acc5: 94.2708 (95.0990)  time: 0.5467  data: 0.0152  max mem: 19734
Test:  [ 70/261]  eta: 0:02:55  loss: 0.9431 (0.8650)  acc1: 77.6042 (80.2524)  acc5: 96.3542 (95.2905)  time: 0.5255  data: 0.0166  max mem: 19734
Test:  [ 80/261]  eta: 0:02:36  loss: 0.8629 (0.8662)  acc1: 80.7292 (80.3627)  acc5: 96.8750 (95.4154)  time: 0.5496  data: 0.0702  max mem: 19734
Test:  [ 90/261]  eta: 0:02:18  loss: 0.8242 (0.8528)  acc1: 84.8958 (80.7979)  acc5: 96.3542 (95.5300)  time: 0.4258  data: 0.0674  max mem: 19734
Test:  [100/261]  eta: 0:02:13  loss: 0.7904 (0.8552)  acc1: 84.8958 (80.8168)  acc5: 95.3125 (95.5807)  time: 0.6935  data: 0.3793  max mem: 19734
Test:  [110/261]  eta: 0:01:57  loss: 0.8781 (0.8782)  acc1: 77.0833 (80.2881)  acc5: 94.7917 (95.2609)  time: 0.6370  data: 0.3827  max mem: 19734
Test:  [120/261]  eta: 0:01:43  loss: 1.2483 (0.9178)  acc1: 71.3542 (79.3948)  acc5: 90.1042 (94.7357)  time: 0.2573  data: 0.0175  max mem: 19734
Test:  [130/261]  eta: 0:01:33  loss: 1.3685 (0.9629)  acc1: 68.2292 (78.4828)  acc5: 87.5000 (94.1317)  time: 0.3340  data: 0.0170  max mem: 19734
Test:  [140/261]  eta: 0:01:24  loss: 1.3328 (0.9891)  acc1: 69.2708 (77.8332)  acc5: 89.5833 (93.8830)  time: 0.4980  data: 0.0535  max mem: 19734
Test:  [150/261]  eta: 0:01:15  loss: 1.2286 (0.9940)  acc1: 73.4375 (77.8180)  acc5: 91.1458 (93.7431)  time: 0.4492  data: 0.0509  max mem: 19734
Test:  [160/261]  eta: 0:01:06  loss: 1.0465 (1.0150)  acc1: 77.6042 (77.4521)  acc5: 90.6250 (93.4297)  time: 0.3408  data: 0.0148  max mem: 19734
Test:  [170/261]  eta: 0:00:58  loss: 1.2958 (1.0457)  acc1: 64.5833 (76.6326)  acc5: 88.0208 (93.0647)  time: 0.3722  data: 0.0529  max mem: 19734
Test:  [180/261]  eta: 0:00:50  loss: 1.4080 (1.0623)  acc1: 65.1042 (76.2373)  acc5: 89.5833 (92.9184)  time: 0.3438  data: 0.0489  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.3979 (1.0760)  acc1: 67.1875 (75.9408)  acc5: 91.1458 (92.7792)  time: 0.2913  data: 0.0109  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: 1.3924 (1.0907)  acc1: 71.8750 (75.6297)  acc5: 89.5833 (92.5451)  time: 0.4028  data: 0.2034  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.3669 (1.1046)  acc1: 70.3125 (75.3579)  acc5: 88.0208 (92.3603)  time: 0.3370  data: 0.1976  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4123 (1.1238)  acc1: 67.1875 (74.8987)  acc5: 88.5417 (92.1686)  time: 0.1316  data: 0.0003  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4486 (1.1330)  acc1: 65.1042 (74.6663)  acc5: 89.0625 (92.0725)  time: 0.1215  data: 0.0026  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.2748 (1.1414)  acc1: 68.2292 (74.4640)  acc5: 91.1458 (92.0038)  time: 0.1172  data: 0.0025  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0358 (1.1348)  acc1: 76.5625 (74.6244)  acc5: 93.7500 (92.1149)  time: 0.1149  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9624 (1.1342)  acc1: 77.0833 (74.6180)  acc5: 95.3125 (92.1860)  time: 0.1115  data: 0.0001  max mem: 19734
Test: Total time: 0:02:08 (0.4915 s / it)
* Acc@1 74.618 Acc@5 92.186 loss 1.134
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.62%
Epoch: [30]  [   0/1251]  eta: 4:39:46  lr: 0.000016  loss: 3.7994 (3.7994)  time: 13.4182  data: 12.5840  max mem: 19734
Epoch: [30]  [  10/1251]  eta: 0:43:30  lr: 0.000016  loss: 3.6825 (3.4898)  time: 2.1032  data: 1.2089  max mem: 19734
Epoch: [30]  [  20/1251]  eta: 0:30:28  lr: 0.000016  loss: 3.5464 (3.4584)  time: 0.8885  data: 0.0359  max mem: 19734
Epoch: [30]  [  30/1251]  eta: 0:25:51  lr: 0.000016  loss: 3.3356 (3.4449)  time: 0.8129  data: 0.0004  max mem: 19734
Epoch: [30]  [  40/1251]  eta: 0:23:18  lr: 0.000016  loss: 3.3376 (3.4319)  time: 0.8079  data: 0.0004  max mem: 19734
Epoch: [30]  [  50/1251]  eta: 0:21:42  lr: 0.000016  loss: 3.5366 (3.4374)  time: 0.7955  data: 0.0005  max mem: 19734
Epoch: [30]  [  60/1251]  eta: 0:20:36  lr: 0.000016  loss: 3.5438 (3.4586)  time: 0.7991  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3730, ratio_loss=0.0050, pruning_loss=0.1346, mse_loss=0.6170
Epoch: [30]  [  70/1251]  eta: 0:19:49  lr: 0.000016  loss: 3.3738 (3.4056)  time: 0.8101  data: 0.0006  max mem: 19734
Epoch: [30]  [  80/1251]  eta: 0:19:10  lr: 0.000016  loss: 3.4524 (3.4202)  time: 0.8128  data: 0.0006  max mem: 19734
Epoch: [30]  [  90/1251]  eta: 0:18:37  lr: 0.000016  loss: 3.4430 (3.4132)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [30]  [ 100/1251]  eta: 0:18:09  lr: 0.000016  loss: 3.3919 (3.3988)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [30]  [ 110/1251]  eta: 0:17:44  lr: 0.000016  loss: 3.6474 (3.4258)  time: 0.7990  data: 0.0006  max mem: 19734
Epoch: [30]  [ 120/1251]  eta: 0:17:24  lr: 0.000016  loss: 3.7179 (3.4379)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [30]  [ 130/1251]  eta: 0:17:04  lr: 0.000016  loss: 3.5805 (3.4433)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [30]  [ 140/1251]  eta: 0:16:47  lr: 0.000016  loss: 3.5781 (3.4385)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [30]  [ 150/1251]  eta: 0:16:31  lr: 0.000016  loss: 3.1730 (3.4206)  time: 0.8138  data: 0.0005  max mem: 19734
Epoch: [30]  [ 160/1251]  eta: 0:16:18  lr: 0.000016  loss: 3.4311 (3.4412)  time: 0.8304  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4491, ratio_loss=0.0050, pruning_loss=0.1327, mse_loss=0.5929
Epoch: [30]  [ 170/1251]  eta: 0:16:04  lr: 0.000016  loss: 3.7079 (3.4464)  time: 0.8253  data: 0.0005  max mem: 19734
Epoch: [30]  [ 180/1251]  eta: 0:15:50  lr: 0.000016  loss: 3.5037 (3.4423)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [30]  [ 190/1251]  eta: 0:15:36  lr: 0.000016  loss: 3.5893 (3.4545)  time: 0.8068  data: 0.0005  max mem: 19734
Epoch: [30]  [ 200/1251]  eta: 0:15:24  lr: 0.000016  loss: 3.6390 (3.4549)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [30]  [ 210/1251]  eta: 0:15:11  lr: 0.000016  loss: 3.4499 (3.4462)  time: 0.8047  data: 0.0006  max mem: 19734
Epoch: [30]  [ 220/1251]  eta: 0:15:00  lr: 0.000016  loss: 2.9657 (3.4213)  time: 0.8129  data: 0.0007  max mem: 19734
Epoch: [30]  [ 230/1251]  eta: 0:14:48  lr: 0.000016  loss: 3.2760 (3.4199)  time: 0.8162  data: 0.0006  max mem: 19734
Epoch: [30]  [ 240/1251]  eta: 0:14:37  lr: 0.000016  loss: 3.5099 (3.4253)  time: 0.8082  data: 0.0004  max mem: 19734
Epoch: [30]  [ 250/1251]  eta: 0:14:26  lr: 0.000016  loss: 3.5616 (3.4230)  time: 0.8028  data: 0.0004  max mem: 19734
Epoch: [30]  [ 260/1251]  eta: 0:14:15  lr: 0.000016  loss: 3.1257 (3.4140)  time: 0.8027  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3704, ratio_loss=0.0046, pruning_loss=0.1342, mse_loss=0.6145
Epoch: [30]  [ 270/1251]  eta: 0:14:04  lr: 0.000016  loss: 3.5521 (3.4249)  time: 0.8114  data: 0.0005  max mem: 19734
Epoch: [30]  [ 280/1251]  eta: 0:13:54  lr: 0.000016  loss: 3.5597 (3.4172)  time: 0.8120  data: 0.0006  max mem: 19734
Epoch: [30]  [ 290/1251]  eta: 0:13:43  lr: 0.000016  loss: 3.3432 (3.4113)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [30]  [ 300/1251]  eta: 0:13:34  lr: 0.000016  loss: 3.3609 (3.4099)  time: 0.8143  data: 0.0005  max mem: 19734
Epoch: [30]  [ 310/1251]  eta: 0:13:24  lr: 0.000016  loss: 3.3513 (3.4055)  time: 0.8258  data: 0.0005  max mem: 19734
Epoch: [30]  [ 320/1251]  eta: 0:13:15  lr: 0.000016  loss: 3.3513 (3.4028)  time: 0.8290  data: 0.0005  max mem: 19734
Epoch: [30]  [ 330/1251]  eta: 0:13:05  lr: 0.000016  loss: 3.5752 (3.4109)  time: 0.8151  data: 0.0004  max mem: 19734
Epoch: [30]  [ 340/1251]  eta: 0:12:55  lr: 0.000016  loss: 3.6132 (3.4076)  time: 0.8072  data: 0.0005  max mem: 19734
Epoch: [30]  [ 350/1251]  eta: 0:12:46  lr: 0.000016  loss: 3.5517 (3.4122)  time: 0.8085  data: 0.0005  max mem: 19734
Epoch: [30]  [ 360/1251]  eta: 0:12:36  lr: 0.000016  loss: 3.6040 (3.4137)  time: 0.8117  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3557, ratio_loss=0.0050, pruning_loss=0.1345, mse_loss=0.6206
Epoch: [30]  [ 370/1251]  eta: 0:12:27  lr: 0.000016  loss: 3.4517 (3.4093)  time: 0.8239  data: 0.0004  max mem: 19734
Epoch: [30]  [ 380/1251]  eta: 0:12:18  lr: 0.000016  loss: 3.5185 (3.4102)  time: 0.8173  data: 0.0005  max mem: 19734
Epoch: [30]  [ 390/1251]  eta: 0:12:09  lr: 0.000016  loss: 3.5294 (3.4099)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [30]  [ 400/1251]  eta: 0:11:59  lr: 0.000016  loss: 3.6318 (3.4133)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [30]  [ 410/1251]  eta: 0:11:50  lr: 0.000016  loss: 3.5971 (3.4065)  time: 0.8057  data: 0.0007  max mem: 19734
Epoch: [30]  [ 420/1251]  eta: 0:11:41  lr: 0.000016  loss: 2.9737 (3.4012)  time: 0.8088  data: 0.0007  max mem: 19734
Epoch: [30]  [ 430/1251]  eta: 0:11:32  lr: 0.000016  loss: 3.5373 (3.4032)  time: 0.8088  data: 0.0004  max mem: 19734
Epoch: [30]  [ 440/1251]  eta: 0:11:23  lr: 0.000016  loss: 3.3608 (3.4022)  time: 0.8161  data: 0.0004  max mem: 19734
Epoch: [30]  [ 450/1251]  eta: 0:11:14  lr: 0.000016  loss: 3.2402 (3.3997)  time: 0.8300  data: 0.0005  max mem: 19734
Epoch: [30]  [ 460/1251]  eta: 0:11:06  lr: 0.000016  loss: 3.2884 (3.3988)  time: 0.8291  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3573, ratio_loss=0.0049, pruning_loss=0.1344, mse_loss=0.6094
Epoch: [30]  [ 470/1251]  eta: 0:10:57  lr: 0.000016  loss: 3.5081 (3.4016)  time: 0.8372  data: 0.0005  max mem: 19734
Epoch: [30]  [ 480/1251]  eta: 0:10:48  lr: 0.000016  loss: 3.2669 (3.3961)  time: 0.8271  data: 0.0005  max mem: 19734
Epoch: [30]  [ 490/1251]  eta: 0:10:39  lr: 0.000016  loss: 3.1902 (3.3955)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [30]  [ 500/1251]  eta: 0:10:30  lr: 0.000016  loss: 3.4425 (3.3924)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [30]  [ 510/1251]  eta: 0:10:22  lr: 0.000016  loss: 3.4425 (3.3905)  time: 0.8116  data: 0.0005  max mem: 19734
Epoch: [30]  [ 520/1251]  eta: 0:10:13  lr: 0.000016  loss: 3.6977 (3.3910)  time: 0.8172  data: 0.0005  max mem: 19734
Epoch: [30]  [ 530/1251]  eta: 0:10:04  lr: 0.000016  loss: 3.5542 (3.3925)  time: 0.8116  data: 0.0007  max mem: 19734
Epoch: [30]  [ 540/1251]  eta: 0:09:55  lr: 0.000016  loss: 3.4303 (3.3880)  time: 0.8039  data: 0.0007  max mem: 19734
Epoch: [30]  [ 550/1251]  eta: 0:09:46  lr: 0.000016  loss: 3.2820 (3.3883)  time: 0.8036  data: 0.0006  max mem: 19734
Epoch: [30]  [ 560/1251]  eta: 0:09:38  lr: 0.000016  loss: 3.3391 (3.3878)  time: 0.8057  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3128, ratio_loss=0.0048, pruning_loss=0.1357, mse_loss=0.6072
Epoch: [30]  [ 570/1251]  eta: 0:09:29  lr: 0.000016  loss: 3.4728 (3.3919)  time: 0.8156  data: 0.0005  max mem: 19734
Epoch: [30]  [ 580/1251]  eta: 0:09:21  lr: 0.000016  loss: 3.6747 (3.3957)  time: 0.8155  data: 0.0005  max mem: 19734
Epoch: [30]  [ 590/1251]  eta: 0:09:12  lr: 0.000016  loss: 3.6935 (3.3984)  time: 0.8176  data: 0.0005  max mem: 19734
Epoch: [30]  [ 600/1251]  eta: 0:09:04  lr: 0.000016  loss: 3.7636 (3.4003)  time: 0.8284  data: 0.0005  max mem: 19734
Epoch: [30]  [ 610/1251]  eta: 0:08:55  lr: 0.000016  loss: 3.6878 (3.4015)  time: 0.8208  data: 0.0005  max mem: 19734
Epoch: [30]  [ 620/1251]  eta: 0:08:46  lr: 0.000016  loss: 3.6716 (3.4037)  time: 0.8133  data: 0.0005  max mem: 19734
Epoch: [30]  [ 630/1251]  eta: 0:08:38  lr: 0.000016  loss: 3.5586 (3.4052)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [30]  [ 640/1251]  eta: 0:08:29  lr: 0.000016  loss: 3.3874 (3.4054)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [30]  [ 650/1251]  eta: 0:08:21  lr: 0.000016  loss: 3.3238 (3.4026)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [30]  [ 660/1251]  eta: 0:08:12  lr: 0.000016  loss: 3.3281 (3.4051)  time: 0.8150  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4585, ratio_loss=0.0051, pruning_loss=0.1329, mse_loss=0.6163
Epoch: [30]  [ 670/1251]  eta: 0:08:04  lr: 0.000016  loss: 3.5372 (3.4045)  time: 0.8131  data: 0.0006  max mem: 19734
Epoch: [30]  [ 680/1251]  eta: 0:07:55  lr: 0.000016  loss: 3.6055 (3.4075)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [30]  [ 690/1251]  eta: 0:07:46  lr: 0.000016  loss: 3.6055 (3.4069)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [30]  [ 700/1251]  eta: 0:07:38  lr: 0.000016  loss: 3.4493 (3.4054)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [30]  [ 710/1251]  eta: 0:07:29  lr: 0.000016  loss: 3.5647 (3.4079)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [30]  [ 720/1251]  eta: 0:07:21  lr: 0.000016  loss: 3.4536 (3.4060)  time: 0.8091  data: 0.0004  max mem: 19734
Epoch: [30]  [ 730/1251]  eta: 0:07:12  lr: 0.000016  loss: 3.4240 (3.4106)  time: 0.8014  data: 0.0004  max mem: 19734
Epoch: [30]  [ 740/1251]  eta: 0:07:04  lr: 0.000016  loss: 3.7780 (3.4123)  time: 0.8109  data: 0.0005  max mem: 19734
Epoch: [30]  [ 750/1251]  eta: 0:06:56  lr: 0.000016  loss: 3.6590 (3.4132)  time: 0.8255  data: 0.0005  max mem: 19734
Epoch: [30]  [ 760/1251]  eta: 0:06:47  lr: 0.000016  loss: 3.6259 (3.4138)  time: 0.8254  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4401, ratio_loss=0.0049, pruning_loss=0.1320, mse_loss=0.6213
Epoch: [30]  [ 770/1251]  eta: 0:06:39  lr: 0.000016  loss: 3.3434 (3.4122)  time: 0.8110  data: 0.0004  max mem: 19734
Epoch: [30]  [ 780/1251]  eta: 0:06:30  lr: 0.000016  loss: 3.2575 (3.4081)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [30]  [ 790/1251]  eta: 0:06:22  lr: 0.000016  loss: 3.3665 (3.4059)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [30]  [ 800/1251]  eta: 0:06:13  lr: 0.000016  loss: 3.4007 (3.4050)  time: 0.8000  data: 0.0004  max mem: 19734
Epoch: [30]  [ 810/1251]  eta: 0:06:05  lr: 0.000016  loss: 3.4053 (3.4033)  time: 0.8194  data: 0.0004  max mem: 19734
Epoch: [30]  [ 820/1251]  eta: 0:05:57  lr: 0.000016  loss: 3.2919 (3.4016)  time: 0.8188  data: 0.0004  max mem: 19734
Epoch: [30]  [ 830/1251]  eta: 0:05:48  lr: 0.000016  loss: 3.3975 (3.4036)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [30]  [ 840/1251]  eta: 0:05:40  lr: 0.000016  loss: 3.4723 (3.4044)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [30]  [ 850/1251]  eta: 0:05:31  lr: 0.000016  loss: 3.4682 (3.4036)  time: 0.7979  data: 0.0004  max mem: 19734
Epoch: [30]  [ 860/1251]  eta: 0:05:23  lr: 0.000016  loss: 3.6911 (3.4062)  time: 0.8033  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3293, ratio_loss=0.0048, pruning_loss=0.1343, mse_loss=0.6103
Epoch: [30]  [ 870/1251]  eta: 0:05:15  lr: 0.000016  loss: 3.6911 (3.4034)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [30]  [ 880/1251]  eta: 0:05:06  lr: 0.000016  loss: 3.1440 (3.4034)  time: 0.8076  data: 0.0005  max mem: 19734
Epoch: [30]  [ 890/1251]  eta: 0:04:58  lr: 0.000016  loss: 3.2626 (3.4008)  time: 0.8173  data: 0.0005  max mem: 19734
Epoch: [30]  [ 900/1251]  eta: 0:04:50  lr: 0.000016  loss: 3.4690 (3.4042)  time: 0.8089  data: 0.0004  max mem: 19734
Epoch: [30]  [ 910/1251]  eta: 0:04:41  lr: 0.000016  loss: 3.8158 (3.4064)  time: 0.8124  data: 0.0004  max mem: 19734
Epoch: [30]  [ 920/1251]  eta: 0:04:33  lr: 0.000016  loss: 3.5076 (3.4039)  time: 0.8150  data: 0.0005  max mem: 19734
Epoch: [30]  [ 930/1251]  eta: 0:04:25  lr: 0.000016  loss: 3.0918 (3.4030)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [30]  [ 940/1251]  eta: 0:04:16  lr: 0.000016  loss: 3.3427 (3.4026)  time: 0.8036  data: 0.0006  max mem: 19734
Epoch: [30]  [ 950/1251]  eta: 0:04:08  lr: 0.000016  loss: 3.4534 (3.4038)  time: 0.8097  data: 0.0006  max mem: 19734
Epoch: [30]  [ 960/1251]  eta: 0:04:00  lr: 0.000016  loss: 3.4591 (3.4029)  time: 0.8146  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3630, ratio_loss=0.0052, pruning_loss=0.1343, mse_loss=0.6296
Epoch: [30]  [ 970/1251]  eta: 0:03:51  lr: 0.000016  loss: 3.2862 (3.4024)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [30]  [ 980/1251]  eta: 0:03:43  lr: 0.000016  loss: 3.6848 (3.4034)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [30]  [ 990/1251]  eta: 0:03:35  lr: 0.000016  loss: 3.8215 (3.4070)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [30]  [1000/1251]  eta: 0:03:27  lr: 0.000016  loss: 3.6654 (3.4067)  time: 0.8111  data: 0.0005  max mem: 19734
Epoch: [30]  [1010/1251]  eta: 0:03:18  lr: 0.000016  loss: 3.2728 (3.4050)  time: 0.8104  data: 0.0004  max mem: 19734
Epoch: [30]  [1020/1251]  eta: 0:03:10  lr: 0.000016  loss: 3.1490 (3.4020)  time: 0.7989  data: 0.0004  max mem: 19734
Epoch: [30]  [1030/1251]  eta: 0:03:02  lr: 0.000016  loss: 3.1883 (3.4014)  time: 0.8097  data: 0.0004  max mem: 19734
Epoch: [30]  [1040/1251]  eta: 0:02:53  lr: 0.000016  loss: 3.3853 (3.4015)  time: 0.8259  data: 0.0004  max mem: 19734
Epoch: [30]  [1050/1251]  eta: 0:02:45  lr: 0.000016  loss: 3.4813 (3.4028)  time: 0.8204  data: 0.0005  max mem: 19734
Epoch: [30]  [1060/1251]  eta: 0:02:37  lr: 0.000016  loss: 3.6069 (3.4025)  time: 0.8094  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3723, ratio_loss=0.0051, pruning_loss=0.1333, mse_loss=0.6201
Epoch: [30]  [1070/1251]  eta: 0:02:29  lr: 0.000016  loss: 3.5071 (3.4015)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [30]  [1080/1251]  eta: 0:02:20  lr: 0.000016  loss: 3.2691 (3.4011)  time: 0.7994  data: 0.0006  max mem: 19734
Epoch: [30]  [1090/1251]  eta: 0:02:12  lr: 0.000016  loss: 3.6171 (3.4023)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [30]  [1100/1251]  eta: 0:02:04  lr: 0.000016  loss: 3.5496 (3.4009)  time: 0.8190  data: 0.0004  max mem: 19734
Epoch: [30]  [1110/1251]  eta: 0:01:56  lr: 0.000016  loss: 3.4154 (3.4017)  time: 0.8174  data: 0.0004  max mem: 19734
Epoch: [30]  [1120/1251]  eta: 0:01:47  lr: 0.000016  loss: 3.4549 (3.4010)  time: 0.7999  data: 0.0004  max mem: 19734
Epoch: [30]  [1130/1251]  eta: 0:01:39  lr: 0.000016  loss: 3.3860 (3.4004)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [30]  [1140/1251]  eta: 0:01:31  lr: 0.000016  loss: 3.2744 (3.3994)  time: 0.7996  data: 0.0004  max mem: 19734
Epoch: [30]  [1150/1251]  eta: 0:01:23  lr: 0.000016  loss: 3.3461 (3.3993)  time: 0.8121  data: 0.0004  max mem: 19734
Epoch: [30]  [1160/1251]  eta: 0:01:14  lr: 0.000016  loss: 3.4449 (3.3998)  time: 0.8127  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3706, ratio_loss=0.0052, pruning_loss=0.1341, mse_loss=0.5911
Epoch: [30]  [1170/1251]  eta: 0:01:06  lr: 0.000016  loss: 3.4449 (3.3996)  time: 0.8017  data: 0.0006  max mem: 19734
Epoch: [30]  [1180/1251]  eta: 0:00:58  lr: 0.000016  loss: 3.4951 (3.4008)  time: 0.8225  data: 0.0004  max mem: 19734
Epoch: [30]  [1190/1251]  eta: 0:00:50  lr: 0.000016  loss: 3.4951 (3.3995)  time: 0.8197  data: 0.0009  max mem: 19734
Epoch: [30]  [1200/1251]  eta: 0:00:41  lr: 0.000016  loss: 3.4760 (3.3997)  time: 0.8079  data: 0.0007  max mem: 19734
Epoch: [30]  [1210/1251]  eta: 0:00:33  lr: 0.000016  loss: 3.5727 (3.4016)  time: 0.8054  data: 0.0001  max mem: 19734
Epoch: [30]  [1220/1251]  eta: 0:00:25  lr: 0.000016  loss: 3.6587 (3.4020)  time: 0.7925  data: 0.0001  max mem: 19734
Epoch: [30]  [1230/1251]  eta: 0:00:17  lr: 0.000016  loss: 3.5170 (3.4004)  time: 0.7934  data: 0.0001  max mem: 19734
Epoch: [30]  [1240/1251]  eta: 0:00:09  lr: 0.000016  loss: 3.2249 (3.3986)  time: 0.7938  data: 0.0001  max mem: 19734
Epoch: [30]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.3356 (3.3989)  time: 0.8057  data: 0.0001  max mem: 19734
Epoch: [30] Total time: 0:17:08 (0.8223 s / it)
Averaged stats: lr: 0.000016  loss: 3.3356 (3.4143)
Test:  [  0/261]  eta: 1:58:12  loss: 0.7228 (0.7228)  acc1: 83.3333 (83.3333)  acc5: 95.8333 (95.8333)  time: 27.1733  data: 27.0210  max mem: 19734
Test:  [ 10/261]  eta: 0:15:15  loss: 0.7228 (0.7301)  acc1: 83.3333 (83.6648)  acc5: 96.3542 (96.2121)  time: 3.6467  data: 3.2085  max mem: 19734
Test:  [ 20/261]  eta: 0:08:18  loss: 0.9013 (0.9001)  acc1: 78.6458 (79.4147)  acc5: 94.2708 (94.7917)  time: 0.8116  data: 0.4233  max mem: 19734
Test:  [ 30/261]  eta: 0:05:49  loss: 0.8162 (0.8135)  acc1: 82.8125 (82.4093)  acc5: 94.2708 (95.2789)  time: 0.3380  data: 0.0212  max mem: 19734
Test:  [ 40/261]  eta: 0:04:25  loss: 0.5462 (0.7819)  acc1: 88.5417 (83.1682)  acc5: 96.8750 (95.6174)  time: 0.2889  data: 0.0254  max mem: 19734
Test:  [ 50/261]  eta: 0:03:32  loss: 0.9897 (0.8490)  acc1: 77.0833 (81.1683)  acc5: 93.2292 (95.0980)  time: 0.2272  data: 0.0177  max mem: 19734
Test:  [ 60/261]  eta: 0:02:58  loss: 0.9898 (0.8560)  acc1: 75.5208 (80.8231)  acc5: 93.2292 (95.1332)  time: 0.2530  data: 0.0090  max mem: 19734
Test:  [ 70/261]  eta: 0:02:43  loss: 0.9272 (0.8564)  acc1: 78.1250 (80.4064)  acc5: 96.3542 (95.3345)  time: 0.4727  data: 0.0141  max mem: 19734
Test:  [ 80/261]  eta: 0:02:23  loss: 0.8670 (0.8599)  acc1: 79.6875 (80.4270)  acc5: 96.8750 (95.4540)  time: 0.4974  data: 0.0173  max mem: 19734
Test:  [ 90/261]  eta: 0:02:20  loss: 0.8364 (0.8486)  acc1: 81.7708 (80.7578)  acc5: 95.8333 (95.5128)  time: 0.6937  data: 0.3305  max mem: 19734
Test:  [100/261]  eta: 0:02:09  loss: 0.8216 (0.8511)  acc1: 83.3333 (80.7189)  acc5: 95.3125 (95.5600)  time: 0.8646  data: 0.4839  max mem: 19734
Test:  [110/261]  eta: 0:01:55  loss: 0.8684 (0.8747)  acc1: 77.6042 (80.1943)  acc5: 94.7917 (95.2327)  time: 0.5190  data: 0.1735  max mem: 19734
Test:  [120/261]  eta: 0:01:42  loss: 1.2331 (0.9153)  acc1: 71.3542 (79.2183)  acc5: 89.5833 (94.6711)  time: 0.3055  data: 0.0221  max mem: 19734
Test:  [130/261]  eta: 0:01:29  loss: 1.4366 (0.9609)  acc1: 66.6667 (78.2204)  acc5: 86.9792 (94.0919)  time: 0.2172  data: 0.0187  max mem: 19734
Test:  [140/261]  eta: 0:01:18  loss: 1.3257 (0.9869)  acc1: 68.7500 (77.5525)  acc5: 89.5833 (93.8128)  time: 0.2037  data: 0.0116  max mem: 19734
Test:  [150/261]  eta: 0:01:08  loss: 1.2166 (0.9919)  acc1: 71.8750 (77.5283)  acc5: 90.6250 (93.6845)  time: 0.1869  data: 0.0042  max mem: 19734
Test:  [160/261]  eta: 0:01:00  loss: 1.0592 (1.0122)  acc1: 76.5625 (77.1836)  acc5: 91.6667 (93.3877)  time: 0.2078  data: 0.0044  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.2635 (1.0418)  acc1: 63.5417 (76.4315)  acc5: 88.5417 (93.0556)  time: 0.5489  data: 0.2592  max mem: 19734
Test:  [180/261]  eta: 0:00:49  loss: 1.4833 (1.0582)  acc1: 64.0625 (75.9956)  acc5: 88.5417 (92.9270)  time: 0.7822  data: 0.4687  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.3783 (1.0719)  acc1: 66.1458 (75.7226)  acc5: 90.6250 (92.7656)  time: 0.5372  data: 0.3102  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.3719 (1.0865)  acc1: 70.8333 (75.4664)  acc5: 88.5417 (92.5503)  time: 0.2745  data: 0.1337  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.3407 (1.0999)  acc1: 69.7917 (75.1999)  acc5: 88.0208 (92.3554)  time: 0.1659  data: 0.0411  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.3724 (1.1183)  acc1: 67.1875 (74.7667)  acc5: 87.5000 (92.1427)  time: 0.1221  data: 0.0046  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4459 (1.1279)  acc1: 66.6667 (74.5355)  acc5: 89.0625 (92.0500)  time: 0.1156  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3439 (1.1374)  acc1: 68.7500 (74.2933)  acc5: 90.6250 (91.9606)  time: 0.1153  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 0.9921 (1.1299)  acc1: 76.0417 (74.4895)  acc5: 93.2292 (92.0817)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9471 (1.1292)  acc1: 77.6042 (74.5040)  acc5: 95.3125 (92.1440)  time: 0.1116  data: 0.0001  max mem: 19734
Test: Total time: 0:02:04 (0.4760 s / it)
* Acc@1 74.504 Acc@5 92.144 loss 1.129
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.62%
Epoch: [31]  [   0/1251]  eta: 6:34:24  lr: 0.000016  loss: 2.5986 (2.5986)  time: 18.9165  data: 9.3102  max mem: 19734
Epoch: [31]  [  10/1251]  eta: 0:53:23  lr: 0.000016  loss: 3.7571 (3.4882)  time: 2.5815  data: 0.8469  max mem: 19734
loss info: cls_loss=3.3709, ratio_loss=0.0052, pruning_loss=0.1342, mse_loss=0.6102
Epoch: [31]  [  20/1251]  eta: 0:35:36  lr: 0.000016  loss: 3.5675 (3.4268)  time: 0.8762  data: 0.0005  max mem: 19734
Epoch: [31]  [  30/1251]  eta: 0:29:10  lr: 0.000016  loss: 3.4782 (3.3790)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [31]  [  40/1251]  eta: 0:25:55  lr: 0.000016  loss: 3.1728 (3.3083)  time: 0.8109  data: 0.0004  max mem: 19734
Epoch: [31]  [  50/1251]  eta: 0:23:48  lr: 0.000016  loss: 3.1625 (3.2741)  time: 0.8094  data: 0.0006  max mem: 19734
Epoch: [31]  [  60/1251]  eta: 0:22:22  lr: 0.000016  loss: 3.1625 (3.2593)  time: 0.8058  data: 0.0006  max mem: 19734
Epoch: [31]  [  70/1251]  eta: 0:21:23  lr: 0.000016  loss: 3.2231 (3.2467)  time: 0.8252  data: 0.0006  max mem: 19734
Epoch: [31]  [  80/1251]  eta: 0:20:36  lr: 0.000016  loss: 3.2231 (3.2465)  time: 0.8393  data: 0.0007  max mem: 19734
Epoch: [31]  [  90/1251]  eta: 0:19:53  lr: 0.000016  loss: 3.3051 (3.2794)  time: 0.8220  data: 0.0007  max mem: 19734
Epoch: [31]  [ 100/1251]  eta: 0:19:17  lr: 0.000016  loss: 3.7191 (3.3178)  time: 0.8016  data: 0.0006  max mem: 19734
Epoch: [31]  [ 110/1251]  eta: 0:18:47  lr: 0.000016  loss: 3.6233 (3.3421)  time: 0.8028  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3143, ratio_loss=0.0051, pruning_loss=0.1364, mse_loss=0.6121
Epoch: [31]  [ 120/1251]  eta: 0:18:19  lr: 0.000016  loss: 3.5991 (3.3664)  time: 0.8012  data: 0.0004  max mem: 19734
Epoch: [31]  [ 130/1251]  eta: 0:17:56  lr: 0.000016  loss: 3.5251 (3.3684)  time: 0.8067  data: 0.0005  max mem: 19734
Epoch: [31]  [ 140/1251]  eta: 0:17:34  lr: 0.000016  loss: 3.3963 (3.3543)  time: 0.8075  data: 0.0006  max mem: 19734
Epoch: [31]  [ 150/1251]  eta: 0:17:13  lr: 0.000016  loss: 3.2884 (3.3563)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [31]  [ 160/1251]  eta: 0:16:55  lr: 0.000016  loss: 3.2271 (3.3453)  time: 0.8024  data: 0.0004  max mem: 19734
Epoch: [31]  [ 170/1251]  eta: 0:16:37  lr: 0.000016  loss: 3.4591 (3.3610)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [31]  [ 180/1251]  eta: 0:16:21  lr: 0.000016  loss: 3.7852 (3.3747)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [31]  [ 190/1251]  eta: 0:16:08  lr: 0.000016  loss: 3.5867 (3.3614)  time: 0.8200  data: 0.0004  max mem: 19734
Epoch: [31]  [ 200/1251]  eta: 0:15:53  lr: 0.000016  loss: 3.3496 (3.3735)  time: 0.8209  data: 0.0004  max mem: 19734
Epoch: [31]  [ 210/1251]  eta: 0:15:40  lr: 0.000016  loss: 3.6692 (3.3810)  time: 0.8164  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3907, ratio_loss=0.0049, pruning_loss=0.1341, mse_loss=0.6091
Epoch: [31]  [ 220/1251]  eta: 0:15:27  lr: 0.000016  loss: 3.5355 (3.3775)  time: 0.8236  data: 0.0005  max mem: 19734
Epoch: [31]  [ 230/1251]  eta: 0:15:15  lr: 0.000016  loss: 3.1393 (3.3673)  time: 0.8270  data: 0.0005  max mem: 19734
Epoch: [31]  [ 240/1251]  eta: 0:15:02  lr: 0.000016  loss: 3.1393 (3.3660)  time: 0.8193  data: 0.0005  max mem: 19734
Epoch: [31]  [ 250/1251]  eta: 0:14:49  lr: 0.000016  loss: 3.5578 (3.3695)  time: 0.8015  data: 0.0006  max mem: 19734
Epoch: [31]  [ 260/1251]  eta: 0:14:38  lr: 0.000016  loss: 3.5578 (3.3704)  time: 0.8111  data: 0.0005  max mem: 19734
Epoch: [31]  [ 270/1251]  eta: 0:14:26  lr: 0.000016  loss: 3.7712 (3.3791)  time: 0.8152  data: 0.0005  max mem: 19734
Epoch: [31]  [ 280/1251]  eta: 0:14:16  lr: 0.000016  loss: 3.5901 (3.3773)  time: 0.8179  data: 0.0005  max mem: 19734
Epoch: [31]  [ 290/1251]  eta: 0:14:05  lr: 0.000016  loss: 3.5901 (3.3823)  time: 0.8226  data: 0.0005  max mem: 19734
Epoch: [31]  [ 300/1251]  eta: 0:13:53  lr: 0.000016  loss: 3.6231 (3.3924)  time: 0.8107  data: 0.0006  max mem: 19734
Epoch: [31]  [ 310/1251]  eta: 0:13:42  lr: 0.000016  loss: 3.6665 (3.3987)  time: 0.8022  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4093, ratio_loss=0.0049, pruning_loss=0.1336, mse_loss=0.6033
Epoch: [31]  [ 320/1251]  eta: 0:13:32  lr: 0.000016  loss: 3.5872 (3.3943)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [31]  [ 330/1251]  eta: 0:13:21  lr: 0.000016  loss: 3.5132 (3.4024)  time: 0.8134  data: 0.0005  max mem: 19734
Epoch: [31]  [ 340/1251]  eta: 0:13:11  lr: 0.000016  loss: 3.7754 (3.4077)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [31]  [ 350/1251]  eta: 0:13:00  lr: 0.000016  loss: 3.5456 (3.4049)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [31]  [ 360/1251]  eta: 0:12:51  lr: 0.000016  loss: 3.3988 (3.4014)  time: 0.8226  data: 0.0005  max mem: 19734
Epoch: [31]  [ 370/1251]  eta: 0:12:41  lr: 0.000016  loss: 3.2285 (3.3956)  time: 0.8337  data: 0.0004  max mem: 19734
Epoch: [31]  [ 380/1251]  eta: 0:12:32  lr: 0.000016  loss: 3.4445 (3.3945)  time: 0.8215  data: 0.0004  max mem: 19734
Epoch: [31]  [ 390/1251]  eta: 0:12:22  lr: 0.000016  loss: 3.2934 (3.3913)  time: 0.8131  data: 0.0005  max mem: 19734
Epoch: [31]  [ 400/1251]  eta: 0:12:12  lr: 0.000016  loss: 3.4017 (3.4021)  time: 0.8139  data: 0.0004  max mem: 19734
Epoch: [31]  [ 410/1251]  eta: 0:12:03  lr: 0.000016  loss: 3.5061 (3.4019)  time: 0.8132  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4269, ratio_loss=0.0050, pruning_loss=0.1326, mse_loss=0.6117
Epoch: [31]  [ 420/1251]  eta: 0:11:53  lr: 0.000016  loss: 3.5061 (3.4059)  time: 0.8076  data: 0.0005  max mem: 19734
Epoch: [31]  [ 430/1251]  eta: 0:11:43  lr: 0.000016  loss: 3.6263 (3.4082)  time: 0.8062  data: 0.0004  max mem: 19734
Epoch: [31]  [ 440/1251]  eta: 0:11:34  lr: 0.000016  loss: 3.6098 (3.4078)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [31]  [ 450/1251]  eta: 0:11:24  lr: 0.000016  loss: 3.5587 (3.4091)  time: 0.8064  data: 0.0004  max mem: 19734
Epoch: [31]  [ 460/1251]  eta: 0:11:15  lr: 0.000016  loss: 3.5033 (3.4060)  time: 0.8058  data: 0.0004  max mem: 19734
Epoch: [31]  [ 470/1251]  eta: 0:11:06  lr: 0.000016  loss: 3.2640 (3.4043)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [31]  [ 480/1251]  eta: 0:10:57  lr: 0.000016  loss: 3.6066 (3.4088)  time: 0.8113  data: 0.0005  max mem: 19734
Epoch: [31]  [ 490/1251]  eta: 0:10:47  lr: 0.000016  loss: 3.5467 (3.4058)  time: 0.8097  data: 0.0004  max mem: 19734
Epoch: [31]  [ 500/1251]  eta: 0:10:38  lr: 0.000016  loss: 3.3948 (3.4053)  time: 0.8048  data: 0.0004  max mem: 19734
Epoch: [31]  [ 510/1251]  eta: 0:10:29  lr: 0.000016  loss: 3.3276 (3.4028)  time: 0.8256  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3547, ratio_loss=0.0051, pruning_loss=0.1349, mse_loss=0.6106
Epoch: [31]  [ 520/1251]  eta: 0:10:21  lr: 0.000016  loss: 3.2117 (3.4018)  time: 0.8396  data: 0.0004  max mem: 19734
Epoch: [31]  [ 530/1251]  eta: 0:10:12  lr: 0.000016  loss: 3.3126 (3.4033)  time: 0.8211  data: 0.0004  max mem: 19734
Epoch: [31]  [ 540/1251]  eta: 0:10:03  lr: 0.000016  loss: 3.5040 (3.4063)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [31]  [ 550/1251]  eta: 0:09:54  lr: 0.000016  loss: 3.2637 (3.3991)  time: 0.8120  data: 0.0004  max mem: 19734
Epoch: [31]  [ 560/1251]  eta: 0:09:45  lr: 0.000016  loss: 2.9915 (3.3979)  time: 0.8111  data: 0.0004  max mem: 19734
Epoch: [31]  [ 570/1251]  eta: 0:09:36  lr: 0.000016  loss: 3.4306 (3.3989)  time: 0.8073  data: 0.0004  max mem: 19734
Epoch: [31]  [ 580/1251]  eta: 0:09:27  lr: 0.000016  loss: 3.6141 (3.4023)  time: 0.8096  data: 0.0004  max mem: 19734
Epoch: [31]  [ 590/1251]  eta: 0:09:18  lr: 0.000016  loss: 3.5621 (3.4019)  time: 0.8035  data: 0.0004  max mem: 19734
Epoch: [31]  [ 600/1251]  eta: 0:09:09  lr: 0.000016  loss: 3.4783 (3.4014)  time: 0.7988  data: 0.0004  max mem: 19734
Epoch: [31]  [ 610/1251]  eta: 0:09:00  lr: 0.000016  loss: 3.4540 (3.4020)  time: 0.7978  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4237, ratio_loss=0.0052, pruning_loss=0.1330, mse_loss=0.5835
Epoch: [31]  [ 620/1251]  eta: 0:08:51  lr: 0.000016  loss: 3.7733 (3.4096)  time: 0.7992  data: 0.0004  max mem: 19734
Epoch: [31]  [ 630/1251]  eta: 0:08:43  lr: 0.000016  loss: 3.7733 (3.4115)  time: 0.8109  data: 0.0004  max mem: 19734
Epoch: [31]  [ 640/1251]  eta: 0:08:34  lr: 0.000016  loss: 3.5899 (3.4120)  time: 0.8124  data: 0.0004  max mem: 19734
Epoch: [31]  [ 650/1251]  eta: 0:08:25  lr: 0.000016  loss: 3.7124 (3.4123)  time: 0.8205  data: 0.0004  max mem: 19734
Epoch: [31]  [ 660/1251]  eta: 0:08:17  lr: 0.000016  loss: 3.6736 (3.4126)  time: 0.8337  data: 0.0004  max mem: 19734
Epoch: [31]  [ 670/1251]  eta: 0:08:08  lr: 0.000016  loss: 3.6329 (3.4126)  time: 0.8266  data: 0.0004  max mem: 19734
Epoch: [31]  [ 680/1251]  eta: 0:07:59  lr: 0.000016  loss: 3.5174 (3.4117)  time: 0.8137  data: 0.0004  max mem: 19734
Epoch: [31]  [ 690/1251]  eta: 0:07:51  lr: 0.000016  loss: 3.3684 (3.4120)  time: 0.8033  data: 0.0004  max mem: 19734
Epoch: [31]  [ 700/1251]  eta: 0:07:42  lr: 0.000016  loss: 3.5921 (3.4143)  time: 0.8053  data: 0.0004  max mem: 19734
Epoch: [31]  [ 710/1251]  eta: 0:07:34  lr: 0.000016  loss: 3.5605 (3.4120)  time: 0.8124  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4309, ratio_loss=0.0048, pruning_loss=0.1323, mse_loss=0.6202
Epoch: [31]  [ 720/1251]  eta: 0:07:25  lr: 0.000016  loss: 3.5240 (3.4144)  time: 0.8088  data: 0.0005  max mem: 19734
Epoch: [31]  [ 730/1251]  eta: 0:07:16  lr: 0.000016  loss: 3.6210 (3.4145)  time: 0.8007  data: 0.0004  max mem: 19734
Epoch: [31]  [ 740/1251]  eta: 0:07:08  lr: 0.000016  loss: 3.6411 (3.4189)  time: 0.7992  data: 0.0004  max mem: 19734
Epoch: [31]  [ 750/1251]  eta: 0:06:59  lr: 0.000016  loss: 3.7058 (3.4204)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [31]  [ 760/1251]  eta: 0:06:50  lr: 0.000016  loss: 3.3419 (3.4168)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [31]  [ 770/1251]  eta: 0:06:42  lr: 0.000016  loss: 3.1716 (3.4146)  time: 0.8038  data: 0.0004  max mem: 19734
Epoch: [31]  [ 780/1251]  eta: 0:06:33  lr: 0.000016  loss: 3.2842 (3.4131)  time: 0.8088  data: 0.0004  max mem: 19734
Epoch: [31]  [ 790/1251]  eta: 0:06:25  lr: 0.000016  loss: 3.5204 (3.4148)  time: 0.8181  data: 0.0003  max mem: 19734
Epoch: [31]  [ 800/1251]  eta: 0:06:16  lr: 0.000016  loss: 3.5658 (3.4172)  time: 0.8283  data: 0.0004  max mem: 19734
Epoch: [31]  [ 810/1251]  eta: 0:06:08  lr: 0.000016  loss: 3.5658 (3.4194)  time: 0.8244  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4499, ratio_loss=0.0051, pruning_loss=0.1313, mse_loss=0.6068
Epoch: [31]  [ 820/1251]  eta: 0:06:00  lr: 0.000016  loss: 3.6495 (3.4209)  time: 0.8134  data: 0.0004  max mem: 19734
Epoch: [31]  [ 830/1251]  eta: 0:05:51  lr: 0.000016  loss: 3.5519 (3.4203)  time: 0.8115  data: 0.0004  max mem: 19734
Epoch: [31]  [ 840/1251]  eta: 0:05:43  lr: 0.000016  loss: 3.5301 (3.4224)  time: 0.8054  data: 0.0003  max mem: 19734
Epoch: [31]  [ 850/1251]  eta: 0:05:34  lr: 0.000016  loss: 3.5301 (3.4244)  time: 0.8040  data: 0.0004  max mem: 19734
Epoch: [31]  [ 860/1251]  eta: 0:05:26  lr: 0.000016  loss: 3.4492 (3.4228)  time: 0.8077  data: 0.0004  max mem: 19734
Epoch: [31]  [ 870/1251]  eta: 0:05:17  lr: 0.000016  loss: 3.6371 (3.4250)  time: 0.8036  data: 0.0004  max mem: 19734
Epoch: [31]  [ 880/1251]  eta: 0:05:09  lr: 0.000016  loss: 3.7033 (3.4255)  time: 0.7998  data: 0.0004  max mem: 19734
Epoch: [31]  [ 890/1251]  eta: 0:05:00  lr: 0.000016  loss: 3.4561 (3.4244)  time: 0.8014  data: 0.0004  max mem: 19734
Epoch: [31]  [ 900/1251]  eta: 0:04:52  lr: 0.000016  loss: 3.5754 (3.4252)  time: 0.8057  data: 0.0004  max mem: 19734
Epoch: [31]  [ 910/1251]  eta: 0:04:43  lr: 0.000016  loss: 3.6476 (3.4270)  time: 0.8045  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4720, ratio_loss=0.0047, pruning_loss=0.1314, mse_loss=0.6355
Epoch: [31]  [ 920/1251]  eta: 0:04:35  lr: 0.000016  loss: 3.5574 (3.4271)  time: 0.8089  data: 0.0004  max mem: 19734
Epoch: [31]  [ 930/1251]  eta: 0:04:26  lr: 0.000016  loss: 3.4727 (3.4265)  time: 0.8099  data: 0.0004  max mem: 19734
Epoch: [31]  [ 940/1251]  eta: 0:04:18  lr: 0.000016  loss: 3.5456 (3.4272)  time: 0.8086  data: 0.0004  max mem: 19734
Epoch: [31]  [ 950/1251]  eta: 0:04:10  lr: 0.000016  loss: 3.6866 (3.4288)  time: 0.8196  data: 0.0004  max mem: 19734
Epoch: [31]  [ 960/1251]  eta: 0:04:01  lr: 0.000016  loss: 3.6866 (3.4294)  time: 0.8236  data: 0.0004  max mem: 19734
Epoch: [31]  [ 970/1251]  eta: 0:03:53  lr: 0.000016  loss: 3.6195 (3.4285)  time: 0.8111  data: 0.0004  max mem: 19734
Epoch: [31]  [ 980/1251]  eta: 0:03:45  lr: 0.000016  loss: 3.6195 (3.4298)  time: 0.7982  data: 0.0005  max mem: 19734
Epoch: [31]  [ 990/1251]  eta: 0:03:36  lr: 0.000016  loss: 3.5081 (3.4285)  time: 0.8047  data: 0.0006  max mem: 19734
Epoch: [31]  [1000/1251]  eta: 0:03:28  lr: 0.000016  loss: 3.1383 (3.4256)  time: 0.8181  data: 0.0006  max mem: 19734
Epoch: [31]  [1010/1251]  eta: 0:03:20  lr: 0.000016  loss: 3.3508 (3.4237)  time: 0.8115  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3594, ratio_loss=0.0049, pruning_loss=0.1339, mse_loss=0.6071
Epoch: [31]  [1020/1251]  eta: 0:03:11  lr: 0.000016  loss: 3.4095 (3.4228)  time: 0.7995  data: 0.0007  max mem: 19734
Epoch: [31]  [1030/1251]  eta: 0:03:03  lr: 0.000016  loss: 3.4095 (3.4231)  time: 0.8018  data: 0.0006  max mem: 19734
Epoch: [31]  [1040/1251]  eta: 0:02:54  lr: 0.000016  loss: 3.4218 (3.4246)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [31]  [1050/1251]  eta: 0:02:46  lr: 0.000016  loss: 3.6049 (3.4249)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [31]  [1060/1251]  eta: 0:02:38  lr: 0.000016  loss: 3.4835 (3.4235)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [31]  [1070/1251]  eta: 0:02:30  lr: 0.000016  loss: 3.4287 (3.4226)  time: 0.8143  data: 0.0006  max mem: 19734
Epoch: [31]  [1080/1251]  eta: 0:02:21  lr: 0.000016  loss: 3.5037 (3.4233)  time: 0.8129  data: 0.0006  max mem: 19734
Epoch: [31]  [1090/1251]  eta: 0:02:13  lr: 0.000016  loss: 3.5633 (3.4234)  time: 0.8068  data: 0.0004  max mem: 19734
Epoch: [31]  [1100/1251]  eta: 0:02:05  lr: 0.000016  loss: 3.4082 (3.4220)  time: 0.8417  data: 0.0006  max mem: 19734
Epoch: [31]  [1110/1251]  eta: 0:01:56  lr: 0.000016  loss: 3.4082 (3.4205)  time: 0.8433  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3718, ratio_loss=0.0049, pruning_loss=0.1332, mse_loss=0.6313
Epoch: [31]  [1120/1251]  eta: 0:01:48  lr: 0.000016  loss: 3.4120 (3.4199)  time: 0.8111  data: 0.0005  max mem: 19734
Epoch: [31]  [1130/1251]  eta: 0:01:40  lr: 0.000016  loss: 3.4666 (3.4196)  time: 0.8004  data: 0.0007  max mem: 19734
Epoch: [31]  [1140/1251]  eta: 0:01:31  lr: 0.000016  loss: 3.5621 (3.4199)  time: 0.8029  data: 0.0007  max mem: 19734
Epoch: [31]  [1150/1251]  eta: 0:01:23  lr: 0.000016  loss: 3.3856 (3.4189)  time: 0.8105  data: 0.0004  max mem: 19734
Epoch: [31]  [1160/1251]  eta: 0:01:15  lr: 0.000016  loss: 3.5770 (3.4207)  time: 0.8064  data: 0.0004  max mem: 19734
Epoch: [31]  [1170/1251]  eta: 0:01:07  lr: 0.000016  loss: 3.6122 (3.4209)  time: 0.7988  data: 0.0004  max mem: 19734
Epoch: [31]  [1180/1251]  eta: 0:00:58  lr: 0.000016  loss: 3.2228 (3.4180)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [31]  [1190/1251]  eta: 0:00:50  lr: 0.000016  loss: 3.0251 (3.4181)  time: 0.8022  data: 0.0010  max mem: 19734
Epoch: [31]  [1200/1251]  eta: 0:00:42  lr: 0.000016  loss: 3.6661 (3.4192)  time: 0.7962  data: 0.0009  max mem: 19734
Epoch: [31]  [1210/1251]  eta: 0:00:33  lr: 0.000016  loss: 3.7124 (3.4217)  time: 0.7907  data: 0.0001  max mem: 19734
loss info: cls_loss=3.4021, ratio_loss=0.0051, pruning_loss=0.1344, mse_loss=0.6035
Epoch: [31]  [1220/1251]  eta: 0:00:25  lr: 0.000016  loss: 3.5974 (3.4206)  time: 0.7987  data: 0.0002  max mem: 19734
Epoch: [31]  [1230/1251]  eta: 0:00:17  lr: 0.000016  loss: 3.5606 (3.4214)  time: 0.7984  data: 0.0002  max mem: 19734
Epoch: [31]  [1240/1251]  eta: 0:00:09  lr: 0.000016  loss: 3.3857 (3.4197)  time: 0.8060  data: 0.0003  max mem: 19734
Epoch: [31]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.0863 (3.4179)  time: 0.8184  data: 0.0002  max mem: 19734
Epoch: [31] Total time: 0:17:14 (0.8267 s / it)
Averaged stats: lr: 0.000016  loss: 3.0863 (3.4202)
Test:  [  0/261]  eta: 1:26:30  loss: 0.7071 (0.7071)  acc1: 83.8542 (83.8542)  acc5: 97.3958 (97.3958)  time: 19.8857  data: 19.6762  max mem: 19734
Test:  [ 10/261]  eta: 0:15:38  loss: 0.7071 (0.7121)  acc1: 86.4583 (84.1383)  acc5: 97.3958 (96.7330)  time: 3.7396  data: 3.2858  max mem: 19734
Test:  [ 20/261]  eta: 0:09:39  loss: 0.8834 (0.8894)  acc1: 79.6875 (79.2907)  acc5: 94.7917 (95.1637)  time: 1.5299  data: 1.0833  max mem: 19734
Test:  [ 30/261]  eta: 0:06:50  loss: 0.7986 (0.8047)  acc1: 84.3750 (82.2413)  acc5: 95.3125 (95.6485)  time: 0.6998  data: 0.2721  max mem: 19734
Test:  [ 40/261]  eta: 0:05:17  loss: 0.5615 (0.7753)  acc1: 88.5417 (83.0793)  acc5: 96.8750 (95.9096)  time: 0.4232  data: 0.1014  max mem: 19734
Test:  [ 50/261]  eta: 0:04:20  loss: 0.9190 (0.8394)  acc1: 77.0833 (81.1172)  acc5: 95.8333 (95.4759)  time: 0.3975  data: 0.0972  max mem: 19734
Test:  [ 60/261]  eta: 0:03:36  loss: 0.9921 (0.8492)  acc1: 75.0000 (80.6865)  acc5: 93.7500 (95.4064)  time: 0.3326  data: 0.0144  max mem: 19734
Test:  [ 70/261]  eta: 0:03:05  loss: 0.9322 (0.8489)  acc1: 77.0833 (80.2670)  acc5: 95.3125 (95.5546)  time: 0.3018  data: 0.0926  max mem: 19734
Test:  [ 80/261]  eta: 0:02:42  loss: 0.8154 (0.8513)  acc1: 80.2083 (80.4270)  acc5: 96.3542 (95.6404)  time: 0.3570  data: 0.0937  max mem: 19734
Test:  [ 90/261]  eta: 0:02:22  loss: 0.8154 (0.8396)  acc1: 83.8542 (80.7578)  acc5: 95.8333 (95.6845)  time: 0.3297  data: 0.0139  max mem: 19734
Test:  [100/261]  eta: 0:02:13  loss: 0.8123 (0.8431)  acc1: 83.3333 (80.7034)  acc5: 95.8333 (95.7250)  time: 0.5476  data: 0.3138  max mem: 19734
Test:  [110/261]  eta: 0:01:56  loss: 0.8511 (0.8681)  acc1: 78.1250 (80.1286)  acc5: 94.7917 (95.3923)  time: 0.5005  data: 0.3115  max mem: 19734
Test:  [120/261]  eta: 0:01:42  loss: 1.2397 (0.9083)  acc1: 70.8333 (79.1925)  acc5: 89.5833 (94.8562)  time: 0.2279  data: 0.0127  max mem: 19734
Test:  [130/261]  eta: 0:01:30  loss: 1.3692 (0.9542)  acc1: 67.7083 (78.2284)  acc5: 88.5417 (94.2390)  time: 0.2185  data: 0.0113  max mem: 19734
Test:  [140/261]  eta: 0:01:25  loss: 1.3094 (0.9808)  acc1: 68.7500 (77.5783)  acc5: 89.0625 (93.9495)  time: 0.5957  data: 0.4008  max mem: 19734
Test:  [150/261]  eta: 0:01:16  loss: 1.2581 (0.9855)  acc1: 72.9167 (77.5869)  acc5: 91.6667 (93.8121)  time: 0.6700  data: 0.4065  max mem: 19734
Test:  [160/261]  eta: 0:01:05  loss: 1.0301 (1.0062)  acc1: 78.1250 (77.2677)  acc5: 91.6667 (93.5138)  time: 0.2497  data: 0.0151  max mem: 19734
Test:  [170/261]  eta: 0:01:00  loss: 1.3590 (1.0375)  acc1: 64.5833 (76.4742)  acc5: 87.5000 (93.1500)  time: 0.5472  data: 0.3931  max mem: 19734
Test:  [180/261]  eta: 0:00:51  loss: 1.4587 (1.0561)  acc1: 64.0625 (76.0244)  acc5: 88.0208 (92.9616)  time: 0.5443  data: 0.3923  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: 1.3778 (1.0694)  acc1: 66.1458 (75.7499)  acc5: 90.6250 (92.7983)  time: 0.1534  data: 0.0083  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: 1.3546 (1.0845)  acc1: 71.3542 (75.4146)  acc5: 89.5833 (92.5736)  time: 0.1314  data: 0.0029  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.3546 (1.0985)  acc1: 70.8333 (75.1555)  acc5: 88.0208 (92.3529)  time: 0.1167  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4221 (1.1173)  acc1: 67.7083 (74.6771)  acc5: 88.0208 (92.1522)  time: 0.1154  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4520 (1.1272)  acc1: 65.6250 (74.4386)  acc5: 89.5833 (92.0409)  time: 0.1144  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3375 (1.1368)  acc1: 66.6667 (74.1939)  acc5: 90.6250 (91.9627)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0717 (1.1297)  acc1: 75.0000 (74.3713)  acc5: 93.7500 (92.0651)  time: 0.1150  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9046 (1.1291)  acc1: 77.6042 (74.3660)  acc5: 94.7917 (92.1320)  time: 0.1117  data: 0.0001  max mem: 19734
Test: Total time: 0:02:05 (0.4818 s / it)
* Acc@1 74.366 Acc@5 92.132 loss 1.129
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.62%
Epoch: [32]  [   0/1251]  eta: 6:51:26  lr: 0.000016  loss: 3.2403 (3.2403)  time: 19.7336  data: 15.5425  max mem: 19734
Epoch: [32]  [  10/1251]  eta: 0:54:51  lr: 0.000016  loss: 3.4401 (3.2380)  time: 2.6523  data: 1.4147  max mem: 19734
Epoch: [32]  [  20/1251]  eta: 0:36:23  lr: 0.000016  loss: 3.4605 (3.3262)  time: 0.8754  data: 0.0013  max mem: 19734
Epoch: [32]  [  30/1251]  eta: 0:29:49  lr: 0.000016  loss: 3.4605 (3.3551)  time: 0.8130  data: 0.0005  max mem: 19734
Epoch: [32]  [  40/1251]  eta: 0:26:17  lr: 0.000016  loss: 3.5129 (3.3983)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [32]  [  50/1251]  eta: 0:24:06  lr: 0.000016  loss: 3.5936 (3.3998)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [32]  [  60/1251]  eta: 0:22:36  lr: 0.000016  loss: 3.6435 (3.4360)  time: 0.8027  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3666, ratio_loss=0.0053, pruning_loss=0.1346, mse_loss=0.6028
Epoch: [32]  [  70/1251]  eta: 0:21:28  lr: 0.000016  loss: 3.5873 (3.4121)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [32]  [  80/1251]  eta: 0:20:35  lr: 0.000016  loss: 3.5873 (3.4142)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [32]  [  90/1251]  eta: 0:19:52  lr: 0.000016  loss: 3.4085 (3.3901)  time: 0.8002  data: 0.0006  max mem: 19734
Epoch: [32]  [ 100/1251]  eta: 0:19:18  lr: 0.000016  loss: 3.4085 (3.3905)  time: 0.8111  data: 0.0006  max mem: 19734
Epoch: [32]  [ 110/1251]  eta: 0:18:47  lr: 0.000016  loss: 3.5006 (3.4044)  time: 0.8131  data: 0.0005  max mem: 19734
Epoch: [32]  [ 120/1251]  eta: 0:18:22  lr: 0.000016  loss: 3.6492 (3.4156)  time: 0.8138  data: 0.0005  max mem: 19734
Epoch: [32]  [ 130/1251]  eta: 0:18:01  lr: 0.000016  loss: 3.6666 (3.4150)  time: 0.8324  data: 0.0008  max mem: 19734
Epoch: [32]  [ 140/1251]  eta: 0:17:39  lr: 0.000016  loss: 3.5297 (3.4186)  time: 0.8235  data: 0.0008  max mem: 19734
Epoch: [32]  [ 150/1251]  eta: 0:17:22  lr: 0.000016  loss: 3.4469 (3.4156)  time: 0.8319  data: 0.0005  max mem: 19734
Epoch: [32]  [ 160/1251]  eta: 0:17:03  lr: 0.000016  loss: 3.2234 (3.3963)  time: 0.8345  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3638, ratio_loss=0.0049, pruning_loss=0.1320, mse_loss=0.6104
Epoch: [32]  [ 170/1251]  eta: 0:16:45  lr: 0.000016  loss: 3.3122 (3.4019)  time: 0.8063  data: 0.0006  max mem: 19734
Epoch: [32]  [ 180/1251]  eta: 0:16:30  lr: 0.000016  loss: 3.3874 (3.3962)  time: 0.8127  data: 0.0005  max mem: 19734
Epoch: [32]  [ 190/1251]  eta: 0:16:14  lr: 0.000016  loss: 3.3874 (3.4006)  time: 0.8144  data: 0.0005  max mem: 19734
Epoch: [32]  [ 200/1251]  eta: 0:15:59  lr: 0.000016  loss: 3.5205 (3.4063)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [32]  [ 210/1251]  eta: 0:15:44  lr: 0.000016  loss: 3.2887 (3.3919)  time: 0.8024  data: 0.0006  max mem: 19734
Epoch: [32]  [ 220/1251]  eta: 0:15:30  lr: 0.000016  loss: 3.2739 (3.3893)  time: 0.8045  data: 0.0006  max mem: 19734
Epoch: [32]  [ 230/1251]  eta: 0:15:17  lr: 0.000016  loss: 3.6161 (3.4005)  time: 0.8033  data: 0.0007  max mem: 19734
Epoch: [32]  [ 240/1251]  eta: 0:15:04  lr: 0.000016  loss: 3.7687 (3.4170)  time: 0.8022  data: 0.0006  max mem: 19734
Epoch: [32]  [ 250/1251]  eta: 0:14:52  lr: 0.000016  loss: 3.7047 (3.4252)  time: 0.8118  data: 0.0005  max mem: 19734
Epoch: [32]  [ 260/1251]  eta: 0:14:40  lr: 0.000016  loss: 3.6740 (3.4380)  time: 0.8117  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4823, ratio_loss=0.0050, pruning_loss=0.1306, mse_loss=0.6227
Epoch: [32]  [ 270/1251]  eta: 0:14:28  lr: 0.000016  loss: 3.6786 (3.4419)  time: 0.8050  data: 0.0006  max mem: 19734
Epoch: [32]  [ 280/1251]  eta: 0:14:18  lr: 0.000016  loss: 3.6951 (3.4433)  time: 0.8297  data: 0.0007  max mem: 19734
Epoch: [32]  [ 290/1251]  eta: 0:14:07  lr: 0.000016  loss: 3.3206 (3.4371)  time: 0.8415  data: 0.0007  max mem: 19734
Epoch: [32]  [ 300/1251]  eta: 0:13:57  lr: 0.000016  loss: 3.3506 (3.4373)  time: 0.8308  data: 0.0005  max mem: 19734
Epoch: [32]  [ 310/1251]  eta: 0:13:46  lr: 0.000016  loss: 3.4917 (3.4328)  time: 0.8159  data: 0.0005  max mem: 19734
Epoch: [32]  [ 320/1251]  eta: 0:13:35  lr: 0.000016  loss: 3.4001 (3.4256)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [32]  [ 330/1251]  eta: 0:13:24  lr: 0.000016  loss: 3.4096 (3.4220)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [32]  [ 340/1251]  eta: 0:13:13  lr: 0.000016  loss: 3.4637 (3.4235)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [32]  [ 350/1251]  eta: 0:13:03  lr: 0.000016  loss: 3.5780 (3.4275)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [32]  [ 360/1251]  eta: 0:12:53  lr: 0.000016  loss: 3.5780 (3.4245)  time: 0.8031  data: 0.0008  max mem: 19734
loss info: cls_loss=3.3912, ratio_loss=0.0051, pruning_loss=0.1324, mse_loss=0.5945
Epoch: [32]  [ 370/1251]  eta: 0:12:43  lr: 0.000016  loss: 3.4733 (3.4231)  time: 0.8053  data: 0.0008  max mem: 19734
Epoch: [32]  [ 380/1251]  eta: 0:12:32  lr: 0.000016  loss: 3.4361 (3.4233)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [32]  [ 390/1251]  eta: 0:12:23  lr: 0.000016  loss: 3.4361 (3.4242)  time: 0.8170  data: 0.0005  max mem: 19734
Epoch: [32]  [ 400/1251]  eta: 0:12:13  lr: 0.000016  loss: 3.3373 (3.4210)  time: 0.8161  data: 0.0005  max mem: 19734
Epoch: [32]  [ 410/1251]  eta: 0:12:04  lr: 0.000016  loss: 3.3373 (3.4213)  time: 0.8161  data: 0.0005  max mem: 19734
Epoch: [32]  [ 420/1251]  eta: 0:11:54  lr: 0.000016  loss: 3.6739 (3.4260)  time: 0.8248  data: 0.0005  max mem: 19734
Epoch: [32]  [ 430/1251]  eta: 0:11:45  lr: 0.000016  loss: 3.6000 (3.4285)  time: 0.8179  data: 0.0005  max mem: 19734
Epoch: [32]  [ 440/1251]  eta: 0:11:36  lr: 0.000016  loss: 3.5703 (3.4315)  time: 0.8207  data: 0.0005  max mem: 19734
Epoch: [32]  [ 450/1251]  eta: 0:11:27  lr: 0.000016  loss: 3.4526 (3.4331)  time: 0.8282  data: 0.0005  max mem: 19734
Epoch: [32]  [ 460/1251]  eta: 0:11:17  lr: 0.000016  loss: 3.4526 (3.4332)  time: 0.8164  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4245, ratio_loss=0.0053, pruning_loss=0.1323, mse_loss=0.6122
Epoch: [32]  [ 470/1251]  eta: 0:11:08  lr: 0.000016  loss: 3.3922 (3.4336)  time: 0.8118  data: 0.0006  max mem: 19734
Epoch: [32]  [ 480/1251]  eta: 0:10:59  lr: 0.000016  loss: 3.5258 (3.4345)  time: 0.8143  data: 0.0006  max mem: 19734
Epoch: [32]  [ 490/1251]  eta: 0:10:49  lr: 0.000016  loss: 3.6981 (3.4382)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [32]  [ 500/1251]  eta: 0:10:40  lr: 0.000016  loss: 3.6897 (3.4382)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [32]  [ 510/1251]  eta: 0:10:31  lr: 0.000016  loss: 3.1692 (3.4297)  time: 0.8031  data: 0.0008  max mem: 19734
Epoch: [32]  [ 520/1251]  eta: 0:10:22  lr: 0.000016  loss: 3.0998 (3.4273)  time: 0.8023  data: 0.0007  max mem: 19734
Epoch: [32]  [ 530/1251]  eta: 0:10:12  lr: 0.000016  loss: 3.1468 (3.4250)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [32]  [ 540/1251]  eta: 0:10:03  lr: 0.000016  loss: 3.3562 (3.4292)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [32]  [ 550/1251]  eta: 0:09:54  lr: 0.000016  loss: 3.6148 (3.4299)  time: 0.8131  data: 0.0005  max mem: 19734
Epoch: [32]  [ 560/1251]  eta: 0:09:45  lr: 0.000016  loss: 3.4712 (3.4294)  time: 0.8116  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3766, ratio_loss=0.0050, pruning_loss=0.1328, mse_loss=0.6009
Epoch: [32]  [ 570/1251]  eta: 0:09:37  lr: 0.000016  loss: 3.5591 (3.4273)  time: 0.8286  data: 0.0005  max mem: 19734
Epoch: [32]  [ 580/1251]  eta: 0:09:28  lr: 0.000016  loss: 3.4914 (3.4221)  time: 0.8229  data: 0.0004  max mem: 19734
Epoch: [32]  [ 590/1251]  eta: 0:09:19  lr: 0.000016  loss: 3.4914 (3.4233)  time: 0.8206  data: 0.0005  max mem: 19734
Epoch: [32]  [ 600/1251]  eta: 0:09:10  lr: 0.000016  loss: 3.4486 (3.4184)  time: 0.8171  data: 0.0006  max mem: 19734
Epoch: [32]  [ 610/1251]  eta: 0:09:01  lr: 0.000016  loss: 3.4564 (3.4212)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [32]  [ 620/1251]  eta: 0:08:53  lr: 0.000016  loss: 3.5403 (3.4183)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [32]  [ 630/1251]  eta: 0:08:44  lr: 0.000016  loss: 3.4952 (3.4181)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [32]  [ 640/1251]  eta: 0:08:35  lr: 0.000016  loss: 3.5013 (3.4198)  time: 0.8003  data: 0.0004  max mem: 19734
Epoch: [32]  [ 650/1251]  eta: 0:08:26  lr: 0.000016  loss: 3.4831 (3.4170)  time: 0.8028  data: 0.0004  max mem: 19734
Epoch: [32]  [ 660/1251]  eta: 0:08:17  lr: 0.000016  loss: 3.2284 (3.4138)  time: 0.8019  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3150, ratio_loss=0.0049, pruning_loss=0.1327, mse_loss=0.6232
Epoch: [32]  [ 670/1251]  eta: 0:08:09  lr: 0.000016  loss: 3.3239 (3.4129)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [32]  [ 680/1251]  eta: 0:08:00  lr: 0.000016  loss: 3.4287 (3.4105)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [32]  [ 690/1251]  eta: 0:07:51  lr: 0.000016  loss: 3.5490 (3.4141)  time: 0.8127  data: 0.0004  max mem: 19734
Epoch: [32]  [ 700/1251]  eta: 0:07:43  lr: 0.000016  loss: 3.5490 (3.4128)  time: 0.8140  data: 0.0004  max mem: 19734
Epoch: [32]  [ 710/1251]  eta: 0:07:34  lr: 0.000016  loss: 3.4083 (3.4113)  time: 0.8247  data: 0.0004  max mem: 19734
Epoch: [32]  [ 720/1251]  eta: 0:07:26  lr: 0.000016  loss: 3.4479 (3.4120)  time: 0.8296  data: 0.0004  max mem: 19734
Epoch: [32]  [ 730/1251]  eta: 0:07:17  lr: 0.000016  loss: 3.4479 (3.4100)  time: 0.8178  data: 0.0004  max mem: 19734
Epoch: [32]  [ 740/1251]  eta: 0:07:08  lr: 0.000016  loss: 3.2194 (3.4064)  time: 0.8199  data: 0.0005  max mem: 19734
Epoch: [32]  [ 750/1251]  eta: 0:07:00  lr: 0.000016  loss: 3.2194 (3.4058)  time: 0.8110  data: 0.0008  max mem: 19734
Epoch: [32]  [ 760/1251]  eta: 0:06:51  lr: 0.000016  loss: 3.3893 (3.4038)  time: 0.8033  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3172, ratio_loss=0.0051, pruning_loss=0.1324, mse_loss=0.5909
Epoch: [32]  [ 770/1251]  eta: 0:06:43  lr: 0.000016  loss: 3.3001 (3.4017)  time: 0.8120  data: 0.0005  max mem: 19734
Epoch: [32]  [ 780/1251]  eta: 0:06:34  lr: 0.000016  loss: 3.4279 (3.4028)  time: 0.8116  data: 0.0005  max mem: 19734
Epoch: [32]  [ 790/1251]  eta: 0:06:26  lr: 0.000016  loss: 3.4328 (3.4022)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [32]  [ 800/1251]  eta: 0:06:17  lr: 0.000016  loss: 3.2320 (3.3983)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [32]  [ 810/1251]  eta: 0:06:08  lr: 0.000016  loss: 3.0540 (3.3958)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [32]  [ 820/1251]  eta: 0:06:00  lr: 0.000016  loss: 3.2112 (3.3937)  time: 0.8023  data: 0.0004  max mem: 19734
Epoch: [32]  [ 830/1251]  eta: 0:05:51  lr: 0.000016  loss: 3.4483 (3.3965)  time: 0.8081  data: 0.0007  max mem: 19734
Epoch: [32]  [ 840/1251]  eta: 0:05:43  lr: 0.000016  loss: 3.6613 (3.3963)  time: 0.8057  data: 0.0007  max mem: 19734
Epoch: [32]  [ 850/1251]  eta: 0:05:34  lr: 0.000016  loss: 3.5201 (3.3965)  time: 0.8063  data: 0.0006  max mem: 19734
Epoch: [32]  [ 860/1251]  eta: 0:05:26  lr: 0.000016  loss: 3.3839 (3.3942)  time: 0.8316  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3273, ratio_loss=0.0050, pruning_loss=0.1336, mse_loss=0.6175
Epoch: [32]  [ 870/1251]  eta: 0:05:18  lr: 0.000016  loss: 3.6026 (3.3976)  time: 0.8286  data: 0.0005  max mem: 19734
Epoch: [32]  [ 880/1251]  eta: 0:05:09  lr: 0.000016  loss: 3.6109 (3.3946)  time: 0.8157  data: 0.0005  max mem: 19734
Epoch: [32]  [ 890/1251]  eta: 0:05:01  lr: 0.000016  loss: 3.2833 (3.3952)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [32]  [ 900/1251]  eta: 0:04:52  lr: 0.000016  loss: 3.3687 (3.3945)  time: 0.8061  data: 0.0005  max mem: 19734
Epoch: [32]  [ 910/1251]  eta: 0:04:44  lr: 0.000016  loss: 3.1852 (3.3919)  time: 0.8061  data: 0.0007  max mem: 19734
Epoch: [32]  [ 920/1251]  eta: 0:04:35  lr: 0.000016  loss: 3.4127 (3.3909)  time: 0.8074  data: 0.0007  max mem: 19734
Epoch: [32]  [ 930/1251]  eta: 0:04:27  lr: 0.000016  loss: 3.4511 (3.3908)  time: 0.8019  data: 0.0004  max mem: 19734
Epoch: [32]  [ 940/1251]  eta: 0:04:19  lr: 0.000016  loss: 3.5308 (3.3922)  time: 0.7998  data: 0.0004  max mem: 19734
Epoch: [32]  [ 950/1251]  eta: 0:04:10  lr: 0.000016  loss: 3.4012 (3.3928)  time: 0.7986  data: 0.0004  max mem: 19734
Epoch: [32]  [ 960/1251]  eta: 0:04:02  lr: 0.000016  loss: 3.4012 (3.3940)  time: 0.7977  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3466, ratio_loss=0.0049, pruning_loss=0.1336, mse_loss=0.5780
Epoch: [32]  [ 970/1251]  eta: 0:03:53  lr: 0.000016  loss: 3.6952 (3.3941)  time: 0.7981  data: 0.0005  max mem: 19734
Epoch: [32]  [ 980/1251]  eta: 0:03:45  lr: 0.000016  loss: 3.5348 (3.3946)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [32]  [ 990/1251]  eta: 0:03:36  lr: 0.000016  loss: 3.5354 (3.3961)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [32]  [1000/1251]  eta: 0:03:28  lr: 0.000016  loss: 3.6024 (3.3989)  time: 0.8150  data: 0.0006  max mem: 19734
Epoch: [32]  [1010/1251]  eta: 0:03:20  lr: 0.000016  loss: 3.6050 (3.3987)  time: 0.8268  data: 0.0005  max mem: 19734
Epoch: [32]  [1020/1251]  eta: 0:03:11  lr: 0.000016  loss: 3.5536 (3.3998)  time: 0.8128  data: 0.0005  max mem: 19734
Epoch: [32]  [1030/1251]  eta: 0:03:03  lr: 0.000016  loss: 3.4172 (3.3990)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [32]  [1040/1251]  eta: 0:02:55  lr: 0.000016  loss: 3.4573 (3.4009)  time: 0.8179  data: 0.0005  max mem: 19734
Epoch: [32]  [1050/1251]  eta: 0:02:46  lr: 0.000016  loss: 3.6236 (3.4021)  time: 0.8087  data: 0.0006  max mem: 19734
Epoch: [32]  [1060/1251]  eta: 0:02:38  lr: 0.000016  loss: 3.6866 (3.4031)  time: 0.8073  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5004, ratio_loss=0.0051, pruning_loss=0.1308, mse_loss=0.5996
Epoch: [32]  [1070/1251]  eta: 0:02:30  lr: 0.000016  loss: 3.6866 (3.4046)  time: 0.8068  data: 0.0006  max mem: 19734
Epoch: [32]  [1080/1251]  eta: 0:02:21  lr: 0.000016  loss: 3.6966 (3.4052)  time: 0.7982  data: 0.0006  max mem: 19734
Epoch: [32]  [1090/1251]  eta: 0:02:13  lr: 0.000016  loss: 3.7821 (3.4063)  time: 0.7984  data: 0.0004  max mem: 19734
Epoch: [32]  [1100/1251]  eta: 0:02:05  lr: 0.000016  loss: 3.6569 (3.4076)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [32]  [1110/1251]  eta: 0:01:56  lr: 0.000016  loss: 3.6275 (3.4104)  time: 0.8021  data: 0.0006  max mem: 19734
Epoch: [32]  [1120/1251]  eta: 0:01:48  lr: 0.000016  loss: 3.8052 (3.4122)  time: 0.8117  data: 0.0005  max mem: 19734
Epoch: [32]  [1130/1251]  eta: 0:01:40  lr: 0.000016  loss: 3.6083 (3.4120)  time: 0.8102  data: 0.0006  max mem: 19734
Epoch: [32]  [1140/1251]  eta: 0:01:31  lr: 0.000016  loss: 3.4972 (3.4123)  time: 0.7990  data: 0.0007  max mem: 19734
Epoch: [32]  [1150/1251]  eta: 0:01:23  lr: 0.000016  loss: 3.4316 (3.4124)  time: 0.8215  data: 0.0006  max mem: 19734
Epoch: [32]  [1160/1251]  eta: 0:01:15  lr: 0.000016  loss: 3.4316 (3.4126)  time: 0.8312  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4778, ratio_loss=0.0051, pruning_loss=0.1302, mse_loss=0.5944
Epoch: [32]  [1170/1251]  eta: 0:01:07  lr: 0.000016  loss: 3.2973 (3.4118)  time: 0.8166  data: 0.0005  max mem: 19734
Epoch: [32]  [1180/1251]  eta: 0:00:58  lr: 0.000016  loss: 3.3148 (3.4118)  time: 0.8075  data: 0.0006  max mem: 19734
Epoch: [32]  [1190/1251]  eta: 0:00:50  lr: 0.000016  loss: 3.4200 (3.4113)  time: 0.8101  data: 0.0012  max mem: 19734
Epoch: [32]  [1200/1251]  eta: 0:00:42  lr: 0.000016  loss: 3.2417 (3.4097)  time: 0.8067  data: 0.0010  max mem: 19734
Epoch: [32]  [1210/1251]  eta: 0:00:33  lr: 0.000016  loss: 3.1881 (3.4074)  time: 0.8008  data: 0.0001  max mem: 19734
Epoch: [32]  [1220/1251]  eta: 0:00:25  lr: 0.000016  loss: 3.4664 (3.4077)  time: 0.7990  data: 0.0001  max mem: 19734
Epoch: [32]  [1230/1251]  eta: 0:00:17  lr: 0.000016  loss: 3.4273 (3.4067)  time: 0.7946  data: 0.0001  max mem: 19734
Epoch: [32]  [1240/1251]  eta: 0:00:09  lr: 0.000016  loss: 3.5168 (3.4074)  time: 0.7943  data: 0.0001  max mem: 19734
Epoch: [32]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.6627 (3.4099)  time: 0.8014  data: 0.0002  max mem: 19734
Epoch: [32] Total time: 0:17:15 (0.8275 s / it)
Averaged stats: lr: 0.000016  loss: 3.6627 (3.4164)
Test:  [  0/261]  eta: 2:53:28  loss: 0.7437 (0.7437)  acc1: 83.3333 (83.3333)  acc5: 95.8333 (95.8333)  time: 39.8788  data: 39.7064  max mem: 19734
Test:  [ 10/261]  eta: 0:16:42  loss: 0.7437 (0.7360)  acc1: 85.4167 (83.9489)  acc5: 96.8750 (96.2595)  time: 3.9925  data: 3.6184  max mem: 19734
Test:  [ 20/261]  eta: 0:09:21  loss: 0.9023 (0.9086)  acc1: 78.6458 (78.9187)  acc5: 93.7500 (94.6925)  time: 0.4520  data: 0.0151  max mem: 19734
Test:  [ 30/261]  eta: 0:06:21  loss: 0.8258 (0.8247)  acc1: 82.2917 (81.7876)  acc5: 94.7917 (95.2621)  time: 0.3628  data: 0.0223  max mem: 19734
Test:  [ 40/261]  eta: 0:05:01  loss: 0.5922 (0.7893)  acc1: 86.9792 (82.8379)  acc5: 96.8750 (95.5920)  time: 0.3544  data: 0.1781  max mem: 19734
Test:  [ 50/261]  eta: 0:04:03  loss: 0.9030 (0.8533)  acc1: 78.1250 (80.9845)  acc5: 94.7917 (95.2308)  time: 0.3894  data: 0.1966  max mem: 19734
Test:  [ 60/261]  eta: 0:03:22  loss: 0.9811 (0.8632)  acc1: 76.5625 (80.6096)  acc5: 94.2708 (95.2100)  time: 0.2759  data: 0.0381  max mem: 19734
Test:  [ 70/261]  eta: 0:03:16  loss: 0.9238 (0.8616)  acc1: 77.6042 (80.2157)  acc5: 95.8333 (95.4152)  time: 0.6956  data: 0.3433  max mem: 19734
Test:  [ 80/261]  eta: 0:02:54  loss: 0.7995 (0.8642)  acc1: 80.2083 (80.3498)  acc5: 96.8750 (95.4925)  time: 0.8237  data: 0.3432  max mem: 19734
Test:  [ 90/261]  eta: 0:02:33  loss: 0.7995 (0.8510)  acc1: 82.8125 (80.7006)  acc5: 95.8333 (95.5586)  time: 0.4512  data: 0.0168  max mem: 19734
Test:  [100/261]  eta: 0:02:17  loss: 0.8292 (0.8543)  acc1: 83.3333 (80.6209)  acc5: 95.3125 (95.6064)  time: 0.4162  data: 0.0340  max mem: 19734
Test:  [110/261]  eta: 0:02:06  loss: 0.8484 (0.8775)  acc1: 77.0833 (80.1567)  acc5: 94.2708 (95.2843)  time: 0.5430  data: 0.1068  max mem: 19734
Test:  [120/261]  eta: 0:01:51  loss: 1.2212 (0.9168)  acc1: 72.3958 (79.2398)  acc5: 90.1042 (94.7228)  time: 0.4841  data: 0.0909  max mem: 19734
Test:  [130/261]  eta: 0:01:39  loss: 1.3651 (0.9620)  acc1: 68.2292 (78.2840)  acc5: 86.4583 (94.1595)  time: 0.3440  data: 0.0201  max mem: 19734
Test:  [140/261]  eta: 0:01:36  loss: 1.3163 (0.9879)  acc1: 68.2292 (77.5598)  acc5: 90.1042 (93.8941)  time: 0.8195  data: 0.5139  max mem: 19734
Test:  [150/261]  eta: 0:01:24  loss: 1.2264 (0.9929)  acc1: 70.8333 (77.5214)  acc5: 91.1458 (93.7362)  time: 0.7716  data: 0.5506  max mem: 19734
Test:  [160/261]  eta: 0:01:13  loss: 1.0165 (1.0124)  acc1: 79.6875 (77.2127)  acc5: 92.1875 (93.4491)  time: 0.2105  data: 0.0520  max mem: 19734
Test:  [170/261]  eta: 0:01:03  loss: 1.3174 (1.0427)  acc1: 64.0625 (76.4133)  acc5: 86.9792 (93.0982)  time: 0.2094  data: 0.0736  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 1.4548 (1.0603)  acc1: 64.5833 (76.0186)  acc5: 90.1042 (92.9357)  time: 0.2017  data: 0.0810  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: 1.4240 (1.0741)  acc1: 67.7083 (75.7526)  acc5: 90.6250 (92.7847)  time: 0.1273  data: 0.0120  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 1.3492 (1.0900)  acc1: 71.3542 (75.4094)  acc5: 89.0625 (92.5529)  time: 0.1222  data: 0.0006  max mem: 19734
Test:  [210/261]  eta: 0:00:30  loss: 1.3676 (1.1036)  acc1: 69.7917 (75.1358)  acc5: 88.0208 (92.3405)  time: 0.1217  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.4649 (1.1222)  acc1: 67.7083 (74.6795)  acc5: 88.5417 (92.1522)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4161 (1.1313)  acc1: 65.1042 (74.4679)  acc5: 89.5833 (92.0432)  time: 0.1154  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.3557 (1.1408)  acc1: 68.7500 (74.2242)  acc5: 90.6250 (91.9649)  time: 0.1154  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0983 (1.1339)  acc1: 74.4792 (74.3754)  acc5: 93.2292 (92.0630)  time: 0.1152  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9323 (1.1328)  acc1: 77.0833 (74.4060)  acc5: 95.3125 (92.1360)  time: 0.1121  data: 0.0001  max mem: 19734
Test: Total time: 0:02:10 (0.4993 s / it)
* Acc@1 74.406 Acc@5 92.136 loss 1.133
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.62%
Epoch: [33]  [   0/1251]  eta: 6:49:18  lr: 0.000016  loss: 3.6618 (3.6618)  time: 19.6309  data: 15.6336  max mem: 19734
Epoch: [33]  [  10/1251]  eta: 0:55:37  lr: 0.000016  loss: 3.6618 (3.5221)  time: 2.6893  data: 1.4230  max mem: 19734
loss info: cls_loss=3.3858, ratio_loss=0.0049, pruning_loss=0.1315, mse_loss=0.5926
Epoch: [33]  [  20/1251]  eta: 0:36:53  lr: 0.000016  loss: 3.6542 (3.5459)  time: 0.9063  data: 0.0014  max mem: 19734
Epoch: [33]  [  30/1251]  eta: 0:30:05  lr: 0.000016  loss: 3.5413 (3.4827)  time: 0.8129  data: 0.0007  max mem: 19734
Epoch: [33]  [  40/1251]  eta: 0:26:41  lr: 0.000016  loss: 3.5058 (3.4219)  time: 0.8231  data: 0.0005  max mem: 19734
Epoch: [33]  [  50/1251]  eta: 0:24:29  lr: 0.000016  loss: 3.4945 (3.3985)  time: 0.8275  data: 0.0005  max mem: 19734
Epoch: [33]  [  60/1251]  eta: 0:22:57  lr: 0.000016  loss: 3.4677 (3.3630)  time: 0.8172  data: 0.0005  max mem: 19734
Epoch: [33]  [  70/1251]  eta: 0:21:46  lr: 0.000016  loss: 3.3943 (3.3692)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [33]  [  80/1251]  eta: 0:20:52  lr: 0.000016  loss: 3.6713 (3.3988)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [33]  [  90/1251]  eta: 0:20:07  lr: 0.000016  loss: 3.7346 (3.3969)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [33]  [ 100/1251]  eta: 0:19:31  lr: 0.000016  loss: 3.4977 (3.4035)  time: 0.8083  data: 0.0005  max mem: 19734
Epoch: [33]  [ 110/1251]  eta: 0:18:59  lr: 0.000016  loss: 3.4898 (3.4031)  time: 0.8111  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3552, ratio_loss=0.0049, pruning_loss=0.1316, mse_loss=0.5755
Epoch: [33]  [ 120/1251]  eta: 0:18:31  lr: 0.000016  loss: 3.4364 (3.4026)  time: 0.8072  data: 0.0005  max mem: 19734
Epoch: [33]  [ 130/1251]  eta: 0:18:06  lr: 0.000016  loss: 3.4364 (3.4012)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [33]  [ 140/1251]  eta: 0:17:43  lr: 0.000016  loss: 3.2538 (3.3893)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [33]  [ 150/1251]  eta: 0:17:22  lr: 0.000016  loss: 3.3232 (3.3921)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [33]  [ 160/1251]  eta: 0:17:03  lr: 0.000016  loss: 3.4126 (3.3945)  time: 0.8084  data: 0.0006  max mem: 19734
Epoch: [33]  [ 170/1251]  eta: 0:16:45  lr: 0.000016  loss: 3.6830 (3.3963)  time: 0.8084  data: 0.0006  max mem: 19734
Epoch: [33]  [ 180/1251]  eta: 0:16:30  lr: 0.000016  loss: 3.5795 (3.3904)  time: 0.8164  data: 0.0005  max mem: 19734
Epoch: [33]  [ 190/1251]  eta: 0:16:15  lr: 0.000016  loss: 3.4844 (3.3864)  time: 0.8279  data: 0.0005  max mem: 19734
Epoch: [33]  [ 200/1251]  eta: 0:16:01  lr: 0.000016  loss: 3.4844 (3.3889)  time: 0.8269  data: 0.0005  max mem: 19734
Epoch: [33]  [ 210/1251]  eta: 0:15:47  lr: 0.000016  loss: 3.5161 (3.3899)  time: 0.8164  data: 0.0008  max mem: 19734
loss info: cls_loss=3.3281, ratio_loss=0.0051, pruning_loss=0.1323, mse_loss=0.5631
Epoch: [33]  [ 220/1251]  eta: 0:15:33  lr: 0.000016  loss: 3.5161 (3.3775)  time: 0.8023  data: 0.0008  max mem: 19734
Epoch: [33]  [ 230/1251]  eta: 0:15:19  lr: 0.000016  loss: 3.5420 (3.3868)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [33]  [ 240/1251]  eta: 0:15:06  lr: 0.000016  loss: 3.5542 (3.3831)  time: 0.7991  data: 0.0005  max mem: 19734
Epoch: [33]  [ 250/1251]  eta: 0:14:53  lr: 0.000016  loss: 3.5281 (3.3835)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [33]  [ 260/1251]  eta: 0:14:41  lr: 0.000016  loss: 3.4684 (3.3820)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [33]  [ 270/1251]  eta: 0:14:29  lr: 0.000016  loss: 3.5069 (3.3820)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [33]  [ 280/1251]  eta: 0:14:17  lr: 0.000016  loss: 3.7320 (3.3951)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [33]  [ 290/1251]  eta: 0:14:06  lr: 0.000016  loss: 3.7320 (3.3999)  time: 0.8064  data: 0.0006  max mem: 19734
Epoch: [33]  [ 300/1251]  eta: 0:13:55  lr: 0.000016  loss: 3.5811 (3.4094)  time: 0.8064  data: 0.0007  max mem: 19734
Epoch: [33]  [ 310/1251]  eta: 0:13:44  lr: 0.000016  loss: 3.6690 (3.4105)  time: 0.8085  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4627, ratio_loss=0.0049, pruning_loss=0.1294, mse_loss=0.5989
Epoch: [33]  [ 320/1251]  eta: 0:13:34  lr: 0.000016  loss: 3.3846 (3.4089)  time: 0.8181  data: 0.0005  max mem: 19734
Epoch: [33]  [ 330/1251]  eta: 0:13:24  lr: 0.000016  loss: 3.3653 (3.4011)  time: 0.8271  data: 0.0005  max mem: 19734
Epoch: [33]  [ 340/1251]  eta: 0:13:14  lr: 0.000016  loss: 3.3503 (3.4012)  time: 0.8258  data: 0.0005  max mem: 19734
Epoch: [33]  [ 350/1251]  eta: 0:13:04  lr: 0.000016  loss: 3.2400 (3.3989)  time: 0.8312  data: 0.0005  max mem: 19734
Epoch: [33]  [ 360/1251]  eta: 0:12:54  lr: 0.000016  loss: 3.2374 (3.3902)  time: 0.8243  data: 0.0006  max mem: 19734
Epoch: [33]  [ 370/1251]  eta: 0:12:43  lr: 0.000016  loss: 3.3463 (3.3888)  time: 0.8022  data: 0.0006  max mem: 19734
Epoch: [33]  [ 380/1251]  eta: 0:12:33  lr: 0.000016  loss: 3.4210 (3.3902)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [33]  [ 390/1251]  eta: 0:12:24  lr: 0.000016  loss: 3.6370 (3.3894)  time: 0.8094  data: 0.0004  max mem: 19734
Epoch: [33]  [ 400/1251]  eta: 0:12:14  lr: 0.000016  loss: 3.5764 (3.3865)  time: 0.8107  data: 0.0005  max mem: 19734
Epoch: [33]  [ 410/1251]  eta: 0:12:04  lr: 0.000016  loss: 3.3036 (3.3877)  time: 0.8052  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2834, ratio_loss=0.0050, pruning_loss=0.1337, mse_loss=0.6063
Epoch: [33]  [ 420/1251]  eta: 0:11:54  lr: 0.000016  loss: 3.3036 (3.3837)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [33]  [ 430/1251]  eta: 0:11:44  lr: 0.000016  loss: 3.5039 (3.3840)  time: 0.8033  data: 0.0006  max mem: 19734
Epoch: [33]  [ 440/1251]  eta: 0:11:35  lr: 0.000016  loss: 3.1313 (3.3789)  time: 0.8014  data: 0.0006  max mem: 19734
Epoch: [33]  [ 450/1251]  eta: 0:11:25  lr: 0.000016  loss: 3.2411 (3.3804)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [33]  [ 460/1251]  eta: 0:11:16  lr: 0.000016  loss: 3.5794 (3.3826)  time: 0.8176  data: 0.0005  max mem: 19734
Epoch: [33]  [ 470/1251]  eta: 0:11:07  lr: 0.000016  loss: 3.5044 (3.3765)  time: 0.8205  data: 0.0005  max mem: 19734
Epoch: [33]  [ 480/1251]  eta: 0:10:58  lr: 0.000016  loss: 3.3905 (3.3780)  time: 0.8342  data: 0.0005  max mem: 19734
Epoch: [33]  [ 490/1251]  eta: 0:10:49  lr: 0.000016  loss: 3.3905 (3.3739)  time: 0.8285  data: 0.0005  max mem: 19734
Epoch: [33]  [ 500/1251]  eta: 0:10:40  lr: 0.000016  loss: 3.5626 (3.3748)  time: 0.8097  data: 0.0005  max mem: 19734
Epoch: [33]  [ 510/1251]  eta: 0:10:31  lr: 0.000016  loss: 3.6333 (3.3773)  time: 0.8116  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3168, ratio_loss=0.0048, pruning_loss=0.1328, mse_loss=0.6137
Epoch: [33]  [ 520/1251]  eta: 0:10:22  lr: 0.000016  loss: 3.3573 (3.3761)  time: 0.8054  data: 0.0006  max mem: 19734
Epoch: [33]  [ 530/1251]  eta: 0:10:13  lr: 0.000016  loss: 3.4151 (3.3758)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [33]  [ 540/1251]  eta: 0:10:04  lr: 0.000016  loss: 3.4151 (3.3711)  time: 0.8153  data: 0.0004  max mem: 19734
Epoch: [33]  [ 550/1251]  eta: 0:09:55  lr: 0.000016  loss: 3.2171 (3.3710)  time: 0.8159  data: 0.0005  max mem: 19734
Epoch: [33]  [ 560/1251]  eta: 0:09:46  lr: 0.000016  loss: 3.1182 (3.3653)  time: 0.8068  data: 0.0004  max mem: 19734
Epoch: [33]  [ 570/1251]  eta: 0:09:37  lr: 0.000016  loss: 3.1587 (3.3655)  time: 0.8092  data: 0.0004  max mem: 19734
Epoch: [33]  [ 580/1251]  eta: 0:09:28  lr: 0.000016  loss: 3.5274 (3.3650)  time: 0.8083  data: 0.0004  max mem: 19734
Epoch: [33]  [ 590/1251]  eta: 0:09:19  lr: 0.000016  loss: 3.4617 (3.3656)  time: 0.8068  data: 0.0004  max mem: 19734
Epoch: [33]  [ 600/1251]  eta: 0:09:10  lr: 0.000016  loss: 3.4098 (3.3665)  time: 0.8193  data: 0.0005  max mem: 19734
Epoch: [33]  [ 610/1251]  eta: 0:09:01  lr: 0.000016  loss: 3.5265 (3.3676)  time: 0.8254  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2952, ratio_loss=0.0047, pruning_loss=0.1333, mse_loss=0.6077
Epoch: [33]  [ 620/1251]  eta: 0:08:53  lr: 0.000016  loss: 3.5265 (3.3687)  time: 0.8286  data: 0.0005  max mem: 19734
Epoch: [33]  [ 630/1251]  eta: 0:08:44  lr: 0.000016  loss: 3.7434 (3.3754)  time: 0.8302  data: 0.0005  max mem: 19734
Epoch: [33]  [ 640/1251]  eta: 0:08:36  lr: 0.000016  loss: 3.4270 (3.3700)  time: 0.8191  data: 0.0005  max mem: 19734
Epoch: [33]  [ 650/1251]  eta: 0:08:27  lr: 0.000016  loss: 3.2856 (3.3701)  time: 0.8060  data: 0.0004  max mem: 19734
Epoch: [33]  [ 660/1251]  eta: 0:08:18  lr: 0.000016  loss: 3.3880 (3.3713)  time: 0.8010  data: 0.0004  max mem: 19734
Epoch: [33]  [ 670/1251]  eta: 0:08:09  lr: 0.000016  loss: 3.5298 (3.3758)  time: 0.8079  data: 0.0005  max mem: 19734
Epoch: [33]  [ 680/1251]  eta: 0:08:00  lr: 0.000016  loss: 3.5954 (3.3787)  time: 0.8062  data: 0.0005  max mem: 19734
Epoch: [33]  [ 690/1251]  eta: 0:07:52  lr: 0.000016  loss: 3.4150 (3.3764)  time: 0.8106  data: 0.0006  max mem: 19734
Epoch: [33]  [ 700/1251]  eta: 0:07:43  lr: 0.000016  loss: 3.5101 (3.3797)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [33]  [ 710/1251]  eta: 0:07:34  lr: 0.000016  loss: 3.7947 (3.3831)  time: 0.8013  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4723, ratio_loss=0.0052, pruning_loss=0.1293, mse_loss=0.5671
Epoch: [33]  [ 720/1251]  eta: 0:07:26  lr: 0.000016  loss: 3.6214 (3.3841)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [33]  [ 730/1251]  eta: 0:07:17  lr: 0.000016  loss: 3.4785 (3.3848)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [33]  [ 740/1251]  eta: 0:07:08  lr: 0.000016  loss: 3.2803 (3.3826)  time: 0.8076  data: 0.0006  max mem: 19734
Epoch: [33]  [ 750/1251]  eta: 0:07:00  lr: 0.000016  loss: 3.5852 (3.3845)  time: 0.8136  data: 0.0005  max mem: 19734
Epoch: [33]  [ 760/1251]  eta: 0:06:51  lr: 0.000016  loss: 3.6462 (3.3869)  time: 0.8223  data: 0.0005  max mem: 19734
Epoch: [33]  [ 770/1251]  eta: 0:06:43  lr: 0.000016  loss: 3.5017 (3.3893)  time: 0.8399  data: 0.0005  max mem: 19734
Epoch: [33]  [ 780/1251]  eta: 0:06:34  lr: 0.000016  loss: 3.5017 (3.3896)  time: 0.8278  data: 0.0005  max mem: 19734
Epoch: [33]  [ 790/1251]  eta: 0:06:26  lr: 0.000016  loss: 3.7176 (3.3907)  time: 0.8212  data: 0.0005  max mem: 19734
Epoch: [33]  [ 800/1251]  eta: 0:06:17  lr: 0.000016  loss: 3.5312 (3.3887)  time: 0.8212  data: 0.0005  max mem: 19734
Epoch: [33]  [ 810/1251]  eta: 0:06:09  lr: 0.000016  loss: 3.5312 (3.3877)  time: 0.7997  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4018, ratio_loss=0.0049, pruning_loss=0.1314, mse_loss=0.5986
Epoch: [33]  [ 820/1251]  eta: 0:06:00  lr: 0.000016  loss: 3.5633 (3.3892)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [33]  [ 830/1251]  eta: 0:05:52  lr: 0.000016  loss: 3.3562 (3.3881)  time: 0.8157  data: 0.0004  max mem: 19734
Epoch: [33]  [ 840/1251]  eta: 0:05:43  lr: 0.000016  loss: 3.5208 (3.3910)  time: 0.8121  data: 0.0004  max mem: 19734
Epoch: [33]  [ 850/1251]  eta: 0:05:35  lr: 0.000016  loss: 3.5377 (3.3901)  time: 0.8005  data: 0.0004  max mem: 19734
Epoch: [33]  [ 860/1251]  eta: 0:05:26  lr: 0.000016  loss: 3.4566 (3.3911)  time: 0.8011  data: 0.0004  max mem: 19734
Epoch: [33]  [ 870/1251]  eta: 0:05:18  lr: 0.000016  loss: 3.4488 (3.3898)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [33]  [ 880/1251]  eta: 0:05:09  lr: 0.000016  loss: 3.2469 (3.3875)  time: 0.8004  data: 0.0004  max mem: 19734
Epoch: [33]  [ 890/1251]  eta: 0:05:01  lr: 0.000016  loss: 3.5006 (3.3878)  time: 0.8025  data: 0.0004  max mem: 19734
Epoch: [33]  [ 900/1251]  eta: 0:04:52  lr: 0.000016  loss: 3.7233 (3.3908)  time: 0.8225  data: 0.0004  max mem: 19734
Epoch: [33]  [ 910/1251]  eta: 0:04:44  lr: 0.000016  loss: 3.7677 (3.3930)  time: 0.8222  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4408, ratio_loss=0.0050, pruning_loss=0.1300, mse_loss=0.6133
Epoch: [33]  [ 920/1251]  eta: 0:04:36  lr: 0.000016  loss: 3.6202 (3.3957)  time: 0.8285  data: 0.0005  max mem: 19734
Epoch: [33]  [ 930/1251]  eta: 0:04:27  lr: 0.000016  loss: 3.5199 (3.3946)  time: 0.8265  data: 0.0005  max mem: 19734
Epoch: [33]  [ 940/1251]  eta: 0:04:19  lr: 0.000016  loss: 3.4027 (3.3950)  time: 0.8210  data: 0.0005  max mem: 19734
Epoch: [33]  [ 950/1251]  eta: 0:04:11  lr: 0.000016  loss: 3.4703 (3.3962)  time: 0.8208  data: 0.0005  max mem: 19734
Epoch: [33]  [ 960/1251]  eta: 0:04:02  lr: 0.000016  loss: 3.5071 (3.3960)  time: 0.7979  data: 0.0005  max mem: 19734
Epoch: [33]  [ 970/1251]  eta: 0:03:54  lr: 0.000016  loss: 3.3776 (3.3966)  time: 0.7967  data: 0.0005  max mem: 19734
Epoch: [33]  [ 980/1251]  eta: 0:03:45  lr: 0.000016  loss: 3.6007 (3.3981)  time: 0.8049  data: 0.0004  max mem: 19734
Epoch: [33]  [ 990/1251]  eta: 0:03:37  lr: 0.000016  loss: 3.3785 (3.3942)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [33]  [1000/1251]  eta: 0:03:28  lr: 0.000016  loss: 3.3785 (3.3952)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [33]  [1010/1251]  eta: 0:03:20  lr: 0.000016  loss: 3.5362 (3.3924)  time: 0.8021  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3389, ratio_loss=0.0048, pruning_loss=0.1327, mse_loss=0.5960
Epoch: [33]  [1020/1251]  eta: 0:03:12  lr: 0.000016  loss: 3.5051 (3.3934)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [33]  [1030/1251]  eta: 0:03:03  lr: 0.000016  loss: 3.5713 (3.3940)  time: 0.8041  data: 0.0006  max mem: 19734
Epoch: [33]  [1040/1251]  eta: 0:02:55  lr: 0.000016  loss: 3.6915 (3.3965)  time: 0.8083  data: 0.0005  max mem: 19734
Epoch: [33]  [1050/1251]  eta: 0:02:47  lr: 0.000016  loss: 3.5386 (3.3973)  time: 0.8170  data: 0.0005  max mem: 19734
Epoch: [33]  [1060/1251]  eta: 0:02:38  lr: 0.000016  loss: 3.4907 (3.3986)  time: 0.8199  data: 0.0005  max mem: 19734
Epoch: [33]  [1070/1251]  eta: 0:02:30  lr: 0.000016  loss: 3.4013 (3.3960)  time: 0.8212  data: 0.0005  max mem: 19734
Epoch: [33]  [1080/1251]  eta: 0:02:22  lr: 0.000016  loss: 3.4525 (3.3966)  time: 0.8149  data: 0.0005  max mem: 19734
Epoch: [33]  [1090/1251]  eta: 0:02:13  lr: 0.000016  loss: 3.4913 (3.3960)  time: 0.8115  data: 0.0005  max mem: 19734
Epoch: [33]  [1100/1251]  eta: 0:02:05  lr: 0.000016  loss: 3.1587 (3.3943)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [33]  [1110/1251]  eta: 0:01:57  lr: 0.000016  loss: 3.5644 (3.3955)  time: 0.7986  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4055, ratio_loss=0.0050, pruning_loss=0.1301, mse_loss=0.6082
Epoch: [33]  [1120/1251]  eta: 0:01:48  lr: 0.000016  loss: 3.5904 (3.3966)  time: 0.8083  data: 0.0004  max mem: 19734
Epoch: [33]  [1130/1251]  eta: 0:01:40  lr: 0.000016  loss: 3.4386 (3.3969)  time: 0.8075  data: 0.0005  max mem: 19734
Epoch: [33]  [1140/1251]  eta: 0:01:32  lr: 0.000016  loss: 3.4081 (3.3965)  time: 0.7989  data: 0.0005  max mem: 19734
Epoch: [33]  [1150/1251]  eta: 0:01:23  lr: 0.000016  loss: 3.4867 (3.3966)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [33]  [1160/1251]  eta: 0:01:15  lr: 0.000016  loss: 3.5040 (3.3965)  time: 0.7996  data: 0.0004  max mem: 19734
Epoch: [33]  [1170/1251]  eta: 0:01:07  lr: 0.000016  loss: 3.4337 (3.3976)  time: 0.7984  data: 0.0004  max mem: 19734
Epoch: [33]  [1180/1251]  eta: 0:00:58  lr: 0.000016  loss: 3.5621 (3.3983)  time: 0.7993  data: 0.0004  max mem: 19734
Epoch: [33]  [1190/1251]  eta: 0:00:50  lr: 0.000016  loss: 3.5621 (3.3988)  time: 0.8072  data: 0.0009  max mem: 19734
Epoch: [33]  [1200/1251]  eta: 0:00:42  lr: 0.000016  loss: 3.5895 (3.4003)  time: 0.8104  data: 0.0008  max mem: 19734
Epoch: [33]  [1210/1251]  eta: 0:00:33  lr: 0.000016  loss: 3.5859 (3.4002)  time: 0.8135  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4537, ratio_loss=0.0049, pruning_loss=0.1300, mse_loss=0.5907
Epoch: [33]  [1220/1251]  eta: 0:00:25  lr: 0.000016  loss: 3.5980 (3.4029)  time: 0.8141  data: 0.0002  max mem: 19734
Epoch: [33]  [1230/1251]  eta: 0:00:17  lr: 0.000016  loss: 3.5980 (3.4024)  time: 0.8062  data: 0.0001  max mem: 19734
Epoch: [33]  [1240/1251]  eta: 0:00:09  lr: 0.000016  loss: 3.5248 (3.4028)  time: 0.7995  data: 0.0001  max mem: 19734
Epoch: [33]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.4716 (3.4024)  time: 0.7939  data: 0.0001  max mem: 19734
Epoch: [33] Total time: 0:17:16 (0.8281 s / it)
Averaged stats: lr: 0.000016  loss: 3.4716 (3.4082)
Test:  [  0/261]  eta: 1:31:26  loss: 0.7403 (0.7403)  acc1: 81.2500 (81.2500)  acc5: 95.8333 (95.8333)  time: 21.0217  data: 20.7635  max mem: 19734
Test:  [ 10/261]  eta: 0:14:43  loss: 0.7195 (0.7174)  acc1: 83.3333 (83.9015)  acc5: 96.8750 (96.6383)  time: 3.5219  data: 3.2358  max mem: 19734
Test:  [ 20/261]  eta: 0:08:01  loss: 0.9086 (0.8979)  acc1: 80.2083 (78.9683)  acc5: 94.7917 (95.0397)  time: 1.0483  data: 0.7461  max mem: 19734
Test:  [ 30/261]  eta: 0:05:38  loss: 0.8273 (0.8153)  acc1: 81.7708 (81.8884)  acc5: 95.3125 (95.4301)  time: 0.3344  data: 0.0097  max mem: 19734
Test:  [ 40/261]  eta: 0:05:00  loss: 0.5975 (0.7809)  acc1: 86.9792 (82.9776)  acc5: 96.8750 (95.6428)  time: 0.6870  data: 0.3096  max mem: 19734
Test:  [ 50/261]  eta: 0:04:14  loss: 0.8875 (0.8430)  acc1: 78.6458 (81.1989)  acc5: 95.3125 (95.2308)  time: 0.8000  data: 0.3164  max mem: 19734
Test:  [ 60/261]  eta: 0:03:29  loss: 0.9555 (0.8512)  acc1: 75.0000 (80.8402)  acc5: 94.2708 (95.2698)  time: 0.3918  data: 0.0166  max mem: 19734
Test:  [ 70/261]  eta: 0:03:15  loss: 0.8952 (0.8543)  acc1: 78.1250 (80.3110)  acc5: 96.3542 (95.4299)  time: 0.5607  data: 0.3438  max mem: 19734
Test:  [ 80/261]  eta: 0:02:57  loss: 0.8408 (0.8568)  acc1: 79.1667 (80.4077)  acc5: 96.8750 (95.5054)  time: 0.7900  data: 0.3797  max mem: 19734
Test:  [ 90/261]  eta: 0:02:36  loss: 0.7871 (0.8445)  acc1: 84.3750 (80.7979)  acc5: 96.3542 (95.5930)  time: 0.5208  data: 0.0500  max mem: 19734
Test:  [100/261]  eta: 0:02:25  loss: 0.7871 (0.8477)  acc1: 84.3750 (80.7653)  acc5: 95.3125 (95.6425)  time: 0.6108  data: 0.2458  max mem: 19734
Test:  [110/261]  eta: 0:02:06  loss: 0.8875 (0.8702)  acc1: 77.0833 (80.3163)  acc5: 94.7917 (95.3125)  time: 0.5051  data: 0.2398  max mem: 19734
Test:  [120/261]  eta: 0:01:52  loss: 1.1682 (0.9109)  acc1: 70.8333 (79.3216)  acc5: 89.5833 (94.7701)  time: 0.2437  data: 0.0070  max mem: 19734
Test:  [130/261]  eta: 0:01:40  loss: 1.4117 (0.9573)  acc1: 68.2292 (78.3954)  acc5: 86.9792 (94.1436)  time: 0.3422  data: 0.0160  max mem: 19734
Test:  [140/261]  eta: 0:01:33  loss: 1.3591 (0.9842)  acc1: 68.2292 (77.7076)  acc5: 89.5833 (93.8904)  time: 0.6108  data: 0.3655  max mem: 19734
Test:  [150/261]  eta: 0:01:21  loss: 1.2573 (0.9889)  acc1: 71.8750 (77.6663)  acc5: 91.6667 (93.7741)  time: 0.5495  data: 0.3584  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: 1.0079 (1.0071)  acc1: 78.1250 (77.3648)  acc5: 92.1875 (93.5009)  time: 0.1875  data: 0.0129  max mem: 19734
Test:  [170/261]  eta: 0:01:02  loss: 1.2305 (1.0380)  acc1: 64.0625 (76.5747)  acc5: 86.9792 (93.1408)  time: 0.3662  data: 0.2418  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 1.4112 (1.0551)  acc1: 64.0625 (76.1913)  acc5: 88.0208 (92.9731)  time: 0.3564  data: 0.2359  max mem: 19734
Test:  [190/261]  eta: 0:00:44  loss: 1.3784 (1.0680)  acc1: 68.2292 (75.9571)  acc5: 91.6667 (92.8120)  time: 0.1225  data: 0.0021  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 1.3870 (1.0823)  acc1: 72.3958 (75.6426)  acc5: 90.1042 (92.6021)  time: 0.1185  data: 0.0006  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.3946 (1.0965)  acc1: 69.7917 (75.3530)  acc5: 88.0208 (92.3973)  time: 0.1148  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.4354 (1.1162)  acc1: 67.7083 (74.8327)  acc5: 89.0625 (92.1969)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4599 (1.1261)  acc1: 66.1458 (74.6145)  acc5: 89.0625 (92.1018)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.2844 (1.1350)  acc1: 67.7083 (74.3517)  acc5: 90.6250 (92.0276)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0360 (1.1279)  acc1: 74.4792 (74.5103)  acc5: 92.7083 (92.1356)  time: 0.1150  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9409 (1.1278)  acc1: 76.5625 (74.5360)  acc5: 95.3125 (92.2100)  time: 0.1120  data: 0.0001  max mem: 19734
Test: Total time: 0:02:09 (0.4945 s / it)
* Acc@1 74.536 Acc@5 92.210 loss 1.128
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.62%
Epoch: [34]  [   0/1251]  eta: 6:26:10  lr: 0.000015  loss: 3.5813 (3.5813)  time: 18.5219  data: 8.6279  max mem: 19734
Epoch: [34]  [  10/1251]  eta: 0:54:15  lr: 0.000015  loss: 3.6307 (3.4764)  time: 2.6237  data: 0.8253  max mem: 19734
Epoch: [34]  [  20/1251]  eta: 0:35:57  lr: 0.000015  loss: 3.3898 (3.3458)  time: 0.9143  data: 0.0228  max mem: 19734
Epoch: [34]  [  30/1251]  eta: 0:29:20  lr: 0.000015  loss: 3.0013 (3.2068)  time: 0.7924  data: 0.0005  max mem: 19734
Epoch: [34]  [  40/1251]  eta: 0:25:55  lr: 0.000015  loss: 3.4630 (3.3060)  time: 0.7932  data: 0.0005  max mem: 19734
Epoch: [34]  [  50/1251]  eta: 0:23:48  lr: 0.000015  loss: 3.4721 (3.3235)  time: 0.7966  data: 0.0005  max mem: 19734
Epoch: [34]  [  60/1251]  eta: 0:22:18  lr: 0.000015  loss: 3.4711 (3.3406)  time: 0.7952  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3277, ratio_loss=0.0047, pruning_loss=0.1325, mse_loss=0.6012
Epoch: [34]  [  70/1251]  eta: 0:21:16  lr: 0.000015  loss: 3.4205 (3.2954)  time: 0.8050  data: 0.0007  max mem: 19734
Epoch: [34]  [  80/1251]  eta: 0:20:24  lr: 0.000015  loss: 3.0675 (3.2892)  time: 0.8056  data: 0.0007  max mem: 19734
Epoch: [34]  [  90/1251]  eta: 0:19:48  lr: 0.000015  loss: 3.2610 (3.2888)  time: 0.8200  data: 0.0005  max mem: 19734
Epoch: [34]  [ 100/1251]  eta: 0:19:14  lr: 0.000015  loss: 3.7390 (3.3189)  time: 0.8290  data: 0.0005  max mem: 19734
Epoch: [34]  [ 110/1251]  eta: 0:18:45  lr: 0.000015  loss: 3.8170 (3.3447)  time: 0.8201  data: 0.0005  max mem: 19734
Epoch: [34]  [ 120/1251]  eta: 0:18:19  lr: 0.000015  loss: 3.6038 (3.3513)  time: 0.8158  data: 0.0004  max mem: 19734
Epoch: [34]  [ 130/1251]  eta: 0:17:54  lr: 0.000015  loss: 3.5668 (3.3423)  time: 0.7997  data: 0.0004  max mem: 19734
Epoch: [34]  [ 140/1251]  eta: 0:17:32  lr: 0.000015  loss: 3.5124 (3.3466)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [34]  [ 150/1251]  eta: 0:17:12  lr: 0.000015  loss: 3.4511 (3.3364)  time: 0.8061  data: 0.0005  max mem: 19734
Epoch: [34]  [ 160/1251]  eta: 0:16:53  lr: 0.000015  loss: 3.2606 (3.3302)  time: 0.8027  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3344, ratio_loss=0.0047, pruning_loss=0.1337, mse_loss=0.6015
Epoch: [34]  [ 170/1251]  eta: 0:16:36  lr: 0.000015  loss: 3.2606 (3.3290)  time: 0.7953  data: 0.0004  max mem: 19734
Epoch: [34]  [ 180/1251]  eta: 0:16:19  lr: 0.000015  loss: 3.4235 (3.3350)  time: 0.7957  data: 0.0004  max mem: 19734
Epoch: [34]  [ 190/1251]  eta: 0:16:04  lr: 0.000015  loss: 3.4235 (3.3309)  time: 0.8000  data: 0.0004  max mem: 19734
Epoch: [34]  [ 200/1251]  eta: 0:15:49  lr: 0.000015  loss: 3.6680 (3.3515)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [34]  [ 210/1251]  eta: 0:15:35  lr: 0.000015  loss: 3.6896 (3.3640)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [34]  [ 220/1251]  eta: 0:15:23  lr: 0.000015  loss: 3.4398 (3.3557)  time: 0.8175  data: 0.0006  max mem: 19734
Epoch: [34]  [ 230/1251]  eta: 0:15:10  lr: 0.000015  loss: 3.4340 (3.3586)  time: 0.8136  data: 0.0006  max mem: 19734
Epoch: [34]  [ 240/1251]  eta: 0:14:59  lr: 0.000015  loss: 3.4978 (3.3574)  time: 0.8269  data: 0.0005  max mem: 19734
Epoch: [34]  [ 250/1251]  eta: 0:14:48  lr: 0.000015  loss: 3.2978 (3.3502)  time: 0.8360  data: 0.0005  max mem: 19734
Epoch: [34]  [ 260/1251]  eta: 0:14:36  lr: 0.000015  loss: 3.2978 (3.3511)  time: 0.8179  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3687, ratio_loss=0.0049, pruning_loss=0.1313, mse_loss=0.5965
Epoch: [34]  [ 270/1251]  eta: 0:14:24  lr: 0.000015  loss: 3.4829 (3.3624)  time: 0.8101  data: 0.0005  max mem: 19734
Epoch: [34]  [ 280/1251]  eta: 0:14:13  lr: 0.000015  loss: 3.4831 (3.3565)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [34]  [ 290/1251]  eta: 0:14:01  lr: 0.000015  loss: 3.4560 (3.3655)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [34]  [ 300/1251]  eta: 0:13:51  lr: 0.000015  loss: 3.4560 (3.3638)  time: 0.8094  data: 0.0005  max mem: 19734
Epoch: [34]  [ 310/1251]  eta: 0:13:40  lr: 0.000015  loss: 3.4217 (3.3642)  time: 0.8074  data: 0.0005  max mem: 19734
Epoch: [34]  [ 320/1251]  eta: 0:13:29  lr: 0.000015  loss: 3.3444 (3.3607)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [34]  [ 330/1251]  eta: 0:13:19  lr: 0.000015  loss: 3.5664 (3.3689)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [34]  [ 340/1251]  eta: 0:13:08  lr: 0.000015  loss: 3.6293 (3.3746)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [34]  [ 350/1251]  eta: 0:12:58  lr: 0.000015  loss: 3.5643 (3.3693)  time: 0.8034  data: 0.0006  max mem: 19734
Epoch: [34]  [ 360/1251]  eta: 0:12:48  lr: 0.000015  loss: 3.1076 (3.3612)  time: 0.8011  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3729, ratio_loss=0.0049, pruning_loss=0.1309, mse_loss=0.5976
Epoch: [34]  [ 370/1251]  eta: 0:12:38  lr: 0.000015  loss: 3.1783 (3.3645)  time: 0.8093  data: 0.0005  max mem: 19734
Epoch: [34]  [ 380/1251]  eta: 0:12:29  lr: 0.000015  loss: 3.5343 (3.3670)  time: 0.8280  data: 0.0005  max mem: 19734
Epoch: [34]  [ 390/1251]  eta: 0:12:20  lr: 0.000015  loss: 3.2787 (3.3618)  time: 0.8468  data: 0.0005  max mem: 19734
Epoch: [34]  [ 400/1251]  eta: 0:12:11  lr: 0.000015  loss: 3.1371 (3.3588)  time: 0.8398  data: 0.0005  max mem: 19734
Epoch: [34]  [ 410/1251]  eta: 0:12:01  lr: 0.000015  loss: 3.0663 (3.3497)  time: 0.8132  data: 0.0005  max mem: 19734
Epoch: [34]  [ 420/1251]  eta: 0:11:51  lr: 0.000015  loss: 3.0036 (3.3508)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [34]  [ 430/1251]  eta: 0:11:42  lr: 0.000015  loss: 3.3667 (3.3484)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [34]  [ 440/1251]  eta: 0:11:32  lr: 0.000015  loss: 3.3667 (3.3502)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [34]  [ 450/1251]  eta: 0:11:23  lr: 0.000015  loss: 3.4156 (3.3519)  time: 0.8122  data: 0.0005  max mem: 19734
Epoch: [34]  [ 460/1251]  eta: 0:11:14  lr: 0.000015  loss: 3.4156 (3.3491)  time: 0.8110  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2767, ratio_loss=0.0051, pruning_loss=0.1320, mse_loss=0.5949
Epoch: [34]  [ 470/1251]  eta: 0:11:05  lr: 0.000015  loss: 3.5047 (3.3508)  time: 0.8067  data: 0.0005  max mem: 19734
Epoch: [34]  [ 480/1251]  eta: 0:10:55  lr: 0.000015  loss: 3.5398 (3.3538)  time: 0.8061  data: 0.0005  max mem: 19734
Epoch: [34]  [ 490/1251]  eta: 0:10:46  lr: 0.000015  loss: 3.4916 (3.3516)  time: 0.7991  data: 0.0004  max mem: 19734
Epoch: [34]  [ 500/1251]  eta: 0:10:37  lr: 0.000015  loss: 3.5256 (3.3578)  time: 0.7977  data: 0.0004  max mem: 19734
Epoch: [34]  [ 510/1251]  eta: 0:10:28  lr: 0.000015  loss: 3.6120 (3.3608)  time: 0.8028  data: 0.0004  max mem: 19734
Epoch: [34]  [ 520/1251]  eta: 0:10:18  lr: 0.000015  loss: 3.5436 (3.3629)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [34]  [ 530/1251]  eta: 0:10:10  lr: 0.000015  loss: 3.5734 (3.3673)  time: 0.8115  data: 0.0006  max mem: 19734
Epoch: [34]  [ 540/1251]  eta: 0:10:01  lr: 0.000015  loss: 3.6163 (3.3677)  time: 0.8325  data: 0.0005  max mem: 19734
Epoch: [34]  [ 550/1251]  eta: 0:09:52  lr: 0.000015  loss: 3.5717 (3.3724)  time: 0.8284  data: 0.0005  max mem: 19734
Epoch: [34]  [ 560/1251]  eta: 0:09:43  lr: 0.000015  loss: 3.5470 (3.3752)  time: 0.8050  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4722, ratio_loss=0.0050, pruning_loss=0.1272, mse_loss=0.5834
Epoch: [34]  [ 570/1251]  eta: 0:09:34  lr: 0.000015  loss: 3.5415 (3.3743)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [34]  [ 580/1251]  eta: 0:09:25  lr: 0.000015  loss: 3.6190 (3.3766)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [34]  [ 590/1251]  eta: 0:09:17  lr: 0.000015  loss: 3.4095 (3.3766)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [34]  [ 600/1251]  eta: 0:09:08  lr: 0.000015  loss: 3.2231 (3.3695)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [34]  [ 610/1251]  eta: 0:08:59  lr: 0.000015  loss: 3.3636 (3.3679)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [34]  [ 620/1251]  eta: 0:08:50  lr: 0.000015  loss: 3.4203 (3.3671)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [34]  [ 630/1251]  eta: 0:08:41  lr: 0.000015  loss: 3.5479 (3.3691)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [34]  [ 640/1251]  eta: 0:08:33  lr: 0.000015  loss: 3.6402 (3.3717)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [34]  [ 650/1251]  eta: 0:08:24  lr: 0.000015  loss: 3.6613 (3.3767)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [34]  [ 660/1251]  eta: 0:08:15  lr: 0.000015  loss: 3.6429 (3.3773)  time: 0.8097  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3824, ratio_loss=0.0048, pruning_loss=0.1299, mse_loss=0.6187
Epoch: [34]  [ 670/1251]  eta: 0:08:07  lr: 0.000015  loss: 3.5611 (3.3785)  time: 0.8178  data: 0.0006  max mem: 19734
Epoch: [34]  [ 680/1251]  eta: 0:07:58  lr: 0.000015  loss: 3.5338 (3.3815)  time: 0.8360  data: 0.0004  max mem: 19734
Epoch: [34]  [ 690/1251]  eta: 0:07:50  lr: 0.000015  loss: 3.5338 (3.3827)  time: 0.8338  data: 0.0004  max mem: 19734
Epoch: [34]  [ 700/1251]  eta: 0:07:41  lr: 0.000015  loss: 3.5140 (3.3848)  time: 0.8150  data: 0.0004  max mem: 19734
Epoch: [34]  [ 710/1251]  eta: 0:07:33  lr: 0.000015  loss: 3.6397 (3.3891)  time: 0.8122  data: 0.0004  max mem: 19734
Epoch: [34]  [ 720/1251]  eta: 0:07:24  lr: 0.000015  loss: 3.7136 (3.3919)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [34]  [ 730/1251]  eta: 0:07:15  lr: 0.000015  loss: 3.6237 (3.3925)  time: 0.8043  data: 0.0006  max mem: 19734
Epoch: [34]  [ 740/1251]  eta: 0:07:07  lr: 0.000015  loss: 3.5715 (3.3942)  time: 0.8136  data: 0.0005  max mem: 19734
Epoch: [34]  [ 750/1251]  eta: 0:06:58  lr: 0.000015  loss: 3.6264 (3.3962)  time: 0.8165  data: 0.0004  max mem: 19734
Epoch: [34]  [ 760/1251]  eta: 0:06:50  lr: 0.000015  loss: 3.7123 (3.3999)  time: 0.8058  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5198, ratio_loss=0.0049, pruning_loss=0.1269, mse_loss=0.5948
Epoch: [34]  [ 770/1251]  eta: 0:06:41  lr: 0.000015  loss: 3.4281 (3.3954)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [34]  [ 780/1251]  eta: 0:06:33  lr: 0.000015  loss: 3.3349 (3.3965)  time: 0.8030  data: 0.0004  max mem: 19734
Epoch: [34]  [ 790/1251]  eta: 0:06:24  lr: 0.000015  loss: 3.6271 (3.3959)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [34]  [ 800/1251]  eta: 0:06:16  lr: 0.000015  loss: 3.4601 (3.3972)  time: 0.8127  data: 0.0006  max mem: 19734
Epoch: [34]  [ 810/1251]  eta: 0:06:07  lr: 0.000015  loss: 3.4601 (3.3987)  time: 0.8114  data: 0.0005  max mem: 19734
Epoch: [34]  [ 820/1251]  eta: 0:05:59  lr: 0.000015  loss: 3.4637 (3.3971)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [34]  [ 830/1251]  eta: 0:05:51  lr: 0.000015  loss: 3.4637 (3.3977)  time: 0.8279  data: 0.0005  max mem: 19734
Epoch: [34]  [ 840/1251]  eta: 0:05:42  lr: 0.000015  loss: 3.5374 (3.3986)  time: 0.8349  data: 0.0005  max mem: 19734
Epoch: [34]  [ 850/1251]  eta: 0:05:34  lr: 0.000015  loss: 3.5959 (3.3997)  time: 0.8194  data: 0.0006  max mem: 19734
Epoch: [34]  [ 860/1251]  eta: 0:05:25  lr: 0.000015  loss: 3.5677 (3.3987)  time: 0.8041  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3763, ratio_loss=0.0046, pruning_loss=0.1296, mse_loss=0.5921
Epoch: [34]  [ 870/1251]  eta: 0:05:17  lr: 0.000015  loss: 3.5655 (3.4001)  time: 0.8046  data: 0.0005  max mem: 19734
Epoch: [34]  [ 880/1251]  eta: 0:05:08  lr: 0.000015  loss: 3.5008 (3.4010)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [34]  [ 890/1251]  eta: 0:05:00  lr: 0.000015  loss: 3.6295 (3.4034)  time: 0.8039  data: 0.0005  max mem: 19734
Epoch: [34]  [ 900/1251]  eta: 0:04:51  lr: 0.000015  loss: 3.6868 (3.4050)  time: 0.7994  data: 0.0004  max mem: 19734
Epoch: [34]  [ 910/1251]  eta: 0:04:43  lr: 0.000015  loss: 3.4497 (3.4055)  time: 0.7988  data: 0.0004  max mem: 19734
Epoch: [34]  [ 920/1251]  eta: 0:04:35  lr: 0.000015  loss: 3.4892 (3.4070)  time: 0.7980  data: 0.0006  max mem: 19734
Epoch: [34]  [ 930/1251]  eta: 0:04:26  lr: 0.000015  loss: 3.5464 (3.4070)  time: 0.7978  data: 0.0007  max mem: 19734
Epoch: [34]  [ 940/1251]  eta: 0:04:18  lr: 0.000015  loss: 3.4585 (3.4064)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [34]  [ 950/1251]  eta: 0:04:09  lr: 0.000015  loss: 3.3637 (3.4073)  time: 0.8157  data: 0.0004  max mem: 19734
Epoch: [34]  [ 960/1251]  eta: 0:04:01  lr: 0.000015  loss: 3.3169 (3.4041)  time: 0.8166  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4161, ratio_loss=0.0045, pruning_loss=0.1294, mse_loss=0.5904
Epoch: [34]  [ 970/1251]  eta: 0:03:53  lr: 0.000015  loss: 3.0417 (3.4027)  time: 0.8227  data: 0.0004  max mem: 19734
Epoch: [34]  [ 980/1251]  eta: 0:03:44  lr: 0.000015  loss: 3.3923 (3.4037)  time: 0.8231  data: 0.0004  max mem: 19734
Epoch: [34]  [ 990/1251]  eta: 0:03:36  lr: 0.000015  loss: 3.4921 (3.4053)  time: 0.8189  data: 0.0005  max mem: 19734
Epoch: [34]  [1000/1251]  eta: 0:03:28  lr: 0.000015  loss: 3.3689 (3.4031)  time: 0.8203  data: 0.0005  max mem: 19734
Epoch: [34]  [1010/1251]  eta: 0:03:19  lr: 0.000015  loss: 3.1879 (3.4028)  time: 0.8089  data: 0.0006  max mem: 19734
Epoch: [34]  [1020/1251]  eta: 0:03:11  lr: 0.000015  loss: 3.4273 (3.4048)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [34]  [1030/1251]  eta: 0:03:03  lr: 0.000015  loss: 3.4433 (3.4040)  time: 0.8102  data: 0.0004  max mem: 19734
Epoch: [34]  [1040/1251]  eta: 0:02:54  lr: 0.000015  loss: 3.4066 (3.4032)  time: 0.8109  data: 0.0005  max mem: 19734
Epoch: [34]  [1050/1251]  eta: 0:02:46  lr: 0.000015  loss: 3.4066 (3.4014)  time: 0.8029  data: 0.0005  max mem: 19734
Epoch: [34]  [1060/1251]  eta: 0:02:38  lr: 0.000015  loss: 3.4449 (3.4028)  time: 0.8052  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4005, ratio_loss=0.0049, pruning_loss=0.1296, mse_loss=0.6058
Epoch: [34]  [1070/1251]  eta: 0:02:29  lr: 0.000015  loss: 3.6030 (3.4042)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [34]  [1080/1251]  eta: 0:02:21  lr: 0.000015  loss: 3.5536 (3.4036)  time: 0.8030  data: 0.0004  max mem: 19734
Epoch: [34]  [1090/1251]  eta: 0:02:13  lr: 0.000015  loss: 3.0554 (3.4014)  time: 0.8038  data: 0.0004  max mem: 19734
Epoch: [34]  [1100/1251]  eta: 0:02:05  lr: 0.000015  loss: 3.4094 (3.4027)  time: 0.8163  data: 0.0004  max mem: 19734
Epoch: [34]  [1110/1251]  eta: 0:01:56  lr: 0.000015  loss: 3.4998 (3.4016)  time: 0.8262  data: 0.0005  max mem: 19734
Epoch: [34]  [1120/1251]  eta: 0:01:48  lr: 0.000015  loss: 3.4938 (3.4034)  time: 0.8313  data: 0.0005  max mem: 19734
Epoch: [34]  [1130/1251]  eta: 0:01:40  lr: 0.000015  loss: 3.4396 (3.4027)  time: 0.8319  data: 0.0004  max mem: 19734
Epoch: [34]  [1140/1251]  eta: 0:01:31  lr: 0.000015  loss: 3.2622 (3.4027)  time: 0.8126  data: 0.0004  max mem: 19734
Epoch: [34]  [1150/1251]  eta: 0:01:23  lr: 0.000015  loss: 3.5026 (3.4037)  time: 0.8002  data: 0.0004  max mem: 19734
Epoch: [34]  [1160/1251]  eta: 0:01:15  lr: 0.000015  loss: 3.6310 (3.4066)  time: 0.8027  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4115, ratio_loss=0.0049, pruning_loss=0.1293, mse_loss=0.6130
Epoch: [34]  [1170/1251]  eta: 0:01:06  lr: 0.000015  loss: 3.5478 (3.4058)  time: 0.8041  data: 0.0004  max mem: 19734
Epoch: [34]  [1180/1251]  eta: 0:00:58  lr: 0.000015  loss: 3.4562 (3.4068)  time: 0.8089  data: 0.0004  max mem: 19734
Epoch: [34]  [1190/1251]  eta: 0:00:50  lr: 0.000015  loss: 3.6127 (3.4079)  time: 0.8093  data: 0.0011  max mem: 19734
Epoch: [34]  [1200/1251]  eta: 0:00:42  lr: 0.000015  loss: 3.4865 (3.4044)  time: 0.7980  data: 0.0009  max mem: 19734
Epoch: [34]  [1210/1251]  eta: 0:00:33  lr: 0.000015  loss: 3.3586 (3.4062)  time: 0.7941  data: 0.0001  max mem: 19734
Epoch: [34]  [1220/1251]  eta: 0:00:25  lr: 0.000015  loss: 3.5852 (3.4068)  time: 0.7948  data: 0.0001  max mem: 19734
Epoch: [34]  [1230/1251]  eta: 0:00:17  lr: 0.000015  loss: 3.5453 (3.4060)  time: 0.7927  data: 0.0001  max mem: 19734
Epoch: [34]  [1240/1251]  eta: 0:00:09  lr: 0.000015  loss: 3.3952 (3.4035)  time: 0.8005  data: 0.0001  max mem: 19734
Epoch: [34]  [1250/1251]  eta: 0:00:00  lr: 0.000015  loss: 3.4689 (3.4037)  time: 0.8015  data: 0.0001  max mem: 19734
Epoch: [34] Total time: 0:17:13 (0.8261 s / it)
Averaged stats: lr: 0.000015  loss: 3.4689 (3.4138)
Test:  [  0/261]  eta: 0:48:28  loss: 0.7569 (0.7569)  acc1: 81.7708 (81.7708)  acc5: 96.8750 (96.8750)  time: 11.1438  data: 11.0183  max mem: 19734
Test:  [ 10/261]  eta: 0:12:55  loss: 0.7569 (0.7308)  acc1: 83.3333 (84.0436)  acc5: 96.8750 (96.7330)  time: 3.0898  data: 2.8238  max mem: 19734
Test:  [ 20/261]  eta: 0:07:28  loss: 0.9160 (0.9054)  acc1: 78.6458 (79.5387)  acc5: 94.7917 (94.9901)  time: 1.3978  data: 1.0114  max mem: 19734
Test:  [ 30/261]  eta: 0:05:18  loss: 0.8654 (0.8223)  acc1: 81.7708 (82.2413)  acc5: 94.2708 (95.3629)  time: 0.4369  data: 0.0202  max mem: 19734
Test:  [ 40/261]  eta: 0:04:28  loss: 0.5554 (0.7845)  acc1: 88.5417 (83.1682)  acc5: 96.8750 (95.6047)  time: 0.5375  data: 0.1926  max mem: 19734
Test:  [ 50/261]  eta: 0:03:53  loss: 0.9186 (0.8501)  acc1: 78.1250 (81.1479)  acc5: 95.3125 (95.1593)  time: 0.6900  data: 0.1906  max mem: 19734
Test:  [ 60/261]  eta: 0:03:16  loss: 0.9799 (0.8619)  acc1: 75.0000 (80.6609)  acc5: 94.2708 (95.1674)  time: 0.4956  data: 0.0169  max mem: 19734
Test:  [ 70/261]  eta: 0:03:00  loss: 0.9049 (0.8614)  acc1: 78.1250 (80.2670)  acc5: 95.8333 (95.3712)  time: 0.5285  data: 0.2467  max mem: 19734
Test:  [ 80/261]  eta: 0:02:38  loss: 0.8484 (0.8631)  acc1: 80.2083 (80.3755)  acc5: 96.8750 (95.4733)  time: 0.5549  data: 0.3020  max mem: 19734
Test:  [ 90/261]  eta: 0:02:15  loss: 0.8458 (0.8493)  acc1: 82.2917 (80.7349)  acc5: 95.8333 (95.5414)  time: 0.2602  data: 0.0700  max mem: 19734
Test:  [100/261]  eta: 0:02:00  loss: 0.8086 (0.8507)  acc1: 83.8542 (80.7395)  acc5: 94.7917 (95.5910)  time: 0.2324  data: 0.0153  max mem: 19734
Test:  [110/261]  eta: 0:01:49  loss: 0.8723 (0.8738)  acc1: 78.1250 (80.1849)  acc5: 94.7917 (95.2984)  time: 0.4205  data: 0.1862  max mem: 19734
Test:  [120/261]  eta: 0:01:39  loss: 1.2310 (0.9148)  acc1: 71.3542 (79.1968)  acc5: 89.5833 (94.7185)  time: 0.5130  data: 0.1846  max mem: 19734
Test:  [130/261]  eta: 0:01:31  loss: 1.3587 (0.9594)  acc1: 67.1875 (78.2323)  acc5: 86.4583 (94.1237)  time: 0.5287  data: 0.1618  max mem: 19734
Test:  [140/261]  eta: 0:01:22  loss: 1.3071 (0.9867)  acc1: 67.7083 (77.5598)  acc5: 89.0625 (93.8571)  time: 0.5378  data: 0.2835  max mem: 19734
Test:  [150/261]  eta: 0:01:13  loss: 1.2386 (0.9903)  acc1: 71.8750 (77.5800)  acc5: 91.1458 (93.6948)  time: 0.4173  data: 0.1687  max mem: 19734
Test:  [160/261]  eta: 0:01:04  loss: 0.9937 (1.0101)  acc1: 78.6458 (77.2289)  acc5: 91.1458 (93.3974)  time: 0.2937  data: 0.0502  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.2671 (1.0397)  acc1: 65.6250 (76.4833)  acc5: 86.9792 (93.0799)  time: 0.2434  data: 0.0146  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: 1.4129 (1.0569)  acc1: 65.1042 (76.1079)  acc5: 89.5833 (92.9357)  time: 0.1978  data: 0.0285  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: 1.3965 (1.0697)  acc1: 68.2292 (75.8590)  acc5: 91.1458 (92.7874)  time: 0.1652  data: 0.0289  max mem: 19734
Test:  [200/261]  eta: 0:00:33  loss: 1.3483 (1.0850)  acc1: 72.3958 (75.5493)  acc5: 89.0625 (92.5399)  time: 0.1465  data: 0.0066  max mem: 19734
Test:  [210/261]  eta: 0:00:26  loss: 1.3440 (1.0979)  acc1: 70.8333 (75.3036)  acc5: 88.5417 (92.3479)  time: 0.1833  data: 0.0519  max mem: 19734
Test:  [220/261]  eta: 0:00:20  loss: 1.3684 (1.1168)  acc1: 67.1875 (74.8327)  acc5: 88.5417 (92.1569)  time: 0.1788  data: 0.0630  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.4251 (1.1262)  acc1: 66.6667 (74.6257)  acc5: 90.6250 (92.0612)  time: 0.1275  data: 0.0126  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.2899 (1.1352)  acc1: 69.7917 (74.4208)  acc5: 90.6250 (91.9800)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0556 (1.1277)  acc1: 75.0000 (74.5954)  acc5: 94.2708 (92.0900)  time: 0.1152  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9686 (1.1278)  acc1: 76.0417 (74.6120)  acc5: 94.7917 (92.1500)  time: 0.1122  data: 0.0001  max mem: 19734
Test: Total time: 0:01:57 (0.4510 s / it)
* Acc@1 74.612 Acc@5 92.150 loss 1.128
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.62%
Epoch: [35]  [   0/1251]  eta: 6:02:05  lr: 0.000015  loss: 3.7489 (3.7489)  time: 17.3669  data: 8.6394  max mem: 19734
Epoch: [35]  [  10/1251]  eta: 0:53:13  lr: 0.000015  loss: 3.7066 (3.4077)  time: 2.5733  data: 0.8133  max mem: 19734
loss info: cls_loss=3.3220, ratio_loss=0.0048, pruning_loss=0.1317, mse_loss=0.6182
Epoch: [35]  [  20/1251]  eta: 0:35:37  lr: 0.000015  loss: 3.6496 (3.2976)  time: 0.9553  data: 0.0157  max mem: 19734
Epoch: [35]  [  30/1251]  eta: 0:29:10  lr: 0.000015  loss: 3.4731 (3.3665)  time: 0.8073  data: 0.0007  max mem: 19734
Epoch: [35]  [  40/1251]  eta: 0:25:49  lr: 0.000015  loss: 3.5658 (3.4202)  time: 0.7995  data: 0.0007  max mem: 19734
Epoch: [35]  [  50/1251]  eta: 0:23:43  lr: 0.000015  loss: 3.4946 (3.3959)  time: 0.7990  data: 0.0006  max mem: 19734
Epoch: [35]  [  60/1251]  eta: 0:22:16  lr: 0.000015  loss: 3.4894 (3.4056)  time: 0.7986  data: 0.0006  max mem: 19734
Epoch: [35]  [  70/1251]  eta: 0:21:12  lr: 0.000015  loss: 3.6202 (3.4255)  time: 0.8044  data: 0.0006  max mem: 19734
Epoch: [35]  [  80/1251]  eta: 0:20:21  lr: 0.000015  loss: 3.4723 (3.4056)  time: 0.8034  data: 0.0007  max mem: 19734
Epoch: [35]  [  90/1251]  eta: 0:19:40  lr: 0.000015  loss: 3.3608 (3.3970)  time: 0.8023  data: 0.0008  max mem: 19734
Epoch: [35]  [ 100/1251]  eta: 0:19:06  lr: 0.000015  loss: 3.4824 (3.4076)  time: 0.8054  data: 0.0007  max mem: 19734
Epoch: [35]  [ 110/1251]  eta: 0:18:36  lr: 0.000015  loss: 3.4824 (3.3893)  time: 0.8045  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4017, ratio_loss=0.0045, pruning_loss=0.1302, mse_loss=0.5730
Epoch: [35]  [ 120/1251]  eta: 0:18:10  lr: 0.000015  loss: 3.5193 (3.4057)  time: 0.8032  data: 0.0007  max mem: 19734
Epoch: [35]  [ 130/1251]  eta: 0:17:48  lr: 0.000015  loss: 3.5534 (3.4068)  time: 0.8093  data: 0.0007  max mem: 19734
Epoch: [35]  [ 140/1251]  eta: 0:17:28  lr: 0.000015  loss: 3.5534 (3.4094)  time: 0.8184  data: 0.0006  max mem: 19734
Epoch: [35]  [ 150/1251]  eta: 0:17:12  lr: 0.000015  loss: 3.5906 (3.4202)  time: 0.8388  data: 0.0006  max mem: 19734
Epoch: [35]  [ 160/1251]  eta: 0:16:54  lr: 0.000015  loss: 3.4735 (3.4072)  time: 0.8307  data: 0.0006  max mem: 19734
Epoch: [35]  [ 170/1251]  eta: 0:16:38  lr: 0.000015  loss: 3.4735 (3.4053)  time: 0.8148  data: 0.0007  max mem: 19734
Epoch: [35]  [ 180/1251]  eta: 0:16:21  lr: 0.000015  loss: 3.5293 (3.4017)  time: 0.8130  data: 0.0007  max mem: 19734
Epoch: [35]  [ 190/1251]  eta: 0:16:06  lr: 0.000015  loss: 3.5707 (3.4107)  time: 0.8014  data: 0.0007  max mem: 19734
Epoch: [35]  [ 200/1251]  eta: 0:15:51  lr: 0.000015  loss: 3.4612 (3.4056)  time: 0.8038  data: 0.0006  max mem: 19734
Epoch: [35]  [ 210/1251]  eta: 0:15:37  lr: 0.000015  loss: 3.4156 (3.4104)  time: 0.8048  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4144, ratio_loss=0.0047, pruning_loss=0.1288, mse_loss=0.6015
Epoch: [35]  [ 220/1251]  eta: 0:15:24  lr: 0.000015  loss: 3.6331 (3.4193)  time: 0.8106  data: 0.0007  max mem: 19734
Epoch: [35]  [ 230/1251]  eta: 0:15:11  lr: 0.000015  loss: 3.3099 (3.4119)  time: 0.8102  data: 0.0007  max mem: 19734
Epoch: [35]  [ 240/1251]  eta: 0:14:58  lr: 0.000015  loss: 3.2654 (3.4067)  time: 0.8044  data: 0.0006  max mem: 19734
Epoch: [35]  [ 250/1251]  eta: 0:14:46  lr: 0.000015  loss: 3.4038 (3.4119)  time: 0.8039  data: 0.0006  max mem: 19734
Epoch: [35]  [ 260/1251]  eta: 0:14:34  lr: 0.000015  loss: 3.4828 (3.4107)  time: 0.8046  data: 0.0006  max mem: 19734
Epoch: [35]  [ 270/1251]  eta: 0:14:23  lr: 0.000015  loss: 3.6193 (3.4224)  time: 0.8051  data: 0.0006  max mem: 19734
Epoch: [35]  [ 280/1251]  eta: 0:14:12  lr: 0.000015  loss: 3.4374 (3.4152)  time: 0.8108  data: 0.0007  max mem: 19734
Epoch: [35]  [ 290/1251]  eta: 0:14:01  lr: 0.000015  loss: 3.4208 (3.4174)  time: 0.8163  data: 0.0007  max mem: 19734
Epoch: [35]  [ 300/1251]  eta: 0:13:51  lr: 0.000015  loss: 3.5417 (3.4141)  time: 0.8320  data: 0.0006  max mem: 19734
Epoch: [35]  [ 310/1251]  eta: 0:13:41  lr: 0.000015  loss: 3.4046 (3.4104)  time: 0.8354  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3876, ratio_loss=0.0047, pruning_loss=0.1298, mse_loss=0.5978
Epoch: [35]  [ 320/1251]  eta: 0:13:30  lr: 0.000015  loss: 3.4642 (3.4111)  time: 0.8118  data: 0.0006  max mem: 19734
Epoch: [35]  [ 330/1251]  eta: 0:13:19  lr: 0.000015  loss: 3.5738 (3.4203)  time: 0.8027  data: 0.0006  max mem: 19734
Epoch: [35]  [ 340/1251]  eta: 0:13:09  lr: 0.000015  loss: 3.6116 (3.4269)  time: 0.8074  data: 0.0006  max mem: 19734
Epoch: [35]  [ 350/1251]  eta: 0:12:59  lr: 0.000015  loss: 3.6104 (3.4226)  time: 0.8076  data: 0.0006  max mem: 19734
Epoch: [35]  [ 360/1251]  eta: 0:12:49  lr: 0.000015  loss: 3.4690 (3.4244)  time: 0.8136  data: 0.0005  max mem: 19734
Epoch: [35]  [ 370/1251]  eta: 0:12:39  lr: 0.000015  loss: 3.6309 (3.4282)  time: 0.8142  data: 0.0006  max mem: 19734
Epoch: [35]  [ 380/1251]  eta: 0:12:29  lr: 0.000015  loss: 3.6309 (3.4325)  time: 0.8045  data: 0.0006  max mem: 19734
Epoch: [35]  [ 390/1251]  eta: 0:12:19  lr: 0.000015  loss: 3.5335 (3.4323)  time: 0.8051  data: 0.0006  max mem: 19734
Epoch: [35]  [ 400/1251]  eta: 0:12:10  lr: 0.000015  loss: 3.5564 (3.4371)  time: 0.8082  data: 0.0006  max mem: 19734
Epoch: [35]  [ 410/1251]  eta: 0:12:00  lr: 0.000015  loss: 3.7563 (3.4412)  time: 0.8095  data: 0.0007  max mem: 19734
loss info: cls_loss=3.5312, ratio_loss=0.0047, pruning_loss=0.1260, mse_loss=0.5890
Epoch: [35]  [ 420/1251]  eta: 0:11:51  lr: 0.000015  loss: 3.5936 (3.4412)  time: 0.8135  data: 0.0007  max mem: 19734
Epoch: [35]  [ 430/1251]  eta: 0:11:41  lr: 0.000015  loss: 3.3952 (3.4367)  time: 0.8101  data: 0.0006  max mem: 19734
Epoch: [35]  [ 440/1251]  eta: 0:11:32  lr: 0.000015  loss: 3.5366 (3.4379)  time: 0.8167  data: 0.0006  max mem: 19734
Epoch: [35]  [ 450/1251]  eta: 0:11:24  lr: 0.000015  loss: 3.5493 (3.4369)  time: 0.8427  data: 0.0006  max mem: 19734
Epoch: [35]  [ 460/1251]  eta: 0:11:15  lr: 0.000015  loss: 3.5493 (3.4416)  time: 0.8425  data: 0.0006  max mem: 19734
Epoch: [35]  [ 470/1251]  eta: 0:11:06  lr: 0.000015  loss: 3.7084 (3.4449)  time: 0.8180  data: 0.0009  max mem: 19734
Epoch: [35]  [ 480/1251]  eta: 0:10:56  lr: 0.000015  loss: 3.4817 (3.4416)  time: 0.8056  data: 0.0010  max mem: 19734
Epoch: [35]  [ 490/1251]  eta: 0:10:47  lr: 0.000015  loss: 3.3439 (3.4429)  time: 0.8077  data: 0.0008  max mem: 19734
Epoch: [35]  [ 500/1251]  eta: 0:10:38  lr: 0.000015  loss: 3.4897 (3.4393)  time: 0.8037  data: 0.0007  max mem: 19734
Epoch: [35]  [ 510/1251]  eta: 0:10:29  lr: 0.000015  loss: 3.3144 (3.4370)  time: 0.8106  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3847, ratio_loss=0.0046, pruning_loss=0.1305, mse_loss=0.5968
Epoch: [35]  [ 520/1251]  eta: 0:10:20  lr: 0.000015  loss: 3.5427 (3.4382)  time: 0.8120  data: 0.0007  max mem: 19734
Epoch: [35]  [ 530/1251]  eta: 0:10:11  lr: 0.000015  loss: 3.5266 (3.4372)  time: 0.8014  data: 0.0007  max mem: 19734
Epoch: [35]  [ 540/1251]  eta: 0:10:02  lr: 0.000015  loss: 3.3690 (3.4363)  time: 0.8029  data: 0.0007  max mem: 19734
Epoch: [35]  [ 550/1251]  eta: 0:09:53  lr: 0.000015  loss: 3.3690 (3.4346)  time: 0.8057  data: 0.0007  max mem: 19734
Epoch: [35]  [ 560/1251]  eta: 0:09:44  lr: 0.000015  loss: 3.4821 (3.4343)  time: 0.8030  data: 0.0007  max mem: 19734
Epoch: [35]  [ 570/1251]  eta: 0:09:35  lr: 0.000015  loss: 3.5601 (3.4383)  time: 0.8088  data: 0.0006  max mem: 19734
Epoch: [35]  [ 580/1251]  eta: 0:09:26  lr: 0.000015  loss: 3.7330 (3.4408)  time: 0.8191  data: 0.0007  max mem: 19734
Epoch: [35]  [ 590/1251]  eta: 0:09:18  lr: 0.000015  loss: 3.5787 (3.4398)  time: 0.8322  data: 0.0007  max mem: 19734
Epoch: [35]  [ 600/1251]  eta: 0:09:09  lr: 0.000015  loss: 3.5787 (3.4414)  time: 0.8419  data: 0.0006  max mem: 19734
Epoch: [35]  [ 610/1251]  eta: 0:09:00  lr: 0.000015  loss: 3.6075 (3.4411)  time: 0.8228  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4399, ratio_loss=0.0048, pruning_loss=0.1284, mse_loss=0.6143
Epoch: [35]  [ 620/1251]  eta: 0:08:51  lr: 0.000015  loss: 3.4946 (3.4389)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [35]  [ 630/1251]  eta: 0:08:43  lr: 0.000015  loss: 3.3068 (3.4325)  time: 0.8039  data: 0.0006  max mem: 19734
Epoch: [35]  [ 640/1251]  eta: 0:08:34  lr: 0.000015  loss: 3.1786 (3.4299)  time: 0.8020  data: 0.0007  max mem: 19734
Epoch: [35]  [ 650/1251]  eta: 0:08:25  lr: 0.000015  loss: 3.5421 (3.4317)  time: 0.8056  data: 0.0007  max mem: 19734
Epoch: [35]  [ 660/1251]  eta: 0:08:16  lr: 0.000015  loss: 3.5304 (3.4304)  time: 0.8065  data: 0.0006  max mem: 19734
Epoch: [35]  [ 670/1251]  eta: 0:08:08  lr: 0.000015  loss: 3.2386 (3.4256)  time: 0.8056  data: 0.0006  max mem: 19734
Epoch: [35]  [ 680/1251]  eta: 0:07:59  lr: 0.000015  loss: 3.4332 (3.4275)  time: 0.8063  data: 0.0007  max mem: 19734
Epoch: [35]  [ 690/1251]  eta: 0:07:50  lr: 0.000015  loss: 3.5484 (3.4259)  time: 0.8027  data: 0.0006  max mem: 19734
Epoch: [35]  [ 700/1251]  eta: 0:07:42  lr: 0.000015  loss: 3.4323 (3.4227)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [35]  [ 710/1251]  eta: 0:07:33  lr: 0.000015  loss: 3.5162 (3.4253)  time: 0.8042  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3148, ratio_loss=0.0049, pruning_loss=0.1308, mse_loss=0.6153
Epoch: [35]  [ 720/1251]  eta: 0:07:25  lr: 0.000015  loss: 3.6720 (3.4251)  time: 0.8246  data: 0.0008  max mem: 19734
Epoch: [35]  [ 730/1251]  eta: 0:07:16  lr: 0.000015  loss: 3.5199 (3.4237)  time: 0.8361  data: 0.0007  max mem: 19734
Epoch: [35]  [ 740/1251]  eta: 0:07:08  lr: 0.000015  loss: 3.3460 (3.4195)  time: 0.8524  data: 0.0006  max mem: 19734
Epoch: [35]  [ 750/1251]  eta: 0:07:00  lr: 0.000015  loss: 2.9148 (3.4149)  time: 0.8526  data: 0.0006  max mem: 19734
Epoch: [35]  [ 760/1251]  eta: 0:06:51  lr: 0.000015  loss: 3.7146 (3.4200)  time: 0.8154  data: 0.0006  max mem: 19734
Epoch: [35]  [ 770/1251]  eta: 0:06:42  lr: 0.000015  loss: 3.6004 (3.4166)  time: 0.8039  data: 0.0006  max mem: 19734
Epoch: [35]  [ 780/1251]  eta: 0:06:34  lr: 0.000015  loss: 3.4078 (3.4176)  time: 0.8079  data: 0.0006  max mem: 19734
Epoch: [35]  [ 790/1251]  eta: 0:06:25  lr: 0.000015  loss: 3.5901 (3.4158)  time: 0.8082  data: 0.0006  max mem: 19734
Epoch: [35]  [ 800/1251]  eta: 0:06:17  lr: 0.000015  loss: 3.6917 (3.4179)  time: 0.8139  data: 0.0006  max mem: 19734
Epoch: [35]  [ 810/1251]  eta: 0:06:08  lr: 0.000015  loss: 3.4443 (3.4171)  time: 0.8143  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3339, ratio_loss=0.0048, pruning_loss=0.1309, mse_loss=0.5729
Epoch: [35]  [ 820/1251]  eta: 0:06:00  lr: 0.000015  loss: 3.4443 (3.4190)  time: 0.8031  data: 0.0006  max mem: 19734
Epoch: [35]  [ 830/1251]  eta: 0:05:51  lr: 0.000015  loss: 3.5942 (3.4196)  time: 0.8045  data: 0.0007  max mem: 19734
Epoch: [35]  [ 840/1251]  eta: 0:05:43  lr: 0.000015  loss: 3.5942 (3.4199)  time: 0.8057  data: 0.0007  max mem: 19734
Epoch: [35]  [ 850/1251]  eta: 0:05:34  lr: 0.000015  loss: 3.6275 (3.4189)  time: 0.8078  data: 0.0006  max mem: 19734
Epoch: [35]  [ 860/1251]  eta: 0:05:26  lr: 0.000015  loss: 3.5052 (3.4203)  time: 0.8168  data: 0.0007  max mem: 19734
Epoch: [35]  [ 870/1251]  eta: 0:05:17  lr: 0.000015  loss: 3.6312 (3.4210)  time: 0.8146  data: 0.0006  max mem: 19734
Epoch: [35]  [ 880/1251]  eta: 0:05:09  lr: 0.000015  loss: 3.6198 (3.4208)  time: 0.8227  data: 0.0006  max mem: 19734
Epoch: [35]  [ 890/1251]  eta: 0:05:01  lr: 0.000015  loss: 3.3888 (3.4186)  time: 0.8341  data: 0.0005  max mem: 19734
Epoch: [35]  [ 900/1251]  eta: 0:04:52  lr: 0.000015  loss: 3.3059 (3.4183)  time: 0.8214  data: 0.0006  max mem: 19734
Epoch: [35]  [ 910/1251]  eta: 0:04:44  lr: 0.000015  loss: 3.3059 (3.4178)  time: 0.8082  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4035, ratio_loss=0.0045, pruning_loss=0.1291, mse_loss=0.5776
Epoch: [35]  [ 920/1251]  eta: 0:04:35  lr: 0.000015  loss: 3.3858 (3.4177)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [35]  [ 930/1251]  eta: 0:04:27  lr: 0.000015  loss: 3.5967 (3.4192)  time: 0.8077  data: 0.0007  max mem: 19734
Epoch: [35]  [ 940/1251]  eta: 0:04:19  lr: 0.000015  loss: 3.6641 (3.4211)  time: 0.8104  data: 0.0006  max mem: 19734
Epoch: [35]  [ 950/1251]  eta: 0:04:10  lr: 0.000015  loss: 3.6641 (3.4228)  time: 0.8126  data: 0.0005  max mem: 19734
Epoch: [35]  [ 960/1251]  eta: 0:04:02  lr: 0.000015  loss: 3.6434 (3.4233)  time: 0.8082  data: 0.0005  max mem: 19734
Epoch: [35]  [ 970/1251]  eta: 0:03:53  lr: 0.000015  loss: 3.4952 (3.4222)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [35]  [ 980/1251]  eta: 0:03:45  lr: 0.000015  loss: 3.3052 (3.4209)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [35]  [ 990/1251]  eta: 0:03:37  lr: 0.000015  loss: 3.3765 (3.4208)  time: 0.8007  data: 0.0006  max mem: 19734
Epoch: [35]  [1000/1251]  eta: 0:03:28  lr: 0.000015  loss: 3.5868 (3.4213)  time: 0.8034  data: 0.0010  max mem: 19734
Epoch: [35]  [1010/1251]  eta: 0:03:20  lr: 0.000015  loss: 3.5683 (3.4212)  time: 0.8129  data: 0.0009  max mem: 19734
loss info: cls_loss=3.4463, ratio_loss=0.0049, pruning_loss=0.1281, mse_loss=0.5821
Epoch: [35]  [1020/1251]  eta: 0:03:11  lr: 0.000015  loss: 3.5963 (3.4238)  time: 0.8144  data: 0.0005  max mem: 19734
Epoch: [35]  [1030/1251]  eta: 0:03:03  lr: 0.000015  loss: 3.5914 (3.4205)  time: 0.8274  data: 0.0006  max mem: 19734
Epoch: [35]  [1040/1251]  eta: 0:02:55  lr: 0.000015  loss: 3.2125 (3.4188)  time: 0.8402  data: 0.0006  max mem: 19734
Epoch: [35]  [1050/1251]  eta: 0:02:46  lr: 0.000015  loss: 3.5458 (3.4199)  time: 0.8181  data: 0.0006  max mem: 19734
Epoch: [35]  [1060/1251]  eta: 0:02:38  lr: 0.000015  loss: 3.4316 (3.4178)  time: 0.8042  data: 0.0007  max mem: 19734
Epoch: [35]  [1070/1251]  eta: 0:02:30  lr: 0.000015  loss: 3.2254 (3.4161)  time: 0.8057  data: 0.0007  max mem: 19734
Epoch: [35]  [1080/1251]  eta: 0:02:21  lr: 0.000015  loss: 3.4559 (3.4137)  time: 0.8013  data: 0.0006  max mem: 19734
Epoch: [35]  [1090/1251]  eta: 0:02:13  lr: 0.000015  loss: 3.2118 (3.4123)  time: 0.8084  data: 0.0004  max mem: 19734
Epoch: [35]  [1100/1251]  eta: 0:02:05  lr: 0.000015  loss: 3.5978 (3.4141)  time: 0.8105  data: 0.0005  max mem: 19734
Epoch: [35]  [1110/1251]  eta: 0:01:56  lr: 0.000015  loss: 3.6210 (3.4151)  time: 0.8041  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3068, ratio_loss=0.0047, pruning_loss=0.1315, mse_loss=0.5669
Epoch: [35]  [1120/1251]  eta: 0:01:48  lr: 0.000015  loss: 3.5592 (3.4127)  time: 0.8039  data: 0.0006  max mem: 19734
Epoch: [35]  [1130/1251]  eta: 0:01:40  lr: 0.000015  loss: 3.2381 (3.4127)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [35]  [1140/1251]  eta: 0:01:31  lr: 0.000015  loss: 3.4854 (3.4126)  time: 0.7981  data: 0.0005  max mem: 19734
Epoch: [35]  [1150/1251]  eta: 0:01:23  lr: 0.000015  loss: 3.4854 (3.4111)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [35]  [1160/1251]  eta: 0:01:15  lr: 0.000015  loss: 3.6067 (3.4125)  time: 0.8097  data: 0.0006  max mem: 19734
Epoch: [35]  [1170/1251]  eta: 0:01:07  lr: 0.000015  loss: 3.6883 (3.4136)  time: 0.8134  data: 0.0006  max mem: 19734
Epoch: [35]  [1180/1251]  eta: 0:00:58  lr: 0.000015  loss: 3.5672 (3.4127)  time: 0.8232  data: 0.0007  max mem: 19734
Epoch: [35]  [1190/1251]  eta: 0:00:50  lr: 0.000015  loss: 3.4877 (3.4141)  time: 0.8232  data: 0.0010  max mem: 19734
Epoch: [35]  [1200/1251]  eta: 0:00:42  lr: 0.000015  loss: 3.4250 (3.4109)  time: 0.8024  data: 0.0008  max mem: 19734
Epoch: [35]  [1210/1251]  eta: 0:00:33  lr: 0.000015  loss: 3.3138 (3.4113)  time: 0.7921  data: 0.0001  max mem: 19734
loss info: cls_loss=3.3863, ratio_loss=0.0049, pruning_loss=0.1289, mse_loss=0.5683
Epoch: [35]  [1220/1251]  eta: 0:00:25  lr: 0.000015  loss: 3.5782 (3.4129)  time: 0.7932  data: 0.0001  max mem: 19734
Epoch: [35]  [1230/1251]  eta: 0:00:17  lr: 0.000015  loss: 3.5233 (3.4129)  time: 0.7935  data: 0.0001  max mem: 19734
Epoch: [35]  [1240/1251]  eta: 0:00:09  lr: 0.000015  loss: 3.5233 (3.4124)  time: 0.8013  data: 0.0001  max mem: 19734
Epoch: [35]  [1250/1251]  eta: 0:00:00  lr: 0.000015  loss: 3.5385 (3.4142)  time: 0.8007  data: 0.0001  max mem: 19734
Epoch: [35] Total time: 0:17:15 (0.8276 s / it)
Averaged stats: lr: 0.000015  loss: 3.5385 (3.4127)
Test:  [  0/261]  eta: 2:46:39  loss: 0.7162 (0.7162)  acc1: 83.3333 (83.3333)  acc5: 96.3542 (96.3542)  time: 38.3106  data: 38.1484  max mem: 19734
Test:  [ 10/261]  eta: 0:17:24  loss: 0.7162 (0.7143)  acc1: 84.8958 (84.5644)  acc5: 96.8750 (96.5436)  time: 4.1609  data: 3.4818  max mem: 19734
Test:  [ 20/261]  eta: 0:09:23  loss: 0.9023 (0.8882)  acc1: 78.6458 (79.0675)  acc5: 94.2708 (95.0397)  time: 0.5389  data: 0.0139  max mem: 19734
Test:  [ 30/261]  eta: 0:06:29  loss: 0.7922 (0.8040)  acc1: 80.2083 (82.1069)  acc5: 94.7917 (95.4805)  time: 0.3228  data: 0.0118  max mem: 19734
Test:  [ 40/261]  eta: 0:05:20  loss: 0.5827 (0.7697)  acc1: 88.0208 (83.0793)  acc5: 96.8750 (95.7825)  time: 0.5167  data: 0.2356  max mem: 19734
Test:  [ 50/261]  eta: 0:04:35  loss: 0.8670 (0.8308)  acc1: 79.1667 (81.2806)  acc5: 95.8333 (95.4248)  time: 0.7200  data: 0.2389  max mem: 19734
Test:  [ 60/261]  eta: 0:03:54  loss: 0.9811 (0.8427)  acc1: 76.0417 (80.8829)  acc5: 94.2708 (95.3893)  time: 0.5813  data: 0.0165  max mem: 19734
Test:  [ 70/261]  eta: 0:03:28  loss: 0.9506 (0.8456)  acc1: 78.1250 (80.3991)  acc5: 95.8333 (95.5252)  time: 0.5420  data: 0.1123  max mem: 19734
Test:  [ 80/261]  eta: 0:02:58  loss: 0.8053 (0.8474)  acc1: 80.2083 (80.4977)  acc5: 96.8750 (95.6340)  time: 0.4508  data: 0.1124  max mem: 19734
Test:  [ 90/261]  eta: 0:02:38  loss: 0.7950 (0.8345)  acc1: 83.3333 (80.8150)  acc5: 95.8333 (95.6960)  time: 0.3536  data: 0.0166  max mem: 19734
Test:  [100/261]  eta: 0:02:24  loss: 0.8000 (0.8373)  acc1: 83.3333 (80.8271)  acc5: 95.3125 (95.7457)  time: 0.5443  data: 0.1063  max mem: 19734
Test:  [110/261]  eta: 0:02:08  loss: 0.8839 (0.8585)  acc1: 77.6042 (80.4054)  acc5: 94.7917 (95.5002)  time: 0.4884  data: 0.1036  max mem: 19734
Test:  [120/261]  eta: 0:01:58  loss: 1.2015 (0.8985)  acc1: 71.8750 (79.4335)  acc5: 90.6250 (94.9811)  time: 0.5173  data: 0.1982  max mem: 19734
Test:  [130/261]  eta: 0:01:45  loss: 1.3488 (0.9456)  acc1: 67.1875 (78.4948)  acc5: 88.0208 (94.3225)  time: 0.5539  data: 0.2016  max mem: 19734
Test:  [140/261]  eta: 0:01:31  loss: 1.2919 (0.9743)  acc1: 69.7917 (77.7667)  acc5: 89.5833 (94.0307)  time: 0.2834  data: 0.0138  max mem: 19734
Test:  [150/261]  eta: 0:01:24  loss: 1.2290 (0.9803)  acc1: 71.8750 (77.7456)  acc5: 91.1458 (93.8811)  time: 0.4890  data: 0.3153  max mem: 19734
Test:  [160/261]  eta: 0:01:13  loss: 1.0088 (1.0009)  acc1: 77.6042 (77.3745)  acc5: 91.6667 (93.5688)  time: 0.5066  data: 0.3182  max mem: 19734
Test:  [170/261]  eta: 0:01:03  loss: 1.3006 (1.0294)  acc1: 64.5833 (76.6143)  acc5: 87.5000 (93.2474)  time: 0.1884  data: 0.0321  max mem: 19734
Test:  [180/261]  eta: 0:00:54  loss: 1.4142 (1.0458)  acc1: 64.5833 (76.2057)  acc5: 89.5833 (93.0968)  time: 0.1907  data: 0.0651  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: 1.3334 (1.0583)  acc1: 68.7500 (75.9571)  acc5: 91.1458 (92.9510)  time: 0.1563  data: 0.0404  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 1.3016 (1.0730)  acc1: 71.3542 (75.6556)  acc5: 89.5833 (92.7187)  time: 0.1165  data: 0.0004  max mem: 19734
Test:  [210/261]  eta: 0:00:30  loss: 1.3393 (1.0858)  acc1: 70.8333 (75.3974)  acc5: 88.0208 (92.5010)  time: 0.1162  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.3500 (1.1054)  acc1: 67.7083 (74.9199)  acc5: 87.5000 (92.2888)  time: 0.1146  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.3958 (1.1142)  acc1: 67.1875 (74.7182)  acc5: 89.0625 (92.1988)  time: 0.1144  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.3434 (1.1238)  acc1: 68.7500 (74.5051)  acc5: 91.1458 (92.1227)  time: 0.1145  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 0.9918 (1.1168)  acc1: 75.5208 (74.6680)  acc5: 93.7500 (92.2332)  time: 0.1146  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9765 (1.1166)  acc1: 76.5625 (74.6400)  acc5: 94.7917 (92.2820)  time: 0.1111  data: 0.0001  max mem: 19734
Test: Total time: 0:02:10 (0.4991 s / it)
* Acc@1 74.640 Acc@5 92.282 loss 1.117
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.64%
Epoch: [36]  [   0/1251]  eta: 6:10:49  lr: 0.000015  loss: 3.9309 (3.9309)  time: 17.7856  data: 16.9431  max mem: 19734
Epoch: [36]  [  10/1251]  eta: 0:51:26  lr: 0.000015  loss: 3.5037 (3.3854)  time: 2.4871  data: 1.5420  max mem: 19734
Epoch: [36]  [  20/1251]  eta: 0:34:31  lr: 0.000015  loss: 3.3842 (3.3353)  time: 0.8778  data: 0.0012  max mem: 19734
Epoch: [36]  [  30/1251]  eta: 0:28:27  lr: 0.000015  loss: 3.3510 (3.3559)  time: 0.7993  data: 0.0005  max mem: 19734
Epoch: [36]  [  40/1251]  eta: 0:25:15  lr: 0.000015  loss: 3.5441 (3.4178)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [36]  [  50/1251]  eta: 0:23:19  lr: 0.000015  loss: 3.7137 (3.4908)  time: 0.8052  data: 0.0006  max mem: 19734
Epoch: [36]  [  60/1251]  eta: 0:21:59  lr: 0.000015  loss: 3.6304 (3.4999)  time: 0.8131  data: 0.0008  max mem: 19734
loss info: cls_loss=3.4680, ratio_loss=0.0049, pruning_loss=0.1278, mse_loss=0.6143
Epoch: [36]  [  70/1251]  eta: 0:21:01  lr: 0.000015  loss: 3.4853 (3.4817)  time: 0.8200  data: 0.0008  max mem: 19734
Epoch: [36]  [  80/1251]  eta: 0:20:16  lr: 0.000015  loss: 3.5012 (3.4694)  time: 0.8269  data: 0.0006  max mem: 19734
Epoch: [36]  [  90/1251]  eta: 0:19:35  lr: 0.000015  loss: 3.5341 (3.4713)  time: 0.8142  data: 0.0006  max mem: 19734
Epoch: [36]  [ 100/1251]  eta: 0:19:01  lr: 0.000015  loss: 3.5336 (3.4703)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [36]  [ 110/1251]  eta: 0:18:31  lr: 0.000015  loss: 3.5336 (3.4667)  time: 0.7984  data: 0.0005  max mem: 19734
Epoch: [36]  [ 120/1251]  eta: 0:18:06  lr: 0.000015  loss: 3.4838 (3.4545)  time: 0.8038  data: 0.0004  max mem: 19734
Epoch: [36]  [ 130/1251]  eta: 0:17:43  lr: 0.000015  loss: 3.4838 (3.4553)  time: 0.8067  data: 0.0005  max mem: 19734
Epoch: [36]  [ 140/1251]  eta: 0:17:21  lr: 0.000015  loss: 3.6348 (3.4558)  time: 0.7996  data: 0.0006  max mem: 19734
Epoch: [36]  [ 150/1251]  eta: 0:17:04  lr: 0.000015  loss: 3.5701 (3.4618)  time: 0.8156  data: 0.0006  max mem: 19734
Epoch: [36]  [ 160/1251]  eta: 0:16:46  lr: 0.000015  loss: 3.3122 (3.4399)  time: 0.8185  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3882, ratio_loss=0.0047, pruning_loss=0.1310, mse_loss=0.6208
Epoch: [36]  [ 170/1251]  eta: 0:16:29  lr: 0.000015  loss: 3.4565 (3.4443)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [36]  [ 180/1251]  eta: 0:16:13  lr: 0.000015  loss: 3.6202 (3.4441)  time: 0.7998  data: 0.0006  max mem: 19734
Epoch: [36]  [ 190/1251]  eta: 0:15:59  lr: 0.000015  loss: 3.3135 (3.4409)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [36]  [ 200/1251]  eta: 0:15:44  lr: 0.000015  loss: 3.4755 (3.4430)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [36]  [ 210/1251]  eta: 0:15:31  lr: 0.000015  loss: 3.5439 (3.4367)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [36]  [ 220/1251]  eta: 0:15:20  lr: 0.000015  loss: 3.4909 (3.4318)  time: 0.8263  data: 0.0005  max mem: 19734
Epoch: [36]  [ 230/1251]  eta: 0:15:07  lr: 0.000015  loss: 3.5967 (3.4423)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [36]  [ 240/1251]  eta: 0:14:54  lr: 0.000015  loss: 3.6855 (3.4405)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [36]  [ 250/1251]  eta: 0:14:42  lr: 0.000015  loss: 3.6116 (3.4448)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [36]  [ 260/1251]  eta: 0:14:30  lr: 0.000015  loss: 3.6459 (3.4497)  time: 0.8006  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4543, ratio_loss=0.0043, pruning_loss=0.1290, mse_loss=0.5969
Epoch: [36]  [ 270/1251]  eta: 0:14:19  lr: 0.000015  loss: 3.5772 (3.4560)  time: 0.8066  data: 0.0004  max mem: 19734
Epoch: [36]  [ 280/1251]  eta: 0:14:08  lr: 0.000015  loss: 3.6360 (3.4605)  time: 0.8099  data: 0.0005  max mem: 19734
Epoch: [36]  [ 290/1251]  eta: 0:13:57  lr: 0.000015  loss: 3.4998 (3.4515)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [36]  [ 300/1251]  eta: 0:13:46  lr: 0.000015  loss: 3.3519 (3.4469)  time: 0.8136  data: 0.0005  max mem: 19734
Epoch: [36]  [ 310/1251]  eta: 0:13:36  lr: 0.000015  loss: 3.5710 (3.4493)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [36]  [ 320/1251]  eta: 0:13:25  lr: 0.000015  loss: 3.1360 (3.4339)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [36]  [ 330/1251]  eta: 0:13:15  lr: 0.000015  loss: 3.0245 (3.4325)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [36]  [ 340/1251]  eta: 0:13:05  lr: 0.000015  loss: 3.4161 (3.4296)  time: 0.8129  data: 0.0005  max mem: 19734
Epoch: [36]  [ 350/1251]  eta: 0:12:56  lr: 0.000015  loss: 3.4265 (3.4281)  time: 0.8245  data: 0.0005  max mem: 19734
Epoch: [36]  [ 360/1251]  eta: 0:12:46  lr: 0.000015  loss: 3.6956 (3.4306)  time: 0.8275  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3510, ratio_loss=0.0046, pruning_loss=0.1309, mse_loss=0.5885
Epoch: [36]  [ 370/1251]  eta: 0:12:37  lr: 0.000015  loss: 3.5679 (3.4301)  time: 0.8250  data: 0.0004  max mem: 19734
Epoch: [36]  [ 380/1251]  eta: 0:12:27  lr: 0.000015  loss: 3.4393 (3.4298)  time: 0.8097  data: 0.0004  max mem: 19734
Epoch: [36]  [ 390/1251]  eta: 0:12:17  lr: 0.000015  loss: 3.6560 (3.4325)  time: 0.8004  data: 0.0004  max mem: 19734
Epoch: [36]  [ 400/1251]  eta: 0:12:07  lr: 0.000015  loss: 3.5874 (3.4307)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [36]  [ 410/1251]  eta: 0:11:57  lr: 0.000015  loss: 3.4350 (3.4295)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [36]  [ 420/1251]  eta: 0:11:48  lr: 0.000015  loss: 3.4350 (3.4270)  time: 0.8093  data: 0.0006  max mem: 19734
Epoch: [36]  [ 430/1251]  eta: 0:11:39  lr: 0.000015  loss: 3.3842 (3.4268)  time: 0.8105  data: 0.0005  max mem: 19734
Epoch: [36]  [ 440/1251]  eta: 0:11:29  lr: 0.000015  loss: 3.2141 (3.4232)  time: 0.8084  data: 0.0004  max mem: 19734
Epoch: [36]  [ 450/1251]  eta: 0:11:20  lr: 0.000015  loss: 3.5575 (3.4292)  time: 0.8108  data: 0.0004  max mem: 19734
Epoch: [36]  [ 460/1251]  eta: 0:11:11  lr: 0.000015  loss: 3.6159 (3.4230)  time: 0.8080  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3578, ratio_loss=0.0045, pruning_loss=0.1287, mse_loss=0.5881
Epoch: [36]  [ 470/1251]  eta: 0:11:02  lr: 0.000015  loss: 3.5259 (3.4217)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [36]  [ 480/1251]  eta: 0:10:53  lr: 0.000015  loss: 3.2564 (3.4153)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [36]  [ 490/1251]  eta: 0:10:44  lr: 0.000015  loss: 2.9777 (3.4144)  time: 0.8090  data: 0.0006  max mem: 19734
Epoch: [36]  [ 500/1251]  eta: 0:10:35  lr: 0.000015  loss: 2.8724 (3.4050)  time: 0.8169  data: 0.0005  max mem: 19734
Epoch: [36]  [ 510/1251]  eta: 0:10:26  lr: 0.000015  loss: 3.3841 (3.4072)  time: 0.8254  data: 0.0005  max mem: 19734
Epoch: [36]  [ 520/1251]  eta: 0:10:17  lr: 0.000015  loss: 3.5670 (3.4074)  time: 0.8154  data: 0.0005  max mem: 19734
Epoch: [36]  [ 530/1251]  eta: 0:10:08  lr: 0.000015  loss: 3.5670 (3.4065)  time: 0.8068  data: 0.0005  max mem: 19734
Epoch: [36]  [ 540/1251]  eta: 0:09:59  lr: 0.000015  loss: 3.4167 (3.4032)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [36]  [ 550/1251]  eta: 0:09:50  lr: 0.000015  loss: 3.2683 (3.4011)  time: 0.8001  data: 0.0004  max mem: 19734
Epoch: [36]  [ 560/1251]  eta: 0:09:41  lr: 0.000015  loss: 3.4716 (3.4046)  time: 0.8091  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3103, ratio_loss=0.0045, pruning_loss=0.1295, mse_loss=0.6168
Epoch: [36]  [ 570/1251]  eta: 0:09:32  lr: 0.000015  loss: 3.6040 (3.4041)  time: 0.8108  data: 0.0004  max mem: 19734
Epoch: [36]  [ 580/1251]  eta: 0:09:24  lr: 0.000015  loss: 3.4258 (3.4045)  time: 0.8008  data: 0.0004  max mem: 19734
Epoch: [36]  [ 590/1251]  eta: 0:09:15  lr: 0.000015  loss: 3.3955 (3.4012)  time: 0.8004  data: 0.0004  max mem: 19734
Epoch: [36]  [ 600/1251]  eta: 0:09:06  lr: 0.000015  loss: 3.3756 (3.4005)  time: 0.8100  data: 0.0004  max mem: 19734
Epoch: [36]  [ 610/1251]  eta: 0:08:57  lr: 0.000015  loss: 3.3816 (3.3956)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [36]  [ 620/1251]  eta: 0:08:49  lr: 0.000015  loss: 3.3433 (3.3967)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [36]  [ 630/1251]  eta: 0:08:40  lr: 0.000015  loss: 3.4751 (3.3962)  time: 0.8115  data: 0.0004  max mem: 19734
Epoch: [36]  [ 640/1251]  eta: 0:08:31  lr: 0.000015  loss: 3.5873 (3.4006)  time: 0.8157  data: 0.0004  max mem: 19734
Epoch: [36]  [ 650/1251]  eta: 0:08:23  lr: 0.000015  loss: 3.6117 (3.4031)  time: 0.8142  data: 0.0005  max mem: 19734
Epoch: [36]  [ 660/1251]  eta: 0:08:14  lr: 0.000015  loss: 3.6322 (3.4051)  time: 0.8252  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3986, ratio_loss=0.0047, pruning_loss=0.1275, mse_loss=0.6037
Epoch: [36]  [ 670/1251]  eta: 0:08:06  lr: 0.000015  loss: 3.6772 (3.4042)  time: 0.8173  data: 0.0005  max mem: 19734
Epoch: [36]  [ 680/1251]  eta: 0:07:57  lr: 0.000015  loss: 3.3345 (3.4035)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [36]  [ 690/1251]  eta: 0:07:48  lr: 0.000015  loss: 3.5566 (3.4076)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [36]  [ 700/1251]  eta: 0:07:40  lr: 0.000015  loss: 3.5218 (3.4049)  time: 0.8009  data: 0.0004  max mem: 19734
Epoch: [36]  [ 710/1251]  eta: 0:07:31  lr: 0.000015  loss: 3.4307 (3.4055)  time: 0.8075  data: 0.0004  max mem: 19734
Epoch: [36]  [ 720/1251]  eta: 0:07:23  lr: 0.000015  loss: 3.3118 (3.4038)  time: 0.8072  data: 0.0004  max mem: 19734
Epoch: [36]  [ 730/1251]  eta: 0:07:14  lr: 0.000015  loss: 3.3859 (3.4035)  time: 0.7990  data: 0.0004  max mem: 19734
Epoch: [36]  [ 740/1251]  eta: 0:07:05  lr: 0.000015  loss: 3.5307 (3.4053)  time: 0.7999  data: 0.0004  max mem: 19734
Epoch: [36]  [ 750/1251]  eta: 0:06:57  lr: 0.000015  loss: 3.6173 (3.4068)  time: 0.8039  data: 0.0005  max mem: 19734
Epoch: [36]  [ 760/1251]  eta: 0:06:48  lr: 0.000015  loss: 3.4716 (3.4050)  time: 0.8022  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3444, ratio_loss=0.0045, pruning_loss=0.1313, mse_loss=0.5975
Epoch: [36]  [ 770/1251]  eta: 0:06:40  lr: 0.000015  loss: 2.9742 (3.3991)  time: 0.8110  data: 0.0005  max mem: 19734
Epoch: [36]  [ 780/1251]  eta: 0:06:31  lr: 0.000015  loss: 3.3345 (3.3994)  time: 0.8122  data: 0.0005  max mem: 19734
Epoch: [36]  [ 790/1251]  eta: 0:06:23  lr: 0.000015  loss: 3.6431 (3.4018)  time: 0.8152  data: 0.0005  max mem: 19734
Epoch: [36]  [ 800/1251]  eta: 0:06:15  lr: 0.000015  loss: 3.6700 (3.4027)  time: 0.8246  data: 0.0005  max mem: 19734
Epoch: [36]  [ 810/1251]  eta: 0:06:06  lr: 0.000015  loss: 3.3314 (3.3995)  time: 0.8164  data: 0.0006  max mem: 19734
Epoch: [36]  [ 820/1251]  eta: 0:05:58  lr: 0.000015  loss: 2.9653 (3.3971)  time: 0.8088  data: 0.0006  max mem: 19734
Epoch: [36]  [ 830/1251]  eta: 0:05:49  lr: 0.000015  loss: 3.1919 (3.3936)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [36]  [ 840/1251]  eta: 0:05:41  lr: 0.000015  loss: 3.2103 (3.3936)  time: 0.8000  data: 0.0005  max mem: 19734
Epoch: [36]  [ 850/1251]  eta: 0:05:33  lr: 0.000015  loss: 3.5430 (3.3926)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [36]  [ 860/1251]  eta: 0:05:24  lr: 0.000015  loss: 3.5430 (3.3929)  time: 0.8081  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3248, ratio_loss=0.0047, pruning_loss=0.1306, mse_loss=0.6029
Epoch: [36]  [ 870/1251]  eta: 0:05:16  lr: 0.000015  loss: 3.4542 (3.3924)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [36]  [ 880/1251]  eta: 0:05:07  lr: 0.000015  loss: 3.4679 (3.3929)  time: 0.7995  data: 0.0005  max mem: 19734
Epoch: [36]  [ 890/1251]  eta: 0:04:59  lr: 0.000015  loss: 3.6105 (3.3931)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [36]  [ 900/1251]  eta: 0:04:50  lr: 0.000015  loss: 3.5235 (3.3934)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [36]  [ 910/1251]  eta: 0:04:42  lr: 0.000015  loss: 3.7482 (3.3980)  time: 0.8061  data: 0.0004  max mem: 19734
Epoch: [36]  [ 920/1251]  eta: 0:04:34  lr: 0.000015  loss: 3.7273 (3.3987)  time: 0.8074  data: 0.0005  max mem: 19734
Epoch: [36]  [ 930/1251]  eta: 0:04:25  lr: 0.000015  loss: 3.5938 (3.3993)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [36]  [ 940/1251]  eta: 0:04:17  lr: 0.000015  loss: 3.6355 (3.3999)  time: 0.8170  data: 0.0005  max mem: 19734
Epoch: [36]  [ 950/1251]  eta: 0:04:09  lr: 0.000015  loss: 3.5847 (3.3995)  time: 0.8335  data: 0.0004  max mem: 19734
Epoch: [36]  [ 960/1251]  eta: 0:04:00  lr: 0.000015  loss: 3.5230 (3.4003)  time: 0.8168  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4372, ratio_loss=0.0046, pruning_loss=0.1284, mse_loss=0.6095
Epoch: [36]  [ 970/1251]  eta: 0:03:52  lr: 0.000015  loss: 3.6092 (3.4018)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [36]  [ 980/1251]  eta: 0:03:44  lr: 0.000015  loss: 3.6111 (3.4016)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [36]  [ 990/1251]  eta: 0:03:35  lr: 0.000015  loss: 3.6111 (3.4033)  time: 0.7981  data: 0.0005  max mem: 19734
Epoch: [36]  [1000/1251]  eta: 0:03:27  lr: 0.000015  loss: 3.5298 (3.4032)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [36]  [1010/1251]  eta: 0:03:19  lr: 0.000015  loss: 3.3601 (3.4020)  time: 0.8079  data: 0.0004  max mem: 19734
Epoch: [36]  [1020/1251]  eta: 0:03:10  lr: 0.000015  loss: 3.4308 (3.4022)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [36]  [1030/1251]  eta: 0:03:02  lr: 0.000015  loss: 3.4785 (3.4027)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [36]  [1040/1251]  eta: 0:02:54  lr: 0.000015  loss: 3.4622 (3.4017)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [36]  [1050/1251]  eta: 0:02:46  lr: 0.000015  loss: 3.4741 (3.4014)  time: 0.8106  data: 0.0004  max mem: 19734
Epoch: [36]  [1060/1251]  eta: 0:02:37  lr: 0.000015  loss: 3.5765 (3.4019)  time: 0.8032  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3970, ratio_loss=0.0048, pruning_loss=0.1286, mse_loss=0.5761
Epoch: [36]  [1070/1251]  eta: 0:02:29  lr: 0.000015  loss: 3.5799 (3.4020)  time: 0.8149  data: 0.0004  max mem: 19734
Epoch: [36]  [1080/1251]  eta: 0:02:21  lr: 0.000015  loss: 3.5799 (3.4028)  time: 0.8319  data: 0.0005  max mem: 19734
Epoch: [36]  [1090/1251]  eta: 0:02:12  lr: 0.000015  loss: 3.6283 (3.4029)  time: 0.8217  data: 0.0005  max mem: 19734
Epoch: [36]  [1100/1251]  eta: 0:02:04  lr: 0.000015  loss: 3.4834 (3.4021)  time: 0.8201  data: 0.0005  max mem: 19734
Epoch: [36]  [1110/1251]  eta: 0:01:56  lr: 0.000015  loss: 3.5005 (3.4036)  time: 0.8215  data: 0.0005  max mem: 19734
Epoch: [36]  [1120/1251]  eta: 0:01:48  lr: 0.000015  loss: 3.6412 (3.4041)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [36]  [1130/1251]  eta: 0:01:39  lr: 0.000015  loss: 3.4310 (3.4041)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [36]  [1140/1251]  eta: 0:01:31  lr: 0.000015  loss: 3.6476 (3.4066)  time: 0.8020  data: 0.0006  max mem: 19734
Epoch: [36]  [1150/1251]  eta: 0:01:23  lr: 0.000015  loss: 3.5670 (3.4071)  time: 0.8092  data: 0.0005  max mem: 19734
Epoch: [36]  [1160/1251]  eta: 0:01:15  lr: 0.000015  loss: 3.6705 (3.4109)  time: 0.8075  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4820, ratio_loss=0.0049, pruning_loss=0.1272, mse_loss=0.5583
Epoch: [36]  [1170/1251]  eta: 0:01:06  lr: 0.000015  loss: 3.4461 (3.4093)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [36]  [1180/1251]  eta: 0:00:58  lr: 0.000015  loss: 3.3259 (3.4096)  time: 0.7994  data: 0.0004  max mem: 19734
Epoch: [36]  [1190/1251]  eta: 0:00:50  lr: 0.000015  loss: 3.5193 (3.4097)  time: 0.8054  data: 0.0008  max mem: 19734
Epoch: [36]  [1200/1251]  eta: 0:00:42  lr: 0.000015  loss: 3.5051 (3.4093)  time: 0.8019  data: 0.0007  max mem: 19734
Epoch: [36]  [1210/1251]  eta: 0:00:33  lr: 0.000015  loss: 3.3057 (3.4075)  time: 0.8012  data: 0.0001  max mem: 19734
Epoch: [36]  [1220/1251]  eta: 0:00:25  lr: 0.000015  loss: 3.4548 (3.4089)  time: 0.8017  data: 0.0001  max mem: 19734
Epoch: [36]  [1230/1251]  eta: 0:00:17  lr: 0.000015  loss: 3.4739 (3.4091)  time: 0.8092  data: 0.0001  max mem: 19734
Epoch: [36]  [1240/1251]  eta: 0:00:09  lr: 0.000015  loss: 3.4266 (3.4086)  time: 0.8149  data: 0.0001  max mem: 19734
Epoch: [36]  [1250/1251]  eta: 0:00:00  lr: 0.000015  loss: 3.6411 (3.4109)  time: 0.8022  data: 0.0001  max mem: 19734
Epoch: [36] Total time: 0:17:10 (0.8241 s / it)
Averaged stats: lr: 0.000015  loss: 3.6411 (3.4127)
Test:  [  0/261]  eta: 1:30:39  loss: 0.7040 (0.7040)  acc1: 83.8542 (83.8542)  acc5: 97.3958 (97.3958)  time: 20.8393  data: 20.6703  max mem: 19734
Test:  [ 10/261]  eta: 0:11:20  loss: 0.7040 (0.7182)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.6383)  time: 2.7115  data: 2.5024  max mem: 19734
Test:  [ 20/261]  eta: 0:06:00  loss: 0.8955 (0.8976)  acc1: 80.7292 (79.1915)  acc5: 94.2708 (94.7669)  time: 0.5284  data: 0.3470  max mem: 19734
Test:  [ 30/261]  eta: 0:04:17  loss: 0.8049 (0.8175)  acc1: 82.8125 (82.0229)  acc5: 94.2708 (95.2117)  time: 0.2375  data: 0.0104  max mem: 19734
Test:  [ 40/261]  eta: 0:03:36  loss: 0.5808 (0.7806)  acc1: 87.5000 (82.9395)  acc5: 96.8750 (95.4903)  time: 0.4367  data: 0.1886  max mem: 19734
Test:  [ 50/261]  eta: 0:02:55  loss: 0.9166 (0.8418)  acc1: 78.1250 (81.1070)  acc5: 95.3125 (95.0980)  time: 0.3921  data: 0.1939  max mem: 19734
Test:  [ 60/261]  eta: 0:02:30  loss: 0.9702 (0.8489)  acc1: 77.0833 (80.7719)  acc5: 94.2708 (95.1674)  time: 0.2770  data: 0.0919  max mem: 19734
Test:  [ 70/261]  eta: 0:02:45  loss: 0.9201 (0.8488)  acc1: 78.6458 (80.3110)  acc5: 96.8750 (95.3712)  time: 0.9607  data: 0.7046  max mem: 19734
Test:  [ 80/261]  eta: 0:02:31  loss: 0.8061 (0.8489)  acc1: 80.7292 (80.4077)  acc5: 96.8750 (95.4733)  time: 1.0992  data: 0.7636  max mem: 19734
Test:  [ 90/261]  eta: 0:02:14  loss: 0.7998 (0.8364)  acc1: 83.3333 (80.7521)  acc5: 95.8333 (95.5243)  time: 0.5037  data: 0.1507  max mem: 19734
Test:  [100/261]  eta: 0:02:02  loss: 0.8149 (0.8389)  acc1: 84.8958 (80.7859)  acc5: 94.7917 (95.5600)  time: 0.4657  data: 0.0191  max mem: 19734
Test:  [110/261]  eta: 0:01:50  loss: 0.8627 (0.8615)  acc1: 78.1250 (80.2553)  acc5: 94.2708 (95.2937)  time: 0.4903  data: 0.0615  max mem: 19734
Test:  [120/261]  eta: 0:01:37  loss: 1.1562 (0.9012)  acc1: 72.3958 (79.2958)  acc5: 90.6250 (94.7615)  time: 0.3347  data: 0.0602  max mem: 19734
Test:  [130/261]  eta: 0:01:26  loss: 1.3412 (0.9478)  acc1: 68.7500 (78.3636)  acc5: 87.5000 (94.1436)  time: 0.2546  data: 0.0169  max mem: 19734
Test:  [140/261]  eta: 0:01:19  loss: 1.3012 (0.9746)  acc1: 68.7500 (77.7076)  acc5: 89.5833 (93.8460)  time: 0.4320  data: 0.1813  max mem: 19734
Test:  [150/261]  eta: 0:01:10  loss: 1.2566 (0.9798)  acc1: 72.9167 (77.7111)  acc5: 91.1458 (93.7121)  time: 0.4687  data: 0.1795  max mem: 19734
Test:  [160/261]  eta: 0:01:01  loss: 1.0208 (1.0010)  acc1: 77.6042 (77.3292)  acc5: 91.6667 (93.4071)  time: 0.2844  data: 0.0128  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.2700 (1.0300)  acc1: 65.6250 (76.6051)  acc5: 87.5000 (93.0891)  time: 0.4396  data: 0.2008  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: 1.3953 (1.0465)  acc1: 65.6250 (76.1654)  acc5: 89.0625 (92.9385)  time: 0.4745  data: 0.2033  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: 1.3491 (1.0597)  acc1: 67.7083 (75.9244)  acc5: 90.6250 (92.7629)  time: 0.2478  data: 0.0147  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.3491 (1.0756)  acc1: 71.3542 (75.5856)  acc5: 89.5833 (92.5218)  time: 0.4898  data: 0.2895  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.3321 (1.0887)  acc1: 68.7500 (75.3110)  acc5: 87.5000 (92.3208)  time: 0.4392  data: 0.2817  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.3802 (1.1085)  acc1: 67.1875 (74.8232)  acc5: 88.0208 (92.1215)  time: 0.1216  data: 0.0009  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4116 (1.1175)  acc1: 65.6250 (74.6054)  acc5: 89.5833 (92.0297)  time: 0.1183  data: 0.0009  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3281 (1.1267)  acc1: 67.1875 (74.4035)  acc5: 91.1458 (91.9519)  time: 0.1154  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0507 (1.1187)  acc1: 75.5208 (74.5746)  acc5: 93.7500 (92.0692)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9208 (1.1174)  acc1: 76.5625 (74.5840)  acc5: 95.8333 (92.1340)  time: 0.1114  data: 0.0001  max mem: 19734
Test: Total time: 0:02:04 (0.4778 s / it)
* Acc@1 74.584 Acc@5 92.134 loss 1.117
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.64%
Epoch: [37]  [   0/1251]  eta: 6:37:00  lr: 0.000015  loss: 3.2957 (3.2957)  time: 19.0415  data: 7.8618  max mem: 19734
Epoch: [37]  [  10/1251]  eta: 0:52:29  lr: 0.000015  loss: 3.8216 (3.5997)  time: 2.5378  data: 0.7159  max mem: 19734
loss info: cls_loss=3.4127, ratio_loss=0.0050, pruning_loss=0.1278, mse_loss=0.5580
Epoch: [37]  [  20/1251]  eta: 0:35:02  lr: 0.000015  loss: 3.4213 (3.3315)  time: 0.8414  data: 0.0010  max mem: 19734
Epoch: [37]  [  30/1251]  eta: 0:28:46  lr: 0.000015  loss: 3.4967 (3.4524)  time: 0.7961  data: 0.0007  max mem: 19734
Epoch: [37]  [  40/1251]  eta: 0:25:36  lr: 0.000015  loss: 3.5722 (3.4400)  time: 0.8068  data: 0.0005  max mem: 19734
Epoch: [37]  [  50/1251]  eta: 0:23:39  lr: 0.000015  loss: 3.5554 (3.4668)  time: 0.8213  data: 0.0005  max mem: 19734
Epoch: [37]  [  60/1251]  eta: 0:22:14  lr: 0.000015  loss: 3.5554 (3.4492)  time: 0.8171  data: 0.0006  max mem: 19734
Epoch: [37]  [  70/1251]  eta: 0:21:10  lr: 0.000015  loss: 3.3579 (3.4159)  time: 0.8048  data: 0.0006  max mem: 19734
Epoch: [37]  [  80/1251]  eta: 0:20:19  lr: 0.000015  loss: 3.6385 (3.4584)  time: 0.8018  data: 0.0006  max mem: 19734
Epoch: [37]  [  90/1251]  eta: 0:19:39  lr: 0.000015  loss: 3.6532 (3.4471)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [37]  [ 100/1251]  eta: 0:19:06  lr: 0.000015  loss: 3.3956 (3.4437)  time: 0.8128  data: 0.0006  max mem: 19734
Epoch: [37]  [ 110/1251]  eta: 0:18:39  lr: 0.000015  loss: 3.4139 (3.4450)  time: 0.8211  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4322, ratio_loss=0.0050, pruning_loss=0.1275, mse_loss=0.5830
Epoch: [37]  [ 120/1251]  eta: 0:18:12  lr: 0.000015  loss: 3.3350 (3.4502)  time: 0.8119  data: 0.0004  max mem: 19734
Epoch: [37]  [ 130/1251]  eta: 0:17:52  lr: 0.000015  loss: 3.5213 (3.4470)  time: 0.8246  data: 0.0005  max mem: 19734
Epoch: [37]  [ 140/1251]  eta: 0:17:30  lr: 0.000015  loss: 3.5213 (3.4467)  time: 0.8244  data: 0.0005  max mem: 19734
Epoch: [37]  [ 150/1251]  eta: 0:17:11  lr: 0.000015  loss: 3.3576 (3.4311)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [37]  [ 160/1251]  eta: 0:16:52  lr: 0.000015  loss: 3.0345 (3.4244)  time: 0.8053  data: 0.0005  max mem: 19734
Epoch: [37]  [ 170/1251]  eta: 0:16:35  lr: 0.000015  loss: 3.6303 (3.4339)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [37]  [ 180/1251]  eta: 0:16:19  lr: 0.000015  loss: 3.5374 (3.4305)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [37]  [ 190/1251]  eta: 0:16:04  lr: 0.000015  loss: 3.3103 (3.4202)  time: 0.8075  data: 0.0005  max mem: 19734
Epoch: [37]  [ 200/1251]  eta: 0:15:50  lr: 0.000015  loss: 3.4232 (3.4168)  time: 0.8082  data: 0.0005  max mem: 19734
Epoch: [37]  [ 210/1251]  eta: 0:15:36  lr: 0.000015  loss: 3.3883 (3.4121)  time: 0.8070  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3398, ratio_loss=0.0049, pruning_loss=0.1287, mse_loss=0.5968
Epoch: [37]  [ 220/1251]  eta: 0:15:22  lr: 0.000015  loss: 3.2345 (3.4087)  time: 0.8028  data: 0.0006  max mem: 19734
Epoch: [37]  [ 230/1251]  eta: 0:15:09  lr: 0.000015  loss: 3.1916 (3.4024)  time: 0.8031  data: 0.0006  max mem: 19734
Epoch: [37]  [ 240/1251]  eta: 0:14:57  lr: 0.000015  loss: 3.4287 (3.4016)  time: 0.8047  data: 0.0006  max mem: 19734
Epoch: [37]  [ 250/1251]  eta: 0:14:45  lr: 0.000015  loss: 3.4527 (3.3991)  time: 0.8121  data: 0.0005  max mem: 19734
Epoch: [37]  [ 260/1251]  eta: 0:14:34  lr: 0.000015  loss: 3.5974 (3.3967)  time: 0.8172  data: 0.0005  max mem: 19734
Epoch: [37]  [ 270/1251]  eta: 0:14:23  lr: 0.000015  loss: 3.4626 (3.3930)  time: 0.8211  data: 0.0005  max mem: 19734
Epoch: [37]  [ 280/1251]  eta: 0:14:12  lr: 0.000015  loss: 3.5880 (3.3968)  time: 0.8185  data: 0.0005  max mem: 19734
Epoch: [37]  [ 290/1251]  eta: 0:14:01  lr: 0.000015  loss: 3.7841 (3.4047)  time: 0.8114  data: 0.0004  max mem: 19734
Epoch: [37]  [ 300/1251]  eta: 0:13:50  lr: 0.000015  loss: 3.5903 (3.4030)  time: 0.8041  data: 0.0004  max mem: 19734
Epoch: [37]  [ 310/1251]  eta: 0:13:39  lr: 0.000015  loss: 3.5351 (3.4062)  time: 0.8016  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3675, ratio_loss=0.0047, pruning_loss=0.1310, mse_loss=0.5878
Epoch: [37]  [ 320/1251]  eta: 0:13:28  lr: 0.000015  loss: 3.5539 (3.4077)  time: 0.8035  data: 0.0004  max mem: 19734
Epoch: [37]  [ 330/1251]  eta: 0:13:18  lr: 0.000015  loss: 3.5539 (3.4025)  time: 0.8073  data: 0.0004  max mem: 19734
Epoch: [37]  [ 340/1251]  eta: 0:13:07  lr: 0.000015  loss: 3.2759 (3.3988)  time: 0.8075  data: 0.0004  max mem: 19734
Epoch: [37]  [ 350/1251]  eta: 0:12:58  lr: 0.000015  loss: 3.4811 (3.4036)  time: 0.8100  data: 0.0004  max mem: 19734
Epoch: [37]  [ 360/1251]  eta: 0:12:47  lr: 0.000015  loss: 3.6154 (3.4078)  time: 0.8113  data: 0.0005  max mem: 19734
Epoch: [37]  [ 370/1251]  eta: 0:12:38  lr: 0.000015  loss: 3.5489 (3.4037)  time: 0.8061  data: 0.0004  max mem: 19734
Epoch: [37]  [ 380/1251]  eta: 0:12:28  lr: 0.000015  loss: 3.3988 (3.4018)  time: 0.8047  data: 0.0004  max mem: 19734
Epoch: [37]  [ 390/1251]  eta: 0:12:18  lr: 0.000015  loss: 3.3988 (3.4038)  time: 0.8122  data: 0.0005  max mem: 19734
Epoch: [37]  [ 400/1251]  eta: 0:12:08  lr: 0.000015  loss: 3.3110 (3.3967)  time: 0.8106  data: 0.0005  max mem: 19734
Epoch: [37]  [ 410/1251]  eta: 0:11:59  lr: 0.000015  loss: 3.2981 (3.3963)  time: 0.8082  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3538, ratio_loss=0.0047, pruning_loss=0.1292, mse_loss=0.6055
Epoch: [37]  [ 420/1251]  eta: 0:11:50  lr: 0.000015  loss: 3.0854 (3.3882)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [37]  [ 430/1251]  eta: 0:11:40  lr: 0.000015  loss: 3.5505 (3.3922)  time: 0.8092  data: 0.0006  max mem: 19734
Epoch: [37]  [ 440/1251]  eta: 0:11:31  lr: 0.000015  loss: 3.7306 (3.3924)  time: 0.8012  data: 0.0006  max mem: 19734
Epoch: [37]  [ 450/1251]  eta: 0:11:21  lr: 0.000015  loss: 3.3878 (3.3890)  time: 0.8013  data: 0.0006  max mem: 19734
Epoch: [37]  [ 460/1251]  eta: 0:11:12  lr: 0.000015  loss: 3.0689 (3.3825)  time: 0.7990  data: 0.0006  max mem: 19734
Epoch: [37]  [ 470/1251]  eta: 0:11:03  lr: 0.000015  loss: 3.0689 (3.3813)  time: 0.8034  data: 0.0007  max mem: 19734
Epoch: [37]  [ 480/1251]  eta: 0:10:53  lr: 0.000015  loss: 3.6505 (3.3868)  time: 0.8054  data: 0.0007  max mem: 19734
Epoch: [37]  [ 490/1251]  eta: 0:10:44  lr: 0.000015  loss: 3.5920 (3.3908)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [37]  [ 500/1251]  eta: 0:10:35  lr: 0.000015  loss: 3.5920 (3.3950)  time: 0.8050  data: 0.0007  max mem: 19734
Epoch: [37]  [ 510/1251]  eta: 0:10:26  lr: 0.000015  loss: 3.7453 (3.3998)  time: 0.8052  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3997, ratio_loss=0.0049, pruning_loss=0.1279, mse_loss=0.5749
Epoch: [37]  [ 520/1251]  eta: 0:10:17  lr: 0.000015  loss: 3.7254 (3.3987)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [37]  [ 530/1251]  eta: 0:10:08  lr: 0.000015  loss: 3.6402 (3.4023)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [37]  [ 540/1251]  eta: 0:09:59  lr: 0.000015  loss: 3.6907 (3.4070)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [37]  [ 550/1251]  eta: 0:09:50  lr: 0.000015  loss: 3.5591 (3.4087)  time: 0.8180  data: 0.0005  max mem: 19734
Epoch: [37]  [ 560/1251]  eta: 0:09:41  lr: 0.000015  loss: 3.4531 (3.4087)  time: 0.8091  data: 0.0005  max mem: 19734
Epoch: [37]  [ 570/1251]  eta: 0:09:33  lr: 0.000015  loss: 3.4422 (3.4066)  time: 0.8084  data: 0.0004  max mem: 19734
Epoch: [37]  [ 580/1251]  eta: 0:09:24  lr: 0.000015  loss: 3.4074 (3.4060)  time: 0.8086  data: 0.0004  max mem: 19734
Epoch: [37]  [ 590/1251]  eta: 0:09:15  lr: 0.000015  loss: 3.4910 (3.4034)  time: 0.8011  data: 0.0004  max mem: 19734
Epoch: [37]  [ 600/1251]  eta: 0:09:06  lr: 0.000015  loss: 3.6973 (3.4080)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [37]  [ 610/1251]  eta: 0:08:57  lr: 0.000015  loss: 3.5568 (3.4049)  time: 0.8010  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3981, ratio_loss=0.0048, pruning_loss=0.1280, mse_loss=0.5992
Epoch: [37]  [ 620/1251]  eta: 0:08:49  lr: 0.000015  loss: 3.3566 (3.4030)  time: 0.8088  data: 0.0004  max mem: 19734
Epoch: [37]  [ 630/1251]  eta: 0:08:40  lr: 0.000015  loss: 3.2105 (3.3984)  time: 0.8086  data: 0.0004  max mem: 19734
Epoch: [37]  [ 640/1251]  eta: 0:08:31  lr: 0.000015  loss: 3.5963 (3.4021)  time: 0.8056  data: 0.0004  max mem: 19734
Epoch: [37]  [ 650/1251]  eta: 0:08:23  lr: 0.000015  loss: 3.6603 (3.4050)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [37]  [ 660/1251]  eta: 0:08:14  lr: 0.000015  loss: 3.6605 (3.4090)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [37]  [ 670/1251]  eta: 0:08:05  lr: 0.000015  loss: 3.6654 (3.4112)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [37]  [ 680/1251]  eta: 0:07:57  lr: 0.000015  loss: 3.5697 (3.4149)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [37]  [ 690/1251]  eta: 0:07:48  lr: 0.000015  loss: 3.5513 (3.4136)  time: 0.8060  data: 0.0006  max mem: 19734
Epoch: [37]  [ 700/1251]  eta: 0:07:39  lr: 0.000015  loss: 3.0220 (3.4095)  time: 0.8114  data: 0.0007  max mem: 19734
Epoch: [37]  [ 710/1251]  eta: 0:07:31  lr: 0.000015  loss: 3.1541 (3.4104)  time: 0.8131  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4430, ratio_loss=0.0048, pruning_loss=0.1278, mse_loss=0.5837
Epoch: [37]  [ 720/1251]  eta: 0:07:22  lr: 0.000015  loss: 3.7276 (3.4153)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [37]  [ 730/1251]  eta: 0:07:14  lr: 0.000015  loss: 3.8016 (3.4172)  time: 0.7991  data: 0.0006  max mem: 19734
Epoch: [37]  [ 740/1251]  eta: 0:07:05  lr: 0.000015  loss: 3.7533 (3.4193)  time: 0.7988  data: 0.0006  max mem: 19734
Epoch: [37]  [ 750/1251]  eta: 0:06:57  lr: 0.000015  loss: 3.5808 (3.4185)  time: 0.8006  data: 0.0006  max mem: 19734
Epoch: [37]  [ 760/1251]  eta: 0:06:48  lr: 0.000015  loss: 3.3941 (3.4200)  time: 0.8019  data: 0.0004  max mem: 19734
Epoch: [37]  [ 770/1251]  eta: 0:06:40  lr: 0.000015  loss: 3.4029 (3.4184)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [37]  [ 780/1251]  eta: 0:06:31  lr: 0.000015  loss: 3.1140 (3.4147)  time: 0.8085  data: 0.0005  max mem: 19734
Epoch: [37]  [ 790/1251]  eta: 0:06:23  lr: 0.000015  loss: 3.2612 (3.4151)  time: 0.8066  data: 0.0004  max mem: 19734
Epoch: [37]  [ 800/1251]  eta: 0:06:14  lr: 0.000015  loss: 3.5232 (3.4165)  time: 0.8091  data: 0.0004  max mem: 19734
Epoch: [37]  [ 810/1251]  eta: 0:06:06  lr: 0.000015  loss: 3.4961 (3.4149)  time: 0.7996  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4212, ratio_loss=0.0048, pruning_loss=0.1277, mse_loss=0.5758
Epoch: [37]  [ 820/1251]  eta: 0:05:57  lr: 0.000015  loss: 3.5308 (3.4156)  time: 0.7992  data: 0.0004  max mem: 19734
Epoch: [37]  [ 830/1251]  eta: 0:05:49  lr: 0.000015  loss: 3.5838 (3.4161)  time: 0.8075  data: 0.0004  max mem: 19734
Epoch: [37]  [ 840/1251]  eta: 0:05:41  lr: 0.000015  loss: 3.0161 (3.4099)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [37]  [ 850/1251]  eta: 0:05:32  lr: 0.000015  loss: 3.3656 (3.4115)  time: 0.8055  data: 0.0006  max mem: 19734
Epoch: [37]  [ 860/1251]  eta: 0:05:24  lr: 0.000015  loss: 3.4278 (3.4115)  time: 0.8234  data: 0.0005  max mem: 19734
Epoch: [37]  [ 870/1251]  eta: 0:05:15  lr: 0.000015  loss: 3.4432 (3.4117)  time: 0.8170  data: 0.0006  max mem: 19734
Epoch: [37]  [ 880/1251]  eta: 0:05:07  lr: 0.000015  loss: 3.4655 (3.4121)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [37]  [ 890/1251]  eta: 0:04:59  lr: 0.000015  loss: 3.4195 (3.4117)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [37]  [ 900/1251]  eta: 0:04:50  lr: 0.000015  loss: 3.3945 (3.4124)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [37]  [ 910/1251]  eta: 0:04:42  lr: 0.000015  loss: 3.3328 (3.4081)  time: 0.8090  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3320, ratio_loss=0.0051, pruning_loss=0.1289, mse_loss=0.5742
Epoch: [37]  [ 920/1251]  eta: 0:04:34  lr: 0.000015  loss: 2.9720 (3.4071)  time: 0.8091  data: 0.0006  max mem: 19734
Epoch: [37]  [ 930/1251]  eta: 0:04:25  lr: 0.000015  loss: 3.5660 (3.4091)  time: 0.7992  data: 0.0006  max mem: 19734
Epoch: [37]  [ 940/1251]  eta: 0:04:17  lr: 0.000015  loss: 3.6423 (3.4120)  time: 0.8077  data: 0.0008  max mem: 19734
Epoch: [37]  [ 950/1251]  eta: 0:04:08  lr: 0.000015  loss: 3.6286 (3.4122)  time: 0.8083  data: 0.0008  max mem: 19734
Epoch: [37]  [ 960/1251]  eta: 0:04:00  lr: 0.000015  loss: 3.5397 (3.4119)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [37]  [ 970/1251]  eta: 0:03:52  lr: 0.000015  loss: 3.5156 (3.4132)  time: 0.7985  data: 0.0006  max mem: 19734
Epoch: [37]  [ 980/1251]  eta: 0:03:43  lr: 0.000015  loss: 3.4344 (3.4128)  time: 0.8088  data: 0.0005  max mem: 19734
Epoch: [37]  [ 990/1251]  eta: 0:03:35  lr: 0.000015  loss: 3.6148 (3.4150)  time: 0.8166  data: 0.0004  max mem: 19734
Epoch: [37]  [1000/1251]  eta: 0:03:27  lr: 0.000015  loss: 3.6071 (3.4151)  time: 0.8138  data: 0.0005  max mem: 19734
Epoch: [37]  [1010/1251]  eta: 0:03:19  lr: 0.000015  loss: 3.4448 (3.4140)  time: 0.8178  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4553, ratio_loss=0.0046, pruning_loss=0.1261, mse_loss=0.5643
Epoch: [37]  [1020/1251]  eta: 0:03:10  lr: 0.000015  loss: 3.5336 (3.4150)  time: 0.8107  data: 0.0005  max mem: 19734
Epoch: [37]  [1030/1251]  eta: 0:03:02  lr: 0.000015  loss: 3.5522 (3.4145)  time: 0.7991  data: 0.0005  max mem: 19734
Epoch: [37]  [1040/1251]  eta: 0:02:54  lr: 0.000015  loss: 3.6384 (3.4155)  time: 0.7994  data: 0.0003  max mem: 19734
Epoch: [37]  [1050/1251]  eta: 0:02:45  lr: 0.000015  loss: 3.4770 (3.4140)  time: 0.7992  data: 0.0004  max mem: 19734
Epoch: [37]  [1060/1251]  eta: 0:02:37  lr: 0.000015  loss: 3.3107 (3.4139)  time: 0.8067  data: 0.0004  max mem: 19734
Epoch: [37]  [1070/1251]  eta: 0:02:29  lr: 0.000015  loss: 3.7038 (3.4169)  time: 0.8085  data: 0.0004  max mem: 19734
Epoch: [37]  [1080/1251]  eta: 0:02:21  lr: 0.000015  loss: 3.7038 (3.4187)  time: 0.8022  data: 0.0004  max mem: 19734
Epoch: [37]  [1090/1251]  eta: 0:02:12  lr: 0.000015  loss: 3.4895 (3.4175)  time: 0.8041  data: 0.0004  max mem: 19734
Epoch: [37]  [1100/1251]  eta: 0:02:04  lr: 0.000015  loss: 3.2554 (3.4176)  time: 0.8041  data: 0.0004  max mem: 19734
Epoch: [37]  [1110/1251]  eta: 0:01:56  lr: 0.000015  loss: 3.4163 (3.4186)  time: 0.8016  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4375, ratio_loss=0.0053, pruning_loss=0.1274, mse_loss=0.5724
Epoch: [37]  [1120/1251]  eta: 0:01:47  lr: 0.000015  loss: 3.4787 (3.4177)  time: 0.8139  data: 0.0004  max mem: 19734
Epoch: [37]  [1130/1251]  eta: 0:01:39  lr: 0.000015  loss: 3.7231 (3.4201)  time: 0.8116  data: 0.0004  max mem: 19734
Epoch: [37]  [1140/1251]  eta: 0:01:31  lr: 0.000015  loss: 3.7231 (3.4198)  time: 0.8156  data: 0.0004  max mem: 19734
Epoch: [37]  [1150/1251]  eta: 0:01:23  lr: 0.000015  loss: 3.5351 (3.4215)  time: 0.8303  data: 0.0004  max mem: 19734
Epoch: [37]  [1160/1251]  eta: 0:01:14  lr: 0.000015  loss: 3.5691 (3.4206)  time: 0.8128  data: 0.0004  max mem: 19734
Epoch: [37]  [1170/1251]  eta: 0:01:06  lr: 0.000015  loss: 3.4302 (3.4192)  time: 0.7999  data: 0.0003  max mem: 19734
Epoch: [37]  [1180/1251]  eta: 0:00:58  lr: 0.000015  loss: 3.4733 (3.4202)  time: 0.8005  data: 0.0004  max mem: 19734
Epoch: [37]  [1190/1251]  eta: 0:00:50  lr: 0.000015  loss: 3.8053 (3.4225)  time: 0.7997  data: 0.0011  max mem: 19734
Epoch: [37]  [1200/1251]  eta: 0:00:41  lr: 0.000015  loss: 3.5777 (3.4220)  time: 0.7965  data: 0.0010  max mem: 19734
Epoch: [37]  [1210/1251]  eta: 0:00:33  lr: 0.000015  loss: 3.3026 (3.4210)  time: 0.7983  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4278, ratio_loss=0.0048, pruning_loss=0.1276, mse_loss=0.5685
Epoch: [37]  [1220/1251]  eta: 0:00:25  lr: 0.000015  loss: 3.3026 (3.4203)  time: 0.7971  data: 0.0001  max mem: 19734
Epoch: [37]  [1230/1251]  eta: 0:00:17  lr: 0.000015  loss: 3.3107 (3.4191)  time: 0.8010  data: 0.0001  max mem: 19734
Epoch: [37]  [1240/1251]  eta: 0:00:09  lr: 0.000015  loss: 3.5374 (3.4200)  time: 0.7987  data: 0.0001  max mem: 19734
Epoch: [37]  [1250/1251]  eta: 0:00:00  lr: 0.000015  loss: 3.6443 (3.4209)  time: 0.7893  data: 0.0001  max mem: 19734
Epoch: [37] Total time: 0:17:09 (0.8227 s / it)
Averaged stats: lr: 0.000015  loss: 3.6443 (3.4111)
Test:  [  0/261]  eta: 3:04:57  loss: 0.6915 (0.6915)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 42.5178  data: 42.1848  max mem: 19734
Test:  [ 10/261]  eta: 0:17:44  loss: 0.6915 (0.7257)  acc1: 84.3750 (83.9489)  acc5: 96.8750 (96.2595)  time: 4.2422  data: 3.8554  max mem: 19734
Test:  [ 20/261]  eta: 0:09:10  loss: 0.8820 (0.8994)  acc1: 78.1250 (78.9931)  acc5: 93.7500 (94.7173)  time: 0.2724  data: 0.0162  max mem: 19734
Test:  [ 30/261]  eta: 0:06:11  loss: 0.7995 (0.8143)  acc1: 83.3333 (81.8212)  acc5: 94.7917 (95.2621)  time: 0.1580  data: 0.0128  max mem: 19734
Test:  [ 40/261]  eta: 0:05:18  loss: 0.5645 (0.7804)  acc1: 88.5417 (82.7871)  acc5: 96.3542 (95.5412)  time: 0.5585  data: 0.2898  max mem: 19734
Test:  [ 50/261]  eta: 0:04:11  loss: 0.9277 (0.8441)  acc1: 77.6042 (80.8824)  acc5: 95.3125 (95.1287)  time: 0.5532  data: 0.2843  max mem: 19734
Test:  [ 60/261]  eta: 0:03:29  loss: 0.9752 (0.8511)  acc1: 76.5625 (80.5926)  acc5: 94.7917 (95.1503)  time: 0.2294  data: 0.0078  max mem: 19734
Test:  [ 70/261]  eta: 0:03:11  loss: 0.8733 (0.8552)  acc1: 78.6458 (80.1643)  acc5: 95.8333 (95.3565)  time: 0.5176  data: 0.1842  max mem: 19734
Test:  [ 80/261]  eta: 0:02:45  loss: 0.8305 (0.8569)  acc1: 79.1667 (80.2533)  acc5: 96.8750 (95.4604)  time: 0.5183  data: 0.2158  max mem: 19734
Test:  [ 90/261]  eta: 0:02:32  loss: 0.8117 (0.8434)  acc1: 83.8542 (80.6319)  acc5: 96.3542 (95.5472)  time: 0.5033  data: 0.3014  max mem: 19734
Test:  [100/261]  eta: 0:02:18  loss: 0.8281 (0.8460)  acc1: 83.8542 (80.6312)  acc5: 95.3125 (95.6168)  time: 0.6446  data: 0.4579  max mem: 19734
Test:  [110/261]  eta: 0:02:03  loss: 0.8694 (0.8686)  acc1: 76.0417 (80.1520)  acc5: 94.7917 (95.3078)  time: 0.4895  data: 0.2313  max mem: 19734
Test:  [120/261]  eta: 0:01:52  loss: 1.2287 (0.9092)  acc1: 70.3125 (79.1796)  acc5: 90.1042 (94.7831)  time: 0.4861  data: 0.0445  max mem: 19734
Test:  [130/261]  eta: 0:01:41  loss: 1.4223 (0.9553)  acc1: 66.6667 (78.2363)  acc5: 87.5000 (94.1953)  time: 0.5342  data: 0.1680  max mem: 19734
Test:  [140/261]  eta: 0:01:30  loss: 1.2761 (0.9809)  acc1: 66.6667 (77.5451)  acc5: 89.5833 (93.9421)  time: 0.4391  data: 0.1788  max mem: 19734
Test:  [150/261]  eta: 0:01:20  loss: 1.1990 (0.9855)  acc1: 72.9167 (77.5662)  acc5: 91.1458 (93.7983)  time: 0.3730  data: 0.0269  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: 1.0559 (1.0066)  acc1: 77.6042 (77.2063)  acc5: 92.1875 (93.4912)  time: 0.3731  data: 0.0582  max mem: 19734
Test:  [170/261]  eta: 0:01:01  loss: 1.2715 (1.0357)  acc1: 65.1042 (76.4498)  acc5: 88.5417 (93.1652)  time: 0.3228  data: 0.1259  max mem: 19734
Test:  [180/261]  eta: 0:00:52  loss: 1.4461 (1.0510)  acc1: 66.1458 (76.1079)  acc5: 89.0625 (93.0421)  time: 0.1975  data: 0.0781  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: 1.3895 (1.0649)  acc1: 66.6667 (75.8535)  acc5: 90.6250 (92.8638)  time: 0.1178  data: 0.0014  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: 1.3455 (1.0804)  acc1: 71.3542 (75.5312)  acc5: 89.0625 (92.6332)  time: 0.1155  data: 0.0005  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.3217 (1.0935)  acc1: 70.8333 (75.2617)  acc5: 88.5417 (92.4442)  time: 0.1155  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.3753 (1.1132)  acc1: 67.1875 (74.7408)  acc5: 88.5417 (92.2394)  time: 0.1155  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4133 (1.1220)  acc1: 65.6250 (74.5265)  acc5: 88.5417 (92.1537)  time: 0.1153  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3023 (1.1312)  acc1: 68.2292 (74.2976)  acc5: 91.1458 (92.0924)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0036 (1.1241)  acc1: 76.0417 (74.4501)  acc5: 93.7500 (92.1979)  time: 0.1152  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9332 (1.1232)  acc1: 77.6042 (74.4660)  acc5: 95.3125 (92.2500)  time: 0.1121  data: 0.0001  max mem: 19734
Test: Total time: 0:02:06 (0.4839 s / it)
* Acc@1 74.466 Acc@5 92.250 loss 1.123
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.64%
Epoch: [38]  [   0/1251]  eta: 6:20:16  lr: 0.000014  loss: 3.5139 (3.5139)  time: 18.2386  data: 8.5167  max mem: 19734
Epoch: [38]  [  10/1251]  eta: 0:55:41  lr: 0.000014  loss: 3.4374 (3.3533)  time: 2.6928  data: 0.8775  max mem: 19734
Epoch: [38]  [  20/1251]  eta: 0:36:57  lr: 0.000014  loss: 3.5410 (3.4538)  time: 0.9792  data: 0.0570  max mem: 19734
Epoch: [38]  [  30/1251]  eta: 0:30:21  lr: 0.000014  loss: 3.5597 (3.4103)  time: 0.8313  data: 0.0006  max mem: 19734
Epoch: [38]  [  40/1251]  eta: 0:26:49  lr: 0.000014  loss: 3.4879 (3.3915)  time: 0.8338  data: 0.0005  max mem: 19734
Epoch: [38]  [  50/1251]  eta: 0:24:30  lr: 0.000014  loss: 3.4613 (3.3779)  time: 0.8098  data: 0.0009  max mem: 19734
Epoch: [38]  [  60/1251]  eta: 0:22:55  lr: 0.000014  loss: 3.4410 (3.3999)  time: 0.7966  data: 0.0010  max mem: 19734
loss info: cls_loss=3.3944, ratio_loss=0.0048, pruning_loss=0.1268, mse_loss=0.6112
Epoch: [38]  [  70/1251]  eta: 0:21:45  lr: 0.000014  loss: 3.5588 (3.4230)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [38]  [  80/1251]  eta: 0:20:51  lr: 0.000014  loss: 3.4339 (3.3976)  time: 0.8071  data: 0.0005  max mem: 19734
Epoch: [38]  [  90/1251]  eta: 0:20:10  lr: 0.000014  loss: 3.4339 (3.3825)  time: 0.8207  data: 0.0006  max mem: 19734
Epoch: [38]  [ 100/1251]  eta: 0:19:32  lr: 0.000014  loss: 3.4811 (3.3819)  time: 0.8173  data: 0.0006  max mem: 19734
Epoch: [38]  [ 110/1251]  eta: 0:18:59  lr: 0.000014  loss: 3.6612 (3.3998)  time: 0.7984  data: 0.0006  max mem: 19734
Epoch: [38]  [ 120/1251]  eta: 0:18:31  lr: 0.000014  loss: 3.6280 (3.3974)  time: 0.7979  data: 0.0005  max mem: 19734
Epoch: [38]  [ 130/1251]  eta: 0:18:05  lr: 0.000014  loss: 3.6064 (3.4098)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [38]  [ 140/1251]  eta: 0:17:42  lr: 0.000014  loss: 3.5472 (3.3984)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [38]  [ 150/1251]  eta: 0:17:22  lr: 0.000014  loss: 3.5472 (3.4065)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [38]  [ 160/1251]  eta: 0:17:03  lr: 0.000014  loss: 3.6089 (3.4065)  time: 0.8087  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3950, ratio_loss=0.0046, pruning_loss=0.1269, mse_loss=0.5830
Epoch: [38]  [ 170/1251]  eta: 0:16:48  lr: 0.000014  loss: 3.6845 (3.4153)  time: 0.8295  data: 0.0007  max mem: 19734
Epoch: [38]  [ 180/1251]  eta: 0:16:33  lr: 0.000014  loss: 3.6304 (3.4289)  time: 0.8419  data: 0.0005  max mem: 19734
Epoch: [38]  [ 190/1251]  eta: 0:16:18  lr: 0.000014  loss: 3.5870 (3.4304)  time: 0.8305  data: 0.0004  max mem: 19734
Epoch: [38]  [ 200/1251]  eta: 0:16:03  lr: 0.000014  loss: 3.3546 (3.4250)  time: 0.8141  data: 0.0005  max mem: 19734
Epoch: [38]  [ 210/1251]  eta: 0:15:48  lr: 0.000014  loss: 3.5676 (3.4355)  time: 0.8071  data: 0.0005  max mem: 19734
Epoch: [38]  [ 220/1251]  eta: 0:15:34  lr: 0.000014  loss: 3.6651 (3.4437)  time: 0.8051  data: 0.0006  max mem: 19734
Epoch: [38]  [ 230/1251]  eta: 0:15:20  lr: 0.000014  loss: 3.5857 (3.4411)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [38]  [ 240/1251]  eta: 0:15:08  lr: 0.000014  loss: 3.5292 (3.4463)  time: 0.8136  data: 0.0006  max mem: 19734
Epoch: [38]  [ 250/1251]  eta: 0:14:55  lr: 0.000014  loss: 3.6287 (3.4539)  time: 0.8144  data: 0.0006  max mem: 19734
Epoch: [38]  [ 260/1251]  eta: 0:14:43  lr: 0.000014  loss: 3.8353 (3.4662)  time: 0.8023  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5425, ratio_loss=0.0046, pruning_loss=0.1239, mse_loss=0.6081
Epoch: [38]  [ 270/1251]  eta: 0:14:31  lr: 0.000014  loss: 3.5440 (3.4580)  time: 0.8036  data: 0.0007  max mem: 19734
Epoch: [38]  [ 280/1251]  eta: 0:14:19  lr: 0.000014  loss: 3.2010 (3.4509)  time: 0.8049  data: 0.0008  max mem: 19734
Epoch: [38]  [ 290/1251]  eta: 0:14:07  lr: 0.000014  loss: 3.3045 (3.4532)  time: 0.8054  data: 0.0007  max mem: 19734
Epoch: [38]  [ 300/1251]  eta: 0:13:57  lr: 0.000014  loss: 3.2564 (3.4459)  time: 0.8134  data: 0.0006  max mem: 19734
Epoch: [38]  [ 310/1251]  eta: 0:13:46  lr: 0.000014  loss: 3.2564 (3.4447)  time: 0.8175  data: 0.0005  max mem: 19734
Epoch: [38]  [ 320/1251]  eta: 0:13:36  lr: 0.000014  loss: 3.3715 (3.4388)  time: 0.8265  data: 0.0004  max mem: 19734
Epoch: [38]  [ 330/1251]  eta: 0:13:26  lr: 0.000014  loss: 3.5439 (3.4473)  time: 0.8377  data: 0.0005  max mem: 19734
Epoch: [38]  [ 340/1251]  eta: 0:13:15  lr: 0.000014  loss: 3.6489 (3.4477)  time: 0.8188  data: 0.0006  max mem: 19734
Epoch: [38]  [ 350/1251]  eta: 0:13:05  lr: 0.000014  loss: 3.4505 (3.4418)  time: 0.8029  data: 0.0006  max mem: 19734
Epoch: [38]  [ 360/1251]  eta: 0:12:55  lr: 0.000014  loss: 3.6639 (3.4451)  time: 0.8065  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3701, ratio_loss=0.0044, pruning_loss=0.1268, mse_loss=0.5921
Epoch: [38]  [ 370/1251]  eta: 0:12:44  lr: 0.000014  loss: 3.6806 (3.4465)  time: 0.8106  data: 0.0005  max mem: 19734
Epoch: [38]  [ 380/1251]  eta: 0:12:34  lr: 0.000014  loss: 3.6206 (3.4472)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [38]  [ 390/1251]  eta: 0:12:25  lr: 0.000014  loss: 3.3613 (3.4426)  time: 0.8201  data: 0.0005  max mem: 19734
Epoch: [38]  [ 400/1251]  eta: 0:12:15  lr: 0.000014  loss: 3.3979 (3.4438)  time: 0.8228  data: 0.0005  max mem: 19734
Epoch: [38]  [ 410/1251]  eta: 0:12:05  lr: 0.000014  loss: 3.5905 (3.4431)  time: 0.8080  data: 0.0006  max mem: 19734
Epoch: [38]  [ 420/1251]  eta: 0:11:55  lr: 0.000014  loss: 3.3846 (3.4421)  time: 0.8035  data: 0.0006  max mem: 19734
Epoch: [38]  [ 430/1251]  eta: 0:11:46  lr: 0.000014  loss: 3.3846 (3.4386)  time: 0.8065  data: 0.0006  max mem: 19734
Epoch: [38]  [ 440/1251]  eta: 0:11:36  lr: 0.000014  loss: 3.2625 (3.4323)  time: 0.8084  data: 0.0006  max mem: 19734
Epoch: [38]  [ 450/1251]  eta: 0:11:27  lr: 0.000014  loss: 3.4660 (3.4369)  time: 0.8103  data: 0.0006  max mem: 19734
Epoch: [38]  [ 460/1251]  eta: 0:11:18  lr: 0.000014  loss: 3.6466 (3.4346)  time: 0.8168  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3735, ratio_loss=0.0046, pruning_loss=0.1285, mse_loss=0.5704
Epoch: [38]  [ 470/1251]  eta: 0:11:09  lr: 0.000014  loss: 3.6072 (3.4389)  time: 0.8279  data: 0.0006  max mem: 19734
Epoch: [38]  [ 480/1251]  eta: 0:11:00  lr: 0.000014  loss: 3.8065 (3.4431)  time: 0.8290  data: 0.0005  max mem: 19734
Epoch: [38]  [ 490/1251]  eta: 0:10:50  lr: 0.000014  loss: 3.4886 (3.4394)  time: 0.8124  data: 0.0005  max mem: 19734
Epoch: [38]  [ 500/1251]  eta: 0:10:41  lr: 0.000014  loss: 3.4819 (3.4442)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [38]  [ 510/1251]  eta: 0:10:32  lr: 0.000014  loss: 3.5697 (3.4447)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [38]  [ 520/1251]  eta: 0:10:22  lr: 0.000014  loss: 3.5056 (3.4438)  time: 0.8042  data: 0.0005  max mem: 19734
Epoch: [38]  [ 530/1251]  eta: 0:10:13  lr: 0.000014  loss: 3.4738 (3.4420)  time: 0.8071  data: 0.0005  max mem: 19734
Epoch: [38]  [ 540/1251]  eta: 0:10:04  lr: 0.000014  loss: 3.3783 (3.4381)  time: 0.8135  data: 0.0005  max mem: 19734
Epoch: [38]  [ 550/1251]  eta: 0:09:55  lr: 0.000014  loss: 3.1560 (3.4353)  time: 0.8123  data: 0.0005  max mem: 19734
Epoch: [38]  [ 560/1251]  eta: 0:09:46  lr: 0.000014  loss: 3.1503 (3.4279)  time: 0.8083  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3719, ratio_loss=0.0046, pruning_loss=0.1281, mse_loss=0.5753
Epoch: [38]  [ 570/1251]  eta: 0:09:37  lr: 0.000014  loss: 3.2027 (3.4286)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [38]  [ 580/1251]  eta: 0:09:28  lr: 0.000014  loss: 3.5986 (3.4270)  time: 0.7975  data: 0.0005  max mem: 19734
Epoch: [38]  [ 590/1251]  eta: 0:09:19  lr: 0.000014  loss: 3.2502 (3.4300)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [38]  [ 600/1251]  eta: 0:09:10  lr: 0.000014  loss: 3.3355 (3.4283)  time: 0.8152  data: 0.0005  max mem: 19734
Epoch: [38]  [ 610/1251]  eta: 0:09:02  lr: 0.000014  loss: 3.4512 (3.4264)  time: 0.8294  data: 0.0005  max mem: 19734
Epoch: [38]  [ 620/1251]  eta: 0:08:53  lr: 0.000014  loss: 3.6070 (3.4321)  time: 0.8283  data: 0.0005  max mem: 19734
Epoch: [38]  [ 630/1251]  eta: 0:08:44  lr: 0.000014  loss: 3.5953 (3.4336)  time: 0.8174  data: 0.0006  max mem: 19734
Epoch: [38]  [ 640/1251]  eta: 0:08:36  lr: 0.000014  loss: 3.2630 (3.4292)  time: 0.8086  data: 0.0006  max mem: 19734
Epoch: [38]  [ 650/1251]  eta: 0:08:27  lr: 0.000014  loss: 3.4708 (3.4314)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [38]  [ 660/1251]  eta: 0:08:18  lr: 0.000014  loss: 3.4892 (3.4249)  time: 0.8042  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4012, ratio_loss=0.0047, pruning_loss=0.1286, mse_loss=0.5722
Epoch: [38]  [ 670/1251]  eta: 0:08:09  lr: 0.000014  loss: 3.3038 (3.4246)  time: 0.8159  data: 0.0005  max mem: 19734
Epoch: [38]  [ 680/1251]  eta: 0:08:01  lr: 0.000014  loss: 3.5677 (3.4272)  time: 0.8184  data: 0.0005  max mem: 19734
Epoch: [38]  [ 690/1251]  eta: 0:07:52  lr: 0.000014  loss: 3.5677 (3.4273)  time: 0.8057  data: 0.0004  max mem: 19734
Epoch: [38]  [ 700/1251]  eta: 0:07:43  lr: 0.000014  loss: 3.6484 (3.4328)  time: 0.8013  data: 0.0004  max mem: 19734
Epoch: [38]  [ 710/1251]  eta: 0:07:34  lr: 0.000014  loss: 3.6971 (3.4355)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [38]  [ 720/1251]  eta: 0:07:26  lr: 0.000014  loss: 3.5600 (3.4329)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [38]  [ 730/1251]  eta: 0:07:17  lr: 0.000014  loss: 3.5284 (3.4305)  time: 0.8003  data: 0.0005  max mem: 19734
Epoch: [38]  [ 740/1251]  eta: 0:07:09  lr: 0.000014  loss: 3.4029 (3.4302)  time: 0.8139  data: 0.0005  max mem: 19734
Epoch: [38]  [ 750/1251]  eta: 0:07:00  lr: 0.000014  loss: 3.8211 (3.4360)  time: 0.8226  data: 0.0004  max mem: 19734
Epoch: [38]  [ 760/1251]  eta: 0:06:51  lr: 0.000014  loss: 3.6056 (3.4335)  time: 0.8191  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4585, ratio_loss=0.0047, pruning_loss=0.1259, mse_loss=0.5629
Epoch: [38]  [ 770/1251]  eta: 0:06:43  lr: 0.000014  loss: 3.2175 (3.4302)  time: 0.8310  data: 0.0005  max mem: 19734
Epoch: [38]  [ 780/1251]  eta: 0:06:34  lr: 0.000014  loss: 2.9964 (3.4273)  time: 0.8217  data: 0.0005  max mem: 19734
Epoch: [38]  [ 790/1251]  eta: 0:06:26  lr: 0.000014  loss: 3.3361 (3.4260)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [38]  [ 800/1251]  eta: 0:06:17  lr: 0.000014  loss: 3.5182 (3.4293)  time: 0.8007  data: 0.0007  max mem: 19734
Epoch: [38]  [ 810/1251]  eta: 0:06:09  lr: 0.000014  loss: 3.5864 (3.4280)  time: 0.8025  data: 0.0007  max mem: 19734
Epoch: [38]  [ 820/1251]  eta: 0:06:00  lr: 0.000014  loss: 3.5275 (3.4272)  time: 0.8131  data: 0.0005  max mem: 19734
Epoch: [38]  [ 830/1251]  eta: 0:05:52  lr: 0.000014  loss: 3.5712 (3.4283)  time: 0.8175  data: 0.0005  max mem: 19734
Epoch: [38]  [ 840/1251]  eta: 0:05:43  lr: 0.000014  loss: 3.7054 (3.4303)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [38]  [ 850/1251]  eta: 0:05:35  lr: 0.000014  loss: 3.7331 (3.4303)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [38]  [ 860/1251]  eta: 0:05:26  lr: 0.000014  loss: 3.6188 (3.4327)  time: 0.8018  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4247, ratio_loss=0.0047, pruning_loss=0.1285, mse_loss=0.5879
Epoch: [38]  [ 870/1251]  eta: 0:05:18  lr: 0.000014  loss: 3.6188 (3.4325)  time: 0.8047  data: 0.0006  max mem: 19734
Epoch: [38]  [ 880/1251]  eta: 0:05:09  lr: 0.000014  loss: 3.3918 (3.4309)  time: 0.8051  data: 0.0007  max mem: 19734
Epoch: [38]  [ 890/1251]  eta: 0:05:01  lr: 0.000014  loss: 3.3918 (3.4290)  time: 0.8105  data: 0.0007  max mem: 19734
Epoch: [38]  [ 900/1251]  eta: 0:04:52  lr: 0.000014  loss: 3.6264 (3.4331)  time: 0.8193  data: 0.0005  max mem: 19734
Epoch: [38]  [ 910/1251]  eta: 0:04:44  lr: 0.000014  loss: 3.7399 (3.4356)  time: 0.8209  data: 0.0005  max mem: 19734
Epoch: [38]  [ 920/1251]  eta: 0:04:36  lr: 0.000014  loss: 3.5210 (3.4347)  time: 0.8272  data: 0.0005  max mem: 19734
Epoch: [38]  [ 930/1251]  eta: 0:04:27  lr: 0.000014  loss: 3.4975 (3.4344)  time: 0.8148  data: 0.0005  max mem: 19734
Epoch: [38]  [ 940/1251]  eta: 0:04:19  lr: 0.000014  loss: 3.5106 (3.4332)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [38]  [ 950/1251]  eta: 0:04:10  lr: 0.000014  loss: 3.5515 (3.4335)  time: 0.8010  data: 0.0006  max mem: 19734
Epoch: [38]  [ 960/1251]  eta: 0:04:02  lr: 0.000014  loss: 3.3660 (3.4326)  time: 0.8019  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4128, ratio_loss=0.0051, pruning_loss=0.1278, mse_loss=0.5752
Epoch: [38]  [ 970/1251]  eta: 0:03:54  lr: 0.000014  loss: 3.4636 (3.4348)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [38]  [ 980/1251]  eta: 0:03:45  lr: 0.000014  loss: 3.5188 (3.4348)  time: 0.8132  data: 0.0005  max mem: 19734
Epoch: [38]  [ 990/1251]  eta: 0:03:37  lr: 0.000014  loss: 3.2808 (3.4331)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [38]  [1000/1251]  eta: 0:03:28  lr: 0.000014  loss: 3.2151 (3.4333)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [38]  [1010/1251]  eta: 0:03:20  lr: 0.000014  loss: 3.2151 (3.4304)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [38]  [1020/1251]  eta: 0:03:12  lr: 0.000014  loss: 3.3467 (3.4318)  time: 0.8021  data: 0.0005  max mem: 19734
Epoch: [38]  [1030/1251]  eta: 0:03:03  lr: 0.000014  loss: 3.5165 (3.4311)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [38]  [1040/1251]  eta: 0:02:55  lr: 0.000014  loss: 3.4682 (3.4311)  time: 0.8180  data: 0.0005  max mem: 19734
Epoch: [38]  [1050/1251]  eta: 0:02:47  lr: 0.000014  loss: 3.4682 (3.4317)  time: 0.8312  data: 0.0006  max mem: 19734
Epoch: [38]  [1060/1251]  eta: 0:02:38  lr: 0.000014  loss: 3.6212 (3.4309)  time: 0.8345  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3788, ratio_loss=0.0048, pruning_loss=0.1291, mse_loss=0.5921
Epoch: [38]  [1070/1251]  eta: 0:02:30  lr: 0.000014  loss: 3.6212 (3.4307)  time: 0.8244  data: 0.0005  max mem: 19734
Epoch: [38]  [1080/1251]  eta: 0:02:22  lr: 0.000014  loss: 3.7890 (3.4319)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [38]  [1090/1251]  eta: 0:02:13  lr: 0.000014  loss: 3.6717 (3.4327)  time: 0.8019  data: 0.0006  max mem: 19734
Epoch: [38]  [1100/1251]  eta: 0:02:05  lr: 0.000014  loss: 3.4230 (3.4309)  time: 0.8042  data: 0.0006  max mem: 19734
Epoch: [38]  [1110/1251]  eta: 0:01:57  lr: 0.000014  loss: 3.4026 (3.4316)  time: 0.8099  data: 0.0005  max mem: 19734
Epoch: [38]  [1120/1251]  eta: 0:01:48  lr: 0.000014  loss: 3.3245 (3.4291)  time: 0.8229  data: 0.0005  max mem: 19734
Epoch: [38]  [1130/1251]  eta: 0:01:40  lr: 0.000014  loss: 3.3245 (3.4275)  time: 0.8151  data: 0.0006  max mem: 19734
Epoch: [38]  [1140/1251]  eta: 0:01:32  lr: 0.000014  loss: 3.5206 (3.4281)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [38]  [1150/1251]  eta: 0:01:23  lr: 0.000014  loss: 3.5504 (3.4301)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [38]  [1160/1251]  eta: 0:01:15  lr: 0.000014  loss: 3.4548 (3.4292)  time: 0.8024  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3944, ratio_loss=0.0049, pruning_loss=0.1296, mse_loss=0.5809
Epoch: [38]  [1170/1251]  eta: 0:01:07  lr: 0.000014  loss: 3.3628 (3.4288)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [38]  [1180/1251]  eta: 0:00:58  lr: 0.000014  loss: 3.0829 (3.4256)  time: 0.8141  data: 0.0007  max mem: 19734
Epoch: [38]  [1190/1251]  eta: 0:00:50  lr: 0.000014  loss: 3.1440 (3.4261)  time: 0.8258  data: 0.0012  max mem: 19734
Epoch: [38]  [1200/1251]  eta: 0:00:42  lr: 0.000014  loss: 3.5624 (3.4246)  time: 0.8189  data: 0.0009  max mem: 19734
Epoch: [38]  [1210/1251]  eta: 0:00:33  lr: 0.000014  loss: 3.6372 (3.4254)  time: 0.8111  data: 0.0002  max mem: 19734
Epoch: [38]  [1220/1251]  eta: 0:00:25  lr: 0.000014  loss: 3.6603 (3.4269)  time: 0.8032  data: 0.0002  max mem: 19734
Epoch: [38]  [1230/1251]  eta: 0:00:17  lr: 0.000014  loss: 3.6614 (3.4275)  time: 0.7925  data: 0.0001  max mem: 19734
Epoch: [38]  [1240/1251]  eta: 0:00:09  lr: 0.000014  loss: 3.5376 (3.4278)  time: 0.7931  data: 0.0002  max mem: 19734
Epoch: [38]  [1250/1251]  eta: 0:00:00  lr: 0.000014  loss: 3.4253 (3.4259)  time: 0.7934  data: 0.0002  max mem: 19734
Epoch: [38] Total time: 0:17:16 (0.8285 s / it)
Averaged stats: lr: 0.000014  loss: 3.4253 (3.4104)
Test:  [  0/261]  eta: 2:23:36  loss: 0.6576 (0.6576)  acc1: 83.8542 (83.8542)  acc5: 96.3542 (96.3542)  time: 33.0151  data: 32.8668  max mem: 19734
Test:  [ 10/261]  eta: 0:17:37  loss: 0.6576 (0.7254)  acc1: 85.4167 (83.5227)  acc5: 96.8750 (96.4962)  time: 4.2131  data: 4.0014  max mem: 19734
Test:  [ 20/261]  eta: 0:09:28  loss: 0.9188 (0.9016)  acc1: 78.1250 (78.6458)  acc5: 94.2708 (94.8413)  time: 0.8252  data: 0.5657  max mem: 19734
Test:  [ 30/261]  eta: 0:06:35  loss: 0.8106 (0.8204)  acc1: 82.8125 (81.6364)  acc5: 94.7917 (95.1949)  time: 0.3333  data: 0.0183  max mem: 19734
Test:  [ 40/261]  eta: 0:05:58  loss: 0.5908 (0.7839)  acc1: 88.5417 (82.8379)  acc5: 96.8750 (95.5285)  time: 0.8498  data: 0.5170  max mem: 19734
Test:  [ 50/261]  eta: 0:04:53  loss: 0.8991 (0.8443)  acc1: 78.1250 (81.0355)  acc5: 95.3125 (95.0776)  time: 0.8989  data: 0.5178  max mem: 19734
Test:  [ 60/261]  eta: 0:04:03  loss: 0.9888 (0.8510)  acc1: 75.5208 (80.6523)  acc5: 94.2708 (95.0990)  time: 0.3700  data: 0.0185  max mem: 19734
Test:  [ 70/261]  eta: 0:03:35  loss: 0.9178 (0.8525)  acc1: 76.5625 (80.3477)  acc5: 96.3542 (95.2758)  time: 0.4484  data: 0.0177  max mem: 19734
Test:  [ 80/261]  eta: 0:03:08  loss: 0.8118 (0.8532)  acc1: 79.1667 (80.4591)  acc5: 96.8750 (95.3832)  time: 0.5278  data: 0.0155  max mem: 19734
Test:  [ 90/261]  eta: 0:02:44  loss: 0.8118 (0.8392)  acc1: 82.2917 (80.8780)  acc5: 95.8333 (95.4613)  time: 0.3781  data: 0.0094  max mem: 19734
Test:  [100/261]  eta: 0:02:26  loss: 0.8219 (0.8418)  acc1: 83.8542 (80.8581)  acc5: 95.8333 (95.5342)  time: 0.3676  data: 0.0095  max mem: 19734
Test:  [110/261]  eta: 0:02:10  loss: 0.8518 (0.8637)  acc1: 76.5625 (80.3679)  acc5: 94.7917 (95.2797)  time: 0.4230  data: 0.0210  max mem: 19734
Test:  [120/261]  eta: 0:01:55  loss: 1.2155 (0.9037)  acc1: 71.3542 (79.3991)  acc5: 90.1042 (94.7142)  time: 0.3441  data: 0.0426  max mem: 19734
Test:  [130/261]  eta: 0:01:40  loss: 1.3665 (0.9488)  acc1: 67.7083 (78.4550)  acc5: 86.9792 (94.1317)  time: 0.2453  data: 0.0294  max mem: 19734
Test:  [140/261]  eta: 0:01:28  loss: 1.3298 (0.9759)  acc1: 66.6667 (77.7704)  acc5: 90.1042 (93.8682)  time: 0.2125  data: 0.0071  max mem: 19734
Test:  [150/261]  eta: 0:01:20  loss: 1.1748 (0.9801)  acc1: 71.8750 (77.7697)  acc5: 91.6667 (93.7604)  time: 0.4587  data: 0.2186  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: 1.0002 (0.9997)  acc1: 78.1250 (77.4457)  acc5: 92.1875 (93.5074)  time: 0.4400  data: 0.2327  max mem: 19734
Test:  [170/261]  eta: 0:01:00  loss: 1.3012 (1.0290)  acc1: 66.1458 (76.7057)  acc5: 89.5833 (93.1591)  time: 0.1500  data: 0.0217  max mem: 19734
Test:  [180/261]  eta: 0:00:51  loss: 1.3738 (1.0454)  acc1: 66.1458 (76.3122)  acc5: 89.0625 (92.9875)  time: 0.1414  data: 0.0194  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: 1.3730 (1.0591)  acc1: 68.7500 (76.0608)  acc5: 90.1042 (92.8229)  time: 0.1347  data: 0.0171  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.3892 (1.0754)  acc1: 71.3542 (75.7178)  acc5: 89.5833 (92.5684)  time: 0.1163  data: 0.0007  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.3892 (1.0883)  acc1: 69.2708 (75.4418)  acc5: 88.5417 (92.3850)  time: 0.1154  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.3949 (1.1068)  acc1: 68.2292 (74.9434)  acc5: 89.0625 (92.1969)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.3997 (1.1155)  acc1: 67.1875 (74.7159)  acc5: 89.5833 (92.1063)  time: 0.1152  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3149 (1.1246)  acc1: 68.2292 (74.5073)  acc5: 91.1458 (92.0297)  time: 0.1155  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0225 (1.1166)  acc1: 75.0000 (74.6867)  acc5: 93.2292 (92.1439)  time: 0.1155  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9335 (1.1163)  acc1: 77.6042 (74.6780)  acc5: 95.3125 (92.2000)  time: 0.1121  data: 0.0001  max mem: 19734
Test: Total time: 0:02:04 (0.4753 s / it)
* Acc@1 74.678 Acc@5 92.200 loss 1.116
Accuracy of the network on the 50000 test images: 74.7%
Max accuracy: 74.68%
Epoch: [39]  [   0/1251]  eta: 2:23:53  lr: 0.000014  loss: 3.1949 (3.1949)  time: 6.9013  data: 6.0894  max mem: 19734
loss info: cls_loss=3.3840, ratio_loss=0.0051, pruning_loss=0.1284, mse_loss=0.6094
Epoch: [39]  [  10/1251]  eta: 0:31:00  lr: 0.000014  loss: 3.6574 (3.5006)  time: 1.4990  data: 0.5542  max mem: 19734
Epoch: [39]  [  20/1251]  eta: 0:23:52  lr: 0.000014  loss: 3.5518 (3.5356)  time: 0.8766  data: 0.0006  max mem: 19734
Epoch: [39]  [  30/1251]  eta: 0:21:20  lr: 0.000014  loss: 3.4847 (3.4812)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [39]  [  40/1251]  eta: 0:19:54  lr: 0.000014  loss: 3.3344 (3.4315)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [39]  [  50/1251]  eta: 0:19:00  lr: 0.000014  loss: 3.3152 (3.4022)  time: 0.7964  data: 0.0005  max mem: 19734
Epoch: [39]  [  60/1251]  eta: 0:18:25  lr: 0.000014  loss: 3.3408 (3.3807)  time: 0.8086  data: 0.0005  max mem: 19734
Epoch: [39]  [  70/1251]  eta: 0:17:54  lr: 0.000014  loss: 3.3224 (3.3726)  time: 0.8089  data: 0.0006  max mem: 19734
Epoch: [39]  [  80/1251]  eta: 0:17:35  lr: 0.000014  loss: 3.2188 (3.3604)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [39]  [  90/1251]  eta: 0:17:16  lr: 0.000014  loss: 3.5860 (3.3897)  time: 0.8323  data: 0.0005  max mem: 19734
Epoch: [39]  [ 100/1251]  eta: 0:16:57  lr: 0.000014  loss: 3.6884 (3.4140)  time: 0.8124  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4096, ratio_loss=0.0049, pruning_loss=0.1274, mse_loss=0.5715
Epoch: [39]  [ 110/1251]  eta: 0:16:41  lr: 0.000014  loss: 3.7349 (3.4356)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [39]  [ 120/1251]  eta: 0:16:25  lr: 0.000014  loss: 3.7424 (3.4354)  time: 0.8083  data: 0.0005  max mem: 19734
Epoch: [39]  [ 130/1251]  eta: 0:16:11  lr: 0.000014  loss: 3.6262 (3.4668)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [39]  [ 140/1251]  eta: 0:15:58  lr: 0.000014  loss: 3.5637 (3.4566)  time: 0.8095  data: 0.0008  max mem: 19734
Epoch: [39]  [ 150/1251]  eta: 0:15:44  lr: 0.000014  loss: 3.4739 (3.4572)  time: 0.8054  data: 0.0008  max mem: 19734
Epoch: [39]  [ 160/1251]  eta: 0:15:32  lr: 0.000014  loss: 3.5144 (3.4525)  time: 0.7985  data: 0.0005  max mem: 19734
Epoch: [39]  [ 170/1251]  eta: 0:15:20  lr: 0.000014  loss: 3.3260 (3.4430)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [39]  [ 180/1251]  eta: 0:15:09  lr: 0.000014  loss: 3.3260 (3.4421)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [39]  [ 190/1251]  eta: 0:14:58  lr: 0.000014  loss: 3.5970 (3.4528)  time: 0.8037  data: 0.0006  max mem: 19734
Epoch: [39]  [ 200/1251]  eta: 0:14:47  lr: 0.000014  loss: 3.6042 (3.4469)  time: 0.8016  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4252, ratio_loss=0.0049, pruning_loss=0.1256, mse_loss=0.5657
Epoch: [39]  [ 210/1251]  eta: 0:14:37  lr: 0.000014  loss: 3.4580 (3.4395)  time: 0.8126  data: 0.0005  max mem: 19734
Epoch: [39]  [ 220/1251]  eta: 0:14:28  lr: 0.000014  loss: 3.5961 (3.4489)  time: 0.8232  data: 0.0005  max mem: 19734
Epoch: [39]  [ 230/1251]  eta: 0:14:19  lr: 0.000014  loss: 3.5961 (3.4426)  time: 0.8245  data: 0.0005  max mem: 19734
Epoch: [39]  [ 240/1251]  eta: 0:14:09  lr: 0.000014  loss: 3.4545 (3.4469)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [39]  [ 250/1251]  eta: 0:13:59  lr: 0.000014  loss: 3.3919 (3.4376)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [39]  [ 260/1251]  eta: 0:13:50  lr: 0.000014  loss: 3.3919 (3.4420)  time: 0.8073  data: 0.0004  max mem: 19734
Epoch: [39]  [ 270/1251]  eta: 0:13:40  lr: 0.000014  loss: 3.6426 (3.4434)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [39]  [ 280/1251]  eta: 0:13:30  lr: 0.000014  loss: 3.5370 (3.4430)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [39]  [ 290/1251]  eta: 0:13:22  lr: 0.000014  loss: 3.2636 (3.4308)  time: 0.8093  data: 0.0005  max mem: 19734
Epoch: [39]  [ 300/1251]  eta: 0:13:12  lr: 0.000014  loss: 3.2329 (3.4326)  time: 0.8095  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4018, ratio_loss=0.0047, pruning_loss=0.1262, mse_loss=0.5798
Epoch: [39]  [ 310/1251]  eta: 0:13:03  lr: 0.000014  loss: 3.4901 (3.4334)  time: 0.8040  data: 0.0005  max mem: 19734
Epoch: [39]  [ 320/1251]  eta: 0:12:54  lr: 0.000014  loss: 3.6321 (3.4386)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [39]  [ 330/1251]  eta: 0:12:45  lr: 0.000014  loss: 3.6349 (3.4357)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [39]  [ 340/1251]  eta: 0:12:36  lr: 0.000014  loss: 3.4359 (3.4325)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [39]  [ 350/1251]  eta: 0:12:27  lr: 0.000014  loss: 3.5346 (3.4353)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [39]  [ 360/1251]  eta: 0:12:19  lr: 0.000014  loss: 3.5674 (3.4257)  time: 0.8138  data: 0.0005  max mem: 19734
Epoch: [39]  [ 370/1251]  eta: 0:12:10  lr: 0.000014  loss: 3.5138 (3.4258)  time: 0.8292  data: 0.0005  max mem: 19734
Epoch: [39]  [ 380/1251]  eta: 0:12:02  lr: 0.000014  loss: 3.6090 (3.4313)  time: 0.8236  data: 0.0005  max mem: 19734
Epoch: [39]  [ 390/1251]  eta: 0:11:53  lr: 0.000014  loss: 3.5846 (3.4312)  time: 0.8153  data: 0.0005  max mem: 19734
Epoch: [39]  [ 400/1251]  eta: 0:11:44  lr: 0.000014  loss: 3.6256 (3.4345)  time: 0.8112  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4267, ratio_loss=0.0046, pruning_loss=0.1265, mse_loss=0.6103
Epoch: [39]  [ 410/1251]  eta: 0:11:36  lr: 0.000014  loss: 3.6256 (3.4363)  time: 0.8166  data: 0.0005  max mem: 19734
Epoch: [39]  [ 420/1251]  eta: 0:11:27  lr: 0.000014  loss: 3.5584 (3.4381)  time: 0.8214  data: 0.0005  max mem: 19734
Epoch: [39]  [ 430/1251]  eta: 0:11:19  lr: 0.000014  loss: 3.6929 (3.4407)  time: 0.8147  data: 0.0005  max mem: 19734
Epoch: [39]  [ 440/1251]  eta: 0:11:10  lr: 0.000014  loss: 3.6929 (3.4429)  time: 0.8084  data: 0.0006  max mem: 19734
Epoch: [39]  [ 450/1251]  eta: 0:11:01  lr: 0.000014  loss: 3.3895 (3.4390)  time: 0.8010  data: 0.0006  max mem: 19734
Epoch: [39]  [ 460/1251]  eta: 0:10:53  lr: 0.000014  loss: 3.1971 (3.4319)  time: 0.7980  data: 0.0005  max mem: 19734
Epoch: [39]  [ 470/1251]  eta: 0:10:44  lr: 0.000014  loss: 3.4030 (3.4348)  time: 0.7981  data: 0.0005  max mem: 19734
Epoch: [39]  [ 480/1251]  eta: 0:10:35  lr: 0.000014  loss: 3.4867 (3.4387)  time: 0.7983  data: 0.0005  max mem: 19734
Epoch: [39]  [ 490/1251]  eta: 0:10:27  lr: 0.000014  loss: 3.5678 (3.4401)  time: 0.7984  data: 0.0004  max mem: 19734
Epoch: [39]  [ 500/1251]  eta: 0:10:18  lr: 0.000014  loss: 3.4808 (3.4342)  time: 0.8105  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4106, ratio_loss=0.0046, pruning_loss=0.1267, mse_loss=0.5856
Epoch: [39]  [ 510/1251]  eta: 0:10:10  lr: 0.000014  loss: 3.3586 (3.4350)  time: 0.8107  data: 0.0005  max mem: 19734
Epoch: [39]  [ 520/1251]  eta: 0:10:02  lr: 0.000014  loss: 3.5099 (3.4388)  time: 0.8155  data: 0.0005  max mem: 19734
Epoch: [39]  [ 530/1251]  eta: 0:09:54  lr: 0.000014  loss: 3.4489 (3.4342)  time: 0.8398  data: 0.0004  max mem: 19734
Epoch: [39]  [ 540/1251]  eta: 0:09:45  lr: 0.000014  loss: 3.2247 (3.4311)  time: 0.8240  data: 0.0004  max mem: 19734
Epoch: [39]  [ 550/1251]  eta: 0:09:37  lr: 0.000014  loss: 3.4316 (3.4322)  time: 0.8014  data: 0.0006  max mem: 19734
Epoch: [39]  [ 560/1251]  eta: 0:09:28  lr: 0.000014  loss: 3.4942 (3.4319)  time: 0.8064  data: 0.0006  max mem: 19734
Epoch: [39]  [ 570/1251]  eta: 0:09:20  lr: 0.000014  loss: 3.6071 (3.4368)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [39]  [ 580/1251]  eta: 0:09:11  lr: 0.000014  loss: 3.7890 (3.4396)  time: 0.8017  data: 0.0005  max mem: 19734
Epoch: [39]  [ 590/1251]  eta: 0:09:03  lr: 0.000014  loss: 3.7890 (3.4416)  time: 0.8043  data: 0.0004  max mem: 19734
Epoch: [39]  [ 600/1251]  eta: 0:08:54  lr: 0.000014  loss: 3.7561 (3.4452)  time: 0.8010  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5125, ratio_loss=0.0046, pruning_loss=0.1246, mse_loss=0.5644
Epoch: [39]  [ 610/1251]  eta: 0:08:46  lr: 0.000014  loss: 3.7462 (3.4507)  time: 0.7992  data: 0.0006  max mem: 19734
Epoch: [39]  [ 620/1251]  eta: 0:08:38  lr: 0.000014  loss: 3.6940 (3.4484)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [39]  [ 630/1251]  eta: 0:08:29  lr: 0.000014  loss: 3.6228 (3.4490)  time: 0.7997  data: 0.0006  max mem: 19734
Epoch: [39]  [ 640/1251]  eta: 0:08:21  lr: 0.000014  loss: 3.2920 (3.4436)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [39]  [ 650/1251]  eta: 0:08:13  lr: 0.000014  loss: 3.2062 (3.4423)  time: 0.8127  data: 0.0005  max mem: 19734
Epoch: [39]  [ 660/1251]  eta: 0:08:04  lr: 0.000014  loss: 3.2844 (3.4401)  time: 0.8219  data: 0.0005  max mem: 19734
Epoch: [39]  [ 670/1251]  eta: 0:07:56  lr: 0.000014  loss: 3.6645 (3.4441)  time: 0.8293  data: 0.0005  max mem: 19734
Epoch: [39]  [ 680/1251]  eta: 0:07:48  lr: 0.000014  loss: 3.6574 (3.4414)  time: 0.8301  data: 0.0005  max mem: 19734
Epoch: [39]  [ 690/1251]  eta: 0:07:40  lr: 0.000014  loss: 3.4093 (3.4378)  time: 0.8110  data: 0.0005  max mem: 19734
Epoch: [39]  [ 700/1251]  eta: 0:07:31  lr: 0.000014  loss: 3.3812 (3.4344)  time: 0.8070  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3181, ratio_loss=0.0048, pruning_loss=0.1273, mse_loss=0.5771
Epoch: [39]  [ 710/1251]  eta: 0:07:23  lr: 0.000014  loss: 3.4096 (3.4347)  time: 0.8106  data: 0.0004  max mem: 19734
Epoch: [39]  [ 720/1251]  eta: 0:07:15  lr: 0.000014  loss: 3.4488 (3.4333)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [39]  [ 730/1251]  eta: 0:07:07  lr: 0.000014  loss: 3.5645 (3.4352)  time: 0.8128  data: 0.0005  max mem: 19734
Epoch: [39]  [ 740/1251]  eta: 0:06:58  lr: 0.000014  loss: 3.5980 (3.4318)  time: 0.8137  data: 0.0005  max mem: 19734
Epoch: [39]  [ 750/1251]  eta: 0:06:50  lr: 0.000014  loss: 3.4089 (3.4304)  time: 0.8015  data: 0.0004  max mem: 19734
Epoch: [39]  [ 760/1251]  eta: 0:06:42  lr: 0.000014  loss: 3.5947 (3.4293)  time: 0.7980  data: 0.0004  max mem: 19734
Epoch: [39]  [ 770/1251]  eta: 0:06:33  lr: 0.000014  loss: 3.6566 (3.4293)  time: 0.7973  data: 0.0004  max mem: 19734
Epoch: [39]  [ 780/1251]  eta: 0:06:25  lr: 0.000014  loss: 3.5972 (3.4283)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [39]  [ 790/1251]  eta: 0:06:17  lr: 0.000014  loss: 3.3979 (3.4242)  time: 0.8089  data: 0.0005  max mem: 19734
Epoch: [39]  [ 800/1251]  eta: 0:06:09  lr: 0.000014  loss: 3.3979 (3.4260)  time: 0.8058  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3422, ratio_loss=0.0046, pruning_loss=0.1274, mse_loss=0.5840
Epoch: [39]  [ 810/1251]  eta: 0:06:00  lr: 0.000014  loss: 3.4384 (3.4256)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [39]  [ 820/1251]  eta: 0:05:52  lr: 0.000014  loss: 3.4721 (3.4253)  time: 0.8283  data: 0.0005  max mem: 19734
Epoch: [39]  [ 830/1251]  eta: 0:05:44  lr: 0.000014  loss: 3.4144 (3.4232)  time: 0.8180  data: 0.0005  max mem: 19734
Epoch: [39]  [ 840/1251]  eta: 0:05:36  lr: 0.000014  loss: 3.3416 (3.4217)  time: 0.7988  data: 0.0005  max mem: 19734
Epoch: [39]  [ 850/1251]  eta: 0:05:27  lr: 0.000014  loss: 3.2186 (3.4199)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [39]  [ 860/1251]  eta: 0:05:19  lr: 0.000014  loss: 3.6030 (3.4220)  time: 0.8026  data: 0.0005  max mem: 19734
Epoch: [39]  [ 870/1251]  eta: 0:05:11  lr: 0.000014  loss: 3.6159 (3.4214)  time: 0.8088  data: 0.0007  max mem: 19734
Epoch: [39]  [ 880/1251]  eta: 0:05:03  lr: 0.000014  loss: 3.5747 (3.4200)  time: 0.8101  data: 0.0006  max mem: 19734
Epoch: [39]  [ 890/1251]  eta: 0:04:55  lr: 0.000014  loss: 3.3797 (3.4198)  time: 0.8003  data: 0.0005  max mem: 19734
Epoch: [39]  [ 900/1251]  eta: 0:04:46  lr: 0.000014  loss: 3.4174 (3.4216)  time: 0.7984  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3687, ratio_loss=0.0049, pruning_loss=0.1269, mse_loss=0.5470
Epoch: [39]  [ 910/1251]  eta: 0:04:38  lr: 0.000014  loss: 3.5745 (3.4214)  time: 0.7994  data: 0.0005  max mem: 19734
Epoch: [39]  [ 920/1251]  eta: 0:04:30  lr: 0.000014  loss: 3.0615 (3.4185)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [39]  [ 930/1251]  eta: 0:04:22  lr: 0.000014  loss: 3.0565 (3.4171)  time: 0.8021  data: 0.0006  max mem: 19734
Epoch: [39]  [ 940/1251]  eta: 0:04:13  lr: 0.000014  loss: 3.3795 (3.4157)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [39]  [ 950/1251]  eta: 0:04:05  lr: 0.000014  loss: 3.4071 (3.4177)  time: 0.8058  data: 0.0004  max mem: 19734
Epoch: [39]  [ 960/1251]  eta: 0:03:57  lr: 0.000014  loss: 3.4495 (3.4163)  time: 0.8178  data: 0.0004  max mem: 19734
Epoch: [39]  [ 970/1251]  eta: 0:03:49  lr: 0.000014  loss: 3.1028 (3.4139)  time: 0.8301  data: 0.0004  max mem: 19734
Epoch: [39]  [ 980/1251]  eta: 0:03:41  lr: 0.000014  loss: 3.3094 (3.4144)  time: 0.8122  data: 0.0005  max mem: 19734
Epoch: [39]  [ 990/1251]  eta: 0:03:33  lr: 0.000014  loss: 3.6584 (3.4140)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [39]  [1000/1251]  eta: 0:03:24  lr: 0.000014  loss: 3.4336 (3.4137)  time: 0.8088  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3174, ratio_loss=0.0050, pruning_loss=0.1273, mse_loss=0.5523
Epoch: [39]  [1010/1251]  eta: 0:03:16  lr: 0.000014  loss: 3.3509 (3.4130)  time: 0.8097  data: 0.0004  max mem: 19734
Epoch: [39]  [1020/1251]  eta: 0:03:08  lr: 0.000014  loss: 3.4295 (3.4130)  time: 0.8131  data: 0.0007  max mem: 19734
Epoch: [39]  [1030/1251]  eta: 0:03:00  lr: 0.000014  loss: 3.6749 (3.4159)  time: 0.8113  data: 0.0007  max mem: 19734
Epoch: [39]  [1040/1251]  eta: 0:02:52  lr: 0.000014  loss: 3.6602 (3.4160)  time: 0.7985  data: 0.0005  max mem: 19734
Epoch: [39]  [1050/1251]  eta: 0:02:43  lr: 0.000014  loss: 3.3657 (3.4163)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [39]  [1060/1251]  eta: 0:02:35  lr: 0.000014  loss: 3.5264 (3.4157)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [39]  [1070/1251]  eta: 0:02:27  lr: 0.000014  loss: 3.5308 (3.4151)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [39]  [1080/1251]  eta: 0:02:19  lr: 0.000014  loss: 3.5308 (3.4166)  time: 0.7983  data: 0.0005  max mem: 19734
Epoch: [39]  [1090/1251]  eta: 0:02:11  lr: 0.000014  loss: 3.6229 (3.4187)  time: 0.8036  data: 0.0004  max mem: 19734
Epoch: [39]  [1100/1251]  eta: 0:02:03  lr: 0.000014  loss: 3.4846 (3.4190)  time: 0.8019  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4719, ratio_loss=0.0047, pruning_loss=0.1256, mse_loss=0.6192
Epoch: [39]  [1110/1251]  eta: 0:01:54  lr: 0.000014  loss: 3.5429 (3.4200)  time: 0.8160  data: 0.0006  max mem: 19734
Epoch: [39]  [1120/1251]  eta: 0:01:46  lr: 0.000014  loss: 3.6566 (3.4207)  time: 0.8299  data: 0.0004  max mem: 19734
Epoch: [39]  [1130/1251]  eta: 0:01:38  lr: 0.000014  loss: 3.6651 (3.4212)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [39]  [1140/1251]  eta: 0:01:30  lr: 0.000014  loss: 3.7129 (3.4222)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [39]  [1150/1251]  eta: 0:01:22  lr: 0.000014  loss: 3.7592 (3.4247)  time: 0.8070  data: 0.0004  max mem: 19734
Epoch: [39]  [1160/1251]  eta: 0:01:14  lr: 0.000014  loss: 3.8101 (3.4267)  time: 0.8069  data: 0.0004  max mem: 19734
Epoch: [39]  [1170/1251]  eta: 0:01:06  lr: 0.000014  loss: 3.5640 (3.4244)  time: 0.8104  data: 0.0005  max mem: 19734
Epoch: [39]  [1180/1251]  eta: 0:00:57  lr: 0.000014  loss: 3.2030 (3.4235)  time: 0.8112  data: 0.0005  max mem: 19734
Epoch: [39]  [1190/1251]  eta: 0:00:49  lr: 0.000014  loss: 3.6530 (3.4265)  time: 0.7971  data: 0.0007  max mem: 19734
Epoch: [39]  [1200/1251]  eta: 0:00:41  lr: 0.000014  loss: 3.6530 (3.4278)  time: 0.7953  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4758, ratio_loss=0.0050, pruning_loss=0.1256, mse_loss=0.5654
Epoch: [39]  [1210/1251]  eta: 0:00:33  lr: 0.000014  loss: 3.4927 (3.4261)  time: 0.7945  data: 0.0001  max mem: 19734
Epoch: [39]  [1220/1251]  eta: 0:00:25  lr: 0.000014  loss: 3.3039 (3.4242)  time: 0.7918  data: 0.0001  max mem: 19734
Epoch: [39]  [1230/1251]  eta: 0:00:17  lr: 0.000014  loss: 3.1784 (3.4232)  time: 0.7923  data: 0.0001  max mem: 19734
Epoch: [39]  [1240/1251]  eta: 0:00:08  lr: 0.000014  loss: 3.2692 (3.4227)  time: 0.8013  data: 0.0001  max mem: 19734
Epoch: [39]  [1250/1251]  eta: 0:00:00  lr: 0.000014  loss: 3.2692 (3.4194)  time: 0.8090  data: 0.0001  max mem: 19734
Epoch: [39] Total time: 0:16:59 (0.8149 s / it)
Averaged stats: lr: 0.000014  loss: 3.2692 (3.4079)
Test:  [  0/261]  eta: 2:05:37  loss: 0.6555 (0.6555)  acc1: 84.3750 (84.3750)  acc5: 97.9167 (97.9167)  time: 28.8793  data: 28.7306  max mem: 19734
Test:  [ 10/261]  eta: 0:13:05  loss: 0.6555 (0.7340)  acc1: 84.3750 (83.5227)  acc5: 96.8750 (96.4962)  time: 3.1302  data: 2.9146  max mem: 19734
Test:  [ 20/261]  eta: 0:07:07  loss: 0.9667 (0.8954)  acc1: 77.6042 (79.1915)  acc5: 94.2708 (94.8413)  time: 0.4190  data: 0.1727  max mem: 19734
Test:  [ 30/261]  eta: 0:04:51  loss: 0.8028 (0.8136)  acc1: 84.3750 (82.0565)  acc5: 94.2708 (95.2621)  time: 0.2336  data: 0.0144  max mem: 19734
Test:  [ 40/261]  eta: 0:04:02  loss: 0.5621 (0.7804)  acc1: 88.0208 (83.1047)  acc5: 96.8750 (95.4776)  time: 0.3833  data: 0.2114  max mem: 19734
Test:  [ 50/261]  eta: 0:03:25  loss: 0.9270 (0.8434)  acc1: 79.6875 (81.2194)  acc5: 94.7917 (95.0878)  time: 0.5338  data: 0.2116  max mem: 19734
Test:  [ 60/261]  eta: 0:02:51  loss: 0.9697 (0.8520)  acc1: 76.5625 (80.8572)  acc5: 94.2708 (95.1417)  time: 0.3507  data: 0.0189  max mem: 19734
Test:  [ 70/261]  eta: 0:02:29  loss: 0.9072 (0.8534)  acc1: 78.1250 (80.4724)  acc5: 96.3542 (95.3052)  time: 0.2819  data: 0.0463  max mem: 19734
Test:  [ 80/261]  eta: 0:02:17  loss: 0.8408 (0.8551)  acc1: 80.2083 (80.5877)  acc5: 96.8750 (95.4090)  time: 0.4892  data: 0.0580  max mem: 19734
Test:  [ 90/261]  eta: 0:02:03  loss: 0.8140 (0.8414)  acc1: 83.3333 (80.9581)  acc5: 96.3542 (95.4899)  time: 0.5257  data: 0.0334  max mem: 19734
Test:  [100/261]  eta: 0:02:03  loss: 0.8102 (0.8446)  acc1: 83.8542 (80.8890)  acc5: 95.8333 (95.5446)  time: 0.7918  data: 0.3721  max mem: 19734
Test:  [110/261]  eta: 0:01:48  loss: 0.8743 (0.8680)  acc1: 78.1250 (80.3819)  acc5: 94.2708 (95.2468)  time: 0.7070  data: 0.3700  max mem: 19734
Test:  [120/261]  eta: 0:01:40  loss: 1.1900 (0.9092)  acc1: 72.3958 (79.4206)  acc5: 89.5833 (94.6970)  time: 0.4472  data: 0.0241  max mem: 19734
Test:  [130/261]  eta: 0:01:31  loss: 1.3905 (0.9534)  acc1: 67.1875 (78.4828)  acc5: 86.9792 (94.0959)  time: 0.5476  data: 0.0230  max mem: 19734
Test:  [140/261]  eta: 0:01:27  loss: 1.2906 (0.9791)  acc1: 67.7083 (77.8443)  acc5: 90.1042 (93.8239)  time: 0.7681  data: 0.3821  max mem: 19734
Test:  [150/261]  eta: 0:01:17  loss: 1.2097 (0.9836)  acc1: 72.3958 (77.8491)  acc5: 91.1458 (93.7017)  time: 0.7110  data: 0.3820  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: 0.9816 (1.0034)  acc1: 79.6875 (77.5039)  acc5: 91.6667 (93.4136)  time: 0.4881  data: 0.0811  max mem: 19734
Test:  [170/261]  eta: 0:01:01  loss: 1.3054 (1.0336)  acc1: 66.1458 (76.7757)  acc5: 88.5417 (93.0556)  time: 0.5210  data: 0.1635  max mem: 19734
Test:  [180/261]  eta: 0:00:52  loss: 1.4497 (1.0507)  acc1: 65.6250 (76.3553)  acc5: 89.5833 (92.9328)  time: 0.3083  data: 0.0978  max mem: 19734
Test:  [190/261]  eta: 0:00:44  loss: 1.4140 (1.0645)  acc1: 67.1875 (76.0880)  acc5: 91.1458 (92.7711)  time: 0.1703  data: 0.0134  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: 1.3854 (1.0801)  acc1: 71.8750 (75.7748)  acc5: 89.5833 (92.5321)  time: 0.1216  data: 0.0060  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: 1.3854 (1.0934)  acc1: 70.8333 (75.4937)  acc5: 88.5417 (92.3381)  time: 0.1152  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4088 (1.1132)  acc1: 67.1875 (74.9788)  acc5: 88.0208 (92.1215)  time: 0.1150  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4595 (1.1231)  acc1: 65.6250 (74.7137)  acc5: 89.5833 (92.0184)  time: 0.1153  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3492 (1.1316)  acc1: 68.2292 (74.5008)  acc5: 89.5833 (91.9433)  time: 0.1154  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0366 (1.1244)  acc1: 75.5208 (74.6618)  acc5: 92.1875 (92.0630)  time: 0.1152  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9310 (1.1244)  acc1: 77.0833 (74.6500)  acc5: 95.3125 (92.1300)  time: 0.1118  data: 0.0001  max mem: 19734
Test: Total time: 0:02:07 (0.4877 s / it)
* Acc@1 74.650 Acc@5 92.130 loss 1.124
Accuracy of the network on the 50000 test images: 74.7%
Max accuracy: 74.68%
Epoch: [40]  [   0/1251]  eta: 6:33:59  lr: 0.000014  loss: 2.5962 (2.5962)  time: 18.8968  data: 9.5724  max mem: 19734
Epoch: [40]  [  10/1251]  eta: 0:54:00  lr: 0.000014  loss: 3.5414 (3.4008)  time: 2.6114  data: 0.8860  max mem: 19734
Epoch: [40]  [  20/1251]  eta: 0:35:52  lr: 0.000014  loss: 3.6402 (3.4617)  time: 0.8908  data: 0.0092  max mem: 19734
Epoch: [40]  [  30/1251]  eta: 0:29:21  lr: 0.000014  loss: 3.5715 (3.5010)  time: 0.7994  data: 0.0008  max mem: 19734
Epoch: [40]  [  40/1251]  eta: 0:25:57  lr: 0.000014  loss: 3.4343 (3.4510)  time: 0.8002  data: 0.0006  max mem: 19734
Epoch: [40]  [  50/1251]  eta: 0:23:52  lr: 0.000014  loss: 3.3991 (3.4288)  time: 0.8064  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3299, ratio_loss=0.0048, pruning_loss=0.1289, mse_loss=0.5840
Epoch: [40]  [  60/1251]  eta: 0:22:24  lr: 0.000014  loss: 3.4520 (3.4190)  time: 0.8061  data: 0.0006  max mem: 19734
Epoch: [40]  [  70/1251]  eta: 0:21:17  lr: 0.000014  loss: 3.2451 (3.3621)  time: 0.7984  data: 0.0006  max mem: 19734
Epoch: [40]  [  80/1251]  eta: 0:20:26  lr: 0.000014  loss: 3.3787 (3.3879)  time: 0.7995  data: 0.0007  max mem: 19734
Epoch: [40]  [  90/1251]  eta: 0:19:44  lr: 0.000014  loss: 3.3787 (3.3501)  time: 0.8028  data: 0.0007  max mem: 19734
Epoch: [40]  [ 100/1251]  eta: 0:19:08  lr: 0.000014  loss: 3.2727 (3.3490)  time: 0.7998  data: 0.0005  max mem: 19734
Epoch: [40]  [ 110/1251]  eta: 0:18:38  lr: 0.000014  loss: 3.3075 (3.3487)  time: 0.7987  data: 0.0006  max mem: 19734
Epoch: [40]  [ 120/1251]  eta: 0:18:13  lr: 0.000014  loss: 3.5803 (3.3610)  time: 0.8073  data: 0.0005  max mem: 19734
Epoch: [40]  [ 130/1251]  eta: 0:17:50  lr: 0.000014  loss: 3.6582 (3.3660)  time: 0.8142  data: 0.0005  max mem: 19734
Epoch: [40]  [ 140/1251]  eta: 0:17:30  lr: 0.000014  loss: 3.2808 (3.3592)  time: 0.8204  data: 0.0006  max mem: 19734
Epoch: [40]  [ 150/1251]  eta: 0:17:13  lr: 0.000014  loss: 3.4010 (3.3669)  time: 0.8299  data: 0.0006  max mem: 19734
loss info: cls_loss=3.2913, ratio_loss=0.0046, pruning_loss=0.1291, mse_loss=0.5775
Epoch: [40]  [ 160/1251]  eta: 0:16:55  lr: 0.000014  loss: 3.4195 (3.3593)  time: 0.8262  data: 0.0006  max mem: 19734
Epoch: [40]  [ 170/1251]  eta: 0:16:38  lr: 0.000014  loss: 3.4428 (3.3654)  time: 0.8100  data: 0.0005  max mem: 19734
Epoch: [40]  [ 180/1251]  eta: 0:16:22  lr: 0.000014  loss: 3.4240 (3.3627)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [40]  [ 190/1251]  eta: 0:16:07  lr: 0.000014  loss: 3.4148 (3.3646)  time: 0.8088  data: 0.0006  max mem: 19734
Epoch: [40]  [ 200/1251]  eta: 0:15:53  lr: 0.000014  loss: 3.4390 (3.3606)  time: 0.8211  data: 0.0006  max mem: 19734
Epoch: [40]  [ 210/1251]  eta: 0:15:39  lr: 0.000014  loss: 3.6707 (3.3677)  time: 0.8167  data: 0.0006  max mem: 19734
Epoch: [40]  [ 220/1251]  eta: 0:15:25  lr: 0.000014  loss: 3.6707 (3.3717)  time: 0.8033  data: 0.0007  max mem: 19734
Epoch: [40]  [ 230/1251]  eta: 0:15:12  lr: 0.000014  loss: 3.4379 (3.3763)  time: 0.8042  data: 0.0007  max mem: 19734
Epoch: [40]  [ 240/1251]  eta: 0:14:59  lr: 0.000014  loss: 3.4719 (3.3811)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [40]  [ 250/1251]  eta: 0:14:47  lr: 0.000014  loss: 3.6126 (3.3929)  time: 0.8013  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4177, ratio_loss=0.0046, pruning_loss=0.1252, mse_loss=0.5876
Epoch: [40]  [ 260/1251]  eta: 0:14:35  lr: 0.000014  loss: 3.5608 (3.3879)  time: 0.8072  data: 0.0007  max mem: 19734
Epoch: [40]  [ 270/1251]  eta: 0:14:23  lr: 0.000014  loss: 3.3646 (3.3835)  time: 0.8070  data: 0.0007  max mem: 19734
Epoch: [40]  [ 280/1251]  eta: 0:14:12  lr: 0.000014  loss: 3.0404 (3.3793)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [40]  [ 290/1251]  eta: 0:14:02  lr: 0.000014  loss: 3.3649 (3.3841)  time: 0.8218  data: 0.0006  max mem: 19734
Epoch: [40]  [ 300/1251]  eta: 0:13:52  lr: 0.000014  loss: 3.5608 (3.3877)  time: 0.8245  data: 0.0006  max mem: 19734
Epoch: [40]  [ 310/1251]  eta: 0:13:41  lr: 0.000014  loss: 3.6308 (3.3961)  time: 0.8198  data: 0.0005  max mem: 19734
Epoch: [40]  [ 320/1251]  eta: 0:13:30  lr: 0.000014  loss: 3.4124 (3.3849)  time: 0.8094  data: 0.0006  max mem: 19734
Epoch: [40]  [ 330/1251]  eta: 0:13:20  lr: 0.000014  loss: 3.3393 (3.3886)  time: 0.8037  data: 0.0006  max mem: 19734
Epoch: [40]  [ 340/1251]  eta: 0:13:10  lr: 0.000014  loss: 3.6520 (3.3970)  time: 0.8124  data: 0.0006  max mem: 19734
Epoch: [40]  [ 350/1251]  eta: 0:12:59  lr: 0.000014  loss: 3.6156 (3.3929)  time: 0.8131  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4316, ratio_loss=0.0048, pruning_loss=0.1267, mse_loss=0.5728
Epoch: [40]  [ 360/1251]  eta: 0:12:49  lr: 0.000014  loss: 3.6559 (3.4062)  time: 0.8052  data: 0.0006  max mem: 19734
Epoch: [40]  [ 370/1251]  eta: 0:12:39  lr: 0.000014  loss: 3.7452 (3.4125)  time: 0.8046  data: 0.0006  max mem: 19734
Epoch: [40]  [ 380/1251]  eta: 0:12:29  lr: 0.000014  loss: 3.6824 (3.4206)  time: 0.8045  data: 0.0006  max mem: 19734
Epoch: [40]  [ 390/1251]  eta: 0:12:19  lr: 0.000014  loss: 3.6824 (3.4244)  time: 0.8031  data: 0.0006  max mem: 19734
Epoch: [40]  [ 400/1251]  eta: 0:12:10  lr: 0.000014  loss: 3.6969 (3.4303)  time: 0.8024  data: 0.0005  max mem: 19734
Epoch: [40]  [ 410/1251]  eta: 0:12:00  lr: 0.000014  loss: 3.6828 (3.4311)  time: 0.8153  data: 0.0005  max mem: 19734
Epoch: [40]  [ 420/1251]  eta: 0:11:51  lr: 0.000014  loss: 3.6824 (3.4353)  time: 0.8157  data: 0.0006  max mem: 19734
Epoch: [40]  [ 430/1251]  eta: 0:11:42  lr: 0.000014  loss: 3.7644 (3.4382)  time: 0.8196  data: 0.0005  max mem: 19734
Epoch: [40]  [ 440/1251]  eta: 0:11:33  lr: 0.000014  loss: 3.5769 (3.4394)  time: 0.8378  data: 0.0007  max mem: 19734
Epoch: [40]  [ 450/1251]  eta: 0:11:23  lr: 0.000014  loss: 3.3274 (3.4277)  time: 0.8211  data: 0.0007  max mem: 19734
loss info: cls_loss=3.5079, ratio_loss=0.0048, pruning_loss=0.1233, mse_loss=0.5668
Epoch: [40]  [ 460/1251]  eta: 0:11:14  lr: 0.000014  loss: 3.0085 (3.4304)  time: 0.8112  data: 0.0006  max mem: 19734
Epoch: [40]  [ 470/1251]  eta: 0:11:05  lr: 0.000014  loss: 3.5237 (3.4296)  time: 0.8112  data: 0.0005  max mem: 19734
Epoch: [40]  [ 480/1251]  eta: 0:10:56  lr: 0.000014  loss: 3.5194 (3.4298)  time: 0.8045  data: 0.0005  max mem: 19734
Epoch: [40]  [ 490/1251]  eta: 0:10:47  lr: 0.000014  loss: 3.5055 (3.4270)  time: 0.8127  data: 0.0006  max mem: 19734
Epoch: [40]  [ 500/1251]  eta: 0:10:37  lr: 0.000014  loss: 3.5202 (3.4313)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [40]  [ 510/1251]  eta: 0:10:28  lr: 0.000014  loss: 3.6500 (3.4340)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [40]  [ 520/1251]  eta: 0:10:19  lr: 0.000014  loss: 3.7480 (3.4362)  time: 0.8011  data: 0.0007  max mem: 19734
Epoch: [40]  [ 530/1251]  eta: 0:10:10  lr: 0.000014  loss: 3.6231 (3.4384)  time: 0.8010  data: 0.0009  max mem: 19734
Epoch: [40]  [ 540/1251]  eta: 0:10:01  lr: 0.000014  loss: 3.5757 (3.4373)  time: 0.8018  data: 0.0007  max mem: 19734
Epoch: [40]  [ 550/1251]  eta: 0:09:52  lr: 0.000014  loss: 3.6312 (3.4387)  time: 0.8003  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4154, ratio_loss=0.0045, pruning_loss=0.1247, mse_loss=0.5736
Epoch: [40]  [ 560/1251]  eta: 0:09:43  lr: 0.000014  loss: 3.2306 (3.4332)  time: 0.8084  data: 0.0006  max mem: 19734
Epoch: [40]  [ 570/1251]  eta: 0:09:34  lr: 0.000014  loss: 3.3360 (3.4324)  time: 0.8147  data: 0.0005  max mem: 19734
Epoch: [40]  [ 580/1251]  eta: 0:09:26  lr: 0.000014  loss: 3.4685 (3.4279)  time: 0.8245  data: 0.0005  max mem: 19734
Epoch: [40]  [ 590/1251]  eta: 0:09:17  lr: 0.000014  loss: 2.8810 (3.4198)  time: 0.8363  data: 0.0005  max mem: 19734
Epoch: [40]  [ 600/1251]  eta: 0:09:09  lr: 0.000014  loss: 3.0836 (3.4226)  time: 0.8311  data: 0.0006  max mem: 19734
Epoch: [40]  [ 610/1251]  eta: 0:09:00  lr: 0.000014  loss: 3.5533 (3.4207)  time: 0.8135  data: 0.0006  max mem: 19734
Epoch: [40]  [ 620/1251]  eta: 0:08:51  lr: 0.000014  loss: 3.1135 (3.4181)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [40]  [ 630/1251]  eta: 0:08:42  lr: 0.000014  loss: 3.6658 (3.4212)  time: 0.8003  data: 0.0005  max mem: 19734
Epoch: [40]  [ 640/1251]  eta: 0:08:33  lr: 0.000014  loss: 3.7150 (3.4198)  time: 0.8022  data: 0.0006  max mem: 19734
Epoch: [40]  [ 650/1251]  eta: 0:08:24  lr: 0.000014  loss: 3.6448 (3.4229)  time: 0.8011  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3640, ratio_loss=0.0048, pruning_loss=0.1277, mse_loss=0.5626
Epoch: [40]  [ 660/1251]  eta: 0:08:16  lr: 0.000014  loss: 3.6448 (3.4245)  time: 0.7982  data: 0.0005  max mem: 19734
Epoch: [40]  [ 670/1251]  eta: 0:08:07  lr: 0.000014  loss: 3.5462 (3.4230)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [40]  [ 680/1251]  eta: 0:07:58  lr: 0.000014  loss: 3.5500 (3.4259)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [40]  [ 690/1251]  eta: 0:07:50  lr: 0.000014  loss: 3.7735 (3.4289)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [40]  [ 700/1251]  eta: 0:07:41  lr: 0.000014  loss: 3.5720 (3.4306)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [40]  [ 710/1251]  eta: 0:07:32  lr: 0.000014  loss: 3.4794 (3.4272)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [40]  [ 720/1251]  eta: 0:07:24  lr: 0.000014  loss: 3.2260 (3.4252)  time: 0.8212  data: 0.0006  max mem: 19734
Epoch: [40]  [ 730/1251]  eta: 0:07:15  lr: 0.000014  loss: 3.5664 (3.4255)  time: 0.8210  data: 0.0007  max mem: 19734
Epoch: [40]  [ 740/1251]  eta: 0:07:07  lr: 0.000014  loss: 3.4096 (3.4208)  time: 0.8292  data: 0.0007  max mem: 19734
Epoch: [40]  [ 750/1251]  eta: 0:06:58  lr: 0.000014  loss: 3.0950 (3.4188)  time: 0.8273  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3600, ratio_loss=0.0045, pruning_loss=0.1273, mse_loss=0.5791
Epoch: [40]  [ 760/1251]  eta: 0:06:50  lr: 0.000014  loss: 3.2582 (3.4170)  time: 0.8025  data: 0.0006  max mem: 19734
Epoch: [40]  [ 770/1251]  eta: 0:06:41  lr: 0.000014  loss: 3.4512 (3.4176)  time: 0.7973  data: 0.0006  max mem: 19734
Epoch: [40]  [ 780/1251]  eta: 0:06:33  lr: 0.000014  loss: 3.4524 (3.4162)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [40]  [ 790/1251]  eta: 0:06:24  lr: 0.000014  loss: 3.6775 (3.4193)  time: 0.8062  data: 0.0005  max mem: 19734
Epoch: [40]  [ 800/1251]  eta: 0:06:16  lr: 0.000014  loss: 3.6897 (3.4188)  time: 0.8005  data: 0.0006  max mem: 19734
Epoch: [40]  [ 810/1251]  eta: 0:06:07  lr: 0.000014  loss: 3.6539 (3.4207)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [40]  [ 820/1251]  eta: 0:05:59  lr: 0.000014  loss: 3.6991 (3.4226)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [40]  [ 830/1251]  eta: 0:05:50  lr: 0.000014  loss: 3.5952 (3.4245)  time: 0.7980  data: 0.0005  max mem: 19734
Epoch: [40]  [ 840/1251]  eta: 0:05:42  lr: 0.000014  loss: 3.4930 (3.4270)  time: 0.7984  data: 0.0005  max mem: 19734
Epoch: [40]  [ 850/1251]  eta: 0:05:33  lr: 0.000014  loss: 3.6023 (3.4271)  time: 0.8091  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5087, ratio_loss=0.0050, pruning_loss=0.1243, mse_loss=0.5707
Epoch: [40]  [ 860/1251]  eta: 0:05:25  lr: 0.000014  loss: 3.7255 (3.4303)  time: 0.8077  data: 0.0006  max mem: 19734
Epoch: [40]  [ 870/1251]  eta: 0:05:17  lr: 0.000014  loss: 3.5941 (3.4303)  time: 0.8229  data: 0.0007  max mem: 19734
Epoch: [40]  [ 880/1251]  eta: 0:05:08  lr: 0.000014  loss: 3.5941 (3.4331)  time: 0.8348  data: 0.0006  max mem: 19734
Epoch: [40]  [ 890/1251]  eta: 0:05:00  lr: 0.000014  loss: 3.5602 (3.4325)  time: 0.8183  data: 0.0006  max mem: 19734
Epoch: [40]  [ 900/1251]  eta: 0:04:51  lr: 0.000014  loss: 3.5187 (3.4328)  time: 0.8114  data: 0.0006  max mem: 19734
Epoch: [40]  [ 910/1251]  eta: 0:04:43  lr: 0.000014  loss: 3.5980 (3.4335)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [40]  [ 920/1251]  eta: 0:04:35  lr: 0.000014  loss: 3.5275 (3.4344)  time: 0.8000  data: 0.0006  max mem: 19734
Epoch: [40]  [ 930/1251]  eta: 0:04:26  lr: 0.000014  loss: 3.5205 (3.4345)  time: 0.8096  data: 0.0006  max mem: 19734
Epoch: [40]  [ 940/1251]  eta: 0:04:18  lr: 0.000014  loss: 3.5135 (3.4346)  time: 0.8112  data: 0.0005  max mem: 19734
Epoch: [40]  [ 950/1251]  eta: 0:04:09  lr: 0.000014  loss: 3.3726 (3.4305)  time: 0.8035  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3817, ratio_loss=0.0051, pruning_loss=0.1263, mse_loss=0.5745
Epoch: [40]  [ 960/1251]  eta: 0:04:01  lr: 0.000014  loss: 3.0487 (3.4284)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [40]  [ 970/1251]  eta: 0:03:53  lr: 0.000014  loss: 3.3390 (3.4274)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [40]  [ 980/1251]  eta: 0:03:44  lr: 0.000014  loss: 3.5220 (3.4276)  time: 0.8008  data: 0.0006  max mem: 19734
Epoch: [40]  [ 990/1251]  eta: 0:03:36  lr: 0.000014  loss: 3.5339 (3.4279)  time: 0.8006  data: 0.0008  max mem: 19734
Epoch: [40]  [1000/1251]  eta: 0:03:28  lr: 0.000014  loss: 3.6366 (3.4302)  time: 0.8124  data: 0.0009  max mem: 19734
Epoch: [40]  [1010/1251]  eta: 0:03:19  lr: 0.000014  loss: 3.5972 (3.4294)  time: 0.8213  data: 0.0007  max mem: 19734
Epoch: [40]  [1020/1251]  eta: 0:03:11  lr: 0.000014  loss: 3.4556 (3.4300)  time: 0.8175  data: 0.0006  max mem: 19734
Epoch: [40]  [1030/1251]  eta: 0:03:03  lr: 0.000014  loss: 3.5639 (3.4287)  time: 0.8157  data: 0.0006  max mem: 19734
Epoch: [40]  [1040/1251]  eta: 0:02:54  lr: 0.000014  loss: 3.3052 (3.4274)  time: 0.8079  data: 0.0005  max mem: 19734
Epoch: [40]  [1050/1251]  eta: 0:02:46  lr: 0.000014  loss: 3.5528 (3.4282)  time: 0.8049  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4081, ratio_loss=0.0047, pruning_loss=0.1252, mse_loss=0.5649
Epoch: [40]  [1060/1251]  eta: 0:02:38  lr: 0.000014  loss: 3.4663 (3.4278)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [40]  [1070/1251]  eta: 0:02:29  lr: 0.000014  loss: 3.4053 (3.4270)  time: 0.7989  data: 0.0006  max mem: 19734
Epoch: [40]  [1080/1251]  eta: 0:02:21  lr: 0.000014  loss: 3.4467 (3.4272)  time: 0.8073  data: 0.0006  max mem: 19734
Epoch: [40]  [1090/1251]  eta: 0:02:13  lr: 0.000014  loss: 3.5596 (3.4241)  time: 0.8077  data: 0.0006  max mem: 19734
Epoch: [40]  [1100/1251]  eta: 0:02:04  lr: 0.000014  loss: 3.3442 (3.4219)  time: 0.8002  data: 0.0006  max mem: 19734
Epoch: [40]  [1110/1251]  eta: 0:01:56  lr: 0.000014  loss: 3.3484 (3.4209)  time: 0.7994  data: 0.0004  max mem: 19734
Epoch: [40]  [1120/1251]  eta: 0:01:48  lr: 0.000014  loss: 3.5256 (3.4216)  time: 0.8005  data: 0.0005  max mem: 19734
Epoch: [40]  [1130/1251]  eta: 0:01:39  lr: 0.000014  loss: 3.6406 (3.4220)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [40]  [1140/1251]  eta: 0:01:31  lr: 0.000014  loss: 3.5274 (3.4202)  time: 0.8100  data: 0.0006  max mem: 19734
Epoch: [40]  [1150/1251]  eta: 0:01:23  lr: 0.000014  loss: 3.3613 (3.4199)  time: 0.8100  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3178, ratio_loss=0.0047, pruning_loss=0.1271, mse_loss=0.5795
Epoch: [40]  [1160/1251]  eta: 0:01:15  lr: 0.000014  loss: 3.5111 (3.4205)  time: 0.8108  data: 0.0005  max mem: 19734
Epoch: [40]  [1170/1251]  eta: 0:01:06  lr: 0.000014  loss: 3.3737 (3.4175)  time: 0.8160  data: 0.0006  max mem: 19734
Epoch: [40]  [1180/1251]  eta: 0:00:58  lr: 0.000014  loss: 2.8548 (3.4142)  time: 0.8189  data: 0.0006  max mem: 19734
Epoch: [40]  [1190/1251]  eta: 0:00:50  lr: 0.000014  loss: 2.8150 (3.4111)  time: 0.8192  data: 0.0010  max mem: 19734
Epoch: [40]  [1200/1251]  eta: 0:00:42  lr: 0.000014  loss: 3.1359 (3.4099)  time: 0.8006  data: 0.0007  max mem: 19734
Epoch: [40]  [1210/1251]  eta: 0:00:33  lr: 0.000014  loss: 2.9547 (3.4065)  time: 0.7900  data: 0.0001  max mem: 19734
Epoch: [40]  [1220/1251]  eta: 0:00:25  lr: 0.000014  loss: 3.4614 (3.4067)  time: 0.7903  data: 0.0001  max mem: 19734
Epoch: [40]  [1230/1251]  eta: 0:00:17  lr: 0.000014  loss: 3.5011 (3.4063)  time: 0.7968  data: 0.0002  max mem: 19734
Epoch: [40]  [1240/1251]  eta: 0:00:09  lr: 0.000014  loss: 3.5011 (3.4062)  time: 0.7980  data: 0.0002  max mem: 19734
Epoch: [40]  [1250/1251]  eta: 0:00:00  lr: 0.000014  loss: 3.4966 (3.4059)  time: 0.7936  data: 0.0002  max mem: 19734
Epoch: [40] Total time: 0:17:12 (0.8251 s / it)
Averaged stats: lr: 0.000014  loss: 3.4966 (3.4013)
Test:  [  0/261]  eta: 0:59:23  loss: 0.6471 (0.6471)  acc1: 85.4167 (85.4167)  acc5: 96.8750 (96.8750)  time: 13.6527  data: 13.3641  max mem: 19734
Test:  [ 10/261]  eta: 0:08:40  loss: 0.6471 (0.7187)  acc1: 85.4167 (83.6648)  acc5: 96.8750 (96.5436)  time: 2.0737  data: 1.9095  max mem: 19734
Test:  [ 20/261]  eta: 0:05:18  loss: 0.9303 (0.9000)  acc1: 77.6042 (78.9435)  acc5: 93.7500 (94.6925)  time: 0.7062  data: 0.5035  max mem: 19734
Test:  [ 30/261]  eta: 0:03:43  loss: 0.8324 (0.8159)  acc1: 83.3333 (81.8548)  acc5: 94.2708 (95.1949)  time: 0.3563  data: 0.1269  max mem: 19734
Test:  [ 40/261]  eta: 0:03:12  loss: 0.5779 (0.7777)  acc1: 88.5417 (82.9141)  acc5: 97.3958 (95.5158)  time: 0.3973  data: 0.1500  max mem: 19734
Test:  [ 50/261]  eta: 0:03:06  loss: 0.8916 (0.8348)  acc1: 77.0833 (81.2194)  acc5: 95.3125 (95.1797)  time: 0.7555  data: 0.5047  max mem: 19734
Test:  [ 60/261]  eta: 0:02:37  loss: 0.9581 (0.8430)  acc1: 76.5625 (80.8231)  acc5: 95.3125 (95.2271)  time: 0.5985  data: 0.3692  max mem: 19734
Test:  [ 70/261]  eta: 0:02:24  loss: 0.9121 (0.8473)  acc1: 77.6042 (80.3477)  acc5: 96.3542 (95.4079)  time: 0.4292  data: 0.0849  max mem: 19734
Test:  [ 80/261]  eta: 0:02:22  loss: 0.7604 (0.8475)  acc1: 79.6875 (80.5105)  acc5: 96.8750 (95.5183)  time: 0.8002  data: 0.4679  max mem: 19734
Test:  [ 90/261]  eta: 0:02:04  loss: 0.7871 (0.8354)  acc1: 83.8542 (80.8551)  acc5: 95.8333 (95.5586)  time: 0.6235  data: 0.4031  max mem: 19734
Test:  [100/261]  eta: 0:01:59  loss: 0.8246 (0.8383)  acc1: 83.8542 (80.7756)  acc5: 95.3125 (95.6013)  time: 0.5649  data: 0.2523  max mem: 19734
Test:  [110/261]  eta: 0:01:45  loss: 0.8564 (0.8614)  acc1: 76.5625 (80.2318)  acc5: 94.2708 (95.3266)  time: 0.5675  data: 0.2672  max mem: 19734
Test:  [120/261]  eta: 0:01:32  loss: 1.1853 (0.9028)  acc1: 71.3542 (79.2829)  acc5: 90.1042 (94.7874)  time: 0.2398  data: 0.0352  max mem: 19734
Test:  [130/261]  eta: 0:01:23  loss: 1.3961 (0.9485)  acc1: 67.7083 (78.3516)  acc5: 86.9792 (94.1595)  time: 0.2846  data: 0.0224  max mem: 19734
Test:  [140/261]  eta: 0:01:20  loss: 1.2342 (0.9742)  acc1: 68.7500 (77.6891)  acc5: 89.0625 (93.8941)  time: 0.6773  data: 0.4013  max mem: 19734
Test:  [150/261]  eta: 0:01:11  loss: 1.1980 (0.9798)  acc1: 72.3958 (77.6283)  acc5: 91.6667 (93.7535)  time: 0.6965  data: 0.3960  max mem: 19734
Test:  [160/261]  eta: 0:01:03  loss: 0.9818 (0.9990)  acc1: 78.1250 (77.3260)  acc5: 92.1875 (93.4686)  time: 0.3706  data: 0.0213  max mem: 19734
Test:  [170/261]  eta: 0:00:55  loss: 1.2724 (1.0286)  acc1: 65.1042 (76.5808)  acc5: 87.5000 (93.1591)  time: 0.3304  data: 0.0174  max mem: 19734
Test:  [180/261]  eta: 0:00:48  loss: 1.3905 (1.0450)  acc1: 65.1042 (76.1971)  acc5: 89.5833 (93.0191)  time: 0.3650  data: 0.0119  max mem: 19734
Test:  [190/261]  eta: 0:00:41  loss: 1.3449 (1.0580)  acc1: 68.2292 (75.9599)  acc5: 90.1042 (92.8501)  time: 0.3916  data: 0.0129  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.3174 (1.0733)  acc1: 70.3125 (75.6297)  acc5: 89.5833 (92.6151)  time: 0.2889  data: 0.0093  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.3295 (1.0864)  acc1: 69.2708 (75.3382)  acc5: 88.0208 (92.4097)  time: 0.3559  data: 0.1632  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4423 (1.1057)  acc1: 66.6667 (74.8445)  acc5: 88.5417 (92.2229)  time: 0.2990  data: 0.1591  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4610 (1.1145)  acc1: 66.6667 (74.6099)  acc5: 89.0625 (92.1334)  time: 0.1163  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.2827 (1.1229)  acc1: 69.2708 (74.4122)  acc5: 91.1458 (92.0622)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0369 (1.1162)  acc1: 74.4792 (74.5767)  acc5: 92.7083 (92.1688)  time: 0.1143  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9286 (1.1160)  acc1: 76.0417 (74.5800)  acc5: 95.3125 (92.2300)  time: 0.1111  data: 0.0001  max mem: 19734
Test: Total time: 0:02:04 (0.4774 s / it)
* Acc@1 74.580 Acc@5 92.230 loss 1.116
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.68%
Epoch: [41]  [   0/1251]  eta: 6:50:12  lr: 0.000013  loss: 3.7288 (3.7288)  time: 19.6742  data: 6.6349  max mem: 19734
loss info: cls_loss=3.2569, ratio_loss=0.0048, pruning_loss=0.1301, mse_loss=0.5705
Epoch: [41]  [  10/1251]  eta: 0:55:47  lr: 0.000013  loss: 3.7288 (3.5640)  time: 2.6977  data: 0.7201  max mem: 19734
Epoch: [41]  [  20/1251]  eta: 0:36:49  lr: 0.000013  loss: 3.5124 (3.4614)  time: 0.9005  data: 0.0647  max mem: 19734
Epoch: [41]  [  30/1251]  eta: 0:30:08  lr: 0.000013  loss: 3.0795 (3.2675)  time: 0.8123  data: 0.0006  max mem: 19734
Epoch: [41]  [  40/1251]  eta: 0:26:32  lr: 0.000013  loss: 3.0593 (3.3327)  time: 0.8124  data: 0.0005  max mem: 19734
Epoch: [41]  [  50/1251]  eta: 0:24:22  lr: 0.000013  loss: 3.7466 (3.3909)  time: 0.8099  data: 0.0005  max mem: 19734
Epoch: [41]  [  60/1251]  eta: 0:22:55  lr: 0.000013  loss: 3.6310 (3.3918)  time: 0.8271  data: 0.0004  max mem: 19734
Epoch: [41]  [  70/1251]  eta: 0:21:49  lr: 0.000013  loss: 3.6204 (3.4229)  time: 0.8290  data: 0.0004  max mem: 19734
Epoch: [41]  [  80/1251]  eta: 0:20:54  lr: 0.000013  loss: 3.5962 (3.4224)  time: 0.8140  data: 0.0005  max mem: 19734
Epoch: [41]  [  90/1251]  eta: 0:20:09  lr: 0.000013  loss: 3.4994 (3.4042)  time: 0.8039  data: 0.0005  max mem: 19734
Epoch: [41]  [ 100/1251]  eta: 0:19:31  lr: 0.000013  loss: 3.5087 (3.4137)  time: 0.8027  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3546, ratio_loss=0.0050, pruning_loss=0.1276, mse_loss=0.5583
Epoch: [41]  [ 110/1251]  eta: 0:19:00  lr: 0.000013  loss: 3.5087 (3.4134)  time: 0.8073  data: 0.0004  max mem: 19734
Epoch: [41]  [ 120/1251]  eta: 0:18:31  lr: 0.000013  loss: 3.4760 (3.4108)  time: 0.8063  data: 0.0004  max mem: 19734
Epoch: [41]  [ 130/1251]  eta: 0:18:06  lr: 0.000013  loss: 3.6193 (3.4377)  time: 0.7998  data: 0.0004  max mem: 19734
Epoch: [41]  [ 140/1251]  eta: 0:17:43  lr: 0.000013  loss: 3.7054 (3.4464)  time: 0.8000  data: 0.0005  max mem: 19734
Epoch: [41]  [ 150/1251]  eta: 0:17:22  lr: 0.000013  loss: 3.5797 (3.4519)  time: 0.8021  data: 0.0006  max mem: 19734
Epoch: [41]  [ 160/1251]  eta: 0:17:03  lr: 0.000013  loss: 3.5267 (3.4503)  time: 0.8024  data: 0.0006  max mem: 19734
Epoch: [41]  [ 170/1251]  eta: 0:16:45  lr: 0.000013  loss: 3.3376 (3.4431)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [41]  [ 180/1251]  eta: 0:16:29  lr: 0.000013  loss: 3.3376 (3.4426)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [41]  [ 190/1251]  eta: 0:16:13  lr: 0.000013  loss: 3.1914 (3.4168)  time: 0.8107  data: 0.0005  max mem: 19734
Epoch: [41]  [ 200/1251]  eta: 0:16:00  lr: 0.000013  loss: 3.3365 (3.4119)  time: 0.8230  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4122, ratio_loss=0.0046, pruning_loss=0.1256, mse_loss=0.5929
Epoch: [41]  [ 210/1251]  eta: 0:15:47  lr: 0.000013  loss: 3.4778 (3.4190)  time: 0.8386  data: 0.0005  max mem: 19734
Epoch: [41]  [ 220/1251]  eta: 0:15:33  lr: 0.000013  loss: 3.6146 (3.4237)  time: 0.8220  data: 0.0005  max mem: 19734
Epoch: [41]  [ 230/1251]  eta: 0:15:19  lr: 0.000013  loss: 3.5583 (3.4256)  time: 0.8053  data: 0.0005  max mem: 19734
Epoch: [41]  [ 240/1251]  eta: 0:15:06  lr: 0.000013  loss: 3.6064 (3.4327)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [41]  [ 250/1251]  eta: 0:14:53  lr: 0.000013  loss: 3.6243 (3.4413)  time: 0.7998  data: 0.0004  max mem: 19734
Epoch: [41]  [ 260/1251]  eta: 0:14:41  lr: 0.000013  loss: 3.3446 (3.4308)  time: 0.8082  data: 0.0004  max mem: 19734
Epoch: [41]  [ 270/1251]  eta: 0:14:29  lr: 0.000013  loss: 3.5570 (3.4371)  time: 0.8077  data: 0.0004  max mem: 19734
Epoch: [41]  [ 280/1251]  eta: 0:14:17  lr: 0.000013  loss: 3.5497 (3.4248)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [41]  [ 290/1251]  eta: 0:14:06  lr: 0.000013  loss: 3.3168 (3.4257)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [41]  [ 300/1251]  eta: 0:13:55  lr: 0.000013  loss: 3.5427 (3.4257)  time: 0.8056  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4309, ratio_loss=0.0049, pruning_loss=0.1252, mse_loss=0.5658
Epoch: [41]  [ 310/1251]  eta: 0:13:44  lr: 0.000013  loss: 3.5445 (3.4262)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [41]  [ 320/1251]  eta: 0:13:34  lr: 0.000013  loss: 3.5606 (3.4334)  time: 0.8139  data: 0.0005  max mem: 19734
Epoch: [41]  [ 330/1251]  eta: 0:13:23  lr: 0.000013  loss: 3.5957 (3.4343)  time: 0.8130  data: 0.0005  max mem: 19734
Epoch: [41]  [ 340/1251]  eta: 0:13:13  lr: 0.000013  loss: 3.4241 (3.4284)  time: 0.8144  data: 0.0005  max mem: 19734
Epoch: [41]  [ 350/1251]  eta: 0:13:03  lr: 0.000013  loss: 3.5293 (3.4389)  time: 0.8286  data: 0.0006  max mem: 19734
Epoch: [41]  [ 360/1251]  eta: 0:12:53  lr: 0.000013  loss: 3.5863 (3.4341)  time: 0.8287  data: 0.0005  max mem: 19734
Epoch: [41]  [ 370/1251]  eta: 0:12:43  lr: 0.000013  loss: 3.2582 (3.4270)  time: 0.8185  data: 0.0005  max mem: 19734
Epoch: [41]  [ 380/1251]  eta: 0:12:33  lr: 0.000013  loss: 3.5082 (3.4319)  time: 0.8062  data: 0.0005  max mem: 19734
Epoch: [41]  [ 390/1251]  eta: 0:12:23  lr: 0.000013  loss: 3.5693 (3.4332)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [41]  [ 400/1251]  eta: 0:12:14  lr: 0.000013  loss: 3.4716 (3.4302)  time: 0.8141  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3884, ratio_loss=0.0047, pruning_loss=0.1267, mse_loss=0.5692
Epoch: [41]  [ 410/1251]  eta: 0:12:04  lr: 0.000013  loss: 3.3235 (3.4244)  time: 0.8146  data: 0.0006  max mem: 19734
Epoch: [41]  [ 420/1251]  eta: 0:11:54  lr: 0.000013  loss: 3.3087 (3.4227)  time: 0.8020  data: 0.0007  max mem: 19734
Epoch: [41]  [ 430/1251]  eta: 0:11:44  lr: 0.000013  loss: 3.2471 (3.4174)  time: 0.7995  data: 0.0007  max mem: 19734
Epoch: [41]  [ 440/1251]  eta: 0:11:35  lr: 0.000013  loss: 3.1100 (3.4135)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [41]  [ 450/1251]  eta: 0:11:25  lr: 0.000013  loss: 3.5008 (3.4172)  time: 0.7992  data: 0.0006  max mem: 19734
Epoch: [41]  [ 460/1251]  eta: 0:11:15  lr: 0.000013  loss: 3.6230 (3.4194)  time: 0.7995  data: 0.0006  max mem: 19734
Epoch: [41]  [ 470/1251]  eta: 0:11:06  lr: 0.000013  loss: 3.4719 (3.4152)  time: 0.8077  data: 0.0005  max mem: 19734
Epoch: [41]  [ 480/1251]  eta: 0:10:57  lr: 0.000013  loss: 3.2716 (3.4104)  time: 0.8101  data: 0.0005  max mem: 19734
Epoch: [41]  [ 490/1251]  eta: 0:10:48  lr: 0.000013  loss: 3.2716 (3.4110)  time: 0.8255  data: 0.0006  max mem: 19734
Epoch: [41]  [ 500/1251]  eta: 0:10:39  lr: 0.000013  loss: 3.5233 (3.4107)  time: 0.8313  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3435, ratio_loss=0.0045, pruning_loss=0.1285, mse_loss=0.5735
Epoch: [41]  [ 510/1251]  eta: 0:10:31  lr: 0.000013  loss: 3.3668 (3.4103)  time: 0.8287  data: 0.0005  max mem: 19734
Epoch: [41]  [ 520/1251]  eta: 0:10:21  lr: 0.000013  loss: 3.3751 (3.4110)  time: 0.8206  data: 0.0005  max mem: 19734
Epoch: [41]  [ 530/1251]  eta: 0:10:12  lr: 0.000013  loss: 3.2710 (3.4088)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [41]  [ 540/1251]  eta: 0:10:03  lr: 0.000013  loss: 3.1947 (3.4073)  time: 0.8015  data: 0.0005  max mem: 19734
Epoch: [41]  [ 550/1251]  eta: 0:09:54  lr: 0.000013  loss: 3.4587 (3.4079)  time: 0.8084  data: 0.0005  max mem: 19734
Epoch: [41]  [ 560/1251]  eta: 0:09:45  lr: 0.000013  loss: 3.4587 (3.4074)  time: 0.8080  data: 0.0004  max mem: 19734
Epoch: [41]  [ 570/1251]  eta: 0:09:36  lr: 0.000013  loss: 3.4788 (3.4063)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [41]  [ 580/1251]  eta: 0:09:27  lr: 0.000013  loss: 3.4788 (3.4055)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [41]  [ 590/1251]  eta: 0:09:18  lr: 0.000013  loss: 3.4295 (3.4036)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [41]  [ 600/1251]  eta: 0:09:09  lr: 0.000013  loss: 3.5773 (3.4072)  time: 0.7995  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3910, ratio_loss=0.0045, pruning_loss=0.1276, mse_loss=0.5768
Epoch: [41]  [ 610/1251]  eta: 0:09:00  lr: 0.000013  loss: 3.7100 (3.4090)  time: 0.8016  data: 0.0007  max mem: 19734
Epoch: [41]  [ 620/1251]  eta: 0:08:52  lr: 0.000013  loss: 3.5683 (3.4097)  time: 0.8132  data: 0.0007  max mem: 19734
Epoch: [41]  [ 630/1251]  eta: 0:08:43  lr: 0.000013  loss: 3.6385 (3.4115)  time: 0.8270  data: 0.0005  max mem: 19734
Epoch: [41]  [ 640/1251]  eta: 0:08:34  lr: 0.000013  loss: 3.7196 (3.4150)  time: 0.8224  data: 0.0005  max mem: 19734
Epoch: [41]  [ 650/1251]  eta: 0:08:26  lr: 0.000013  loss: 3.6688 (3.4172)  time: 0.8234  data: 0.0005  max mem: 19734
Epoch: [41]  [ 660/1251]  eta: 0:08:17  lr: 0.000013  loss: 3.5917 (3.4175)  time: 0.8240  data: 0.0005  max mem: 19734
Epoch: [41]  [ 670/1251]  eta: 0:08:08  lr: 0.000013  loss: 3.5484 (3.4196)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [41]  [ 680/1251]  eta: 0:08:00  lr: 0.000013  loss: 3.3934 (3.4176)  time: 0.8009  data: 0.0005  max mem: 19734
Epoch: [41]  [ 690/1251]  eta: 0:07:51  lr: 0.000013  loss: 3.4371 (3.4186)  time: 0.8043  data: 0.0005  max mem: 19734
Epoch: [41]  [ 700/1251]  eta: 0:07:42  lr: 0.000013  loss: 3.4371 (3.4160)  time: 0.8041  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4281, ratio_loss=0.0045, pruning_loss=0.1254, mse_loss=0.5831
Epoch: [41]  [ 710/1251]  eta: 0:07:34  lr: 0.000013  loss: 3.1631 (3.4153)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [41]  [ 720/1251]  eta: 0:07:25  lr: 0.000013  loss: 3.4633 (3.4166)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [41]  [ 730/1251]  eta: 0:07:16  lr: 0.000013  loss: 3.6072 (3.4165)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [41]  [ 740/1251]  eta: 0:07:08  lr: 0.000013  loss: 3.5571 (3.4149)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [41]  [ 750/1251]  eta: 0:06:59  lr: 0.000013  loss: 3.6602 (3.4179)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [41]  [ 760/1251]  eta: 0:06:51  lr: 0.000013  loss: 3.3325 (3.4144)  time: 0.8129  data: 0.0004  max mem: 19734
Epoch: [41]  [ 770/1251]  eta: 0:06:42  lr: 0.000013  loss: 3.2135 (3.4139)  time: 0.8108  data: 0.0006  max mem: 19734
Epoch: [41]  [ 780/1251]  eta: 0:06:34  lr: 0.000013  loss: 3.3996 (3.4123)  time: 0.8163  data: 0.0006  max mem: 19734
Epoch: [41]  [ 790/1251]  eta: 0:06:25  lr: 0.000013  loss: 3.4411 (3.4140)  time: 0.8274  data: 0.0005  max mem: 19734
Epoch: [41]  [ 800/1251]  eta: 0:06:17  lr: 0.000013  loss: 3.4164 (3.4111)  time: 0.8316  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3838, ratio_loss=0.0046, pruning_loss=0.1275, mse_loss=0.5515
Epoch: [41]  [ 810/1251]  eta: 0:06:08  lr: 0.000013  loss: 3.5035 (3.4132)  time: 0.8201  data: 0.0004  max mem: 19734
Epoch: [41]  [ 820/1251]  eta: 0:06:00  lr: 0.000013  loss: 3.5581 (3.4132)  time: 0.7978  data: 0.0004  max mem: 19734
Epoch: [41]  [ 830/1251]  eta: 0:05:51  lr: 0.000013  loss: 3.4835 (3.4151)  time: 0.7977  data: 0.0004  max mem: 19734
Epoch: [41]  [ 840/1251]  eta: 0:05:43  lr: 0.000013  loss: 3.5708 (3.4179)  time: 0.8047  data: 0.0005  max mem: 19734
Epoch: [41]  [ 850/1251]  eta: 0:05:34  lr: 0.000013  loss: 3.6992 (3.4217)  time: 0.8065  data: 0.0005  max mem: 19734
Epoch: [41]  [ 860/1251]  eta: 0:05:26  lr: 0.000013  loss: 3.6660 (3.4229)  time: 0.8010  data: 0.0005  max mem: 19734
Epoch: [41]  [ 870/1251]  eta: 0:05:17  lr: 0.000013  loss: 3.4887 (3.4247)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [41]  [ 880/1251]  eta: 0:05:09  lr: 0.000013  loss: 3.4887 (3.4255)  time: 0.7982  data: 0.0004  max mem: 19734
Epoch: [41]  [ 890/1251]  eta: 0:05:00  lr: 0.000013  loss: 3.3582 (3.4251)  time: 0.7974  data: 0.0004  max mem: 19734
Epoch: [41]  [ 900/1251]  eta: 0:04:52  lr: 0.000013  loss: 3.2753 (3.4249)  time: 0.7988  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4776, ratio_loss=0.0045, pruning_loss=0.1235, mse_loss=0.5873
Epoch: [41]  [ 910/1251]  eta: 0:04:43  lr: 0.000013  loss: 3.2550 (3.4223)  time: 0.8093  data: 0.0004  max mem: 19734
Epoch: [41]  [ 920/1251]  eta: 0:04:35  lr: 0.000013  loss: 3.4418 (3.4214)  time: 0.8078  data: 0.0004  max mem: 19734
Epoch: [41]  [ 930/1251]  eta: 0:04:27  lr: 0.000013  loss: 3.6829 (3.4238)  time: 0.8159  data: 0.0004  max mem: 19734
Epoch: [41]  [ 940/1251]  eta: 0:04:18  lr: 0.000013  loss: 3.7083 (3.4265)  time: 0.8238  data: 0.0004  max mem: 19734
Epoch: [41]  [ 950/1251]  eta: 0:04:10  lr: 0.000013  loss: 3.6470 (3.4260)  time: 0.8187  data: 0.0004  max mem: 19734
Epoch: [41]  [ 960/1251]  eta: 0:04:01  lr: 0.000013  loss: 3.6915 (3.4283)  time: 0.8121  data: 0.0005  max mem: 19734
Epoch: [41]  [ 970/1251]  eta: 0:03:53  lr: 0.000013  loss: 3.6779 (3.4274)  time: 0.8014  data: 0.0006  max mem: 19734
Epoch: [41]  [ 980/1251]  eta: 0:03:45  lr: 0.000013  loss: 3.4915 (3.4259)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [41]  [ 990/1251]  eta: 0:03:36  lr: 0.000013  loss: 3.4731 (3.4251)  time: 0.8083  data: 0.0005  max mem: 19734
Epoch: [41]  [1000/1251]  eta: 0:03:28  lr: 0.000013  loss: 3.4731 (3.4236)  time: 0.8082  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4028, ratio_loss=0.0046, pruning_loss=0.1265, mse_loss=0.5619
Epoch: [41]  [1010/1251]  eta: 0:03:20  lr: 0.000013  loss: 3.5034 (3.4238)  time: 0.7988  data: 0.0005  max mem: 19734
Epoch: [41]  [1020/1251]  eta: 0:03:11  lr: 0.000013  loss: 3.6460 (3.4267)  time: 0.7975  data: 0.0005  max mem: 19734
Epoch: [41]  [1030/1251]  eta: 0:03:03  lr: 0.000013  loss: 3.6119 (3.4266)  time: 0.7977  data: 0.0005  max mem: 19734
Epoch: [41]  [1040/1251]  eta: 0:02:54  lr: 0.000013  loss: 3.4700 (3.4270)  time: 0.7976  data: 0.0005  max mem: 19734
Epoch: [41]  [1050/1251]  eta: 0:02:46  lr: 0.000013  loss: 3.4700 (3.4251)  time: 0.7985  data: 0.0008  max mem: 19734
Epoch: [41]  [1060/1251]  eta: 0:02:38  lr: 0.000013  loss: 2.9427 (3.4218)  time: 0.8077  data: 0.0008  max mem: 19734
Epoch: [41]  [1070/1251]  eta: 0:02:29  lr: 0.000013  loss: 3.5272 (3.4219)  time: 0.8258  data: 0.0005  max mem: 19734
Epoch: [41]  [1080/1251]  eta: 0:02:21  lr: 0.000013  loss: 3.5784 (3.4241)  time: 0.8156  data: 0.0004  max mem: 19734
Epoch: [41]  [1090/1251]  eta: 0:02:13  lr: 0.000013  loss: 3.5503 (3.4247)  time: 0.8146  data: 0.0004  max mem: 19734
Epoch: [41]  [1100/1251]  eta: 0:02:05  lr: 0.000013  loss: 3.6095 (3.4268)  time: 0.8163  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4442, ratio_loss=0.0049, pruning_loss=0.1257, mse_loss=0.5744
Epoch: [41]  [1110/1251]  eta: 0:01:56  lr: 0.000013  loss: 3.6095 (3.4256)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [41]  [1120/1251]  eta: 0:01:48  lr: 0.000013  loss: 3.5516 (3.4282)  time: 0.7975  data: 0.0005  max mem: 19734
Epoch: [41]  [1130/1251]  eta: 0:01:40  lr: 0.000013  loss: 3.5975 (3.4285)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [41]  [1140/1251]  eta: 0:01:31  lr: 0.000013  loss: 3.5258 (3.4275)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [41]  [1150/1251]  eta: 0:01:23  lr: 0.000013  loss: 3.5962 (3.4293)  time: 0.7968  data: 0.0004  max mem: 19734
Epoch: [41]  [1160/1251]  eta: 0:01:15  lr: 0.000013  loss: 3.6566 (3.4296)  time: 0.7985  data: 0.0004  max mem: 19734
Epoch: [41]  [1170/1251]  eta: 0:01:06  lr: 0.000013  loss: 3.6156 (3.4301)  time: 0.8001  data: 0.0005  max mem: 19734
Epoch: [41]  [1180/1251]  eta: 0:00:58  lr: 0.000013  loss: 3.6156 (3.4319)  time: 0.7976  data: 0.0005  max mem: 19734
Epoch: [41]  [1190/1251]  eta: 0:00:50  lr: 0.000013  loss: 3.5903 (3.4296)  time: 0.7962  data: 0.0012  max mem: 19734
Epoch: [41]  [1200/1251]  eta: 0:00:42  lr: 0.000013  loss: 3.1767 (3.4278)  time: 0.7931  data: 0.0010  max mem: 19734
loss info: cls_loss=3.4288, ratio_loss=0.0047, pruning_loss=0.1253, mse_loss=0.5695
Epoch: [41]  [1210/1251]  eta: 0:00:33  lr: 0.000013  loss: 3.3175 (3.4281)  time: 0.7978  data: 0.0002  max mem: 19734
Epoch: [41]  [1220/1251]  eta: 0:00:25  lr: 0.000013  loss: 3.4522 (3.4277)  time: 0.8111  data: 0.0001  max mem: 19734
Epoch: [41]  [1230/1251]  eta: 0:00:17  lr: 0.000013  loss: 3.5377 (3.4275)  time: 0.8036  data: 0.0001  max mem: 19734
Epoch: [41]  [1240/1251]  eta: 0:00:09  lr: 0.000013  loss: 3.5012 (3.4280)  time: 0.8089  data: 0.0001  max mem: 19734
Epoch: [41]  [1250/1251]  eta: 0:00:00  lr: 0.000013  loss: 3.4567 (3.4283)  time: 0.8087  data: 0.0002  max mem: 19734
Epoch: [41] Total time: 0:17:13 (0.8260 s / it)
Averaged stats: lr: 0.000013  loss: 3.4567 (3.4149)
Test:  [  0/261]  eta: 3:02:25  loss: 0.6989 (0.6989)  acc1: 81.7708 (81.7708)  acc5: 95.8333 (95.8333)  time: 41.9364  data: 41.7848  max mem: 19734
Test:  [ 10/261]  eta: 0:19:08  loss: 0.6989 (0.7174)  acc1: 83.8542 (83.9015)  acc5: 96.8750 (96.4489)  time: 4.5753  data: 4.3287  max mem: 19734
Test:  [ 20/261]  eta: 0:10:38  loss: 0.9022 (0.8904)  acc1: 79.1667 (79.1667)  acc5: 93.7500 (94.8413)  time: 0.6851  data: 0.2996  max mem: 19734
Test:  [ 30/261]  eta: 0:07:29  loss: 0.8446 (0.8099)  acc1: 82.8125 (81.9724)  acc5: 93.7500 (95.2957)  time: 0.4981  data: 0.0171  max mem: 19734
Test:  [ 40/261]  eta: 0:05:45  loss: 0.5709 (0.7750)  acc1: 89.0625 (82.9522)  acc5: 96.8750 (95.5412)  time: 0.4188  data: 0.0152  max mem: 19734
Test:  [ 50/261]  eta: 0:04:37  loss: 0.8618 (0.8333)  acc1: 78.6458 (81.2092)  acc5: 94.7917 (95.1185)  time: 0.3352  data: 0.0096  max mem: 19734
Test:  [ 60/261]  eta: 0:03:48  loss: 0.9714 (0.8435)  acc1: 76.5625 (80.7889)  acc5: 93.7500 (95.1503)  time: 0.2728  data: 0.0092  max mem: 19734
Test:  [ 70/261]  eta: 0:03:15  loss: 0.9166 (0.8478)  acc1: 78.1250 (80.3037)  acc5: 96.3542 (95.3345)  time: 0.2808  data: 0.0116  max mem: 19734
Test:  [ 80/261]  eta: 0:02:59  loss: 0.8089 (0.8498)  acc1: 80.2083 (80.4141)  acc5: 96.3542 (95.4668)  time: 0.5422  data: 0.2537  max mem: 19734
Test:  [ 90/261]  eta: 0:02:36  loss: 0.8071 (0.8370)  acc1: 84.3750 (80.8207)  acc5: 96.3542 (95.5414)  time: 0.5425  data: 0.2560  max mem: 19734
Test:  [100/261]  eta: 0:02:19  loss: 0.7978 (0.8403)  acc1: 83.8542 (80.7653)  acc5: 95.3125 (95.5910)  time: 0.3621  data: 0.0508  max mem: 19734
Test:  [110/261]  eta: 0:02:16  loss: 0.8643 (0.8647)  acc1: 77.0833 (80.2177)  acc5: 94.2708 (95.2374)  time: 0.8456  data: 0.5571  max mem: 19734
Test:  [120/261]  eta: 0:02:00  loss: 1.1829 (0.9064)  acc1: 70.3125 (79.2485)  acc5: 89.5833 (94.6927)  time: 0.7718  data: 0.5198  max mem: 19734
Test:  [130/261]  eta: 0:01:44  loss: 1.3556 (0.9514)  acc1: 67.7083 (78.3317)  acc5: 87.5000 (94.0840)  time: 0.2205  data: 0.0114  max mem: 19734
Test:  [140/261]  eta: 0:01:35  loss: 1.3139 (0.9781)  acc1: 67.7083 (77.6559)  acc5: 89.5833 (93.8091)  time: 0.4200  data: 0.2337  max mem: 19734
Test:  [150/261]  eta: 0:01:23  loss: 1.2473 (0.9827)  acc1: 70.8333 (77.6456)  acc5: 91.6667 (93.7017)  time: 0.4718  data: 0.2317  max mem: 19734
Test:  [160/261]  eta: 0:01:13  loss: 1.0094 (1.0021)  acc1: 77.6042 (77.3324)  acc5: 91.6667 (93.3942)  time: 0.2682  data: 0.0081  max mem: 19734
Test:  [170/261]  eta: 0:01:03  loss: 1.2870 (1.0316)  acc1: 65.1042 (76.5838)  acc5: 87.5000 (93.0525)  time: 0.2463  data: 0.0160  max mem: 19734
Test:  [180/261]  eta: 0:00:54  loss: 1.4023 (1.0483)  acc1: 66.6667 (76.2230)  acc5: 89.5833 (92.9040)  time: 0.2816  data: 0.1164  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: 1.3884 (1.0629)  acc1: 68.7500 (75.9408)  acc5: 89.5833 (92.7220)  time: 0.2199  data: 0.1047  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 1.3286 (1.0786)  acc1: 72.3958 (75.6193)  acc5: 88.5417 (92.4674)  time: 0.1157  data: 0.0011  max mem: 19734
Test:  [210/261]  eta: 0:00:30  loss: 1.3286 (1.0917)  acc1: 71.3542 (75.3678)  acc5: 88.0208 (92.2813)  time: 0.1152  data: 0.0009  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.4212 (1.1104)  acc1: 68.2292 (74.8963)  acc5: 88.5417 (92.0979)  time: 0.1144  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:17  loss: 1.4250 (1.1195)  acc1: 67.1875 (74.6753)  acc5: 89.5833 (92.0161)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.3046 (1.1280)  acc1: 67.7083 (74.4359)  acc5: 91.1458 (91.9519)  time: 0.1149  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0666 (1.1215)  acc1: 75.0000 (74.5891)  acc5: 93.2292 (92.0651)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9147 (1.1215)  acc1: 75.0000 (74.6120)  acc5: 95.3125 (92.1340)  time: 0.1114  data: 0.0001  max mem: 19734
Test: Total time: 0:02:11 (0.5049 s / it)
* Acc@1 74.612 Acc@5 92.134 loss 1.122
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.68%
Epoch: [42]  [   0/1251]  eta: 6:32:57  lr: 0.000013  loss: 3.6374 (3.6374)  time: 18.8472  data: 15.1228  max mem: 19734
Epoch: [42]  [  10/1251]  eta: 0:55:03  lr: 0.000013  loss: 3.6861 (3.5978)  time: 2.6616  data: 1.3764  max mem: 19734
Epoch: [42]  [  20/1251]  eta: 0:36:31  lr: 0.000013  loss: 3.6998 (3.6046)  time: 0.9269  data: 0.0011  max mem: 19734
Epoch: [42]  [  30/1251]  eta: 0:29:48  lr: 0.000013  loss: 3.6823 (3.5662)  time: 0.8064  data: 0.0006  max mem: 19734
Epoch: [42]  [  40/1251]  eta: 0:26:17  lr: 0.000013  loss: 3.6055 (3.5985)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [42]  [  50/1251]  eta: 0:24:05  lr: 0.000013  loss: 3.5397 (3.5834)  time: 0.7997  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4654, ratio_loss=0.0048, pruning_loss=0.1244, mse_loss=0.5941
Epoch: [42]  [  60/1251]  eta: 0:22:35  lr: 0.000013  loss: 3.3915 (3.5405)  time: 0.7989  data: 0.0005  max mem: 19734
Epoch: [42]  [  70/1251]  eta: 0:21:27  lr: 0.000013  loss: 3.4852 (3.5386)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [42]  [  80/1251]  eta: 0:20:35  lr: 0.000013  loss: 3.5495 (3.5226)  time: 0.8026  data: 0.0007  max mem: 19734
Epoch: [42]  [  90/1251]  eta: 0:19:53  lr: 0.000013  loss: 3.3047 (3.4778)  time: 0.8079  data: 0.0008  max mem: 19734
Epoch: [42]  [ 100/1251]  eta: 0:19:26  lr: 0.000013  loss: 3.2952 (3.4815)  time: 0.8433  data: 0.0006  max mem: 19734
Epoch: [42]  [ 110/1251]  eta: 0:18:55  lr: 0.000013  loss: 3.4025 (3.4520)  time: 0.8482  data: 0.0005  max mem: 19734
Epoch: [42]  [ 120/1251]  eta: 0:18:28  lr: 0.000013  loss: 2.9226 (3.4112)  time: 0.8162  data: 0.0005  max mem: 19734
Epoch: [42]  [ 130/1251]  eta: 0:18:04  lr: 0.000013  loss: 2.9226 (3.3970)  time: 0.8129  data: 0.0005  max mem: 19734
Epoch: [42]  [ 140/1251]  eta: 0:17:42  lr: 0.000013  loss: 3.2814 (3.3917)  time: 0.8078  data: 0.0005  max mem: 19734
Epoch: [42]  [ 150/1251]  eta: 0:17:22  lr: 0.000013  loss: 3.5253 (3.3916)  time: 0.8080  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2935, ratio_loss=0.0049, pruning_loss=0.1291, mse_loss=0.5729
Epoch: [42]  [ 160/1251]  eta: 0:17:02  lr: 0.000013  loss: 3.5445 (3.3973)  time: 0.8052  data: 0.0005  max mem: 19734
Epoch: [42]  [ 170/1251]  eta: 0:16:45  lr: 0.000013  loss: 3.4943 (3.3946)  time: 0.8054  data: 0.0005  max mem: 19734
Epoch: [42]  [ 180/1251]  eta: 0:16:28  lr: 0.000013  loss: 3.5928 (3.4104)  time: 0.8071  data: 0.0005  max mem: 19734
Epoch: [42]  [ 190/1251]  eta: 0:16:12  lr: 0.000013  loss: 3.5312 (3.4086)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [42]  [ 200/1251]  eta: 0:15:57  lr: 0.000013  loss: 3.4645 (3.3987)  time: 0.8011  data: 0.0006  max mem: 19734
Epoch: [42]  [ 210/1251]  eta: 0:15:43  lr: 0.000013  loss: 3.5711 (3.4060)  time: 0.8066  data: 0.0006  max mem: 19734
Epoch: [42]  [ 220/1251]  eta: 0:15:29  lr: 0.000013  loss: 3.6127 (3.4154)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [42]  [ 230/1251]  eta: 0:15:15  lr: 0.000013  loss: 3.3901 (3.4069)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [42]  [ 240/1251]  eta: 0:15:04  lr: 0.000013  loss: 3.2145 (3.3912)  time: 0.8189  data: 0.0004  max mem: 19734
Epoch: [42]  [ 250/1251]  eta: 0:14:54  lr: 0.000013  loss: 3.1754 (3.3908)  time: 0.8485  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3835, ratio_loss=0.0045, pruning_loss=0.1256, mse_loss=0.5734
Epoch: [42]  [ 260/1251]  eta: 0:14:41  lr: 0.000013  loss: 3.3097 (3.3899)  time: 0.8303  data: 0.0005  max mem: 19734
Epoch: [42]  [ 270/1251]  eta: 0:14:29  lr: 0.000013  loss: 3.3908 (3.3940)  time: 0.8071  data: 0.0005  max mem: 19734
Epoch: [42]  [ 280/1251]  eta: 0:14:18  lr: 0.000013  loss: 3.2341 (3.3852)  time: 0.8098  data: 0.0005  max mem: 19734
Epoch: [42]  [ 290/1251]  eta: 0:14:06  lr: 0.000013  loss: 3.1918 (3.3855)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [42]  [ 300/1251]  eta: 0:13:55  lr: 0.000013  loss: 3.3462 (3.3815)  time: 0.8053  data: 0.0005  max mem: 19734
Epoch: [42]  [ 310/1251]  eta: 0:13:45  lr: 0.000013  loss: 3.4360 (3.3809)  time: 0.8165  data: 0.0005  max mem: 19734
Epoch: [42]  [ 320/1251]  eta: 0:13:34  lr: 0.000013  loss: 3.5152 (3.3849)  time: 0.8167  data: 0.0005  max mem: 19734
Epoch: [42]  [ 330/1251]  eta: 0:13:23  lr: 0.000013  loss: 3.5152 (3.3850)  time: 0.8053  data: 0.0005  max mem: 19734
Epoch: [42]  [ 340/1251]  eta: 0:13:13  lr: 0.000013  loss: 3.6490 (3.3908)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [42]  [ 350/1251]  eta: 0:13:02  lr: 0.000013  loss: 3.5099 (3.3906)  time: 0.8026  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3334, ratio_loss=0.0046, pruning_loss=0.1267, mse_loss=0.5798
Epoch: [42]  [ 360/1251]  eta: 0:12:52  lr: 0.000013  loss: 3.1908 (3.3827)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [42]  [ 370/1251]  eta: 0:12:42  lr: 0.000013  loss: 3.0261 (3.3740)  time: 0.8023  data: 0.0005  max mem: 19734
Epoch: [42]  [ 380/1251]  eta: 0:12:32  lr: 0.000013  loss: 3.4094 (3.3794)  time: 0.8051  data: 0.0005  max mem: 19734
Epoch: [42]  [ 390/1251]  eta: 0:12:23  lr: 0.000013  loss: 3.5090 (3.3826)  time: 0.8254  data: 0.0005  max mem: 19734
Epoch: [42]  [ 400/1251]  eta: 0:12:13  lr: 0.000013  loss: 3.6650 (3.3886)  time: 0.8377  data: 0.0005  max mem: 19734
Epoch: [42]  [ 410/1251]  eta: 0:12:04  lr: 0.000013  loss: 3.6650 (3.3866)  time: 0.8290  data: 0.0005  max mem: 19734
Epoch: [42]  [ 420/1251]  eta: 0:11:54  lr: 0.000013  loss: 3.3508 (3.3819)  time: 0.8224  data: 0.0005  max mem: 19734
Epoch: [42]  [ 430/1251]  eta: 0:11:45  lr: 0.000013  loss: 3.2602 (3.3798)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [42]  [ 440/1251]  eta: 0:11:35  lr: 0.000013  loss: 3.5509 (3.3847)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [42]  [ 450/1251]  eta: 0:11:26  lr: 0.000013  loss: 3.6470 (3.3860)  time: 0.8023  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3726, ratio_loss=0.0046, pruning_loss=0.1261, mse_loss=0.5975
Epoch: [42]  [ 460/1251]  eta: 0:11:16  lr: 0.000013  loss: 3.6470 (3.3859)  time: 0.8135  data: 0.0005  max mem: 19734
Epoch: [42]  [ 470/1251]  eta: 0:11:07  lr: 0.000013  loss: 3.3504 (3.3857)  time: 0.8113  data: 0.0005  max mem: 19734
Epoch: [42]  [ 480/1251]  eta: 0:10:58  lr: 0.000013  loss: 3.4438 (3.3866)  time: 0.8032  data: 0.0006  max mem: 19734
Epoch: [42]  [ 490/1251]  eta: 0:10:48  lr: 0.000013  loss: 3.4438 (3.3879)  time: 0.8061  data: 0.0006  max mem: 19734
Epoch: [42]  [ 500/1251]  eta: 0:10:39  lr: 0.000013  loss: 3.3927 (3.3890)  time: 0.8048  data: 0.0005  max mem: 19734
Epoch: [42]  [ 510/1251]  eta: 0:10:30  lr: 0.000013  loss: 3.5450 (3.3874)  time: 0.8035  data: 0.0005  max mem: 19734
Epoch: [42]  [ 520/1251]  eta: 0:10:21  lr: 0.000013  loss: 3.5285 (3.3923)  time: 0.8041  data: 0.0005  max mem: 19734
Epoch: [42]  [ 530/1251]  eta: 0:10:12  lr: 0.000013  loss: 3.4612 (3.3892)  time: 0.8132  data: 0.0005  max mem: 19734
Epoch: [42]  [ 540/1251]  eta: 0:10:03  lr: 0.000013  loss: 3.4244 (3.3909)  time: 0.8294  data: 0.0006  max mem: 19734
Epoch: [42]  [ 550/1251]  eta: 0:09:54  lr: 0.000013  loss: 3.4959 (3.3888)  time: 0.8277  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4034, ratio_loss=0.0048, pruning_loss=0.1255, mse_loss=0.5547
Epoch: [42]  [ 560/1251]  eta: 0:09:45  lr: 0.000013  loss: 3.7559 (3.3940)  time: 0.8194  data: 0.0005  max mem: 19734
Epoch: [42]  [ 570/1251]  eta: 0:09:36  lr: 0.000013  loss: 3.6976 (3.3940)  time: 0.8100  data: 0.0005  max mem: 19734
Epoch: [42]  [ 580/1251]  eta: 0:09:27  lr: 0.000013  loss: 3.6447 (3.3994)  time: 0.7989  data: 0.0005  max mem: 19734
Epoch: [42]  [ 590/1251]  eta: 0:09:18  lr: 0.000013  loss: 3.5190 (3.3956)  time: 0.8002  data: 0.0005  max mem: 19734
Epoch: [42]  [ 600/1251]  eta: 0:09:10  lr: 0.000013  loss: 3.1762 (3.3941)  time: 0.8037  data: 0.0005  max mem: 19734
Epoch: [42]  [ 610/1251]  eta: 0:09:01  lr: 0.000013  loss: 3.4488 (3.3950)  time: 0.8132  data: 0.0006  max mem: 19734
Epoch: [42]  [ 620/1251]  eta: 0:08:52  lr: 0.000013  loss: 3.6185 (3.4015)  time: 0.8108  data: 0.0006  max mem: 19734
Epoch: [42]  [ 630/1251]  eta: 0:08:43  lr: 0.000013  loss: 3.5261 (3.3948)  time: 0.8016  data: 0.0005  max mem: 19734
Epoch: [42]  [ 640/1251]  eta: 0:08:34  lr: 0.000013  loss: 3.2241 (3.3914)  time: 0.8022  data: 0.0005  max mem: 19734
Epoch: [42]  [ 650/1251]  eta: 0:08:25  lr: 0.000013  loss: 3.5498 (3.3939)  time: 0.8031  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3742, ratio_loss=0.0046, pruning_loss=0.1259, mse_loss=0.5838
Epoch: [42]  [ 660/1251]  eta: 0:08:17  lr: 0.000013  loss: 3.5498 (3.3903)  time: 0.8031  data: 0.0006  max mem: 19734
Epoch: [42]  [ 670/1251]  eta: 0:08:08  lr: 0.000013  loss: 3.1046 (3.3872)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [42]  [ 680/1251]  eta: 0:07:59  lr: 0.000013  loss: 3.6071 (3.3879)  time: 0.8119  data: 0.0005  max mem: 19734
Epoch: [42]  [ 690/1251]  eta: 0:07:51  lr: 0.000013  loss: 3.6071 (3.3878)  time: 0.8288  data: 0.0005  max mem: 19734
Epoch: [42]  [ 700/1251]  eta: 0:07:42  lr: 0.000013  loss: 3.5263 (3.3862)  time: 0.8175  data: 0.0007  max mem: 19734
Epoch: [42]  [ 710/1251]  eta: 0:07:34  lr: 0.000013  loss: 3.5263 (3.3846)  time: 0.8221  data: 0.0006  max mem: 19734
Epoch: [42]  [ 720/1251]  eta: 0:07:25  lr: 0.000013  loss: 3.5358 (3.3833)  time: 0.8219  data: 0.0005  max mem: 19734
Epoch: [42]  [ 730/1251]  eta: 0:07:16  lr: 0.000013  loss: 3.2201 (3.3815)  time: 0.7989  data: 0.0006  max mem: 19734
Epoch: [42]  [ 740/1251]  eta: 0:07:08  lr: 0.000013  loss: 3.4575 (3.3835)  time: 0.8010  data: 0.0006  max mem: 19734
Epoch: [42]  [ 750/1251]  eta: 0:06:59  lr: 0.000013  loss: 3.5314 (3.3828)  time: 0.8093  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3196, ratio_loss=0.0046, pruning_loss=0.1277, mse_loss=0.5573
Epoch: [42]  [ 760/1251]  eta: 0:06:51  lr: 0.000013  loss: 3.5036 (3.3843)  time: 0.8070  data: 0.0005  max mem: 19734
Epoch: [42]  [ 770/1251]  eta: 0:06:42  lr: 0.000013  loss: 3.5594 (3.3853)  time: 0.8015  data: 0.0004  max mem: 19734
Epoch: [42]  [ 780/1251]  eta: 0:06:33  lr: 0.000013  loss: 3.3954 (3.3806)  time: 0.8019  data: 0.0007  max mem: 19734
Epoch: [42]  [ 790/1251]  eta: 0:06:25  lr: 0.000013  loss: 3.2277 (3.3793)  time: 0.8015  data: 0.0007  max mem: 19734
Epoch: [42]  [ 800/1251]  eta: 0:06:16  lr: 0.000013  loss: 3.3455 (3.3804)  time: 0.8052  data: 0.0004  max mem: 19734
Epoch: [42]  [ 810/1251]  eta: 0:06:08  lr: 0.000013  loss: 3.3455 (3.3803)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [42]  [ 820/1251]  eta: 0:05:59  lr: 0.000013  loss: 3.4775 (3.3819)  time: 0.8000  data: 0.0005  max mem: 19734
Epoch: [42]  [ 830/1251]  eta: 0:05:51  lr: 0.000013  loss: 3.4775 (3.3823)  time: 0.8318  data: 0.0004  max mem: 19734
Epoch: [42]  [ 840/1251]  eta: 0:05:43  lr: 0.000013  loss: 3.3493 (3.3792)  time: 0.8388  data: 0.0004  max mem: 19734
Epoch: [42]  [ 850/1251]  eta: 0:05:34  lr: 0.000013  loss: 3.2329 (3.3776)  time: 0.8292  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3045, ratio_loss=0.0051, pruning_loss=0.1276, mse_loss=0.6027
Epoch: [42]  [ 860/1251]  eta: 0:05:26  lr: 0.000013  loss: 3.2329 (3.3778)  time: 0.8229  data: 0.0007  max mem: 19734
Epoch: [42]  [ 870/1251]  eta: 0:05:17  lr: 0.000013  loss: 3.5043 (3.3789)  time: 0.8033  data: 0.0006  max mem: 19734
Epoch: [42]  [ 880/1251]  eta: 0:05:09  lr: 0.000013  loss: 3.5043 (3.3799)  time: 0.8059  data: 0.0005  max mem: 19734
Epoch: [42]  [ 890/1251]  eta: 0:05:00  lr: 0.000013  loss: 3.3703 (3.3798)  time: 0.8069  data: 0.0005  max mem: 19734
Epoch: [42]  [ 900/1251]  eta: 0:04:52  lr: 0.000013  loss: 3.5065 (3.3798)  time: 0.8095  data: 0.0005  max mem: 19734
Epoch: [42]  [ 910/1251]  eta: 0:04:44  lr: 0.000013  loss: 3.5480 (3.3798)  time: 0.8058  data: 0.0005  max mem: 19734
Epoch: [42]  [ 920/1251]  eta: 0:04:35  lr: 0.000013  loss: 3.2134 (3.3772)  time: 0.7992  data: 0.0005  max mem: 19734
Epoch: [42]  [ 930/1251]  eta: 0:04:27  lr: 0.000013  loss: 3.2150 (3.3787)  time: 0.8018  data: 0.0005  max mem: 19734
Epoch: [42]  [ 940/1251]  eta: 0:04:18  lr: 0.000013  loss: 3.2624 (3.3766)  time: 0.8044  data: 0.0005  max mem: 19734
Epoch: [42]  [ 950/1251]  eta: 0:04:10  lr: 0.000013  loss: 3.3614 (3.3800)  time: 0.8055  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3848, ratio_loss=0.0051, pruning_loss=0.1274, mse_loss=0.5810
Epoch: [42]  [ 960/1251]  eta: 0:04:01  lr: 0.000013  loss: 3.7238 (3.3813)  time: 0.8066  data: 0.0005  max mem: 19734
Epoch: [42]  [ 970/1251]  eta: 0:03:53  lr: 0.000013  loss: 3.5504 (3.3818)  time: 0.8120  data: 0.0005  max mem: 19734
Epoch: [42]  [ 980/1251]  eta: 0:03:45  lr: 0.000013  loss: 3.5586 (3.3835)  time: 0.8342  data: 0.0005  max mem: 19734
Epoch: [42]  [ 990/1251]  eta: 0:03:36  lr: 0.000013  loss: 3.6488 (3.3867)  time: 0.8284  data: 0.0005  max mem: 19734
Epoch: [42]  [1000/1251]  eta: 0:03:28  lr: 0.000013  loss: 3.5634 (3.3846)  time: 0.8252  data: 0.0005  max mem: 19734
Epoch: [42]  [1010/1251]  eta: 0:03:20  lr: 0.000013  loss: 3.4642 (3.3859)  time: 0.8261  data: 0.0005  max mem: 19734
Epoch: [42]  [1020/1251]  eta: 0:03:11  lr: 0.000013  loss: 3.5274 (3.3849)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [42]  [1030/1251]  eta: 0:03:03  lr: 0.000013  loss: 3.5608 (3.3853)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [42]  [1040/1251]  eta: 0:02:55  lr: 0.000013  loss: 3.5608 (3.3865)  time: 0.8062  data: 0.0004  max mem: 19734
Epoch: [42]  [1050/1251]  eta: 0:02:46  lr: 0.000013  loss: 3.6707 (3.3894)  time: 0.8072  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4523, ratio_loss=0.0049, pruning_loss=0.1230, mse_loss=0.5793
Epoch: [42]  [1060/1251]  eta: 0:02:38  lr: 0.000013  loss: 3.6707 (3.3902)  time: 0.8033  data: 0.0006  max mem: 19734
Epoch: [42]  [1070/1251]  eta: 0:02:30  lr: 0.000013  loss: 3.4189 (3.3900)  time: 0.8019  data: 0.0005  max mem: 19734
Epoch: [42]  [1080/1251]  eta: 0:02:21  lr: 0.000013  loss: 3.4189 (3.3881)  time: 0.8006  data: 0.0004  max mem: 19734
Epoch: [42]  [1090/1251]  eta: 0:02:13  lr: 0.000013  loss: 3.3058 (3.3881)  time: 0.8031  data: 0.0004  max mem: 19734
Epoch: [42]  [1100/1251]  eta: 0:02:05  lr: 0.000013  loss: 3.2824 (3.3861)  time: 0.8037  data: 0.0004  max mem: 19734
Epoch: [42]  [1110/1251]  eta: 0:01:56  lr: 0.000013  loss: 3.3997 (3.3865)  time: 0.8053  data: 0.0005  max mem: 19734
Epoch: [42]  [1120/1251]  eta: 0:01:48  lr: 0.000013  loss: 3.1517 (3.3832)  time: 0.8143  data: 0.0005  max mem: 19734
Epoch: [42]  [1130/1251]  eta: 0:01:40  lr: 0.000013  loss: 3.1081 (3.3815)  time: 0.8265  data: 0.0004  max mem: 19734
Epoch: [42]  [1140/1251]  eta: 0:01:31  lr: 0.000013  loss: 3.2534 (3.3813)  time: 0.8172  data: 0.0005  max mem: 19734
Epoch: [42]  [1150/1251]  eta: 0:01:23  lr: 0.000013  loss: 3.5576 (3.3826)  time: 0.8184  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3078, ratio_loss=0.0048, pruning_loss=0.1270, mse_loss=0.5873
Epoch: [42]  [1160/1251]  eta: 0:01:15  lr: 0.000013  loss: 3.5800 (3.3842)  time: 0.8209  data: 0.0004  max mem: 19734
Epoch: [42]  [1170/1251]  eta: 0:01:07  lr: 0.000013  loss: 3.4446 (3.3839)  time: 0.8036  data: 0.0005  max mem: 19734
Epoch: [42]  [1180/1251]  eta: 0:00:58  lr: 0.000013  loss: 3.5001 (3.3839)  time: 0.8011  data: 0.0005  max mem: 19734
Epoch: [42]  [1190/1251]  eta: 0:00:50  lr: 0.000013  loss: 3.5001 (3.3846)  time: 0.8045  data: 0.0009  max mem: 19734
Epoch: [42]  [1200/1251]  eta: 0:00:42  lr: 0.000013  loss: 3.5352 (3.3851)  time: 0.8005  data: 0.0008  max mem: 19734
Epoch: [42]  [1210/1251]  eta: 0:00:33  lr: 0.000013  loss: 3.4855 (3.3850)  time: 0.7947  data: 0.0002  max mem: 19734
Epoch: [42]  [1220/1251]  eta: 0:00:25  lr: 0.000013  loss: 3.4079 (3.3855)  time: 0.7936  data: 0.0001  max mem: 19734
Epoch: [42]  [1230/1251]  eta: 0:00:17  lr: 0.000013  loss: 3.3908 (3.3843)  time: 0.7926  data: 0.0001  max mem: 19734
Epoch: [42]  [1240/1251]  eta: 0:00:09  lr: 0.000013  loss: 3.3924 (3.3843)  time: 0.7930  data: 0.0001  max mem: 19734
Epoch: [42]  [1250/1251]  eta: 0:00:00  lr: 0.000013  loss: 3.3968 (3.3836)  time: 0.7919  data: 0.0001  max mem: 19734
Epoch: [42] Total time: 0:17:14 (0.8271 s / it)
Averaged stats: lr: 0.000013  loss: 3.3968 (3.3986)
Test:  [  0/261]  eta: 2:58:54  loss: 0.6782 (0.6782)  acc1: 82.2917 (82.2917)  acc5: 96.3542 (96.3542)  time: 41.1284  data: 40.1066  max mem: 19734
Test:  [ 10/261]  eta: 0:17:21  loss: 0.6782 (0.7188)  acc1: 84.3750 (83.9015)  acc5: 96.8750 (96.5909)  time: 4.1476  data: 3.7327  max mem: 19734
Test:  [ 20/261]  eta: 0:09:47  loss: 0.9292 (0.8967)  acc1: 79.1667 (79.1419)  acc5: 94.2708 (94.7421)  time: 0.5043  data: 0.0562  max mem: 19734
Test:  [ 30/261]  eta: 0:06:48  loss: 0.8017 (0.8118)  acc1: 83.8542 (82.0901)  acc5: 94.2708 (95.2453)  time: 0.4569  data: 0.0177  max mem: 19734
Test:  [ 40/261]  eta: 0:05:16  loss: 0.5566 (0.7755)  acc1: 89.5833 (83.1936)  acc5: 96.3542 (95.5158)  time: 0.3785  data: 0.1391  max mem: 19734
Test:  [ 50/261]  eta: 0:04:17  loss: 0.8669 (0.8361)  acc1: 78.1250 (81.3828)  acc5: 95.3125 (95.1491)  time: 0.3705  data: 0.1416  max mem: 19734
Test:  [ 60/261]  eta: 0:03:35  loss: 0.9501 (0.8460)  acc1: 76.5625 (80.9085)  acc5: 93.7500 (95.1844)  time: 0.3378  data: 0.0173  max mem: 19734
Test:  [ 70/261]  eta: 0:03:07  loss: 0.9428 (0.8481)  acc1: 78.1250 (80.3917)  acc5: 96.3542 (95.3565)  time: 0.3721  data: 0.1144  max mem: 19734
Test:  [ 80/261]  eta: 0:02:42  loss: 0.7932 (0.8505)  acc1: 79.6875 (80.4398)  acc5: 96.8750 (95.4604)  time: 0.3561  data: 0.1190  max mem: 19734
Test:  [ 90/261]  eta: 0:02:20  loss: 0.7932 (0.8369)  acc1: 83.3333 (80.8436)  acc5: 96.3542 (95.5472)  time: 0.2692  data: 0.0160  max mem: 19734
Test:  [100/261]  eta: 0:02:04  loss: 0.8242 (0.8413)  acc1: 83.3333 (80.8065)  acc5: 94.7917 (95.5807)  time: 0.2655  data: 0.0149  max mem: 19734
Test:  [110/261]  eta: 0:01:49  loss: 0.8684 (0.8625)  acc1: 77.0833 (80.3397)  acc5: 94.2708 (95.2937)  time: 0.2779  data: 0.0170  max mem: 19734
Test:  [120/261]  eta: 0:01:45  loss: 1.2231 (0.9032)  acc1: 71.3542 (79.4378)  acc5: 89.0625 (94.7745)  time: 0.6201  data: 0.3668  max mem: 19734
Test:  [130/261]  eta: 0:01:34  loss: 1.3723 (0.9477)  acc1: 70.3125 (78.5067)  acc5: 88.0208 (94.2032)  time: 0.6963  data: 0.3720  max mem: 19734
Test:  [140/261]  eta: 0:01:28  loss: 1.3185 (0.9747)  acc1: 69.7917 (77.8184)  acc5: 90.1042 (93.9384)  time: 0.6636  data: 0.2414  max mem: 19734
Test:  [150/261]  eta: 0:01:18  loss: 1.2237 (0.9806)  acc1: 72.3958 (77.7870)  acc5: 91.1458 (93.7914)  time: 0.6222  data: 0.2426  max mem: 19734
Test:  [160/261]  eta: 0:01:11  loss: 1.0049 (1.0010)  acc1: 78.6458 (77.4586)  acc5: 92.1875 (93.4880)  time: 0.5006  data: 0.2404  max mem: 19734
Test:  [170/261]  eta: 0:01:01  loss: 1.3342 (1.0309)  acc1: 66.1458 (76.6874)  acc5: 86.9792 (93.1378)  time: 0.4749  data: 0.2384  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 1.4179 (1.0466)  acc1: 66.1458 (76.3323)  acc5: 89.5833 (93.0047)  time: 0.2615  data: 0.0205  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: 1.3051 (1.0598)  acc1: 69.2708 (76.0908)  acc5: 91.1458 (92.8338)  time: 0.2659  data: 0.0661  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 1.3064 (1.0764)  acc1: 71.3542 (75.7385)  acc5: 89.0625 (92.5865)  time: 0.2106  data: 0.0593  max mem: 19734
Test:  [210/261]  eta: 0:00:30  loss: 1.3703 (1.0896)  acc1: 70.8333 (75.5011)  acc5: 88.5417 (92.3948)  time: 0.1264  data: 0.0014  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.4180 (1.1092)  acc1: 68.2292 (75.0189)  acc5: 88.5417 (92.1875)  time: 0.1158  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4524 (1.1187)  acc1: 66.1458 (74.8106)  acc5: 88.5417 (92.1041)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.2957 (1.1273)  acc1: 67.1875 (74.5613)  acc5: 91.1458 (92.0341)  time: 0.1148  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0174 (1.1205)  acc1: 75.5208 (74.7136)  acc5: 92.7083 (92.1460)  time: 0.1149  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9524 (1.1208)  acc1: 76.5625 (74.7180)  acc5: 94.7917 (92.2180)  time: 0.1118  data: 0.0001  max mem: 19734
Test: Total time: 0:02:10 (0.4992 s / it)
* Acc@1 74.718 Acc@5 92.218 loss 1.121
Accuracy of the network on the 50000 test images: 74.7%
Max accuracy: 74.72%
Epoch: [43]  [   0/1251]  eta: 2:40:35  lr: 0.000013  loss: 3.6667 (3.6667)  time: 7.7020  data: 6.9107  max mem: 19734
loss info: cls_loss=3.3670, ratio_loss=0.0047, pruning_loss=0.1249, mse_loss=0.5681
Epoch: [43]  [  10/1251]  eta: 0:30:34  lr: 0.000013  loss: 3.4674 (3.3900)  time: 1.4784  data: 0.6292  max mem: 19734
Epoch: [43]  [  20/1251]  eta: 0:23:51  lr: 0.000013  loss: 3.4674 (3.4147)  time: 0.8358  data: 0.0009  max mem: 19734
Epoch: [43]  [  30/1251]  eta: 0:21:21  lr: 0.000013  loss: 3.6458 (3.5191)  time: 0.8140  data: 0.0007  max mem: 19734
Epoch: [43]  [  40/1251]  eta: 0:20:03  lr: 0.000013  loss: 3.6350 (3.4866)  time: 0.8157  data: 0.0005  max mem: 19734
Epoch: [43]  [  50/1251]  eta: 0:19:06  lr: 0.000013  loss: 3.6350 (3.5201)  time: 0.8082  data: 0.0005  max mem: 19734
Epoch: [43]  [  60/1251]  eta: 0:18:27  lr: 0.000013  loss: 3.6385 (3.5116)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [43]  [  70/1251]  eta: 0:17:56  lr: 0.000013  loss: 3.5393 (3.4885)  time: 0.8007  data: 0.0005  max mem: 19734
Epoch: [43]  [  80/1251]  eta: 0:17:34  lr: 0.000013  loss: 3.5393 (3.4873)  time: 0.8097  data: 0.0005  max mem: 19734
Epoch: [43]  [  90/1251]  eta: 0:17:12  lr: 0.000013  loss: 3.7184 (3.5041)  time: 0.8097  data: 0.0005  max mem: 19734
Epoch: [43]  [ 100/1251]  eta: 0:16:53  lr: 0.000013  loss: 3.7218 (3.4994)  time: 0.8018  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4801, ratio_loss=0.0045, pruning_loss=0.1243, mse_loss=0.5518
Epoch: [43]  [ 110/1251]  eta: 0:16:35  lr: 0.000013  loss: 3.5718 (3.4923)  time: 0.7988  data: 0.0006  max mem: 19734
Epoch: [43]  [ 120/1251]  eta: 0:16:21  lr: 0.000013  loss: 3.5556 (3.4828)  time: 0.8055  data: 0.0005  max mem: 19734
Epoch: [43]  [ 130/1251]  eta: 0:16:07  lr: 0.000013  loss: 3.4644 (3.4594)  time: 0.8073  data: 0.0004  max mem: 19734
Epoch: [43]  [ 140/1251]  eta: 0:15:53  lr: 0.000013  loss: 3.4644 (3.4550)  time: 0.8003  data: 0.0005  max mem: 19734
Epoch: [43]  [ 150/1251]  eta: 0:15:41  lr: 0.000013  loss: 3.6328 (3.4688)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [43]  [ 160/1251]  eta: 0:15:30  lr: 0.000013  loss: 3.5589 (3.4613)  time: 0.8155  data: 0.0005  max mem: 19734
Epoch: [43]  [ 170/1251]  eta: 0:15:20  lr: 0.000013  loss: 3.1813 (3.4578)  time: 0.8196  data: 0.0005  max mem: 19734
Epoch: [43]  [ 180/1251]  eta: 0:15:10  lr: 0.000013  loss: 3.2673 (3.4482)  time: 0.8249  data: 0.0005  max mem: 19734
Epoch: [43]  [ 190/1251]  eta: 0:14:59  lr: 0.000013  loss: 3.2673 (3.4365)  time: 0.8155  data: 0.0005  max mem: 19734
Epoch: [43]  [ 200/1251]  eta: 0:14:48  lr: 0.000013  loss: 3.5158 (3.4360)  time: 0.8013  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3514, ratio_loss=0.0047, pruning_loss=0.1252, mse_loss=0.5696
Epoch: [43]  [ 210/1251]  eta: 0:14:37  lr: 0.000013  loss: 3.5652 (3.4255)  time: 0.8033  data: 0.0005  max mem: 19734
Epoch: [43]  [ 220/1251]  eta: 0:14:27  lr: 0.000013  loss: 3.5435 (3.4212)  time: 0.8020  data: 0.0005  max mem: 19734
Epoch: [43]  [ 230/1251]  eta: 0:14:17  lr: 0.000013  loss: 3.3367 (3.4152)  time: 0.8090  data: 0.0005  max mem: 19734
Epoch: [43]  [ 240/1251]  eta: 0:14:08  lr: 0.000013  loss: 3.6058 (3.4281)  time: 0.8118  data: 0.0005  max mem: 19734
Epoch: [43]  [ 250/1251]  eta: 0:13:58  lr: 0.000013  loss: 3.5899 (3.4262)  time: 0.8025  data: 0.0005  max mem: 19734
Epoch: [43]  [ 260/1251]  eta: 0:13:48  lr: 0.000013  loss: 3.3324 (3.4219)  time: 0.7987  data: 0.0005  max mem: 19734
Epoch: [43]  [ 270/1251]  eta: 0:13:39  lr: 0.000013  loss: 3.4896 (3.4295)  time: 0.8102  data: 0.0005  max mem: 19734
Epoch: [43]  [ 280/1251]  eta: 0:13:29  lr: 0.000013  loss: 3.6727 (3.4307)  time: 0.8119  data: 0.0006  max mem: 19734
Epoch: [43]  [ 290/1251]  eta: 0:13:20  lr: 0.000013  loss: 3.6467 (3.4354)  time: 0.8008  data: 0.0006  max mem: 19734
Epoch: [43]  [ 300/1251]  eta: 0:13:12  lr: 0.000013  loss: 3.6890 (3.4461)  time: 0.8236  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4088, ratio_loss=0.0049, pruning_loss=0.1236, mse_loss=0.5829
Epoch: [43]  [ 310/1251]  eta: 0:13:03  lr: 0.000013  loss: 3.5840 (3.4309)  time: 0.8328  data: 0.0004  max mem: 19734
Epoch: [43]  [ 320/1251]  eta: 0:12:54  lr: 0.000013  loss: 3.0772 (3.4301)  time: 0.8111  data: 0.0005  max mem: 19734
Epoch: [43]  [ 330/1251]  eta: 0:12:46  lr: 0.000013  loss: 3.0772 (3.4172)  time: 0.8163  data: 0.0005  max mem: 19734
Epoch: [43]  [ 340/1251]  eta: 0:12:37  lr: 0.000013  loss: 3.2026 (3.4157)  time: 0.8178  data: 0.0005  max mem: 19734
Epoch: [43]  [ 350/1251]  eta: 0:12:28  lr: 0.000013  loss: 3.5523 (3.4178)  time: 0.8041  data: 0.0006  max mem: 19734
Epoch: [43]  [ 360/1251]  eta: 0:12:19  lr: 0.000013  loss: 3.5376 (3.4137)  time: 0.8041  data: 0.0006  max mem: 19734
Epoch: [43]  [ 370/1251]  eta: 0:12:10  lr: 0.000013  loss: 3.1830 (3.4074)  time: 0.8087  data: 0.0005  max mem: 19734
Epoch: [43]  [ 380/1251]  eta: 0:12:01  lr: 0.000013  loss: 3.4543 (3.4084)  time: 0.8097  data: 0.0005  max mem: 19734
Epoch: [43]  [ 390/1251]  eta: 0:11:52  lr: 0.000013  loss: 3.5291 (3.4081)  time: 0.8050  data: 0.0005  max mem: 19734
Epoch: [43]  [ 400/1251]  eta: 0:11:44  lr: 0.000013  loss: 3.5759 (3.4119)  time: 0.8029  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3463, ratio_loss=0.0046, pruning_loss=0.1255, mse_loss=0.5765
Epoch: [43]  [ 410/1251]  eta: 0:11:35  lr: 0.000013  loss: 3.6826 (3.4170)  time: 0.8034  data: 0.0009  max mem: 19734
Epoch: [43]  [ 420/1251]  eta: 0:11:26  lr: 0.000013  loss: 3.4975 (3.4158)  time: 0.8123  data: 0.0008  max mem: 19734
Epoch: [43]  [ 430/1251]  eta: 0:11:18  lr: 0.000013  loss: 3.5190 (3.4192)  time: 0.8105  data: 0.0005  max mem: 19734
Epoch: [43]  [ 440/1251]  eta: 0:11:09  lr: 0.000013  loss: 3.5633 (3.4193)  time: 0.8080  data: 0.0005  max mem: 19734
Epoch: [43]  [ 450/1251]  eta: 0:11:01  lr: 0.000013  loss: 3.5692 (3.4212)  time: 0.8183  data: 0.0005  max mem: 19734
Epoch: [43]  [ 460/1251]  eta: 0:10:53  lr: 0.000013  loss: 3.5414 (3.4182)  time: 0.8218  data: 0.0005  max mem: 19734
Epoch: [43]  [ 470/1251]  eta: 0:10:44  lr: 0.000013  loss: 3.5441 (3.4242)  time: 0.8264  data: 0.0005  max mem: 19734
Epoch: [43]  [ 480/1251]  eta: 0:10:36  lr: 0.000013  loss: 3.7746 (3.4256)  time: 0.8182  data: 0.0005  max mem: 19734
Epoch: [43]  [ 490/1251]  eta: 0:10:27  lr: 0.000013  loss: 3.4422 (3.4277)  time: 0.8032  data: 0.0005  max mem: 19734
Epoch: [43]  [ 500/1251]  eta: 0:10:19  lr: 0.000013  loss: 3.3614 (3.4232)  time: 0.8016  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4374, ratio_loss=0.0045, pruning_loss=0.1231, mse_loss=0.5676
Epoch: [43]  [ 510/1251]  eta: 0:10:10  lr: 0.000013  loss: 3.4432 (3.4234)  time: 0.8013  data: 0.0005  max mem: 19734
Epoch: [43]  [ 520/1251]  eta: 0:10:02  lr: 0.000013  loss: 3.4533 (3.4250)  time: 0.8034  data: 0.0005  max mem: 19734
Epoch: [43]  [ 530/1251]  eta: 0:09:53  lr: 0.000013  loss: 3.5663 (3.4256)  time: 0.8038  data: 0.0005  max mem: 19734
Epoch: [43]  [ 540/1251]  eta: 0:09:44  lr: 0.000013  loss: 3.5306 (3.4234)  time: 0.7993  data: 0.0005  max mem: 19734
Epoch: [43]  [ 550/1251]  eta: 0:09:36  lr: 0.000013  loss: 3.1219 (3.4213)  time: 0.7997  data: 0.0005  max mem: 19734
Epoch: [43]  [ 560/1251]  eta: 0:09:27  lr: 0.000013  loss: 3.1014 (3.4129)  time: 0.8031  data: 0.0005  max mem: 19734
Epoch: [43]  [ 570/1251]  eta: 0:09:19  lr: 0.000013  loss: 3.3053 (3.4124)  time: 0.8156  data: 0.0005  max mem: 19734
Epoch: [43]  [ 580/1251]  eta: 0:09:11  lr: 0.000013  loss: 3.3750 (3.4065)  time: 0.8130  data: 0.0005  max mem: 19734
Epoch: [43]  [ 590/1251]  eta: 0:09:03  lr: 0.000013  loss: 3.4805 (3.4097)  time: 0.8064  data: 0.0005  max mem: 19734
Epoch: [43]  [ 600/1251]  eta: 0:08:54  lr: 0.000013  loss: 3.6041 (3.4143)  time: 0.8134  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3458, ratio_loss=0.0048, pruning_loss=0.1266, mse_loss=0.5578
Epoch: [43]  [ 610/1251]  eta: 0:08:46  lr: 0.000013  loss: 3.5168 (3.4088)  time: 0.8195  data: 0.0005  max mem: 19734
Epoch: [43]  [ 620/1251]  eta: 0:08:38  lr: 0.000013  loss: 2.7530 (3.4038)  time: 0.8349  data: 0.0006  max mem: 19734
Epoch: [43]  [ 630/1251]  eta: 0:08:30  lr: 0.000013  loss: 3.3101 (3.4044)  time: 0.8238  data: 0.0006  max mem: 19734
Epoch: [43]  [ 640/1251]  eta: 0:08:21  lr: 0.000013  loss: 3.4897 (3.4053)  time: 0.7999  data: 0.0005  max mem: 19734
Epoch: [43]  [ 650/1251]  eta: 0:08:13  lr: 0.000013  loss: 3.3895 (3.4042)  time: 0.8012  data: 0.0005  max mem: 19734
Epoch: [43]  [ 660/1251]  eta: 0:08:05  lr: 0.000013  loss: 3.2216 (3.3978)  time: 0.8097  data: 0.0006  max mem: 19734
Epoch: [43]  [ 670/1251]  eta: 0:07:56  lr: 0.000013  loss: 3.2216 (3.3994)  time: 0.8106  data: 0.0006  max mem: 19734
Epoch: [43]  [ 680/1251]  eta: 0:07:48  lr: 0.000013  loss: 3.7155 (3.4034)  time: 0.8040  data: 0.0006  max mem: 19734
Epoch: [43]  [ 690/1251]  eta: 0:07:40  lr: 0.000013  loss: 3.6419 (3.4038)  time: 0.8006  data: 0.0006  max mem: 19734
Epoch: [43]  [ 700/1251]  eta: 0:07:31  lr: 0.000013  loss: 3.5021 (3.4036)  time: 0.8008  data: 0.0009  max mem: 19734
loss info: cls_loss=3.3296, ratio_loss=0.0047, pruning_loss=0.1256, mse_loss=0.5566
Epoch: [43]  [ 710/1251]  eta: 0:07:23  lr: 0.000013  loss: 3.5014 (3.4058)  time: 0.8091  data: 0.0008  max mem: 19734
Epoch: [43]  [ 720/1251]  eta: 0:07:15  lr: 0.000013  loss: 3.5014 (3.4040)  time: 0.8110  data: 0.0005  max mem: 19734
Epoch: [43]  [ 730/1251]  eta: 0:07:06  lr: 0.000013  loss: 3.4548 (3.4041)  time: 0.8121  data: 0.0005  max mem: 19734
Epoch: [43]  [ 740/1251]  eta: 0:06:58  lr: 0.000013  loss: 3.2613 (3.3997)  time: 0.8096  data: 0.0005  max mem: 19734
Epoch: [43]  [ 750/1251]  eta: 0:06:50  lr: 0.000013  loss: 3.2613 (3.4008)  time: 0.8264  data: 0.0005  max mem: 19734
Epoch: [43]  [ 760/1251]  eta: 0:06:42  lr: 0.000013  loss: 3.5560 (3.4021)  time: 0.8256  data: 0.0005  max mem: 19734
Epoch: [43]  [ 770/1251]  eta: 0:06:34  lr: 0.000013  loss: 3.5044 (3.4041)  time: 0.8074  data: 0.0004  max mem: 19734
Epoch: [43]  [ 780/1251]  eta: 0:06:25  lr: 0.000013  loss: 3.5044 (3.4077)  time: 0.8113  data: 0.0004  max mem: 19734
Epoch: [43]  [ 790/1251]  eta: 0:06:17  lr: 0.000013  loss: 3.5852 (3.4082)  time: 0.8056  data: 0.0005  max mem: 19734
Epoch: [43]  [ 800/1251]  eta: 0:06:09  lr: 0.000013  loss: 3.5852 (3.4096)  time: 0.8016  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4393, ratio_loss=0.0047, pruning_loss=0.1248, mse_loss=0.5606
Epoch: [43]  [ 810/1251]  eta: 0:06:01  lr: 0.000013  loss: 3.5237 (3.4112)  time: 0.8074  data: 0.0006  max mem: 19734
Epoch: [43]  [ 820/1251]  eta: 0:05:52  lr: 0.000013  loss: 3.4863 (3.4086)  time: 0.8094  data: 0.0005  max mem: 19734
Epoch: [43]  [ 830/1251]  eta: 0:05:44  lr: 0.000013  loss: 3.2974 (3.4085)  time: 0.8027  data: 0.0005  max mem: 19734
Epoch: [43]  [ 840/1251]  eta: 0:05:36  lr: 0.000013  loss: 3.5821 (3.4098)  time: 0.8016  data: 0.0004  max mem: 19734
Epoch: [43]  [ 850/1251]  eta: 0:05:27  lr: 0.000013  loss: 3.5821 (3.4090)  time: 0.8018  data: 0.0004  max mem: 19734
Epoch: [43]  [ 860/1251]  eta: 0:05:19  lr: 0.000013  loss: 3.3279 (3.4073)  time: 0.8082  data: 0.0004  max mem: 19734
Epoch: [43]  [ 870/1251]  eta: 0:05:11  lr: 0.000013  loss: 3.3444 (3.4067)  time: 0.8081  data: 0.0004  max mem: 19734
Epoch: [43]  [ 880/1251]  eta: 0:05:03  lr: 0.000013  loss: 3.3863 (3.4069)  time: 0.8088  data: 0.0005  max mem: 19734
Epoch: [43]  [ 890/1251]  eta: 0:04:55  lr: 0.000013  loss: 3.4153 (3.4080)  time: 0.8152  data: 0.0005  max mem: 19734
Epoch: [43]  [ 900/1251]  eta: 0:04:47  lr: 0.000013  loss: 3.2162 (3.4069)  time: 0.8262  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3252, ratio_loss=0.0045, pruning_loss=0.1274, mse_loss=0.5552
Epoch: [43]  [ 910/1251]  eta: 0:04:38  lr: 0.000013  loss: 3.0122 (3.4027)  time: 0.8390  data: 0.0004  max mem: 19734
Epoch: [43]  [ 920/1251]  eta: 0:04:30  lr: 0.000013  loss: 3.0009 (3.4006)  time: 0.8198  data: 0.0004  max mem: 19734
Epoch: [43]  [ 930/1251]  eta: 0:04:22  lr: 0.000013  loss: 3.4560 (3.4016)  time: 0.7986  data: 0.0004  max mem: 19734
Epoch: [43]  [ 940/1251]  eta: 0:04:14  lr: 0.000013  loss: 3.2087 (3.3981)  time: 0.7976  data: 0.0004  max mem: 19734
Epoch: [43]  [ 950/1251]  eta: 0:04:06  lr: 0.000013  loss: 2.9772 (3.3957)  time: 0.7997  data: 0.0004  max mem: 19734
Epoch: [43]  [ 960/1251]  eta: 0:03:57  lr: 0.000013  loss: 3.4398 (3.3964)  time: 0.8051  data: 0.0004  max mem: 19734
Epoch: [43]  [ 970/1251]  eta: 0:03:49  lr: 0.000013  loss: 3.4898 (3.3942)  time: 0.8038  data: 0.0004  max mem: 19734
Epoch: [43]  [ 980/1251]  eta: 0:03:41  lr: 0.000013  loss: 3.5215 (3.3937)  time: 0.8004  data: 0.0005  max mem: 19734
Epoch: [43]  [ 990/1251]  eta: 0:03:33  lr: 0.000013  loss: 3.5215 (3.3931)  time: 0.7996  data: 0.0005  max mem: 19734
Epoch: [43]  [1000/1251]  eta: 0:03:24  lr: 0.000013  loss: 3.4207 (3.3922)  time: 0.7998  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2729, ratio_loss=0.0047, pruning_loss=0.1259, mse_loss=0.5585
Epoch: [43]  [1010/1251]  eta: 0:03:16  lr: 0.000013  loss: 3.4657 (3.3915)  time: 0.8112  data: 0.0006  max mem: 19734
Epoch: [43]  [1020/1251]  eta: 0:03:08  lr: 0.000013  loss: 3.3911 (3.3912)  time: 0.8092  data: 0.0006  max mem: 19734
Epoch: [43]  [1030/1251]  eta: 0:03:00  lr: 0.000013  loss: 3.3402 (3.3914)  time: 0.8049  data: 0.0005  max mem: 19734
Epoch: [43]  [1040/1251]  eta: 0:02:52  lr: 0.000013  loss: 3.6250 (3.3928)  time: 0.8184  data: 0.0004  max mem: 19734
Epoch: [43]  [1050/1251]  eta: 0:02:44  lr: 0.000013  loss: 3.5323 (3.3908)  time: 0.8178  data: 0.0004  max mem: 19734
Epoch: [43]  [1060/1251]  eta: 0:02:35  lr: 0.000013  loss: 3.5229 (3.3913)  time: 0.8196  data: 0.0004  max mem: 19734
Epoch: [43]  [1070/1251]  eta: 0:02:27  lr: 0.000013  loss: 3.5706 (3.3922)  time: 0.8123  data: 0.0006  max mem: 19734
Epoch: [43]  [1080/1251]  eta: 0:02:19  lr: 0.000013  loss: 3.6348 (3.3947)  time: 0.8002  data: 0.0006  max mem: 19734
Epoch: [43]  [1090/1251]  eta: 0:02:11  lr: 0.000013  loss: 3.5659 (3.3958)  time: 0.8017  data: 0.0004  max mem: 19734
Epoch: [43]  [1100/1251]  eta: 0:02:03  lr: 0.000013  loss: 3.4761 (3.3946)  time: 0.8085  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4234, ratio_loss=0.0045, pruning_loss=0.1240, mse_loss=0.5605
Epoch: [43]  [1110/1251]  eta: 0:01:55  lr: 0.000013  loss: 3.5470 (3.3969)  time: 0.8085  data: 0.0004  max mem: 19734
Epoch: [43]  [1120/1251]  eta: 0:01:46  lr: 0.000013  loss: 3.6053 (3.3971)  time: 0.7993  data: 0.0004  max mem: 19734
Epoch: [43]  [1130/1251]  eta: 0:01:38  lr: 0.000013  loss: 3.2583 (3.3960)  time: 0.7988  data: 0.0005  max mem: 19734
Epoch: [43]  [1140/1251]  eta: 0:01:30  lr: 0.000013  loss: 3.2583 (3.3966)  time: 0.7990  data: 0.0005  max mem: 19734
Epoch: [43]  [1150/1251]  eta: 0:01:22  lr: 0.000013  loss: 3.6375 (3.3985)  time: 0.7988  data: 0.0004  max mem: 19734
Epoch: [43]  [1160/1251]  eta: 0:01:14  lr: 0.000013  loss: 3.4557 (3.3963)  time: 0.8002  data: 0.0004  max mem: 19734
Epoch: [43]  [1170/1251]  eta: 0:01:06  lr: 0.000013  loss: 3.4458 (3.3964)  time: 0.8127  data: 0.0003  max mem: 19734
Epoch: [43]  [1180/1251]  eta: 0:00:57  lr: 0.000013  loss: 3.3725 (3.3942)  time: 0.8189  data: 0.0005  max mem: 19734
Epoch: [43]  [1190/1251]  eta: 0:00:49  lr: 0.000013  loss: 3.4801 (3.3946)  time: 0.8205  data: 0.0007  max mem: 19734
Epoch: [43]  [1200/1251]  eta: 0:00:41  lr: 0.000013  loss: 3.4801 (3.3928)  time: 0.8125  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3403, ratio_loss=0.0046, pruning_loss=0.1265, mse_loss=0.5955
Epoch: [43]  [1210/1251]  eta: 0:00:33  lr: 0.000013  loss: 3.3782 (3.3937)  time: 0.7945  data: 0.0003  max mem: 19734
Epoch: [43]  [1220/1251]  eta: 0:00:25  lr: 0.000013  loss: 3.3607 (3.3921)  time: 0.7908  data: 0.0002  max mem: 19734
Epoch: [43]  [1230/1251]  eta: 0:00:17  lr: 0.000013  loss: 3.3303 (3.3936)  time: 0.7920  data: 0.0002  max mem: 19734
Epoch: [43]  [1240/1251]  eta: 0:00:08  lr: 0.000013  loss: 3.7086 (3.3959)  time: 0.7930  data: 0.0002  max mem: 19734
Epoch: [43]  [1250/1251]  eta: 0:00:00  lr: 0.000013  loss: 3.6082 (3.3949)  time: 0.7945  data: 0.0002  max mem: 19734
Epoch: [43] Total time: 0:16:59 (0.8153 s / it)
Averaged stats: lr: 0.000013  loss: 3.6082 (3.4030)
Test:  [  0/261]  eta: 2:52:46  loss: 0.7183 (0.7183)  acc1: 81.7708 (81.7708)  acc5: 96.3542 (96.3542)  time: 39.7194  data: 39.0383  max mem: 19734
Test:  [ 10/261]  eta: 0:16:14  loss: 0.7152 (0.7228)  acc1: 84.3750 (83.5227)  acc5: 96.8750 (96.2595)  time: 3.8838  data: 3.5604  max mem: 19734
Test:  [ 20/261]  eta: 0:08:39  loss: 0.9021 (0.8979)  acc1: 79.6875 (78.9683)  acc5: 94.2708 (94.6429)  time: 0.2773  data: 0.0113  max mem: 19734
Test:  [ 30/261]  eta: 0:06:01  loss: 0.8128 (0.8152)  acc1: 83.8542 (81.7540)  acc5: 94.7917 (95.1277)  time: 0.2922  data: 0.0151  max mem: 19734
Test:  [ 40/261]  eta: 0:04:50  loss: 0.5474 (0.7780)  acc1: 88.0208 (82.9903)  acc5: 96.3542 (95.4268)  time: 0.4323  data: 0.0213  max mem: 19734
Test:  [ 50/261]  eta: 0:03:57  loss: 0.8817 (0.8362)  acc1: 77.6042 (81.1989)  acc5: 94.7917 (95.1083)  time: 0.4415  data: 0.0202  max mem: 19734
Test:  [ 60/261]  eta: 0:03:16  loss: 0.9298 (0.8443)  acc1: 77.0833 (80.8316)  acc5: 94.2708 (95.1247)  time: 0.2926  data: 0.0118  max mem: 19734
Test:  [ 70/261]  eta: 0:02:47  loss: 0.9246 (0.8468)  acc1: 78.1250 (80.3697)  acc5: 96.3542 (95.3272)  time: 0.2414  data: 0.0111  max mem: 19734
Test:  [ 80/261]  eta: 0:02:55  loss: 0.8082 (0.8490)  acc1: 80.7292 (80.4848)  acc5: 96.3542 (95.4025)  time: 0.9425  data: 0.6147  max mem: 19734
Test:  [ 90/261]  eta: 0:02:35  loss: 0.8082 (0.8378)  acc1: 83.8542 (80.7864)  acc5: 95.8333 (95.4785)  time: 1.0377  data: 0.6163  max mem: 19734
Test:  [100/261]  eta: 0:02:21  loss: 0.8174 (0.8413)  acc1: 83.8542 (80.7292)  acc5: 95.3125 (95.5394)  time: 0.4964  data: 0.0177  max mem: 19734
Test:  [110/261]  eta: 0:02:10  loss: 0.8518 (0.8625)  acc1: 76.0417 (80.3163)  acc5: 94.7917 (95.2468)  time: 0.6393  data: 0.2725  max mem: 19734
Test:  [120/261]  eta: 0:01:54  loss: 1.1969 (0.9018)  acc1: 71.3542 (79.3776)  acc5: 90.1042 (94.7400)  time: 0.4784  data: 0.2739  max mem: 19734
Test:  [130/261]  eta: 0:01:40  loss: 1.3878 (0.9455)  acc1: 69.7917 (78.4868)  acc5: 87.5000 (94.2152)  time: 0.2234  data: 0.0135  max mem: 19734
Test:  [140/261]  eta: 0:01:30  loss: 1.2735 (0.9707)  acc1: 69.7917 (77.8369)  acc5: 90.6250 (93.9642)  time: 0.3577  data: 0.0453  max mem: 19734
Test:  [150/261]  eta: 0:01:20  loss: 1.2224 (0.9765)  acc1: 71.3542 (77.7559)  acc5: 91.6667 (93.8328)  time: 0.4740  data: 0.0472  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: 0.9984 (0.9964)  acc1: 77.0833 (77.4262)  acc5: 91.6667 (93.5171)  time: 0.3773  data: 0.0758  max mem: 19734
Test:  [170/261]  eta: 0:01:01  loss: 1.2783 (1.0263)  acc1: 64.5833 (76.6752)  acc5: 87.5000 (93.2018)  time: 0.2898  data: 0.1223  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 1.3681 (1.0433)  acc1: 65.1042 (76.2978)  acc5: 89.5833 (93.0508)  time: 0.2868  data: 0.1100  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: 1.3663 (1.0571)  acc1: 68.2292 (76.0498)  acc5: 90.6250 (92.8774)  time: 0.3126  data: 0.0945  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 1.2906 (1.0722)  acc1: 71.3542 (75.7255)  acc5: 89.0625 (92.6306)  time: 0.2391  data: 0.0499  max mem: 19734
Test:  [210/261]  eta: 0:00:30  loss: 1.2906 (1.0844)  acc1: 69.7917 (75.4690)  acc5: 88.5417 (92.4566)  time: 0.1652  data: 0.0386  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.3641 (1.1033)  acc1: 67.1875 (74.9929)  acc5: 88.5417 (92.2653)  time: 0.1443  data: 0.0260  max mem: 19734
Test:  [230/261]  eta: 0:00:17  loss: 1.4457 (1.1131)  acc1: 66.6667 (74.7497)  acc5: 89.5833 (92.1762)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.3391 (1.1221)  acc1: 67.1875 (74.5094)  acc5: 90.1042 (92.0794)  time: 0.1147  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0836 (1.1156)  acc1: 75.5208 (74.6555)  acc5: 92.7083 (92.1792)  time: 0.1151  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9307 (1.1163)  acc1: 77.6042 (74.6420)  acc5: 95.3125 (92.2480)  time: 0.1119  data: 0.0001  max mem: 19734
Test: Total time: 0:02:11 (0.5025 s / it)
* Acc@1 74.642 Acc@5 92.248 loss 1.116
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.72%
Epoch: [44]  [   0/1251]  eta: 6:07:43  lr: 0.000013  loss: 3.4114 (3.4114)  time: 17.6363  data: 13.5663  max mem: 19734
Epoch: [44]  [  10/1251]  eta: 0:55:18  lr: 0.000013  loss: 3.4278 (3.3617)  time: 2.6737  data: 1.3047  max mem: 19734
Epoch: [44]  [  20/1251]  eta: 0:36:41  lr: 0.000013  loss: 3.3340 (3.3707)  time: 0.9964  data: 0.0395  max mem: 19734
Epoch: [44]  [  30/1251]  eta: 0:29:57  lr: 0.000013  loss: 3.5368 (3.4743)  time: 0.8111  data: 0.0005  max mem: 19734
Epoch: [44]  [  40/1251]  eta: 0:26:25  lr: 0.000013  loss: 3.5368 (3.4146)  time: 0.8057  data: 0.0005  max mem: 19734
Epoch: [44]  [  50/1251]  eta: 0:24:12  lr: 0.000013  loss: 3.3482 (3.4253)  time: 0.8029  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4095, ratio_loss=0.0045, pruning_loss=0.1246, mse_loss=0.5548
Epoch: [44]  [  60/1251]  eta: 0:22:43  lr: 0.000013  loss: 3.4502 (3.4324)  time: 0.8087  data: 0.0006  max mem: 19734
Epoch: [44]  [  70/1251]  eta: 0:21:38  lr: 0.000013  loss: 3.5349 (3.4243)  time: 0.8177  data: 0.0006  max mem: 19734
Epoch: [44]  [  80/1251]  eta: 0:20:47  lr: 0.000013  loss: 3.6041 (3.4426)  time: 0.8206  data: 0.0007  max mem: 19734
Epoch: [44]  [  90/1251]  eta: 0:20:07  lr: 0.000013  loss: 3.4805 (3.4158)  time: 0.8307  data: 0.0008  max mem: 19734
Epoch: [44]  [ 100/1251]  eta: 0:19:30  lr: 0.000013  loss: 3.4381 (3.4245)  time: 0.8207  data: 0.0005  max mem: 19734
Epoch: [44]  [ 110/1251]  eta: 0:18:57  lr: 0.000013  loss: 3.5649 (3.4088)  time: 0.8006  data: 0.0005  max mem: 19734
Epoch: [44]  [ 120/1251]  eta: 0:18:29  lr: 0.000013  loss: 3.5028 (3.4098)  time: 0.7986  data: 0.0005  max mem: 19734
Epoch: [44]  [ 130/1251]  eta: 0:18:04  lr: 0.000013  loss: 3.6094 (3.4146)  time: 0.8028  data: 0.0005  max mem: 19734
Epoch: [44]  [ 140/1251]  eta: 0:17:41  lr: 0.000013  loss: 3.4685 (3.3916)  time: 0.8057  data: 0.0006  max mem: 19734
Epoch: [44]  [ 150/1251]  eta: 0:17:21  lr: 0.000013  loss: 3.4723 (3.4006)  time: 0.8059  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3646, ratio_loss=0.0045, pruning_loss=0.1245, mse_loss=0.5592
Epoch: [44]  [ 160/1251]  eta: 0:17:02  lr: 0.000013  loss: 3.7033 (3.4190)  time: 0.8060  data: 0.0005  max mem: 19734
Epoch: [44]  [ 170/1251]  eta: 0:16:45  lr: 0.000013  loss: 3.5952 (3.4110)  time: 0.8120  data: 0.0005  max mem: 19734
Epoch: [44]  [ 180/1251]  eta: 0:16:28  lr: 0.000013  loss: 3.2813 (3.4114)  time: 0.8121  data: 0.0006  max mem: 19734
Epoch: [44]  [ 190/1251]  eta: 0:16:12  lr: 0.000013  loss: 3.5074 (3.4159)  time: 0.8015  data: 0.0006  max mem: 19734
Epoch: [44]  [ 200/1251]  eta: 0:15:57  lr: 0.000013  loss: 3.2387 (3.4014)  time: 0.8003  data: 0.0005  max mem: 19734
Epoch: [44]  [ 210/1251]  eta: 0:15:43  lr: 0.000013  loss: 3.3104 (3.4079)  time: 0.8073  data: 0.0006  max mem: 19734
Epoch: [44]  [ 220/1251]  eta: 0:15:31  lr: 0.000013  loss: 3.3965 (3.3961)  time: 0.8222  data: 0.0005  max mem: 19734
Epoch: [44]  [ 230/1251]  eta: 0:15:18  lr: 0.000013  loss: 3.3284 (3.3980)  time: 0.8236  data: 0.0005  max mem: 19734
Epoch: [44]  [ 240/1251]  eta: 0:15:06  lr: 0.000013  loss: 3.5376 (3.3957)  time: 0.8305  data: 0.0005  max mem: 19734
Epoch: [44]  [ 250/1251]  eta: 0:14:54  lr: 0.000013  loss: 3.4749 (3.3916)  time: 0.8236  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3531, ratio_loss=0.0046, pruning_loss=0.1258, mse_loss=0.5694
Epoch: [44]  [ 260/1251]  eta: 0:14:41  lr: 0.000013  loss: 3.4803 (3.3960)  time: 0.8030  data: 0.0006  max mem: 19734
Epoch: [44]  [ 270/1251]  eta: 0:14:29  lr: 0.000013  loss: 3.4820 (3.3941)  time: 0.8065  data: 0.0006  max mem: 19734
Epoch: [44]  [ 280/1251]  eta: 0:14:19  lr: 0.000013  loss: 3.4796 (3.3922)  time: 0.8172  data: 0.0006  max mem: 19734
Epoch: [44]  [ 290/1251]  eta: 0:14:07  lr: 0.000013  loss: 3.6157 (3.3987)  time: 0.8153  data: 0.0006  max mem: 19734
Epoch: [44]  [ 300/1251]  eta: 0:13:56  lr: 0.000013  loss: 3.6672 (3.4037)  time: 0.8014  data: 0.0005  max mem: 19734
Epoch: [44]  [ 310/1251]  eta: 0:13:44  lr: 0.000013  loss: 3.5057 (3.4060)  time: 0.8008  data: 0.0005  max mem: 19734
Epoch: [44]  [ 320/1251]  eta: 0:13:34  lr: 0.000013  loss: 3.5475 (3.4076)  time: 0.8081  data: 0.0005  max mem: 19734
Epoch: [44]  [ 330/1251]  eta: 0:13:23  lr: 0.000013  loss: 3.6031 (3.4147)  time: 0.8107  data: 0.0005  max mem: 19734
Epoch: [44]  [ 340/1251]  eta: 0:13:13  lr: 0.000013  loss: 3.4811 (3.4113)  time: 0.8104  data: 0.0005  max mem: 19734
Epoch: [44]  [ 350/1251]  eta: 0:13:03  lr: 0.000013  loss: 3.4091 (3.4111)  time: 0.8094  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4438, ratio_loss=0.0047, pruning_loss=0.1245, mse_loss=0.5681
Epoch: [44]  [ 360/1251]  eta: 0:12:53  lr: 0.000013  loss: 3.3384 (3.4056)  time: 0.8213  data: 0.0005  max mem: 19734
Epoch: [44]  [ 370/1251]  eta: 0:12:44  lr: 0.000013  loss: 3.3384 (3.4111)  time: 0.8444  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 380/1251]  eta: 0:12:34  lr: 0.000013  loss: 3.6162 (3.3974)  time: 0.8380  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 390/1251]  eta: 0:12:24  lr: 0.000013  loss: 0.0000 (3.3105)  time: 0.7978  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 400/1251]  eta: 0:12:13  lr: 0.000013  loss: 0.0000 (3.2280)  time: 0.7746  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 410/1251]  eta: 0:12:02  lr: 0.000013  loss: 0.0000 (3.1494)  time: 0.7701  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 420/1251]  eta: 0:11:52  lr: 0.000013  loss: 0.0000 (3.0746)  time: 0.7725  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 430/1251]  eta: 0:11:42  lr: 0.000013  loss: 0.0000 (3.0033)  time: 0.7812  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 440/1251]  eta: 0:11:32  lr: 0.000013  loss: 0.0000 (2.9352)  time: 0.7766  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 450/1251]  eta: 0:11:22  lr: 0.000013  loss: 0.0000 (2.8701)  time: 0.7673  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 460/1251]  eta: 0:11:12  lr: 0.000013  loss: 0.0000 (2.8078)  time: 0.7714  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 470/1251]  eta: 0:11:03  lr: 0.000013  loss: 0.0000 (2.7482)  time: 0.7787  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 480/1251]  eta: 0:10:53  lr: 0.000013  loss: 0.0000 (2.6911)  time: 0.7750  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 490/1251]  eta: 0:10:43  lr: 0.000013  loss: 0.0000 (2.6363)  time: 0.7687  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 500/1251]  eta: 0:10:34  lr: 0.000013  loss: 0.0000 (2.5837)  time: 0.7705  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 510/1251]  eta: 0:10:25  lr: 0.000013  loss: 0.0000 (2.5331)  time: 0.7946  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 520/1251]  eta: 0:10:15  lr: 0.000013  loss: 0.0000 (2.4845)  time: 0.7991  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 530/1251]  eta: 0:10:06  lr: 0.000013  loss: 0.0000 (2.4377)  time: 0.7904  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 540/1251]  eta: 0:09:57  lr: 0.000013  loss: 0.0000 (2.3926)  time: 0.7856  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 550/1251]  eta: 0:09:48  lr: 0.000013  loss: 0.0000 (2.3492)  time: 0.7760  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 560/1251]  eta: 0:09:38  lr: 0.000013  loss: 0.0000 (2.3073)  time: 0.7682  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 570/1251]  eta: 0:09:29  lr: 0.000013  loss: 0.0000 (2.2669)  time: 0.7703  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 580/1251]  eta: 0:09:20  lr: 0.000013  loss: 0.0000 (2.2279)  time: 0.7735  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 590/1251]  eta: 0:09:11  lr: 0.000013  loss: 0.0000 (2.1902)  time: 0.7735  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 600/1251]  eta: 0:09:02  lr: 0.000013  loss: 0.0000 (2.1538)  time: 0.7732  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 610/1251]  eta: 0:08:53  lr: 0.000013  loss: 0.0000 (2.1185)  time: 0.7684  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 620/1251]  eta: 0:08:44  lr: 0.000013  loss: 0.0000 (2.0844)  time: 0.7694  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 630/1251]  eta: 0:08:35  lr: 0.000013  loss: 0.0000 (2.0514)  time: 0.7674  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 640/1251]  eta: 0:08:26  lr: 0.000013  loss: 0.0000 (2.0194)  time: 0.7644  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 650/1251]  eta: 0:08:18  lr: 0.000013  loss: 0.0000 (1.9883)  time: 0.7725  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 660/1251]  eta: 0:08:09  lr: 0.000013  loss: 0.0000 (1.9583)  time: 0.7939  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 670/1251]  eta: 0:08:00  lr: 0.000013  loss: 0.0000 (1.9291)  time: 0.7934  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 680/1251]  eta: 0:07:52  lr: 0.000013  loss: 0.0000 (1.9007)  time: 0.7953  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 690/1251]  eta: 0:07:43  lr: 0.000013  loss: 0.0000 (1.8732)  time: 0.7868  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 700/1251]  eta: 0:07:34  lr: 0.000013  loss: 0.0000 (1.8465)  time: 0.7677  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 710/1251]  eta: 0:07:26  lr: 0.000013  loss: 0.0000 (1.8205)  time: 0.7768  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 720/1251]  eta: 0:07:17  lr: 0.000013  loss: 0.0000 (1.7953)  time: 0.7709  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 730/1251]  eta: 0:07:08  lr: 0.000013  loss: 0.0000 (1.7707)  time: 0.7603  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 740/1251]  eta: 0:07:00  lr: 0.000013  loss: 0.0000 (1.7468)  time: 0.7676  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 750/1251]  eta: 0:06:51  lr: 0.000013  loss: 0.0000 (1.7236)  time: 0.7712  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 760/1251]  eta: 0:06:43  lr: 0.000013  loss: 0.0000 (1.7009)  time: 0.7746  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 770/1251]  eta: 0:06:34  lr: 0.000013  loss: 0.0000 (1.6789)  time: 0.7694  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 780/1251]  eta: 0:06:26  lr: 0.000013  loss: 0.0000 (1.6574)  time: 0.7647  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 790/1251]  eta: 0:06:17  lr: 0.000013  loss: 0.0000 (1.6364)  time: 0.7774  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 800/1251]  eta: 0:06:09  lr: 0.000013  loss: 0.0000 (1.6160)  time: 0.7876  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 810/1251]  eta: 0:06:01  lr: 0.000013  loss: 0.0000 (1.5961)  time: 0.7847  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 820/1251]  eta: 0:05:52  lr: 0.000013  loss: 0.0000 (1.5766)  time: 0.7858  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 830/1251]  eta: 0:05:44  lr: 0.000013  loss: 0.0000 (1.5577)  time: 0.7773  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 840/1251]  eta: 0:05:35  lr: 0.000013  loss: 0.0000 (1.5391)  time: 0.7593  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 850/1251]  eta: 0:05:27  lr: 0.000013  loss: 0.0000 (1.5210)  time: 0.7608  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 860/1251]  eta: 0:05:19  lr: 0.000013  loss: 0.0000 (1.5034)  time: 0.7760  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 870/1251]  eta: 0:05:10  lr: 0.000013  loss: 0.0000 (1.4861)  time: 0.7747  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 880/1251]  eta: 0:05:02  lr: 0.000013  loss: 0.0000 (1.4693)  time: 0.7585  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 890/1251]  eta: 0:04:53  lr: 0.000013  loss: 0.0000 (1.4528)  time: 0.7612  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 900/1251]  eta: 0:04:45  lr: 0.000013  loss: 0.0000 (1.4366)  time: 0.7784  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 910/1251]  eta: 0:04:37  lr: 0.000013  loss: 0.0000 (1.4209)  time: 0.7736  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 920/1251]  eta: 0:04:28  lr: 0.000013  loss: 0.0000 (1.4054)  time: 0.7572  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 930/1251]  eta: 0:04:20  lr: 0.000013  loss: 0.0000 (1.3903)  time: 0.7622  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 940/1251]  eta: 0:04:12  lr: 0.000013  loss: 0.0000 (1.3756)  time: 0.7772  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 950/1251]  eta: 0:04:04  lr: 0.000013  loss: 0.0000 (1.3611)  time: 0.7887  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 960/1251]  eta: 0:03:56  lr: 0.000013  loss: 0.0000 (1.3469)  time: 0.7794  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 970/1251]  eta: 0:03:47  lr: 0.000013  loss: 0.0000 (1.3331)  time: 0.7859  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 980/1251]  eta: 0:03:39  lr: 0.000013  loss: 0.0000 (1.3195)  time: 0.7844  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [ 990/1251]  eta: 0:03:31  lr: 0.000013  loss: 0.0000 (1.3062)  time: 0.7631  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1000/1251]  eta: 0:03:23  lr: 0.000013  loss: 0.0000 (1.2931)  time: 0.7588  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1010/1251]  eta: 0:03:15  lr: 0.000013  loss: 0.0000 (1.2803)  time: 0.7664  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1020/1251]  eta: 0:03:06  lr: 0.000013  loss: 0.0000 (1.2678)  time: 0.7703  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1030/1251]  eta: 0:02:58  lr: 0.000013  loss: 0.0000 (1.2555)  time: 0.7618  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1040/1251]  eta: 0:02:50  lr: 0.000013  loss: 0.0000 (1.2434)  time: 0.7577  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1050/1251]  eta: 0:02:42  lr: 0.000013  loss: 0.0000 (1.2316)  time: 0.7672  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1060/1251]  eta: 0:02:34  lr: 0.000013  loss: 0.0000 (1.2200)  time: 0.7682  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1070/1251]  eta: 0:02:25  lr: 0.000013  loss: 0.0000 (1.2086)  time: 0.7567  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1080/1251]  eta: 0:02:17  lr: 0.000013  loss: 0.0000 (1.1974)  time: 0.7564  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1090/1251]  eta: 0:02:09  lr: 0.000013  loss: 0.0000 (1.1864)  time: 0.7792  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1100/1251]  eta: 0:02:01  lr: 0.000013  loss: 0.0000 (1.1757)  time: 0.7947  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1110/1251]  eta: 0:01:53  lr: 0.000013  loss: 0.0000 (1.1651)  time: 0.7799  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1120/1251]  eta: 0:01:45  lr: 0.000013  loss: 0.0000 (1.1547)  time: 0.7725  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1130/1251]  eta: 0:01:37  lr: 0.000013  loss: 0.0000 (1.1445)  time: 0.7719  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1140/1251]  eta: 0:01:29  lr: 0.000013  loss: 0.0000 (1.1345)  time: 0.7665  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1150/1251]  eta: 0:01:21  lr: 0.000013  loss: 0.0000 (1.1246)  time: 0.7666  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1160/1251]  eta: 0:01:13  lr: 0.000013  loss: 0.0000 (1.1149)  time: 0.7649  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1170/1251]  eta: 0:01:05  lr: 0.000013  loss: 0.0000 (1.1054)  time: 0.7659  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1180/1251]  eta: 0:00:57  lr: 0.000013  loss: 0.0000 (1.0960)  time: 0.7664  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1190/1251]  eta: 0:00:48  lr: 0.000013  loss: 0.0000 (1.0868)  time: 0.7593  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1200/1251]  eta: 0:00:40  lr: 0.000013  loss: 0.0000 (1.0778)  time: 0.7614  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1210/1251]  eta: 0:00:32  lr: 0.000013  loss: 0.0000 (1.0689)  time: 0.7613  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1220/1251]  eta: 0:00:24  lr: 0.000013  loss: 0.0000 (1.0601)  time: 0.7514  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1230/1251]  eta: 0:00:16  lr: 0.000013  loss: 0.0000 (1.0515)  time: 0.7453  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1240/1251]  eta: 0:00:08  lr: 0.000013  loss: 0.0000 (1.0430)  time: 0.7626  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [44]  [1250/1251]  eta: 0:00:00  lr: 0.000013  loss: 0.0000 (1.0347)  time: 0.7859  data: 0.0002  max mem: 19734
Epoch: [44] Total time: 0:16:43 (0.8021 s / it)
Averaged stats: lr: 0.000013  loss: 0.0000 (1.0314)
Test:  [  0/261]  eta: 3:00:03  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 41.3917  data: 41.2488  max mem: 19734
Test:  [ 10/261]  eta: 0:16:37  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.9739  data: 3.7733  max mem: 19734
Test:  [ 20/261]  eta: 0:08:47  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2308  data: 0.0229  max mem: 19734
Test:  [ 30/261]  eta: 0:05:58  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2210  data: 0.0183  max mem: 19734
Test:  [ 40/261]  eta: 0:05:04  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5259  data: 0.2428  max mem: 19734
Test:  [ 50/261]  eta: 0:04:16  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6884  data: 0.2421  max mem: 19734
Test:  [ 60/261]  eta: 0:03:44  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.5846  data: 0.0143  max mem: 19734
Test:  [ 70/261]  eta: 0:03:31  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.8416  data: 0.3084  max mem: 19734
Test:  [ 80/261]  eta: 0:03:05  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.7313  data: 0.3107  max mem: 19734
Test:  [ 90/261]  eta: 0:02:45  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4546  data: 0.0142  max mem: 19734
Test:  [100/261]  eta: 0:02:25  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4071  data: 0.0130  max mem: 19734
Test:  [110/261]  eta: 0:02:13  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.5069  data: 0.1372  max mem: 19734
Test:  [120/261]  eta: 0:01:59  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.5921  data: 0.1384  max mem: 19734
Test:  [130/261]  eta: 0:01:46  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.4251  data: 0.0146  max mem: 19734
Test:  [140/261]  eta: 0:01:35  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4340  data: 0.0107  max mem: 19734
Test:  [150/261]  eta: 0:01:25  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4694  data: 0.0678  max mem: 19734
Test:  [160/261]  eta: 0:01:14  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3381  data: 0.1112  max mem: 19734
Test:  [170/261]  eta: 0:01:03  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1825  data: 0.0489  max mem: 19734
Test:  [180/261]  eta: 0:00:54  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1215  data: 0.0026  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1249  data: 0.0132  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1228  data: 0.0114  max mem: 19734
Test:  [210/261]  eta: 0:00:30  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1125  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1088  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1079  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1079  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1081  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1052  data: 0.0001  max mem: 19734
Test: Total time: 0:02:09 (0.4979 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.72%
Loss is nan, stopping training this iteration.
Epoch: [45]  [   0/1251]  eta: 6:30:33  lr: 0.000012  loss: 0.0000 (0.0000)  time: 18.7315  data: 16.6897  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [  10/1251]  eta: 0:54:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 2.6562  data: 1.5191  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [  20/1251]  eta: 0:36:02  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.9083  data: 0.0014  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [  30/1251]  eta: 0:29:26  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7814  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [  40/1251]  eta: 0:25:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7815  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [  50/1251]  eta: 0:23:35  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7631  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [  60/1251]  eta: 0:22:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7725  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [  70/1251]  eta: 0:20:59  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7796  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [  80/1251]  eta: 0:20:04  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7675  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [  90/1251]  eta: 0:19:21  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7661  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 100/1251]  eta: 0:18:45  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7720  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 110/1251]  eta: 0:18:14  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7727  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 120/1251]  eta: 0:17:49  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7803  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 130/1251]  eta: 0:17:26  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7927  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 140/1251]  eta: 0:17:04  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 150/1251]  eta: 0:16:47  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 160/1251]  eta: 0:16:28  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 170/1251]  eta: 0:16:10  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7688  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 180/1251]  eta: 0:15:55  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7849  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 190/1251]  eta: 0:15:40  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7862  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 200/1251]  eta: 0:15:25  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7682  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 210/1251]  eta: 0:15:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7733  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 220/1251]  eta: 0:14:57  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7777  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 230/1251]  eta: 0:14:44  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7682  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 240/1251]  eta: 0:14:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7616  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 250/1251]  eta: 0:14:18  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7614  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 260/1251]  eta: 0:14:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7785  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 270/1251]  eta: 0:13:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7913  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 280/1251]  eta: 0:13:45  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 290/1251]  eta: 0:13:34  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7697  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 300/1251]  eta: 0:13:24  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 310/1251]  eta: 0:13:13  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7808  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 320/1251]  eta: 0:13:02  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7590  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 330/1251]  eta: 0:12:52  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7725  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 340/1251]  eta: 0:12:42  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7813  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 350/1251]  eta: 0:12:32  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7715  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 360/1251]  eta: 0:12:22  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7731  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 370/1251]  eta: 0:12:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7755  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 380/1251]  eta: 0:12:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7731  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 390/1251]  eta: 0:11:53  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7684  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 400/1251]  eta: 0:11:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7622  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 410/1251]  eta: 0:11:34  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 420/1251]  eta: 0:11:25  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7985  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 430/1251]  eta: 0:11:16  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7839  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 440/1251]  eta: 0:11:08  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7897  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 450/1251]  eta: 0:10:58  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7889  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 460/1251]  eta: 0:10:49  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7738  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 470/1251]  eta: 0:10:41  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7763  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 480/1251]  eta: 0:10:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7707  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 490/1251]  eta: 0:10:22  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7691  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 500/1251]  eta: 0:10:13  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7691  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 510/1251]  eta: 0:10:05  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7686  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 520/1251]  eta: 0:09:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7680  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 530/1251]  eta: 0:09:47  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7688  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 540/1251]  eta: 0:09:38  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7721  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 550/1251]  eta: 0:09:30  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7769  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 560/1251]  eta: 0:09:21  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 570/1251]  eta: 0:09:13  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.8007  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 580/1251]  eta: 0:09:05  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7866  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 590/1251]  eta: 0:08:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7793  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 600/1251]  eta: 0:08:48  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7806  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 610/1251]  eta: 0:08:39  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7740  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 620/1251]  eta: 0:08:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 630/1251]  eta: 0:08:22  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7723  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 640/1251]  eta: 0:08:14  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7642  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 650/1251]  eta: 0:08:05  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7768  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 660/1251]  eta: 0:07:57  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7719  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 670/1251]  eta: 0:07:48  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7603  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 680/1251]  eta: 0:07:40  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7678  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 690/1251]  eta: 0:07:32  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7720  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 700/1251]  eta: 0:07:23  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7766  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 710/1251]  eta: 0:07:15  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 720/1251]  eta: 0:07:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7817  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 730/1251]  eta: 0:06:59  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7745  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 740/1251]  eta: 0:06:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 750/1251]  eta: 0:06:42  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7752  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 760/1251]  eta: 0:06:34  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7660  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 770/1251]  eta: 0:06:26  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7674  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 780/1251]  eta: 0:06:17  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7583  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 790/1251]  eta: 0:06:09  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7587  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 800/1251]  eta: 0:06:01  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7744  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 810/1251]  eta: 0:05:53  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7754  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 820/1251]  eta: 0:05:45  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7594  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 830/1251]  eta: 0:05:36  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7591  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 840/1251]  eta: 0:05:28  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7717  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 850/1251]  eta: 0:05:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7773  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 860/1251]  eta: 0:05:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7807  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 870/1251]  eta: 0:05:04  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7758  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 880/1251]  eta: 0:04:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7870  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 890/1251]  eta: 0:04:48  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7848  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 900/1251]  eta: 0:04:40  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7569  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 910/1251]  eta: 0:04:32  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7686  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 920/1251]  eta: 0:04:24  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7756  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 930/1251]  eta: 0:04:15  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7634  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 940/1251]  eta: 0:04:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7578  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 950/1251]  eta: 0:03:59  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7657  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 960/1251]  eta: 0:03:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7687  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 970/1251]  eta: 0:03:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7622  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 980/1251]  eta: 0:03:35  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7565  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [ 990/1251]  eta: 0:03:27  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7697  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1000/1251]  eta: 0:03:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7892  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1010/1251]  eta: 0:03:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7784  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1020/1251]  eta: 0:03:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7696  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1030/1251]  eta: 0:02:55  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7771  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1040/1251]  eta: 0:02:47  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7714  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1050/1251]  eta: 0:02:39  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7603  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1060/1251]  eta: 0:02:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7604  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1070/1251]  eta: 0:02:23  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7688  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1080/1251]  eta: 0:02:15  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7698  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1090/1251]  eta: 0:02:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7614  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1100/1251]  eta: 0:01:59  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7684  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1110/1251]  eta: 0:01:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7733  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1120/1251]  eta: 0:01:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7665  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1130/1251]  eta: 0:01:35  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7712  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1140/1251]  eta: 0:01:27  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7739  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1150/1251]  eta: 0:01:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7786  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1160/1251]  eta: 0:01:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7729  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1170/1251]  eta: 0:01:04  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7753  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1180/1251]  eta: 0:00:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7752  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1190/1251]  eta: 0:00:48  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7632  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1200/1251]  eta: 0:00:40  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7639  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1210/1251]  eta: 0:00:32  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7535  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1220/1251]  eta: 0:00:24  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7480  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1230/1251]  eta: 0:00:16  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7569  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1240/1251]  eta: 0:00:08  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7591  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [45]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7487  data: 0.0002  max mem: 19734
Epoch: [45] Total time: 0:16:28 (0.7903 s / it)
Averaged stats: lr: 0.000012  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:41:36  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 23.3564  data: 22.6520  max mem: 19734
Test:  [ 10/261]  eta: 0:15:31  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.7121  data: 3.4653  max mem: 19734
Test:  [ 20/261]  eta: 0:08:29  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 1.0534  data: 0.7862  max mem: 19734
Test:  [ 30/261]  eta: 0:05:50  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3117  data: 0.0220  max mem: 19734
Test:  [ 40/261]  eta: 0:04:41  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3875  data: 0.0180  max mem: 19734
Test:  [ 50/261]  eta: 0:03:55  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4944  data: 0.0135  max mem: 19734
Test:  [ 60/261]  eta: 0:03:19  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.4155  data: 0.0117  max mem: 19734
Test:  [ 70/261]  eta: 0:03:00  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5035  data: 0.1430  max mem: 19734
Test:  [ 80/261]  eta: 0:02:43  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.6416  data: 0.2659  max mem: 19734
Test:  [ 90/261]  eta: 0:02:28  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.5890  data: 0.2984  max mem: 19734
Test:  [100/261]  eta: 0:02:29  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 1.0375  data: 0.7683  max mem: 19734
Test:  [110/261]  eta: 0:02:12  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.9362  data: 0.6094  max mem: 19734
Test:  [120/261]  eta: 0:02:01  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.5085  data: 0.1429  max mem: 19734
Test:  [130/261]  eta: 0:01:48  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.5546  data: 0.1416  max mem: 19734
Test:  [140/261]  eta: 0:01:39  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.5628  data: 0.1676  max mem: 19734
Test:  [150/261]  eta: 0:01:26  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4684  data: 0.1655  max mem: 19734
Test:  [160/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2107  data: 0.0116  max mem: 19734
Test:  [170/261]  eta: 0:01:04  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1630  data: 0.0106  max mem: 19734
Test:  [180/261]  eta: 0:00:54  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1581  data: 0.0091  max mem: 19734
Test:  [190/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1609  data: 0.0208  max mem: 19734
Test:  [200/261]  eta: 0:00:38  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1369  data: 0.0171  max mem: 19734
Test:  [210/261]  eta: 0:00:30  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1127  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1111  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:17  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1094  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1081  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1082  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1055  data: 0.0001  max mem: 19734
Test: Total time: 0:02:12 (0.5071 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.72%
Loss is nan, stopping training this iteration.
Epoch: [46]  [   0/1251]  eta: 6:34:13  lr: 0.000012  loss: 0.0000 (0.0000)  time: 18.9074  data: 14.8030  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [  10/1251]  eta: 0:52:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 2.5238  data: 1.3463  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [  20/1251]  eta: 0:34:52  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.8395  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [  30/1251]  eta: 0:28:37  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7926  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [  40/1251]  eta: 0:25:25  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7980  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [  50/1251]  eta: 0:23:18  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7888  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [  60/1251]  eta: 0:21:57  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [  70/1251]  eta: 0:20:48  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [  80/1251]  eta: 0:19:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7640  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [  90/1251]  eta: 0:19:16  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7772  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 100/1251]  eta: 0:18:42  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7867  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 110/1251]  eta: 0:18:10  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7721  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 120/1251]  eta: 0:17:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7635  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 130/1251]  eta: 0:17:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7699  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 140/1251]  eta: 0:16:57  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7682  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 150/1251]  eta: 0:16:36  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7636  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 160/1251]  eta: 0:16:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7685  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 170/1251]  eta: 0:16:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 180/1251]  eta: 0:15:49  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7959  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 190/1251]  eta: 0:15:34  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7939  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 200/1251]  eta: 0:15:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7865  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 210/1251]  eta: 0:15:08  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7886  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 220/1251]  eta: 0:14:54  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7797  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 230/1251]  eta: 0:14:41  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7688  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 240/1251]  eta: 0:14:30  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7857  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 250/1251]  eta: 0:14:18  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7954  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 260/1251]  eta: 0:14:06  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7785  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 270/1251]  eta: 0:13:54  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7623  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 280/1251]  eta: 0:13:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7635  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 290/1251]  eta: 0:13:32  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7658  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 300/1251]  eta: 0:13:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7596  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 310/1251]  eta: 0:13:10  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7655  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 320/1251]  eta: 0:13:00  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7791  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 330/1251]  eta: 0:12:50  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7834  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 340/1251]  eta: 0:12:40  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7721  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 350/1251]  eta: 0:12:30  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7713  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 360/1251]  eta: 0:12:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7771  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 370/1251]  eta: 0:12:10  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7678  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 380/1251]  eta: 0:12:01  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7717  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 390/1251]  eta: 0:11:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7703  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 400/1251]  eta: 0:11:42  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7792  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 410/1251]  eta: 0:11:33  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7809  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 420/1251]  eta: 0:11:23  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7607  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 430/1251]  eta: 0:11:14  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7605  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 440/1251]  eta: 0:11:04  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7648  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 450/1251]  eta: 0:10:55  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7620  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 460/1251]  eta: 0:10:46  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7642  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 470/1251]  eta: 0:10:38  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7826  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 480/1251]  eta: 0:10:29  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7922  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 490/1251]  eta: 0:10:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7777  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 500/1251]  eta: 0:10:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7798  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 510/1251]  eta: 0:10:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 520/1251]  eta: 0:09:54  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7707  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 530/1251]  eta: 0:09:45  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7681  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 540/1251]  eta: 0:09:36  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7623  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 550/1251]  eta: 0:09:28  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7709  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 560/1251]  eta: 0:09:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7764  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 570/1251]  eta: 0:09:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7663  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 580/1251]  eta: 0:09:02  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7613  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 590/1251]  eta: 0:08:53  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7630  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 600/1251]  eta: 0:08:45  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7798  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 610/1251]  eta: 0:08:37  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 620/1251]  eta: 0:08:29  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 630/1251]  eta: 0:08:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.8020  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 640/1251]  eta: 0:08:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7982  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 650/1251]  eta: 0:08:04  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7908  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 660/1251]  eta: 0:07:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7637  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 670/1251]  eta: 0:07:47  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7771  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 680/1251]  eta: 0:07:39  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7792  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 690/1251]  eta: 0:07:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7688  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 700/1251]  eta: 0:07:22  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7688  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 710/1251]  eta: 0:07:14  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7679  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 720/1251]  eta: 0:07:06  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7684  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 730/1251]  eta: 0:06:57  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7623  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 740/1251]  eta: 0:06:49  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7610  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 750/1251]  eta: 0:06:41  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7736  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 760/1251]  eta: 0:06:33  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7795  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 770/1251]  eta: 0:06:25  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 780/1251]  eta: 0:06:17  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 790/1251]  eta: 0:06:09  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 800/1251]  eta: 0:06:00  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7828  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 810/1251]  eta: 0:05:52  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7631  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 820/1251]  eta: 0:05:44  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7724  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 830/1251]  eta: 0:05:36  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7763  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 840/1251]  eta: 0:05:28  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7736  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 850/1251]  eta: 0:05:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7674  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 860/1251]  eta: 0:05:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7646  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 870/1251]  eta: 0:05:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7699  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 880/1251]  eta: 0:04:55  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7631  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 890/1251]  eta: 0:04:47  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7687  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 900/1251]  eta: 0:04:39  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7738  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 910/1251]  eta: 0:04:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7938  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 920/1251]  eta: 0:04:23  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7894  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 930/1251]  eta: 0:04:15  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7701  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 940/1251]  eta: 0:04:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7854  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 950/1251]  eta: 0:03:59  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7786  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 960/1251]  eta: 0:03:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7630  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 970/1251]  eta: 0:03:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7693  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 980/1251]  eta: 0:03:35  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7773  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [ 990/1251]  eta: 0:03:27  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7767  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1000/1251]  eta: 0:03:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7690  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1010/1251]  eta: 0:03:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7613  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1020/1251]  eta: 0:03:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7676  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1030/1251]  eta: 0:02:55  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7692  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1040/1251]  eta: 0:02:47  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7703  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1050/1251]  eta: 0:02:39  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7768  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1060/1251]  eta: 0:02:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7924  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1070/1251]  eta: 0:02:23  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1080/1251]  eta: 0:02:15  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7743  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1090/1251]  eta: 0:02:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7750  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1100/1251]  eta: 0:01:59  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7650  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1110/1251]  eta: 0:01:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7706  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1120/1251]  eta: 0:01:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7667  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1130/1251]  eta: 0:01:35  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7639  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1140/1251]  eta: 0:01:27  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7822  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1150/1251]  eta: 0:01:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7768  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1160/1251]  eta: 0:01:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7593  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1170/1251]  eta: 0:01:04  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7631  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1180/1251]  eta: 0:00:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7729  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1190/1251]  eta: 0:00:48  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7679  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1200/1251]  eta: 0:00:40  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7695  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1210/1251]  eta: 0:00:32  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7709  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1220/1251]  eta: 0:00:24  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7560  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1230/1251]  eta: 0:00:16  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7628  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1240/1251]  eta: 0:00:08  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7594  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [46]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7517  data: 0.0003  max mem: 19734
Epoch: [46] Total time: 0:16:29 (0.7907 s / it)
Averaged stats: lr: 0.000012  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:47:24  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 38.4835  data: 37.7957  max mem: 19734
Test:  [ 10/261]  eta: 0:15:10  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.6289  data: 3.4439  max mem: 19734
Test:  [ 20/261]  eta: 0:07:55  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.1463  data: 0.0141  max mem: 19734
Test:  [ 30/261]  eta: 0:05:31  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2283  data: 0.0231  max mem: 19734
Test:  [ 40/261]  eta: 0:04:19  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3406  data: 0.0935  max mem: 19734
Test:  [ 50/261]  eta: 0:03:24  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.2533  data: 0.0857  max mem: 19734
Test:  [ 60/261]  eta: 0:02:48  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1506  data: 0.0092  max mem: 19734
Test:  [ 70/261]  eta: 0:02:42  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5425  data: 0.3375  max mem: 19734
Test:  [ 80/261]  eta: 0:02:22  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.6203  data: 0.4027  max mem: 19734
Test:  [ 90/261]  eta: 0:02:04  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2967  data: 0.0753  max mem: 19734
Test:  [100/261]  eta: 0:01:59  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.5605  data: 0.2594  max mem: 19734
Test:  [110/261]  eta: 0:01:43  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4882  data: 0.2547  max mem: 19734
Test:  [120/261]  eta: 0:01:31  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.1802  data: 0.0416  max mem: 19734
Test:  [130/261]  eta: 0:01:24  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.4373  data: 0.2837  max mem: 19734
Test:  [140/261]  eta: 0:01:23  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.9685  data: 0.7281  max mem: 19734
Test:  [150/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.8716  data: 0.4918  max mem: 19734
Test:  [160/261]  eta: 0:01:06  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.4375  data: 0.0202  max mem: 19734
Test:  [170/261]  eta: 0:00:58  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.4203  data: 0.0924  max mem: 19734
Test:  [180/261]  eta: 0:00:50  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2713  data: 0.0831  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1661  data: 0.0097  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.4427  data: 0.2546  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.4061  data: 0.2483  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1211  data: 0.0003  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1230  data: 0.0008  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1199  data: 0.0007  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1089  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1053  data: 0.0001  max mem: 19734
Test: Total time: 0:02:08 (0.4907 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.72%
Loss is nan, stopping training this iteration.
Epoch: [47]  [   0/1251]  eta: 6:54:01  lr: 0.000012  loss: 0.0000 (0.0000)  time: 19.8571  data: 18.3374  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [  10/1251]  eta: 0:53:57  lr: 0.000012  loss: 0.0000 (0.0000)  time: 2.6091  data: 1.6685  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [  20/1251]  eta: 0:35:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.8252  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [  30/1251]  eta: 0:28:52  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7645  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [  40/1251]  eta: 0:25:27  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7676  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [  50/1251]  eta: 0:23:17  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7674  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [  60/1251]  eta: 0:21:46  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7612  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [  70/1251]  eta: 0:20:44  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7725  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [  80/1251]  eta: 0:19:52  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7767  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [  90/1251]  eta: 0:19:14  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 100/1251]  eta: 0:18:38  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 110/1251]  eta: 0:18:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 120/1251]  eta: 0:17:44  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7874  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 130/1251]  eta: 0:17:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7716  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 140/1251]  eta: 0:16:58  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7675  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 150/1251]  eta: 0:16:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7990  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 160/1251]  eta: 0:16:24  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.8044  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 170/1251]  eta: 0:16:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7702  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 180/1251]  eta: 0:15:50  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7659  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 190/1251]  eta: 0:15:35  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7692  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 200/1251]  eta: 0:15:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7718  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 210/1251]  eta: 0:15:06  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7677  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 220/1251]  eta: 0:14:53  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7742  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 230/1251]  eta: 0:14:42  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7951  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 240/1251]  eta: 0:14:30  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7962  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 250/1251]  eta: 0:14:17  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7739  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 260/1251]  eta: 0:14:06  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7782  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 270/1251]  eta: 0:13:55  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7843  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 280/1251]  eta: 0:13:44  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7713  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 290/1251]  eta: 0:13:32  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7637  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 300/1251]  eta: 0:13:23  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7871  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 310/1251]  eta: 0:13:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7947  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 320/1251]  eta: 0:13:01  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7675  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 330/1251]  eta: 0:12:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7616  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 340/1251]  eta: 0:12:41  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7676  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 350/1251]  eta: 0:12:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7727  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 360/1251]  eta: 0:12:21  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7659  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 370/1251]  eta: 0:12:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7680  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 380/1251]  eta: 0:12:02  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7976  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 390/1251]  eta: 0:11:53  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7916  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 400/1251]  eta: 0:11:44  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7800  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 410/1251]  eta: 0:11:34  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7762  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 420/1251]  eta: 0:11:24  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7613  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 430/1251]  eta: 0:11:15  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7681  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 440/1251]  eta: 0:11:06  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7731  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 450/1251]  eta: 0:10:57  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7738  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 460/1251]  eta: 0:10:48  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7700  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 470/1251]  eta: 0:10:39  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7694  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 480/1251]  eta: 0:10:30  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7661  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 490/1251]  eta: 0:10:21  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7595  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 500/1251]  eta: 0:10:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7626  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 510/1251]  eta: 0:10:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7715  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 520/1251]  eta: 0:09:54  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7708  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 530/1251]  eta: 0:09:46  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7833  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 540/1251]  eta: 0:09:37  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 550/1251]  eta: 0:09:29  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 560/1251]  eta: 0:09:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7775  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 570/1251]  eta: 0:09:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7615  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 580/1251]  eta: 0:09:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7746  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 590/1251]  eta: 0:08:54  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7708  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 600/1251]  eta: 0:08:46  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7672  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 610/1251]  eta: 0:08:37  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7688  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 620/1251]  eta: 0:08:29  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7627  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 630/1251]  eta: 0:08:20  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7625  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 640/1251]  eta: 0:08:12  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7573  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 650/1251]  eta: 0:08:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7624  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 660/1251]  eta: 0:07:55  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 670/1251]  eta: 0:07:47  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7992  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 680/1251]  eta: 0:07:39  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7740  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 690/1251]  eta: 0:07:30  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7690  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 700/1251]  eta: 0:07:22  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 710/1251]  eta: 0:07:14  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7740  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 720/1251]  eta: 0:07:06  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7580  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 730/1251]  eta: 0:06:57  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7677  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 740/1251]  eta: 0:06:49  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7771  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 750/1251]  eta: 0:06:41  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7660  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 760/1251]  eta: 0:06:33  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7579  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 770/1251]  eta: 0:06:24  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7644  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 780/1251]  eta: 0:06:16  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7681  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 790/1251]  eta: 0:06:08  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7604  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 800/1251]  eta: 0:06:00  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7567  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 810/1251]  eta: 0:05:52  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 820/1251]  eta: 0:05:44  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7969  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 830/1251]  eta: 0:05:35  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7730  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 840/1251]  eta: 0:05:27  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7714  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 850/1251]  eta: 0:05:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7777  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 860/1251]  eta: 0:05:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7663  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 870/1251]  eta: 0:05:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7667  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 880/1251]  eta: 0:04:55  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7658  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 890/1251]  eta: 0:04:47  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7745  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 900/1251]  eta: 0:04:39  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7717  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 910/1251]  eta: 0:04:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7558  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 920/1251]  eta: 0:04:23  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7582  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 930/1251]  eta: 0:04:15  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7673  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 940/1251]  eta: 0:04:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7665  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 950/1251]  eta: 0:03:59  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7661  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 960/1251]  eta: 0:03:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7775  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 970/1251]  eta: 0:03:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7877  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 980/1251]  eta: 0:03:35  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7805  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [ 990/1251]  eta: 0:03:27  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7732  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1000/1251]  eta: 0:03:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7686  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1010/1251]  eta: 0:03:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7627  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1020/1251]  eta: 0:03:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7781  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1030/1251]  eta: 0:02:55  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7724  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1040/1251]  eta: 0:02:47  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7646  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1050/1251]  eta: 0:02:39  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7691  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1060/1251]  eta: 0:02:31  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7591  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1070/1251]  eta: 0:02:23  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7546  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1080/1251]  eta: 0:02:15  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7583  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1090/1251]  eta: 0:02:07  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7645  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1100/1251]  eta: 0:01:59  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7780  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1110/1251]  eta: 0:01:51  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7876  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1120/1251]  eta: 0:01:43  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7781  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1130/1251]  eta: 0:01:35  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7837  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1140/1251]  eta: 0:01:27  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7757  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1150/1251]  eta: 0:01:19  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7543  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1160/1251]  eta: 0:01:11  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7633  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1170/1251]  eta: 0:01:03  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7761  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1180/1251]  eta: 0:00:56  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7778  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1190/1251]  eta: 0:00:48  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7635  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1200/1251]  eta: 0:00:40  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7542  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1210/1251]  eta: 0:00:32  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7531  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1220/1251]  eta: 0:00:24  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7480  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1230/1251]  eta: 0:00:16  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7481  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1240/1251]  eta: 0:00:08  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7550  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [47]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 0.0000 (0.0000)  time: 0.7641  data: 0.0002  max mem: 19734
Epoch: [47] Total time: 0:16:26 (0.7888 s / it)
Averaged stats: lr: 0.000012  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:42:21  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 37.3256  data: 37.1816  max mem: 19734
Test:  [ 10/261]  eta: 0:15:45  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.7664  data: 3.4447  max mem: 19734
Test:  [ 20/261]  eta: 0:08:56  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.4732  data: 0.0415  max mem: 19734
Test:  [ 30/261]  eta: 0:06:15  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.4465  data: 0.0215  max mem: 19734
Test:  [ 40/261]  eta: 0:05:07  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5141  data: 0.0739  max mem: 19734
Test:  [ 50/261]  eta: 0:04:07  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4746  data: 0.0672  max mem: 19734
Test:  [ 60/261]  eta: 0:03:24  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2510  data: 0.0159  max mem: 19734
Test:  [ 70/261]  eta: 0:02:58  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3272  data: 0.0100  max mem: 19734
Test:  [ 80/261]  eta: 0:02:36  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4035  data: 0.0116  max mem: 19734
Test:  [ 90/261]  eta: 0:02:17  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3344  data: 0.0594  max mem: 19734
Test:  [100/261]  eta: 0:02:22  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.9710  data: 0.7397  max mem: 19734
Test:  [110/261]  eta: 0:02:06  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.9976  data: 0.6956  max mem: 19734
Test:  [120/261]  eta: 0:01:51  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3175  data: 0.0142  max mem: 19734
Test:  [130/261]  eta: 0:01:39  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3275  data: 0.0199  max mem: 19734
Test:  [140/261]  eta: 0:01:33  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.6682  data: 0.2899  max mem: 19734
Test:  [150/261]  eta: 0:01:22  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.6517  data: 0.2853  max mem: 19734
Test:  [160/261]  eta: 0:01:12  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3255  data: 0.0157  max mem: 19734
Test:  [170/261]  eta: 0:01:02  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2780  data: 0.0135  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2300  data: 0.0116  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1836  data: 0.0094  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1495  data: 0.0077  max mem: 19734
Test:  [210/261]  eta: 0:00:30  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1681  data: 0.0378  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1455  data: 0.0333  max mem: 19734
Test:  [230/261]  eta: 0:00:17  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1099  data: 0.0016  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1093  data: 0.0015  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1081  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1059  data: 0.0008  max mem: 19734
Test: Total time: 0:02:10 (0.5016 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.72%
Loss is nan, stopping training this iteration.
Epoch: [48]  [   0/1251]  eta: 6:41:41  lr: 0.000011  loss: 0.0000 (0.0000)  time: 19.2659  data: 9.2999  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [  10/1251]  eta: 0:54:03  lr: 0.000011  loss: 0.0000 (0.0000)  time: 2.6136  data: 0.8461  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [  20/1251]  eta: 0:35:44  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.8663  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [  30/1251]  eta: 0:29:05  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7789  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [  40/1251]  eta: 0:25:32  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7642  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [  50/1251]  eta: 0:23:29  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7758  data: 0.0007  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [  60/1251]  eta: 0:21:59  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7844  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [  70/1251]  eta: 0:20:51  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7702  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [  80/1251]  eta: 0:19:57  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7651  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [  90/1251]  eta: 0:19:14  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7615  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 100/1251]  eta: 0:18:39  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7703  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 110/1251]  eta: 0:18:09  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7777  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 120/1251]  eta: 0:17:42  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7719  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 130/1251]  eta: 0:17:20  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7776  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 140/1251]  eta: 0:16:59  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7816  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 150/1251]  eta: 0:16:42  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7953  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 160/1251]  eta: 0:16:25  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.8058  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 170/1251]  eta: 0:16:08  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7835  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 180/1251]  eta: 0:15:52  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7727  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 190/1251]  eta: 0:15:37  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 200/1251]  eta: 0:15:24  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7901  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 210/1251]  eta: 0:15:09  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7780  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 220/1251]  eta: 0:14:56  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7710  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 230/1251]  eta: 0:14:43  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7701  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 240/1251]  eta: 0:14:30  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7673  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 250/1251]  eta: 0:14:18  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7680  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 260/1251]  eta: 0:14:06  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7719  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 270/1251]  eta: 0:13:54  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7701  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 280/1251]  eta: 0:13:43  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7747  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 290/1251]  eta: 0:13:32  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7791  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 300/1251]  eta: 0:13:23  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 310/1251]  eta: 0:13:12  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7918  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 320/1251]  eta: 0:13:01  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7697  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 330/1251]  eta: 0:12:51  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7631  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 340/1251]  eta: 0:12:41  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7788  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 350/1251]  eta: 0:12:31  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7747  data: 0.0006  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 360/1251]  eta: 0:12:21  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7589  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 370/1251]  eta: 0:12:11  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7619  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 380/1251]  eta: 0:12:01  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7638  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 390/1251]  eta: 0:11:51  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7616  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 400/1251]  eta: 0:11:42  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7598  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 410/1251]  eta: 0:11:32  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7628  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 420/1251]  eta: 0:11:23  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 430/1251]  eta: 0:11:14  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7813  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 440/1251]  eta: 0:11:05  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7767  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 450/1251]  eta: 0:10:56  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7882  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 460/1251]  eta: 0:10:47  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 470/1251]  eta: 0:10:38  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7680  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 480/1251]  eta: 0:10:29  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7622  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 490/1251]  eta: 0:10:21  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7818  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 500/1251]  eta: 0:10:12  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7815  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 510/1251]  eta: 0:10:03  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7624  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 520/1251]  eta: 0:09:54  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7623  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 530/1251]  eta: 0:09:45  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7651  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 540/1251]  eta: 0:09:36  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7616  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 550/1251]  eta: 0:09:28  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7594  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 560/1251]  eta: 0:09:19  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7634  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 570/1251]  eta: 0:09:11  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7733  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 580/1251]  eta: 0:09:02  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7696  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 590/1251]  eta: 0:08:54  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7748  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 600/1251]  eta: 0:08:45  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 610/1251]  eta: 0:08:37  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7840  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 620/1251]  eta: 0:08:28  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7712  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 630/1251]  eta: 0:08:20  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7661  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 640/1251]  eta: 0:08:12  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7782  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 650/1251]  eta: 0:08:03  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7734  data: 0.0006  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 660/1251]  eta: 0:07:55  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7604  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 670/1251]  eta: 0:07:46  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7598  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 680/1251]  eta: 0:07:38  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7635  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 690/1251]  eta: 0:07:30  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7638  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 700/1251]  eta: 0:07:21  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7585  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 710/1251]  eta: 0:07:13  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7772  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 720/1251]  eta: 0:07:05  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7829  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 730/1251]  eta: 0:06:57  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7811  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 740/1251]  eta: 0:06:49  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 750/1251]  eta: 0:06:41  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7745  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 760/1251]  eta: 0:06:32  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7674  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 770/1251]  eta: 0:06:24  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7656  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 780/1251]  eta: 0:06:16  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7759  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 790/1251]  eta: 0:06:08  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7730  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 800/1251]  eta: 0:06:00  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7632  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 810/1251]  eta: 0:05:51  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7656  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 820/1251]  eta: 0:05:43  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7607  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 830/1251]  eta: 0:05:35  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7590  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 840/1251]  eta: 0:05:27  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7654  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 850/1251]  eta: 0:05:19  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7671  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 860/1251]  eta: 0:05:11  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7799  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 870/1251]  eta: 0:05:03  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7785  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 880/1251]  eta: 0:04:55  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7765  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 890/1251]  eta: 0:04:47  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7784  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 900/1251]  eta: 0:04:39  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7674  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 910/1251]  eta: 0:04:31  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7662  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 920/1251]  eta: 0:04:23  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7715  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 930/1251]  eta: 0:04:15  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7741  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 940/1251]  eta: 0:04:06  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7621  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 950/1251]  eta: 0:03:58  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7581  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 960/1251]  eta: 0:03:50  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7624  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 970/1251]  eta: 0:03:42  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7646  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 980/1251]  eta: 0:03:34  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7585  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [ 990/1251]  eta: 0:03:26  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7601  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1000/1251]  eta: 0:03:18  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7768  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1010/1251]  eta: 0:03:10  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7707  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1020/1251]  eta: 0:03:02  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7637  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1030/1251]  eta: 0:02:54  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7715  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1040/1251]  eta: 0:02:47  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7723  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1050/1251]  eta: 0:02:39  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7757  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1060/1251]  eta: 0:02:31  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7674  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1070/1251]  eta: 0:02:23  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7846  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1080/1251]  eta: 0:02:15  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [48]  [1090/1251]  eta: 0:02:07  lr: 0.000011  loss: 0.0000 (0.0000)  time: 0.7600  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
