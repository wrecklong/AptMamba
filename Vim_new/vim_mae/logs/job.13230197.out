CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
| distributed init (rank 7): env://
| distributed init (rank 5): env://
| distributed init (rank 0): env://
| distributed init (rank 2): env://
| distributed init (rank 4): env://
| distributed init (rank 3): env://
| distributed init (rank 6): env://
| distributed init (rank 1): env://
Namespace(batch_size=128, epochs=100, bce_loss=False, update_freq=1, unscale_lr=False, model='vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.1, sched='cosine', lr=1e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='/cluster/work/cvl/guosun/shangye/pretrained/vim_t_midclstok_76p1acc.pth', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, cosub=False, finetune='', attn_only=False, data_path='/scratch/tmp.13230197.guosun/datasets/imagenet_full_size/', data_set='IMNET', inat_category='name', output_dir='/cluster/work/cvl/guosun/shangye/output/Vim_new/vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_300einit_100e_batch_size128_p0.8_lr0.00001_min_lr1e-6_decoder_pruning_loss3stage_weight0.1_mse_weight0.02_sort_keep_policy_pretrain_mae', log_dir=None, device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=32, pin_mem=True, distributed=True, world_size=8, dist_url='env://', if_amp=False, if_continue_inf=True, if_nan2num=False, if_random_cls_token_position=False, if_random_token_rank=False, base_rate=0.8, lr_scale=0.01, ratio_weight=2.0, pretrained_mae_path='/cluster/work/cvl/guosun/shangye/output/pretrain_mae_vim/vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_mae_300e/checkpoint.pth', local_rank=0, rank=0, gpu=0, dist_backend='nccl')
Creating model: vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2
keep_rate [0.8, 0.6400000000000001, 0.5120000000000001]
VisionMambaPrunning(
  (token_merge_module): TPSModule()
  (score_predictor): ModuleList(
    (0-2): 3 x PredictorLG(
      (in_conv_local): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (in_conv_cls): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (out_conv): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=96, out_features=48, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=48, out_features=2, bias=True)
        (5): LogSoftmax(dim=-1)
      )
    )
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=192, out_features=1000, bias=True)
  (drop_path): Identity()
  (layers): ModuleList(
    (0-23): 24 x Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=192, out_features=768, bias=False)
        (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (act): SiLU()
        (x_proj): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj): Linear(in_features=12, out_features=384, bias=True)
        (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
        (out_proj): Linear(in_features=384, out_features=192, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): Identity()
    )
  )
  (decoder): MAE_Decoder(
    (decoder_embed): Linear(in_features=192, out_features=512, bias=True)
    (decoder_blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)
  )
  (norm_f): RMSNorm()
)
Weights of VisionMambaPrunning not initialized from pretrained model: ['score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'decoder.mask_token', 'decoder.decoder_pos_embed', 'decoder.decoder_embed.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_blocks.0.norm1.weight', 'decoder.decoder_blocks.0.norm1.bias', 'decoder.decoder_blocks.0.attn.qkv.weight', 'decoder.decoder_blocks.0.attn.qkv.bias', 'decoder.decoder_blocks.0.attn.proj.weight', 'decoder.decoder_blocks.0.attn.proj.bias', 'decoder.decoder_blocks.0.norm2.weight', 'decoder.decoder_blocks.0.norm2.bias', 'decoder.decoder_blocks.0.mlp.fc1.weight', 'decoder.decoder_blocks.0.mlp.fc1.bias', 'decoder.decoder_blocks.0.mlp.fc2.weight', 'decoder.decoder_blocks.0.mlp.fc2.bias', 'decoder.decoder_blocks.1.norm1.weight', 'decoder.decoder_blocks.1.norm1.bias', 'decoder.decoder_blocks.1.attn.qkv.weight', 'decoder.decoder_blocks.1.attn.qkv.bias', 'decoder.decoder_blocks.1.attn.proj.weight', 'decoder.decoder_blocks.1.attn.proj.bias', 'decoder.decoder_blocks.1.norm2.weight', 'decoder.decoder_blocks.1.norm2.bias', 'decoder.decoder_blocks.1.mlp.fc1.weight', 'decoder.decoder_blocks.1.mlp.fc1.bias', 'decoder.decoder_blocks.1.mlp.fc2.weight', 'decoder.decoder_blocks.1.mlp.fc2.bias', 'decoder.decoder_blocks.2.norm1.weight', 'decoder.decoder_blocks.2.norm1.bias', 'decoder.decoder_blocks.2.attn.qkv.weight', 'decoder.decoder_blocks.2.attn.qkv.bias', 'decoder.decoder_blocks.2.attn.proj.weight', 'decoder.decoder_blocks.2.attn.proj.bias', 'decoder.decoder_blocks.2.norm2.weight', 'decoder.decoder_blocks.2.norm2.bias', 'decoder.decoder_blocks.2.mlp.fc1.weight', 'decoder.decoder_blocks.2.mlp.fc1.bias', 'decoder.decoder_blocks.2.mlp.fc2.weight', 'decoder.decoder_blocks.2.mlp.fc2.bias', 'decoder.decoder_blocks.3.norm1.weight', 'decoder.decoder_blocks.3.norm1.bias', 'decoder.decoder_blocks.3.attn.qkv.weight', 'decoder.decoder_blocks.3.attn.qkv.bias', 'decoder.decoder_blocks.3.attn.proj.weight', 'decoder.decoder_blocks.3.attn.proj.bias', 'decoder.decoder_blocks.3.norm2.weight', 'decoder.decoder_blocks.3.norm2.bias', 'decoder.decoder_blocks.3.mlp.fc1.weight', 'decoder.decoder_blocks.3.mlp.fc1.bias', 'decoder.decoder_blocks.3.mlp.fc2.weight', 'decoder.decoder_blocks.3.mlp.fc2.bias', 'decoder.decoder_blocks.4.norm1.weight', 'decoder.decoder_blocks.4.norm1.bias', 'decoder.decoder_blocks.4.attn.qkv.weight', 'decoder.decoder_blocks.4.attn.qkv.bias', 'decoder.decoder_blocks.4.attn.proj.weight', 'decoder.decoder_blocks.4.attn.proj.bias', 'decoder.decoder_blocks.4.norm2.weight', 'decoder.decoder_blocks.4.norm2.bias', 'decoder.decoder_blocks.4.mlp.fc1.weight', 'decoder.decoder_blocks.4.mlp.fc1.bias', 'decoder.decoder_blocks.4.mlp.fc2.weight', 'decoder.decoder_blocks.4.mlp.fc2.bias', 'decoder.decoder_blocks.5.norm1.weight', 'decoder.decoder_blocks.5.norm1.bias', 'decoder.decoder_blocks.5.attn.qkv.weight', 'decoder.decoder_blocks.5.attn.qkv.bias', 'decoder.decoder_blocks.5.attn.proj.weight', 'decoder.decoder_blocks.5.attn.proj.bias', 'decoder.decoder_blocks.5.norm2.weight', 'decoder.decoder_blocks.5.norm2.bias', 'decoder.decoder_blocks.5.mlp.fc1.weight', 'decoder.decoder_blocks.5.mlp.fc1.bias', 'decoder.decoder_blocks.5.mlp.fc2.weight', 'decoder.decoder_blocks.5.mlp.fc2.bias', 'decoder.decoder_blocks.6.norm1.weight', 'decoder.decoder_blocks.6.norm1.bias', 'decoder.decoder_blocks.6.attn.qkv.weight', 'decoder.decoder_blocks.6.attn.qkv.bias', 'decoder.decoder_blocks.6.attn.proj.weight', 'decoder.decoder_blocks.6.attn.proj.bias', 'decoder.decoder_blocks.6.norm2.weight', 'decoder.decoder_blocks.6.norm2.bias', 'decoder.decoder_blocks.6.mlp.fc1.weight', 'decoder.decoder_blocks.6.mlp.fc1.bias', 'decoder.decoder_blocks.6.mlp.fc2.weight', 'decoder.decoder_blocks.6.mlp.fc2.bias', 'decoder.decoder_blocks.7.norm1.weight', 'decoder.decoder_blocks.7.norm1.bias', 'decoder.decoder_blocks.7.attn.qkv.weight', 'decoder.decoder_blocks.7.attn.qkv.bias', 'decoder.decoder_blocks.7.attn.proj.weight', 'decoder.decoder_blocks.7.attn.proj.bias', 'decoder.decoder_blocks.7.norm2.weight', 'decoder.decoder_blocks.7.norm2.bias', 'decoder.decoder_blocks.7.mlp.fc1.weight', 'decoder.decoder_blocks.7.mlp.fc1.bias', 'decoder.decoder_blocks.7.mlp.fc2.weight', 'decoder.decoder_blocks.7.mlp.fc2.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_pred.bias']
Weights of VisionMambaPrunning not initialized from pretrained model: ['cls_token', 'pos_embed', 'score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'head.weight', 'head.bias', 'layers.0.mixer.A_log', 'layers.0.mixer.D', 'layers.0.mixer.A_b_log', 'layers.0.mixer.D_b', 'layers.0.mixer.in_proj.weight', 'layers.0.mixer.conv1d.weight', 'layers.0.mixer.conv1d.bias', 'layers.0.mixer.x_proj.weight', 'layers.0.mixer.dt_proj.weight', 'layers.0.mixer.dt_proj.bias', 'layers.0.mixer.conv1d_b.weight', 'layers.0.mixer.conv1d_b.bias', 'layers.0.mixer.x_proj_b.weight', 'layers.0.mixer.dt_proj_b.weight', 'layers.0.mixer.dt_proj_b.bias', 'layers.0.mixer.out_proj.weight', 'layers.0.norm.weight', 'layers.1.mixer.A_log', 'layers.1.mixer.D', 'layers.1.mixer.A_b_log', 'layers.1.mixer.D_b', 'layers.1.mixer.in_proj.weight', 'layers.1.mixer.conv1d.weight', 'layers.1.mixer.conv1d.bias', 'layers.1.mixer.x_proj.weight', 'layers.1.mixer.dt_proj.weight', 'layers.1.mixer.dt_proj.bias', 'layers.1.mixer.conv1d_b.weight', 'layers.1.mixer.conv1d_b.bias', 'layers.1.mixer.x_proj_b.weight', 'layers.1.mixer.dt_proj_b.weight', 'layers.1.mixer.dt_proj_b.bias', 'layers.1.mixer.out_proj.weight', 'layers.1.norm.weight', 'layers.2.mixer.A_log', 'layers.2.mixer.D', 'layers.2.mixer.A_b_log', 'layers.2.mixer.D_b', 'layers.2.mixer.in_proj.weight', 'layers.2.mixer.conv1d.weight', 'layers.2.mixer.conv1d.bias', 'layers.2.mixer.x_proj.weight', 'layers.2.mixer.dt_proj.weight', 'layers.2.mixer.dt_proj.bias', 'layers.2.mixer.conv1d_b.weight', 'layers.2.mixer.conv1d_b.bias', 'layers.2.mixer.x_proj_b.weight', 'layers.2.mixer.dt_proj_b.weight', 'layers.2.mixer.dt_proj_b.bias', 'layers.2.mixer.out_proj.weight', 'layers.2.norm.weight', 'layers.3.mixer.A_log', 'layers.3.mixer.D', 'layers.3.mixer.A_b_log', 'layers.3.mixer.D_b', 'layers.3.mixer.in_proj.weight', 'layers.3.mixer.conv1d.weight', 'layers.3.mixer.conv1d.bias', 'layers.3.mixer.x_proj.weight', 'layers.3.mixer.dt_proj.weight', 'layers.3.mixer.dt_proj.bias', 'layers.3.mixer.conv1d_b.weight', 'layers.3.mixer.conv1d_b.bias', 'layers.3.mixer.x_proj_b.weight', 'layers.3.mixer.dt_proj_b.weight', 'layers.3.mixer.dt_proj_b.bias', 'layers.3.mixer.out_proj.weight', 'layers.3.norm.weight', 'layers.4.mixer.A_log', 'layers.4.mixer.D', 'layers.4.mixer.A_b_log', 'layers.4.mixer.D_b', 'layers.4.mixer.in_proj.weight', 'layers.4.mixer.conv1d.weight', 'layers.4.mixer.conv1d.bias', 'layers.4.mixer.x_proj.weight', 'layers.4.mixer.dt_proj.weight', 'layers.4.mixer.dt_proj.bias', 'layers.4.mixer.conv1d_b.weight', 'layers.4.mixer.conv1d_b.bias', 'layers.4.mixer.x_proj_b.weight', 'layers.4.mixer.dt_proj_b.weight', 'layers.4.mixer.dt_proj_b.bias', 'layers.4.mixer.out_proj.weight', 'layers.4.norm.weight', 'layers.5.mixer.A_log', 'layers.5.mixer.D', 'layers.5.mixer.A_b_log', 'layers.5.mixer.D_b', 'layers.5.mixer.in_proj.weight', 'layers.5.mixer.conv1d.weight', 'layers.5.mixer.conv1d.bias', 'layers.5.mixer.x_proj.weight', 'layers.5.mixer.dt_proj.weight', 'layers.5.mixer.dt_proj.bias', 'layers.5.mixer.conv1d_b.weight', 'layers.5.mixer.conv1d_b.bias', 'layers.5.mixer.x_proj_b.weight', 'layers.5.mixer.dt_proj_b.weight', 'layers.5.mixer.dt_proj_b.bias', 'layers.5.mixer.out_proj.weight', 'layers.5.norm.weight', 'layers.6.mixer.A_log', 'layers.6.mixer.D', 'layers.6.mixer.A_b_log', 'layers.6.mixer.D_b', 'layers.6.mixer.in_proj.weight', 'layers.6.mixer.conv1d.weight', 'layers.6.mixer.conv1d.bias', 'layers.6.mixer.x_proj.weight', 'layers.6.mixer.dt_proj.weight', 'layers.6.mixer.dt_proj.bias', 'layers.6.mixer.conv1d_b.weight', 'layers.6.mixer.conv1d_b.bias', 'layers.6.mixer.x_proj_b.weight', 'layers.6.mixer.dt_proj_b.weight', 'layers.6.mixer.dt_proj_b.bias', 'layers.6.mixer.out_proj.weight', 'layers.6.norm.weight', 'layers.7.mixer.A_log', 'layers.7.mixer.D', 'layers.7.mixer.A_b_log', 'layers.7.mixer.D_b', 'layers.7.mixer.in_proj.weight', 'layers.7.mixer.conv1d.weight', 'layers.7.mixer.conv1d.bias', 'layers.7.mixer.x_proj.weight', 'layers.7.mixer.dt_proj.weight', 'layers.7.mixer.dt_proj.bias', 'layers.7.mixer.conv1d_b.weight', 'layers.7.mixer.conv1d_b.bias', 'layers.7.mixer.x_proj_b.weight', 'layers.7.mixer.dt_proj_b.weight', 'layers.7.mixer.dt_proj_b.bias', 'layers.7.mixer.out_proj.weight', 'layers.7.norm.weight', 'layers.8.mixer.A_log', 'layers.8.mixer.D', 'layers.8.mixer.A_b_log', 'layers.8.mixer.D_b', 'layers.8.mixer.in_proj.weight', 'layers.8.mixer.conv1d.weight', 'layers.8.mixer.conv1d.bias', 'layers.8.mixer.x_proj.weight', 'layers.8.mixer.dt_proj.weight', 'layers.8.mixer.dt_proj.bias', 'layers.8.mixer.conv1d_b.weight', 'layers.8.mixer.conv1d_b.bias', 'layers.8.mixer.x_proj_b.weight', 'layers.8.mixer.dt_proj_b.weight', 'layers.8.mixer.dt_proj_b.bias', 'layers.8.mixer.out_proj.weight', 'layers.8.norm.weight', 'layers.9.mixer.A_log', 'layers.9.mixer.D', 'layers.9.mixer.A_b_log', 'layers.9.mixer.D_b', 'layers.9.mixer.in_proj.weight', 'layers.9.mixer.conv1d.weight', 'layers.9.mixer.conv1d.bias', 'layers.9.mixer.x_proj.weight', 'layers.9.mixer.dt_proj.weight', 'layers.9.mixer.dt_proj.bias', 'layers.9.mixer.conv1d_b.weight', 'layers.9.mixer.conv1d_b.bias', 'layers.9.mixer.x_proj_b.weight', 'layers.9.mixer.dt_proj_b.weight', 'layers.9.mixer.dt_proj_b.bias', 'layers.9.mixer.out_proj.weight', 'layers.9.norm.weight', 'layers.10.mixer.A_log', 'layers.10.mixer.D', 'layers.10.mixer.A_b_log', 'layers.10.mixer.D_b', 'layers.10.mixer.in_proj.weight', 'layers.10.mixer.conv1d.weight', 'layers.10.mixer.conv1d.bias', 'layers.10.mixer.x_proj.weight', 'layers.10.mixer.dt_proj.weight', 'layers.10.mixer.dt_proj.bias', 'layers.10.mixer.conv1d_b.weight', 'layers.10.mixer.conv1d_b.bias', 'layers.10.mixer.x_proj_b.weight', 'layers.10.mixer.dt_proj_b.weight', 'layers.10.mixer.dt_proj_b.bias', 'layers.10.mixer.out_proj.weight', 'layers.10.norm.weight', 'layers.11.mixer.A_log', 'layers.11.mixer.D', 'layers.11.mixer.A_b_log', 'layers.11.mixer.D_b', 'layers.11.mixer.in_proj.weight', 'layers.11.mixer.conv1d.weight', 'layers.11.mixer.conv1d.bias', 'layers.11.mixer.x_proj.weight', 'layers.11.mixer.dt_proj.weight', 'layers.11.mixer.dt_proj.bias', 'layers.11.mixer.conv1d_b.weight', 'layers.11.mixer.conv1d_b.bias', 'layers.11.mixer.x_proj_b.weight', 'layers.11.mixer.dt_proj_b.weight', 'layers.11.mixer.dt_proj_b.bias', 'layers.11.mixer.out_proj.weight', 'layers.11.norm.weight', 'layers.12.mixer.A_log', 'layers.12.mixer.D', 'layers.12.mixer.A_b_log', 'layers.12.mixer.D_b', 'layers.12.mixer.in_proj.weight', 'layers.12.mixer.conv1d.weight', 'layers.12.mixer.conv1d.bias', 'layers.12.mixer.x_proj.weight', 'layers.12.mixer.dt_proj.weight', 'layers.12.mixer.dt_proj.bias', 'layers.12.mixer.conv1d_b.weight', 'layers.12.mixer.conv1d_b.bias', 'layers.12.mixer.x_proj_b.weight', 'layers.12.mixer.dt_proj_b.weight', 'layers.12.mixer.dt_proj_b.bias', 'layers.12.mixer.out_proj.weight', 'layers.12.norm.weight', 'layers.13.mixer.A_log', 'layers.13.mixer.D', 'layers.13.mixer.A_b_log', 'layers.13.mixer.D_b', 'layers.13.mixer.in_proj.weight', 'layers.13.mixer.conv1d.weight', 'layers.13.mixer.conv1d.bias', 'layers.13.mixer.x_proj.weight', 'layers.13.mixer.dt_proj.weight', 'layers.13.mixer.dt_proj.bias', 'layers.13.mixer.conv1d_b.weight', 'layers.13.mixer.conv1d_b.bias', 'layers.13.mixer.x_proj_b.weight', 'layers.13.mixer.dt_proj_b.weight', 'layers.13.mixer.dt_proj_b.bias', 'layers.13.mixer.out_proj.weight', 'layers.13.norm.weight', 'layers.14.mixer.A_log', 'layers.14.mixer.D', 'layers.14.mixer.A_b_log', 'layers.14.mixer.D_b', 'layers.14.mixer.in_proj.weight', 'layers.14.mixer.conv1d.weight', 'layers.14.mixer.conv1d.bias', 'layers.14.mixer.x_proj.weight', 'layers.14.mixer.dt_proj.weight', 'layers.14.mixer.dt_proj.bias', 'layers.14.mixer.conv1d_b.weight', 'layers.14.mixer.conv1d_b.bias', 'layers.14.mixer.x_proj_b.weight', 'layers.14.mixer.dt_proj_b.weight', 'layers.14.mixer.dt_proj_b.bias', 'layers.14.mixer.out_proj.weight', 'layers.14.norm.weight', 'layers.15.mixer.A_log', 'layers.15.mixer.D', 'layers.15.mixer.A_b_log', 'layers.15.mixer.D_b', 'layers.15.mixer.in_proj.weight', 'layers.15.mixer.conv1d.weight', 'layers.15.mixer.conv1d.bias', 'layers.15.mixer.x_proj.weight', 'layers.15.mixer.dt_proj.weight', 'layers.15.mixer.dt_proj.bias', 'layers.15.mixer.conv1d_b.weight', 'layers.15.mixer.conv1d_b.bias', 'layers.15.mixer.x_proj_b.weight', 'layers.15.mixer.dt_proj_b.weight', 'layers.15.mixer.dt_proj_b.bias', 'layers.15.mixer.out_proj.weight', 'layers.15.norm.weight', 'layers.16.mixer.A_log', 'layers.16.mixer.D', 'layers.16.mixer.A_b_log', 'layers.16.mixer.D_b', 'layers.16.mixer.in_proj.weight', 'layers.16.mixer.conv1d.weight', 'layers.16.mixer.conv1d.bias', 'layers.16.mixer.x_proj.weight', 'layers.16.mixer.dt_proj.weight', 'layers.16.mixer.dt_proj.bias', 'layers.16.mixer.conv1d_b.weight', 'layers.16.mixer.conv1d_b.bias', 'layers.16.mixer.x_proj_b.weight', 'layers.16.mixer.dt_proj_b.weight', 'layers.16.mixer.dt_proj_b.bias', 'layers.16.mixer.out_proj.weight', 'layers.16.norm.weight', 'layers.17.mixer.A_log', 'layers.17.mixer.D', 'layers.17.mixer.A_b_log', 'layers.17.mixer.D_b', 'layers.17.mixer.in_proj.weight', 'layers.17.mixer.conv1d.weight', 'layers.17.mixer.conv1d.bias', 'layers.17.mixer.x_proj.weight', 'layers.17.mixer.dt_proj.weight', 'layers.17.mixer.dt_proj.bias', 'layers.17.mixer.conv1d_b.weight', 'layers.17.mixer.conv1d_b.bias', 'layers.17.mixer.x_proj_b.weight', 'layers.17.mixer.dt_proj_b.weight', 'layers.17.mixer.dt_proj_b.bias', 'layers.17.mixer.out_proj.weight', 'layers.17.norm.weight', 'layers.18.mixer.A_log', 'layers.18.mixer.D', 'layers.18.mixer.A_b_log', 'layers.18.mixer.D_b', 'layers.18.mixer.in_proj.weight', 'layers.18.mixer.conv1d.weight', 'layers.18.mixer.conv1d.bias', 'layers.18.mixer.x_proj.weight', 'layers.18.mixer.dt_proj.weight', 'layers.18.mixer.dt_proj.bias', 'layers.18.mixer.conv1d_b.weight', 'layers.18.mixer.conv1d_b.bias', 'layers.18.mixer.x_proj_b.weight', 'layers.18.mixer.dt_proj_b.weight', 'layers.18.mixer.dt_proj_b.bias', 'layers.18.mixer.out_proj.weight', 'layers.18.norm.weight', 'layers.19.mixer.A_log', 'layers.19.mixer.D', 'layers.19.mixer.A_b_log', 'layers.19.mixer.D_b', 'layers.19.mixer.in_proj.weight', 'layers.19.mixer.conv1d.weight', 'layers.19.mixer.conv1d.bias', 'layers.19.mixer.x_proj.weight', 'layers.19.mixer.dt_proj.weight', 'layers.19.mixer.dt_proj.bias', 'layers.19.mixer.conv1d_b.weight', 'layers.19.mixer.conv1d_b.bias', 'layers.19.mixer.x_proj_b.weight', 'layers.19.mixer.dt_proj_b.weight', 'layers.19.mixer.dt_proj_b.bias', 'layers.19.mixer.out_proj.weight', 'layers.19.norm.weight', 'layers.20.mixer.A_log', 'layers.20.mixer.D', 'layers.20.mixer.A_b_log', 'layers.20.mixer.D_b', 'layers.20.mixer.in_proj.weight', 'layers.20.mixer.conv1d.weight', 'layers.20.mixer.conv1d.bias', 'layers.20.mixer.x_proj.weight', 'layers.20.mixer.dt_proj.weight', 'layers.20.mixer.dt_proj.bias', 'layers.20.mixer.conv1d_b.weight', 'layers.20.mixer.conv1d_b.bias', 'layers.20.mixer.x_proj_b.weight', 'layers.20.mixer.dt_proj_b.weight', 'layers.20.mixer.dt_proj_b.bias', 'layers.20.mixer.out_proj.weight', 'layers.20.norm.weight', 'layers.21.mixer.A_log', 'layers.21.mixer.D', 'layers.21.mixer.A_b_log', 'layers.21.mixer.D_b', 'layers.21.mixer.in_proj.weight', 'layers.21.mixer.conv1d.weight', 'layers.21.mixer.conv1d.bias', 'layers.21.mixer.x_proj.weight', 'layers.21.mixer.dt_proj.weight', 'layers.21.mixer.dt_proj.bias', 'layers.21.mixer.conv1d_b.weight', 'layers.21.mixer.conv1d_b.bias', 'layers.21.mixer.x_proj_b.weight', 'layers.21.mixer.dt_proj_b.weight', 'layers.21.mixer.dt_proj_b.bias', 'layers.21.mixer.out_proj.weight', 'layers.21.norm.weight', 'layers.22.mixer.A_log', 'layers.22.mixer.D', 'layers.22.mixer.A_b_log', 'layers.22.mixer.D_b', 'layers.22.mixer.in_proj.weight', 'layers.22.mixer.conv1d.weight', 'layers.22.mixer.conv1d.bias', 'layers.22.mixer.x_proj.weight', 'layers.22.mixer.dt_proj.weight', 'layers.22.mixer.dt_proj.bias', 'layers.22.mixer.conv1d_b.weight', 'layers.22.mixer.conv1d_b.bias', 'layers.22.mixer.x_proj_b.weight', 'layers.22.mixer.dt_proj_b.weight', 'layers.22.mixer.dt_proj_b.bias', 'layers.22.mixer.out_proj.weight', 'layers.22.norm.weight', 'layers.23.mixer.A_log', 'layers.23.mixer.D', 'layers.23.mixer.A_b_log', 'layers.23.mixer.D_b', 'layers.23.mixer.in_proj.weight', 'layers.23.mixer.conv1d.weight', 'layers.23.mixer.conv1d.bias', 'layers.23.mixer.x_proj.weight', 'layers.23.mixer.dt_proj.weight', 'layers.23.mixer.dt_proj.bias', 'layers.23.mixer.conv1d_b.weight', 'layers.23.mixer.conv1d_b.bias', 'layers.23.mixer.x_proj_b.weight', 'layers.23.mixer.dt_proj_b.weight', 'layers.23.mixer.dt_proj_b.bias', 'layers.23.mixer.out_proj.weight', 'layers.23.norm.weight', 'norm_f.weight']
number of params: 33044734
ratio_weight= 2.0 reconstruction_weight 0.02 pruning_weight 0.1
Start training for 100 epochs
Epoch: [0]  [   0/1251]  eta: 10:26:10  lr: 0.000001  loss: 7.5936 (7.5936)  time: 30.0324  data: 13.8836  max mem: 19423
Epoch: [0]  [  10/1251]  eta: 1:06:30  lr: 0.000001  loss: 7.5476 (7.5206)  time: 3.2156  data: 1.2628  max mem: 19733
Epoch: [0]  [  20/1251]  eta: 0:39:04  lr: 0.000001  loss: 7.5249 (7.5285)  time: 0.4984  data: 0.0006  max mem: 19733
Epoch: [0]  [  30/1251]  eta: 0:29:21  lr: 0.000001  loss: 7.5045 (7.5074)  time: 0.4671  data: 0.0004  max mem: 19733
Epoch: [0]  [  40/1251]  eta: 0:24:22  lr: 0.000001  loss: 7.4273 (7.4872)  time: 0.4757  data: 0.0004  max mem: 19733
Epoch: [0]  [  50/1251]  eta: 0:21:14  lr: 0.000001  loss: 7.4001 (7.4616)  time: 0.4708  data: 0.0005  max mem: 19733
Epoch: [0]  [  60/1251]  eta: 0:19:16  lr: 0.000001  loss: 7.3635 (7.4444)  time: 0.4858  data: 0.0005  max mem: 19733
Epoch: [0]  [  70/1251]  eta: 0:17:43  lr: 0.000001  loss: 7.3378 (7.4222)  time: 0.4913  data: 0.0005  max mem: 19733
Epoch: [0]  [  80/1251]  eta: 0:16:31  lr: 0.000001  loss: 7.2540 (7.4006)  time: 0.4691  data: 0.0005  max mem: 19733
Epoch: [0]  [  90/1251]  eta: 0:15:35  lr: 0.000001  loss: 7.2488 (7.3854)  time: 0.4692  data: 0.0005  max mem: 19733
loss info: cls_loss=7.0535, ratio_loss=0.3933, pruning_loss=0.2689, mse_loss=1.2839
Epoch: [0]  [ 100/1251]  eta: 0:14:48  lr: 0.000001  loss: 7.2118 (7.3686)  time: 0.4686  data: 0.0005  max mem: 19733
Epoch: [0]  [ 110/1251]  eta: 0:14:09  lr: 0.000001  loss: 7.1892 (7.3495)  time: 0.4658  data: 0.0005  max mem: 19733
Epoch: [0]  [ 120/1251]  eta: 0:13:36  lr: 0.000001  loss: 7.1556 (7.3285)  time: 0.4672  data: 0.0005  max mem: 19733
Epoch: [0]  [ 130/1251]  eta: 0:13:07  lr: 0.000001  loss: 7.0326 (7.3066)  time: 0.4666  data: 0.0005  max mem: 19733
Epoch: [0]  [ 140/1251]  eta: 0:12:40  lr: 0.000001  loss: 6.9911 (7.2805)  time: 0.4618  data: 0.0005  max mem: 19733
Epoch: [0]  [ 150/1251]  eta: 0:12:17  lr: 0.000001  loss: 6.9091 (7.2525)  time: 0.4563  data: 0.0005  max mem: 19733
Epoch: [0]  [ 160/1251]  eta: 0:11:56  lr: 0.000001  loss: 6.8244 (7.2236)  time: 0.4557  data: 0.0005  max mem: 19733
Epoch: [0]  [ 170/1251]  eta: 0:11:36  lr: 0.000001  loss: 6.7974 (7.1950)  time: 0.4559  data: 0.0006  max mem: 19733
Epoch: [0]  [ 180/1251]  eta: 0:11:20  lr: 0.000001  loss: 6.6988 (7.1645)  time: 0.4669  data: 0.0006  max mem: 19733
Epoch: [0]  [ 190/1251]  eta: 0:11:03  lr: 0.000001  loss: 6.6513 (7.1326)  time: 0.4639  data: 0.0005  max mem: 19733
loss info: cls_loss=6.5214, ratio_loss=0.3879, pruning_loss=0.2685, mse_loss=1.3301
Epoch: [0]  [ 200/1251]  eta: 0:10:49  lr: 0.000001  loss: 6.5423 (7.0987)  time: 0.4605  data: 0.0005  max mem: 19733
Epoch: [0]  [ 210/1251]  eta: 0:10:35  lr: 0.000001  loss: 6.5335 (7.0698)  time: 0.4635  data: 0.0005  max mem: 19733
Epoch: [0]  [ 220/1251]  eta: 0:10:21  lr: 0.000001  loss: 6.5632 (7.0409)  time: 0.4544  data: 0.0005  max mem: 19733
Epoch: [0]  [ 230/1251]  eta: 0:10:09  lr: 0.000001  loss: 6.5632 (7.0108)  time: 0.4522  data: 0.0005  max mem: 19733
Epoch: [0]  [ 240/1251]  eta: 0:09:57  lr: 0.000001  loss: 6.3465 (6.9805)  time: 0.4510  data: 0.0006  max mem: 19733
Epoch: [0]  [ 250/1251]  eta: 0:09:45  lr: 0.000001  loss: 6.3465 (6.9468)  time: 0.4509  data: 0.0005  max mem: 19733
Epoch: [0]  [ 260/1251]  eta: 0:09:34  lr: 0.000001  loss: 6.0341 (6.9131)  time: 0.4504  data: 0.0005  max mem: 19733
Epoch: [0]  [ 270/1251]  eta: 0:09:24  lr: 0.000001  loss: 6.0363 (6.8790)  time: 0.4507  data: 0.0004  max mem: 19733
Epoch: [0]  [ 280/1251]  eta: 0:09:14  lr: 0.000001  loss: 6.3267 (6.8587)  time: 0.4524  data: 0.0004  max mem: 19733
Epoch: [0]  [ 290/1251]  eta: 0:09:04  lr: 0.000001  loss: 6.3079 (6.8214)  time: 0.4549  data: 0.0005  max mem: 19733
loss info: cls_loss=5.8610, ratio_loss=0.3832, pruning_loss=0.2765, mse_loss=1.3229
Epoch: [0]  [ 300/1251]  eta: 0:08:55  lr: 0.000001  loss: 5.6956 (6.7876)  time: 0.4540  data: 0.0008  max mem: 19733
Epoch: [0]  [ 310/1251]  eta: 0:08:46  lr: 0.000001  loss: 5.8156 (6.7645)  time: 0.4512  data: 0.0007  max mem: 19733
Epoch: [0]  [ 320/1251]  eta: 0:08:37  lr: 0.000001  loss: 6.4652 (6.7512)  time: 0.4506  data: 0.0005  max mem: 19733
Epoch: [0]  [ 330/1251]  eta: 0:08:29  lr: 0.000001  loss: 6.3858 (6.7275)  time: 0.4627  data: 0.0005  max mem: 19733
Epoch: [0]  [ 340/1251]  eta: 0:08:22  lr: 0.000001  loss: 6.1022 (6.7084)  time: 0.4741  data: 0.0005  max mem: 19733
Epoch: [0]  [ 350/1251]  eta: 0:08:14  lr: 0.000001  loss: 6.1022 (6.6858)  time: 0.4625  data: 0.0006  max mem: 19733
Epoch: [0]  [ 360/1251]  eta: 0:08:06  lr: 0.000001  loss: 6.0987 (6.6612)  time: 0.4527  data: 0.0005  max mem: 19733
Epoch: [0]  [ 370/1251]  eta: 0:07:58  lr: 0.000001  loss: 5.7796 (6.6379)  time: 0.4527  data: 0.0004  max mem: 19733
Epoch: [0]  [ 380/1251]  eta: 0:07:51  lr: 0.000001  loss: 5.7618 (6.6128)  time: 0.4506  data: 0.0004  max mem: 19733
Epoch: [0]  [ 390/1251]  eta: 0:07:43  lr: 0.000001  loss: 5.6085 (6.5860)  time: 0.4505  data: 0.0005  max mem: 19733
loss info: cls_loss=5.6101, ratio_loss=0.3795, pruning_loss=0.2737, mse_loss=1.2795
Epoch: [0]  [ 400/1251]  eta: 0:07:36  lr: 0.000001  loss: 5.9190 (6.5711)  time: 0.4516  data: 0.0005  max mem: 19733
Epoch: [0]  [ 410/1251]  eta: 0:07:29  lr: 0.000001  loss: 6.0667 (6.5585)  time: 0.4532  data: 0.0005  max mem: 19733
Epoch: [0]  [ 420/1251]  eta: 0:07:22  lr: 0.000001  loss: 6.0663 (6.5375)  time: 0.4536  data: 0.0005  max mem: 19733
Epoch: [0]  [ 430/1251]  eta: 0:07:15  lr: 0.000001  loss: 6.0663 (6.5244)  time: 0.4515  data: 0.0005  max mem: 19733
Epoch: [0]  [ 440/1251]  eta: 0:07:08  lr: 0.000001  loss: 6.0219 (6.5099)  time: 0.4533  data: 0.0005  max mem: 19733
Epoch: [0]  [ 450/1251]  eta: 0:07:02  lr: 0.000001  loss: 5.7500 (6.4897)  time: 0.4530  data: 0.0005  max mem: 19733
Epoch: [0]  [ 460/1251]  eta: 0:06:55  lr: 0.000001  loss: 5.4845 (6.4666)  time: 0.4498  data: 0.0005  max mem: 19733
Epoch: [0]  [ 470/1251]  eta: 0:06:49  lr: 0.000001  loss: 5.1968 (6.4386)  time: 0.4515  data: 0.0005  max mem: 19733
Epoch: [0]  [ 480/1251]  eta: 0:06:43  lr: 0.000001  loss: 5.4087 (6.4200)  time: 0.4632  data: 0.0005  max mem: 19733
Epoch: [0]  [ 490/1251]  eta: 0:06:37  lr: 0.000001  loss: 5.6536 (6.4101)  time: 0.4727  data: 0.0005  max mem: 19733
loss info: cls_loss=5.3733, ratio_loss=0.3741, pruning_loss=0.2778, mse_loss=1.2514
Epoch: [0]  [ 500/1251]  eta: 0:06:30  lr: 0.000001  loss: 5.6536 (6.3928)  time: 0.4624  data: 0.0004  max mem: 19733
Epoch: [0]  [ 510/1251]  eta: 0:06:24  lr: 0.000001  loss: 5.6758 (6.3828)  time: 0.4529  data: 0.0005  max mem: 19733
Epoch: [0]  [ 520/1251]  eta: 0:06:18  lr: 0.000001  loss: 5.8944 (6.3685)  time: 0.4536  data: 0.0005  max mem: 19733
Epoch: [0]  [ 530/1251]  eta: 0:06:12  lr: 0.000001  loss: 5.8305 (6.3479)  time: 0.4534  data: 0.0005  max mem: 19733
Epoch: [0]  [ 540/1251]  eta: 0:06:06  lr: 0.000001  loss: 5.8305 (6.3355)  time: 0.4520  data: 0.0005  max mem: 19733
Epoch: [0]  [ 550/1251]  eta: 0:06:00  lr: 0.000001  loss: 5.6917 (6.3205)  time: 0.4510  data: 0.0005  max mem: 19733
Epoch: [0]  [ 560/1251]  eta: 0:05:54  lr: 0.000001  loss: 5.8463 (6.3119)  time: 0.4501  data: 0.0005  max mem: 19733
Epoch: [0]  [ 570/1251]  eta: 0:05:48  lr: 0.000001  loss: 5.9815 (6.3027)  time: 0.4512  data: 0.0005  max mem: 19733
Epoch: [0]  [ 580/1251]  eta: 0:05:42  lr: 0.000001  loss: 5.8768 (6.2903)  time: 0.4521  data: 0.0005  max mem: 19733
Epoch: [0]  [ 590/1251]  eta: 0:05:37  lr: 0.000001  loss: 5.9055 (6.2802)  time: 0.4541  data: 0.0007  max mem: 19733
loss info: cls_loss=5.3515, ratio_loss=0.3711, pruning_loss=0.2784, mse_loss=1.2695
Epoch: [0]  [ 600/1251]  eta: 0:05:31  lr: 0.000001  loss: 5.8464 (6.2658)  time: 0.4559  data: 0.0007  max mem: 19733
Epoch: [0]  [ 610/1251]  eta: 0:05:25  lr: 0.000001  loss: 5.6004 (6.2542)  time: 0.4541  data: 0.0005  max mem: 19733
Epoch: [0]  [ 620/1251]  eta: 0:05:20  lr: 0.000001  loss: 5.6224 (6.2448)  time: 0.4518  data: 0.0005  max mem: 19733
Epoch: [0]  [ 630/1251]  eta: 0:05:14  lr: 0.000001  loss: 5.8195 (6.2338)  time: 0.4594  data: 0.0005  max mem: 19733
Epoch: [0]  [ 640/1251]  eta: 0:05:09  lr: 0.000001  loss: 5.2689 (6.2185)  time: 0.4709  data: 0.0004  max mem: 19733
Epoch: [0]  [ 650/1251]  eta: 0:05:03  lr: 0.000001  loss: 5.3946 (6.2077)  time: 0.4633  data: 0.0005  max mem: 19733
Epoch: [0]  [ 660/1251]  eta: 0:04:58  lr: 0.000001  loss: 5.2837 (6.1891)  time: 0.4536  data: 0.0005  max mem: 19733
Epoch: [0]  [ 670/1251]  eta: 0:04:52  lr: 0.000001  loss: 5.6315 (6.1831)  time: 0.4547  data: 0.0005  max mem: 19733
Epoch: [0]  [ 680/1251]  eta: 0:04:47  lr: 0.000001  loss: 5.7311 (6.1728)  time: 0.4525  data: 0.0005  max mem: 19733
Epoch: [0]  [ 690/1251]  eta: 0:04:41  lr: 0.000001  loss: 5.1635 (6.1577)  time: 0.4492  data: 0.0005  max mem: 19733
loss info: cls_loss=5.1034, ratio_loss=0.3675, pruning_loss=0.2841, mse_loss=1.2384
Epoch: [0]  [ 700/1251]  eta: 0:04:36  lr: 0.000001  loss: 5.1635 (6.1454)  time: 0.4491  data: 0.0005  max mem: 19733
Epoch: [0]  [ 710/1251]  eta: 0:04:30  lr: 0.000001  loss: 5.8228 (6.1333)  time: 0.4499  data: 0.0005  max mem: 19733
Epoch: [0]  [ 720/1251]  eta: 0:04:25  lr: 0.000001  loss: 5.6347 (6.1195)  time: 0.4523  data: 0.0005  max mem: 19733
Epoch: [0]  [ 730/1251]  eta: 0:04:20  lr: 0.000001  loss: 5.3304 (6.1100)  time: 0.4530  data: 0.0005  max mem: 19733
Epoch: [0]  [ 740/1251]  eta: 0:04:14  lr: 0.000001  loss: 5.6659 (6.1066)  time: 0.4524  data: 0.0005  max mem: 19733
Epoch: [0]  [ 750/1251]  eta: 0:04:09  lr: 0.000001  loss: 5.7225 (6.0943)  time: 0.4548  data: 0.0004  max mem: 19733
Epoch: [0]  [ 760/1251]  eta: 0:04:04  lr: 0.000001  loss: 5.7225 (6.0864)  time: 0.4539  data: 0.0004  max mem: 19733
Epoch: [0]  [ 770/1251]  eta: 0:03:59  lr: 0.000001  loss: 5.7277 (6.0774)  time: 0.4504  data: 0.0004  max mem: 19733
Epoch: [0]  [ 780/1251]  eta: 0:03:54  lr: 0.000001  loss: 5.5105 (6.0692)  time: 0.4822  data: 0.0004  max mem: 19733
Epoch: [0]  [ 790/1251]  eta: 0:03:48  lr: 0.000001  loss: 5.2782 (6.0590)  time: 0.4825  data: 0.0004  max mem: 19733
loss info: cls_loss=5.0962, ratio_loss=0.3646, pruning_loss=0.2838, mse_loss=1.2739
Epoch: [0]  [ 800/1251]  eta: 0:03:43  lr: 0.000001  loss: 5.3448 (6.0512)  time: 0.4505  data: 0.0005  max mem: 19733
Epoch: [0]  [ 810/1251]  eta: 0:03:38  lr: 0.000001  loss: 4.9762 (6.0342)  time: 0.4499  data: 0.0007  max mem: 19733
Epoch: [0]  [ 820/1251]  eta: 0:03:33  lr: 0.000001  loss: 4.9970 (6.0283)  time: 0.4504  data: 0.0006  max mem: 19733
Epoch: [0]  [ 830/1251]  eta: 0:03:28  lr: 0.000001  loss: 5.5993 (6.0152)  time: 0.4506  data: 0.0005  max mem: 19733
Epoch: [0]  [ 840/1251]  eta: 0:03:22  lr: 0.000001  loss: 4.8132 (6.0050)  time: 0.4498  data: 0.0005  max mem: 19733
Epoch: [0]  [ 850/1251]  eta: 0:03:17  lr: 0.000001  loss: 5.1757 (5.9949)  time: 0.4511  data: 0.0005  max mem: 19733
Epoch: [0]  [ 860/1251]  eta: 0:03:12  lr: 0.000001  loss: 5.5582 (5.9867)  time: 0.4515  data: 0.0006  max mem: 19733
Epoch: [0]  [ 870/1251]  eta: 0:03:07  lr: 0.000001  loss: 5.2628 (5.9801)  time: 0.4498  data: 0.0006  max mem: 19733
Epoch: [0]  [ 880/1251]  eta: 0:03:02  lr: 0.000001  loss: 5.2294 (5.9682)  time: 0.4502  data: 0.0006  max mem: 19733
Epoch: [0]  [ 890/1251]  eta: 0:02:57  lr: 0.000001  loss: 4.9940 (5.9579)  time: 0.4504  data: 0.0006  max mem: 19733
loss info: cls_loss=4.8563, ratio_loss=0.3606, pruning_loss=0.2898, mse_loss=1.2446
Epoch: [0]  [ 900/1251]  eta: 0:02:52  lr: 0.000001  loss: 5.3637 (5.9513)  time: 0.4522  data: 0.0006  max mem: 19733
Epoch: [0]  [ 910/1251]  eta: 0:02:47  lr: 0.000001  loss: 5.6232 (5.9450)  time: 0.4524  data: 0.0006  max mem: 19733
Epoch: [0]  [ 920/1251]  eta: 0:02:42  lr: 0.000001  loss: 5.6232 (5.9395)  time: 0.4730  data: 0.0005  max mem: 19733
Epoch: [0]  [ 930/1251]  eta: 0:02:37  lr: 0.000001  loss: 5.6142 (5.9367)  time: 0.4847  data: 0.0005  max mem: 19733
Epoch: [0]  [ 940/1251]  eta: 0:02:32  lr: 0.000001  loss: 5.5305 (5.9280)  time: 0.4611  data: 0.0005  max mem: 19733
Epoch: [0]  [ 950/1251]  eta: 0:02:27  lr: 0.000001  loss: 4.7110 (5.9135)  time: 0.4509  data: 0.0005  max mem: 19733
Epoch: [0]  [ 960/1251]  eta: 0:02:22  lr: 0.000001  loss: 5.0084 (5.9078)  time: 0.4527  data: 0.0005  max mem: 19733
Epoch: [0]  [ 970/1251]  eta: 0:02:17  lr: 0.000001  loss: 5.6474 (5.9028)  time: 0.4533  data: 0.0005  max mem: 19733
Epoch: [0]  [ 980/1251]  eta: 0:02:12  lr: 0.000001  loss: 5.5088 (5.8965)  time: 0.4532  data: 0.0006  max mem: 19733
Epoch: [0]  [ 990/1251]  eta: 0:02:07  lr: 0.000001  loss: 5.6282 (5.8904)  time: 0.4522  data: 0.0006  max mem: 19733
loss info: cls_loss=4.9907, ratio_loss=0.3569, pruning_loss=0.2863, mse_loss=1.2732
Epoch: [0]  [1000/1251]  eta: 0:02:02  lr: 0.000001  loss: 5.4907 (5.8835)  time: 0.4511  data: 0.0006  max mem: 19733
Epoch: [0]  [1010/1251]  eta: 0:01:57  lr: 0.000001  loss: 5.4260 (5.8794)  time: 0.4496  data: 0.0005  max mem: 19733
Epoch: [0]  [1020/1251]  eta: 0:01:52  lr: 0.000001  loss: 5.3788 (5.8718)  time: 0.4511  data: 0.0005  max mem: 19733
Epoch: [0]  [1030/1251]  eta: 0:01:47  lr: 0.000001  loss: 5.1094 (5.8614)  time: 0.4522  data: 0.0005  max mem: 19733
Epoch: [0]  [1040/1251]  eta: 0:01:42  lr: 0.000001  loss: 5.1094 (5.8573)  time: 0.4495  data: 0.0005  max mem: 19733
Epoch: [0]  [1050/1251]  eta: 0:01:37  lr: 0.000001  loss: 5.5475 (5.8505)  time: 0.4509  data: 0.0005  max mem: 19733
Epoch: [0]  [1060/1251]  eta: 0:01:32  lr: 0.000001  loss: 5.4021 (5.8466)  time: 0.4525  data: 0.0005  max mem: 19733
Epoch: [0]  [1070/1251]  eta: 0:01:27  lr: 0.000001  loss: 5.3638 (5.8396)  time: 0.4701  data: 0.0005  max mem: 19733
Epoch: [0]  [1080/1251]  eta: 0:01:23  lr: 0.000001  loss: 5.0207 (5.8335)  time: 0.4772  data: 0.0005  max mem: 19733
Epoch: [0]  [1090/1251]  eta: 0:01:18  lr: 0.000001  loss: 5.3698 (5.8274)  time: 0.4567  data: 0.0005  max mem: 19733
loss info: cls_loss=4.8993, ratio_loss=0.3528, pruning_loss=0.2875, mse_loss=1.2489
Epoch: [0]  [1100/1251]  eta: 0:01:13  lr: 0.000001  loss: 5.3698 (5.8211)  time: 0.4496  data: 0.0006  max mem: 19733
Epoch: [0]  [1110/1251]  eta: 0:01:08  lr: 0.000001  loss: 5.4169 (5.8163)  time: 0.4519  data: 0.0006  max mem: 19733
Epoch: [0]  [1120/1251]  eta: 0:01:03  lr: 0.000001  loss: 5.5776 (5.8130)  time: 0.4519  data: 0.0006  max mem: 19733
Epoch: [0]  [1130/1251]  eta: 0:00:58  lr: 0.000001  loss: 5.4847 (5.8091)  time: 0.4516  data: 0.0005  max mem: 19733
Epoch: [0]  [1140/1251]  eta: 0:00:53  lr: 0.000001  loss: 5.2769 (5.8017)  time: 0.4502  data: 0.0005  max mem: 19733
Epoch: [0]  [1150/1251]  eta: 0:00:48  lr: 0.000001  loss: 5.1307 (5.7966)  time: 0.4491  data: 0.0005  max mem: 19733
Epoch: [0]  [1160/1251]  eta: 0:00:43  lr: 0.000001  loss: 5.4157 (5.7909)  time: 0.4498  data: 0.0006  max mem: 19733
Epoch: [0]  [1170/1251]  eta: 0:00:39  lr: 0.000001  loss: 5.4157 (5.7860)  time: 0.4493  data: 0.0007  max mem: 19733
Epoch: [0]  [1180/1251]  eta: 0:00:34  lr: 0.000001  loss: 5.5913 (5.7841)  time: 0.4497  data: 0.0006  max mem: 19733
Epoch: [0]  [1190/1251]  eta: 0:00:29  lr: 0.000001  loss: 5.5913 (5.7809)  time: 0.4496  data: 0.0007  max mem: 19733
loss info: cls_loss=5.0164, ratio_loss=0.3489, pruning_loss=0.2824, mse_loss=1.2310
Epoch: [0]  [1200/1251]  eta: 0:00:24  lr: 0.000001  loss: 5.4397 (5.7775)  time: 0.4476  data: 0.0005  max mem: 19733
Epoch: [0]  [1210/1251]  eta: 0:00:19  lr: 0.000001  loss: 5.3402 (5.7717)  time: 0.4619  data: 0.0001  max mem: 19733
Epoch: [0]  [1220/1251]  eta: 0:00:14  lr: 0.000001  loss: 5.1045 (5.7657)  time: 0.4594  data: 0.0002  max mem: 19733
Epoch: [0]  [1230/1251]  eta: 0:00:10  lr: 0.000001  loss: 5.1045 (5.7606)  time: 0.4482  data: 0.0002  max mem: 19733
Epoch: [0]  [1240/1251]  eta: 0:00:05  lr: 0.000001  loss: 5.1398 (5.7546)  time: 0.4483  data: 0.0002  max mem: 19733
Epoch: [0]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 5.0013 (5.7474)  time: 0.4420  data: 0.0002  max mem: 19733
Epoch: [0] Total time: 0:10:02 (0.4814 s / it)
Averaged stats: lr: 0.000001  loss: 5.0013 (5.7493)
Test:  [  0/261]  eta: 2:42:27  loss: 1.5858 (1.5858)  acc1: 67.7083 (67.7083)  acc5: 88.5417 (88.5417)  time: 37.3452  data: 34.4905  max mem: 19733
Test:  [ 10/261]  eta: 0:14:59  loss: 1.5165 (1.5195)  acc1: 71.8750 (70.2178)  acc5: 89.5833 (89.2519)  time: 3.5823  data: 3.1488  max mem: 19733
Test:  [ 20/261]  eta: 0:08:04  loss: 1.6198 (1.6506)  acc1: 63.5417 (66.1210)  acc5: 88.0208 (87.7728)  time: 0.2420  data: 0.0229  max mem: 19733
Test:  [ 30/261]  eta: 0:05:30  loss: 1.6307 (1.5737)  acc1: 67.7083 (68.6324)  acc5: 87.5000 (88.2561)  time: 0.2494  data: 0.0271  max mem: 19733
Test:  [ 40/261]  eta: 0:04:36  loss: 1.4268 (1.5803)  acc1: 72.9167 (68.2546)  acc5: 89.0625 (88.2368)  time: 0.4576  data: 0.2676  max mem: 19733
Test:  [ 50/261]  eta: 0:03:39  loss: 1.7651 (1.6575)  acc1: 59.8958 (65.7067)  acc5: 85.4167 (87.1630)  time: 0.4353  data: 0.2618  max mem: 19733
Test:  [ 60/261]  eta: 0:03:00  loss: 1.9211 (1.6916)  acc1: 56.7708 (64.5406)  acc5: 84.3750 (86.8852)  time: 0.1778  data: 0.0100  max mem: 19733
Test:  [ 70/261]  eta: 0:02:34  loss: 1.9156 (1.7035)  acc1: 56.7708 (63.7397)  acc5: 85.9375 (86.9865)  time: 0.2179  data: 0.0183  max mem: 19733
Test:  [ 80/261]  eta: 0:02:12  loss: 1.6319 (1.6804)  acc1: 63.0208 (64.3711)  acc5: 88.5417 (87.3907)  time: 0.2239  data: 0.0227  max mem: 19733
Test:  [ 90/261]  eta: 0:01:56  loss: 1.5037 (1.6474)  acc1: 69.2708 (65.0298)  acc5: 89.5833 (87.8777)  time: 0.2372  data: 0.0662  max mem: 19733
Test:  [100/261]  eta: 0:01:50  loss: 1.4047 (1.6369)  acc1: 70.3125 (65.1764)  acc5: 89.0625 (87.9125)  time: 0.4869  data: 0.3364  max mem: 19733
Test:  [110/261]  eta: 0:01:36  loss: 1.5081 (1.6521)  acc1: 67.7083 (64.9399)  acc5: 87.5000 (87.5563)  time: 0.4248  data: 0.2837  max mem: 19733
Test:  [120/261]  eta: 0:01:27  loss: 1.9440 (1.6843)  acc1: 60.9375 (64.3423)  acc5: 81.2500 (86.9361)  time: 0.3154  data: 0.1850  max mem: 19733
Test:  [130/261]  eta: 0:01:21  loss: 2.0199 (1.7237)  acc1: 55.7292 (63.6609)  acc5: 79.6875 (86.2556)  time: 0.5418  data: 0.4157  max mem: 19733
Test:  [140/261]  eta: 0:01:11  loss: 2.0589 (1.7416)  acc1: 54.6875 (63.1760)  acc5: 81.2500 (85.9597)  time: 0.3774  data: 0.2412  max mem: 19733
Test:  [150/261]  eta: 0:01:01  loss: 1.8246 (1.7423)  acc1: 61.9792 (63.3140)  acc5: 82.2917 (85.7823)  time: 0.1328  data: 0.0184  max mem: 19733
Test:  [160/261]  eta: 0:00:53  loss: 1.7645 (1.7660)  acc1: 64.5833 (62.9885)  acc5: 82.8125 (85.2679)  time: 0.1041  data: 0.0177  max mem: 19733
Test:  [170/261]  eta: 0:00:45  loss: 2.1627 (1.7940)  acc1: 53.1250 (62.4391)  acc5: 77.6042 (84.8623)  time: 0.0955  data: 0.0267  max mem: 19733
Test:  [180/261]  eta: 0:00:38  loss: 2.0747 (1.8092)  acc1: 52.6042 (62.1317)  acc5: 78.6458 (84.6081)  time: 0.0918  data: 0.0288  max mem: 19733
Test:  [190/261]  eta: 0:00:32  loss: 2.0511 (1.8241)  acc1: 56.2500 (61.8374)  acc5: 80.2083 (84.3695)  time: 0.0700  data: 0.0074  max mem: 19733
Test:  [200/261]  eta: 0:00:26  loss: 2.0474 (1.8333)  acc1: 57.2917 (61.6760)  acc5: 80.7292 (84.1884)  time: 0.0680  data: 0.0022  max mem: 19733
Test:  [210/261]  eta: 0:00:21  loss: 1.9604 (1.8404)  acc1: 59.3750 (61.5818)  acc5: 80.7292 (84.0541)  time: 0.0789  data: 0.0122  max mem: 19733
Test:  [220/261]  eta: 0:00:16  loss: 2.1191 (1.8661)  acc1: 56.7708 (61.0294)  acc5: 79.6875 (83.6444)  time: 0.0727  data: 0.0103  max mem: 19733
Test:  [230/261]  eta: 0:00:12  loss: 2.1590 (1.8761)  acc1: 52.0833 (60.7707)  acc5: 77.6042 (83.4483)  time: 0.0615  data: 0.0002  max mem: 19733
Test:  [240/261]  eta: 0:00:07  loss: 1.8962 (1.8763)  acc1: 57.2917 (60.6522)  acc5: 81.7708 (83.4587)  time: 0.0616  data: 0.0002  max mem: 19733
Test:  [250/261]  eta: 0:00:04  loss: 1.7931 (1.8602)  acc1: 61.9792 (60.9832)  acc5: 85.4167 (83.6923)  time: 0.0616  data: 0.0002  max mem: 19733
Test:  [260/261]  eta: 0:00:00  loss: 1.5019 (1.8505)  acc1: 66.6667 (61.2280)  acc5: 90.6250 (83.8920)  time: 0.0601  data: 0.0002  max mem: 19733
Test: Total time: 0:01:32 (0.3546 s / it)
* Acc@1 61.228 Acc@5 83.892 loss 1.851
Accuracy of the network on the 50000 test images: 61.2%
Max accuracy: 61.23%
Epoch: [1]  [   0/1251]  eta: 5:23:59  lr: 0.000001  loss: 5.1448 (5.1448)  time: 15.5393  data: 15.0007  max mem: 19734
Epoch: [1]  [  10/1251]  eta: 0:40:03  lr: 0.000001  loss: 5.4296 (5.2748)  time: 1.9364  data: 1.3723  max mem: 19734
Epoch: [1]  [  20/1251]  eta: 0:25:14  lr: 0.000001  loss: 5.3716 (5.2392)  time: 0.5147  data: 0.0050  max mem: 19734
Epoch: [1]  [  30/1251]  eta: 0:19:56  lr: 0.000001  loss: 5.5572 (5.3335)  time: 0.4533  data: 0.0007  max mem: 19734
Epoch: [1]  [  40/1251]  eta: 0:17:11  lr: 0.000001  loss: 5.5283 (5.2963)  time: 0.4538  data: 0.0007  max mem: 19734
loss info: cls_loss=4.8749, ratio_loss=0.3441, pruning_loss=0.2830, mse_loss=1.2264
Epoch: [1]  [  50/1251]  eta: 0:15:29  lr: 0.000001  loss: 5.3671 (5.2808)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [1]  [  60/1251]  eta: 0:14:22  lr: 0.000001  loss: 5.1816 (5.2176)  time: 0.4630  data: 0.0005  max mem: 19734
Epoch: [1]  [  70/1251]  eta: 0:13:30  lr: 0.000001  loss: 4.6739 (5.1197)  time: 0.4638  data: 0.0005  max mem: 19734
Epoch: [1]  [  80/1251]  eta: 0:12:50  lr: 0.000001  loss: 4.9502 (5.1450)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [1]  [  90/1251]  eta: 0:12:21  lr: 0.000001  loss: 5.5345 (5.1454)  time: 0.4677  data: 0.0005  max mem: 19734
Epoch: [1]  [ 100/1251]  eta: 0:11:56  lr: 0.000001  loss: 5.2161 (5.1595)  time: 0.4784  data: 0.0004  max mem: 19734
Epoch: [1]  [ 110/1251]  eta: 0:11:32  lr: 0.000001  loss: 5.4027 (5.1732)  time: 0.4654  data: 0.0003  max mem: 19734
Epoch: [1]  [ 120/1251]  eta: 0:11:11  lr: 0.000001  loss: 5.4252 (5.1842)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [1]  [ 130/1251]  eta: 0:10:54  lr: 0.000001  loss: 5.4252 (5.1900)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [1]  [ 140/1251]  eta: 0:10:38  lr: 0.000001  loss: 5.3754 (5.1736)  time: 0.4555  data: 0.0006  max mem: 19734
loss info: cls_loss=4.8362, ratio_loss=0.3389, pruning_loss=0.2821, mse_loss=1.2499
Epoch: [1]  [ 150/1251]  eta: 0:10:23  lr: 0.000001  loss: 5.2160 (5.1756)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [1]  [ 160/1251]  eta: 0:10:10  lr: 0.000001  loss: 5.2350 (5.1818)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [1]  [ 170/1251]  eta: 0:09:58  lr: 0.000001  loss: 5.2340 (5.1787)  time: 0.4552  data: 0.0006  max mem: 19734
Epoch: [1]  [ 180/1251]  eta: 0:09:46  lr: 0.000001  loss: 5.1882 (5.1691)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [1]  [ 190/1251]  eta: 0:09:35  lr: 0.000001  loss: 5.0463 (5.1574)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [1]  [ 200/1251]  eta: 0:09:26  lr: 0.000001  loss: 5.2427 (5.1523)  time: 0.4640  data: 0.0007  max mem: 19734
Epoch: [1]  [ 210/1251]  eta: 0:09:17  lr: 0.000001  loss: 5.4127 (5.1581)  time: 0.4635  data: 0.0007  max mem: 19734
Epoch: [1]  [ 220/1251]  eta: 0:09:07  lr: 0.000001  loss: 5.2932 (5.1480)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [1]  [ 230/1251]  eta: 0:08:59  lr: 0.000001  loss: 4.6366 (5.1232)  time: 0.4515  data: 0.0004  max mem: 19734
Epoch: [1]  [ 240/1251]  eta: 0:08:52  lr: 0.000001  loss: 4.5491 (5.1007)  time: 0.4677  data: 0.0004  max mem: 19734
loss info: cls_loss=4.6929, ratio_loss=0.3338, pruning_loss=0.2841, mse_loss=1.2198
Epoch: [1]  [ 250/1251]  eta: 0:08:43  lr: 0.000001  loss: 4.7000 (5.0894)  time: 0.4675  data: 0.0006  max mem: 19734
Epoch: [1]  [ 260/1251]  eta: 0:08:35  lr: 0.000001  loss: 5.1077 (5.1000)  time: 0.4514  data: 0.0006  max mem: 19734
Epoch: [1]  [ 270/1251]  eta: 0:08:28  lr: 0.000001  loss: 5.3707 (5.1042)  time: 0.4506  data: 0.0005  max mem: 19734
Epoch: [1]  [ 280/1251]  eta: 0:08:20  lr: 0.000001  loss: 5.2381 (5.0975)  time: 0.4514  data: 0.0005  max mem: 19734
Epoch: [1]  [ 290/1251]  eta: 0:08:13  lr: 0.000001  loss: 5.3304 (5.0979)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [1]  [ 300/1251]  eta: 0:08:06  lr: 0.000001  loss: 4.9937 (5.0820)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [1]  [ 310/1251]  eta: 0:07:59  lr: 0.000001  loss: 4.9937 (5.0749)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [1]  [ 320/1251]  eta: 0:07:52  lr: 0.000001  loss: 4.8215 (5.0662)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [1]  [ 330/1251]  eta: 0:07:46  lr: 0.000001  loss: 4.8215 (5.0590)  time: 0.4511  data: 0.0006  max mem: 19734
Epoch: [1]  [ 340/1251]  eta: 0:07:39  lr: 0.000001  loss: 5.2144 (5.0639)  time: 0.4505  data: 0.0006  max mem: 19734
loss info: cls_loss=4.7496, ratio_loss=0.3284, pruning_loss=0.2838, mse_loss=1.2407
Epoch: [1]  [ 350/1251]  eta: 0:07:33  lr: 0.000001  loss: 5.3334 (5.0719)  time: 0.4631  data: 0.0005  max mem: 19734
Epoch: [1]  [ 360/1251]  eta: 0:07:27  lr: 0.000001  loss: 5.3645 (5.0731)  time: 0.4647  data: 0.0005  max mem: 19734
Epoch: [1]  [ 370/1251]  eta: 0:07:21  lr: 0.000001  loss: 5.0753 (5.0672)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [1]  [ 380/1251]  eta: 0:07:15  lr: 0.000001  loss: 5.1123 (5.0694)  time: 0.4615  data: 0.0005  max mem: 19734
Epoch: [1]  [ 390/1251]  eta: 0:07:09  lr: 0.000001  loss: 5.1306 (5.0667)  time: 0.4705  data: 0.0005  max mem: 19734
Epoch: [1]  [ 400/1251]  eta: 0:07:03  lr: 0.000001  loss: 4.8398 (5.0460)  time: 0.4628  data: 0.0005  max mem: 19734
Epoch: [1]  [ 410/1251]  eta: 0:06:58  lr: 0.000001  loss: 4.3649 (5.0345)  time: 0.4516  data: 0.0005  max mem: 19734
Epoch: [1]  [ 420/1251]  eta: 0:06:52  lr: 0.000001  loss: 4.5770 (5.0288)  time: 0.4510  data: 0.0005  max mem: 19734
Epoch: [1]  [ 430/1251]  eta: 0:06:46  lr: 0.000001  loss: 5.2340 (5.0285)  time: 0.4509  data: 0.0004  max mem: 19734
Epoch: [1]  [ 440/1251]  eta: 0:06:40  lr: 0.000001  loss: 5.3548 (5.0307)  time: 0.4509  data: 0.0005  max mem: 19734
loss info: cls_loss=4.6380, ratio_loss=0.3242, pruning_loss=0.2831, mse_loss=1.2124
Epoch: [1]  [ 450/1251]  eta: 0:06:34  lr: 0.000001  loss: 5.3548 (5.0362)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [1]  [ 460/1251]  eta: 0:06:29  lr: 0.000001  loss: 5.1809 (5.0308)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [1]  [ 470/1251]  eta: 0:06:23  lr: 0.000001  loss: 4.7549 (5.0258)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [1]  [ 480/1251]  eta: 0:06:18  lr: 0.000001  loss: 5.3811 (5.0312)  time: 0.4525  data: 0.0006  max mem: 19734
Epoch: [1]  [ 490/1251]  eta: 0:06:12  lr: 0.000001  loss: 5.3748 (5.0262)  time: 0.4524  data: 0.0007  max mem: 19734
Epoch: [1]  [ 500/1251]  eta: 0:06:07  lr: 0.000001  loss: 4.5528 (5.0190)  time: 0.4589  data: 0.0005  max mem: 19734
Epoch: [1]  [ 510/1251]  eta: 0:06:02  lr: 0.000001  loss: 4.7253 (5.0167)  time: 0.4593  data: 0.0005  max mem: 19734
Epoch: [1]  [ 520/1251]  eta: 0:05:56  lr: 0.000001  loss: 4.9750 (5.0154)  time: 0.4514  data: 0.0009  max mem: 19734
Epoch: [1]  [ 530/1251]  eta: 0:05:51  lr: 0.000001  loss: 5.2895 (5.0185)  time: 0.4732  data: 0.0008  max mem: 19734
Epoch: [1]  [ 540/1251]  eta: 0:05:46  lr: 0.000001  loss: 5.1384 (5.0145)  time: 0.4741  data: 0.0005  max mem: 19734
loss info: cls_loss=4.6808, ratio_loss=0.3173, pruning_loss=0.2819, mse_loss=1.2313
Epoch: [1]  [ 550/1251]  eta: 0:05:41  lr: 0.000001  loss: 5.0615 (5.0170)  time: 0.4526  data: 0.0006  max mem: 19734
Epoch: [1]  [ 560/1251]  eta: 0:05:35  lr: 0.000001  loss: 5.0714 (5.0138)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [1]  [ 570/1251]  eta: 0:05:30  lr: 0.000001  loss: 4.8140 (5.0113)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [1]  [ 580/1251]  eta: 0:05:25  lr: 0.000001  loss: 5.2167 (5.0154)  time: 0.4504  data: 0.0006  max mem: 19734
Epoch: [1]  [ 590/1251]  eta: 0:05:20  lr: 0.000001  loss: 5.1938 (5.0105)  time: 0.4498  data: 0.0005  max mem: 19734
Epoch: [1]  [ 600/1251]  eta: 0:05:14  lr: 0.000001  loss: 4.7226 (5.0043)  time: 0.4517  data: 0.0005  max mem: 19734
Epoch: [1]  [ 610/1251]  eta: 0:05:09  lr: 0.000001  loss: 4.7248 (5.0028)  time: 0.4522  data: 0.0005  max mem: 19734
Epoch: [1]  [ 620/1251]  eta: 0:05:04  lr: 0.000001  loss: 4.7248 (4.9957)  time: 0.4504  data: 0.0005  max mem: 19734
Epoch: [1]  [ 630/1251]  eta: 0:04:59  lr: 0.000001  loss: 5.1571 (5.0011)  time: 0.4495  data: 0.0005  max mem: 19734
Epoch: [1]  [ 640/1251]  eta: 0:04:54  lr: 0.000001  loss: 5.1339 (4.9928)  time: 0.4503  data: 0.0005  max mem: 19734
loss info: cls_loss=4.5752, ratio_loss=0.3101, pruning_loss=0.2813, mse_loss=1.2338
Epoch: [1]  [ 650/1251]  eta: 0:04:49  lr: 0.000001  loss: 4.7462 (4.9906)  time: 0.4643  data: 0.0005  max mem: 19734
Epoch: [1]  [ 660/1251]  eta: 0:04:44  lr: 0.000001  loss: 5.0321 (4.9910)  time: 0.4644  data: 0.0005  max mem: 19734
Epoch: [1]  [ 670/1251]  eta: 0:04:39  lr: 0.000001  loss: 4.6911 (4.9811)  time: 0.4772  data: 0.0006  max mem: 19734
Epoch: [1]  [ 680/1251]  eta: 0:04:34  lr: 0.000001  loss: 4.4720 (4.9757)  time: 0.4778  data: 0.0005  max mem: 19734
Epoch: [1]  [ 690/1251]  eta: 0:04:29  lr: 0.000001  loss: 4.6190 (4.9720)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [1]  [ 700/1251]  eta: 0:04:24  lr: 0.000001  loss: 4.8199 (4.9674)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [1]  [ 710/1251]  eta: 0:04:19  lr: 0.000001  loss: 4.8928 (4.9640)  time: 0.4509  data: 0.0005  max mem: 19734
Epoch: [1]  [ 720/1251]  eta: 0:04:14  lr: 0.000001  loss: 4.7527 (4.9569)  time: 0.4497  data: 0.0005  max mem: 19734
Epoch: [1]  [ 730/1251]  eta: 0:04:09  lr: 0.000001  loss: 4.3945 (4.9521)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [1]  [ 740/1251]  eta: 0:04:04  lr: 0.000001  loss: 4.7625 (4.9496)  time: 0.4531  data: 0.0004  max mem: 19734
loss info: cls_loss=4.4076, ratio_loss=0.3042, pruning_loss=0.2905, mse_loss=1.2453
Epoch: [1]  [ 750/1251]  eta: 0:03:59  lr: 0.000001  loss: 4.8800 (4.9456)  time: 0.4545  data: 0.0005  max mem: 19734
Epoch: [1]  [ 760/1251]  eta: 0:03:54  lr: 0.000001  loss: 5.0219 (4.9504)  time: 0.4567  data: 0.0005  max mem: 19734
Epoch: [1]  [ 770/1251]  eta: 0:03:49  lr: 0.000001  loss: 5.1594 (4.9478)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [1]  [ 780/1251]  eta: 0:03:44  lr: 0.000001  loss: 5.1803 (4.9493)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [1]  [ 790/1251]  eta: 0:03:39  lr: 0.000001  loss: 5.2534 (4.9483)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [1]  [ 800/1251]  eta: 0:03:35  lr: 0.000001  loss: 4.8154 (4.9453)  time: 0.4607  data: 0.0005  max mem: 19734
Epoch: [1]  [ 810/1251]  eta: 0:03:30  lr: 0.000001  loss: 4.6101 (4.9395)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [1]  [ 820/1251]  eta: 0:03:25  lr: 0.000001  loss: 4.7755 (4.9414)  time: 0.4695  data: 0.0004  max mem: 19734
Epoch: [1]  [ 830/1251]  eta: 0:03:20  lr: 0.000001  loss: 4.8249 (4.9364)  time: 0.4704  data: 0.0005  max mem: 19734
Epoch: [1]  [ 840/1251]  eta: 0:03:15  lr: 0.000001  loss: 4.8596 (4.9389)  time: 0.4533  data: 0.0006  max mem: 19734
loss info: cls_loss=4.6101, ratio_loss=0.2971, pruning_loss=0.2765, mse_loss=1.2008
Epoch: [1]  [ 850/1251]  eta: 0:03:10  lr: 0.000001  loss: 5.0798 (4.9359)  time: 0.4545  data: 0.0009  max mem: 19734
Epoch: [1]  [ 860/1251]  eta: 0:03:06  lr: 0.000001  loss: 4.9272 (4.9350)  time: 0.4541  data: 0.0007  max mem: 19734
Epoch: [1]  [ 870/1251]  eta: 0:03:01  lr: 0.000001  loss: 5.1694 (4.9333)  time: 0.4503  data: 0.0004  max mem: 19734
Epoch: [1]  [ 880/1251]  eta: 0:02:56  lr: 0.000001  loss: 5.0369 (4.9335)  time: 0.4487  data: 0.0009  max mem: 19734
Epoch: [1]  [ 890/1251]  eta: 0:02:51  lr: 0.000001  loss: 5.1856 (4.9349)  time: 0.4501  data: 0.0008  max mem: 19734
Epoch: [1]  [ 900/1251]  eta: 0:02:46  lr: 0.000001  loss: 4.9989 (4.9325)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [1]  [ 910/1251]  eta: 0:02:41  lr: 0.000001  loss: 4.9374 (4.9318)  time: 0.4534  data: 0.0006  max mem: 19734
Epoch: [1]  [ 920/1251]  eta: 0:02:36  lr: 0.000001  loss: 5.0351 (4.9314)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [1]  [ 930/1251]  eta: 0:02:32  lr: 0.000001  loss: 4.9108 (4.9299)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [1]  [ 940/1251]  eta: 0:02:27  lr: 0.000001  loss: 5.0879 (4.9299)  time: 0.4541  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5896, ratio_loss=0.2889, pruning_loss=0.2779, mse_loss=1.2213
Epoch: [1]  [ 950/1251]  eta: 0:02:22  lr: 0.000001  loss: 4.9178 (4.9258)  time: 0.4595  data: 0.0004  max mem: 19734
Epoch: [1]  [ 960/1251]  eta: 0:02:17  lr: 0.000001  loss: 4.6087 (4.9234)  time: 0.4749  data: 0.0004  max mem: 19734
Epoch: [1]  [ 970/1251]  eta: 0:02:13  lr: 0.000001  loss: 4.5434 (4.9186)  time: 0.4670  data: 0.0005  max mem: 19734
Epoch: [1]  [ 980/1251]  eta: 0:02:08  lr: 0.000001  loss: 4.3546 (4.9136)  time: 0.4503  data: 0.0005  max mem: 19734
Epoch: [1]  [ 990/1251]  eta: 0:02:03  lr: 0.000001  loss: 4.5445 (4.9108)  time: 0.4517  data: 0.0005  max mem: 19734
Epoch: [1]  [1000/1251]  eta: 0:01:58  lr: 0.000001  loss: 4.5445 (4.9076)  time: 0.4524  data: 0.0005  max mem: 19734
Epoch: [1]  [1010/1251]  eta: 0:01:53  lr: 0.000001  loss: 4.5307 (4.9043)  time: 0.4538  data: 0.0006  max mem: 19734
Epoch: [1]  [1020/1251]  eta: 0:01:49  lr: 0.000001  loss: 4.5735 (4.9032)  time: 0.4540  data: 0.0006  max mem: 19734
Epoch: [1]  [1030/1251]  eta: 0:01:44  lr: 0.000001  loss: 4.9404 (4.9031)  time: 0.4513  data: 0.0005  max mem: 19734
Epoch: [1]  [1040/1251]  eta: 0:01:39  lr: 0.000001  loss: 4.9404 (4.9007)  time: 0.4492  data: 0.0005  max mem: 19734
loss info: cls_loss=4.3885, ratio_loss=0.2800, pruning_loss=0.2819, mse_loss=1.2297
Epoch: [1]  [1050/1251]  eta: 0:01:34  lr: 0.000001  loss: 4.5412 (4.8954)  time: 0.4489  data: 0.0005  max mem: 19734
Epoch: [1]  [1060/1251]  eta: 0:01:30  lr: 0.000001  loss: 4.7254 (4.8935)  time: 0.4498  data: 0.0005  max mem: 19734
Epoch: [1]  [1070/1251]  eta: 0:01:25  lr: 0.000001  loss: 4.9257 (4.8940)  time: 0.4514  data: 0.0005  max mem: 19734
Epoch: [1]  [1080/1251]  eta: 0:01:20  lr: 0.000001  loss: 5.1351 (4.8950)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [1]  [1090/1251]  eta: 0:01:15  lr: 0.000001  loss: 4.8946 (4.8923)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [1]  [1100/1251]  eta: 0:01:11  lr: 0.000001  loss: 4.6643 (4.8904)  time: 0.4766  data: 0.0005  max mem: 19734
Epoch: [1]  [1110/1251]  eta: 0:01:06  lr: 0.000001  loss: 4.6362 (4.8870)  time: 0.4818  data: 0.0005  max mem: 19734
Epoch: [1]  [1120/1251]  eta: 0:01:01  lr: 0.000001  loss: 4.6340 (4.8842)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [1]  [1130/1251]  eta: 0:00:56  lr: 0.000001  loss: 4.7794 (4.8830)  time: 0.4503  data: 0.0005  max mem: 19734
Epoch: [1]  [1140/1251]  eta: 0:00:52  lr: 0.000001  loss: 4.6938 (4.8785)  time: 0.4528  data: 0.0005  max mem: 19734
loss info: cls_loss=4.4310, ratio_loss=0.2724, pruning_loss=0.2797, mse_loss=1.2214
Epoch: [1]  [1150/1251]  eta: 0:00:47  lr: 0.000001  loss: 4.3700 (4.8777)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [1]  [1160/1251]  eta: 0:00:42  lr: 0.000001  loss: 5.0352 (4.8772)  time: 0.4499  data: 0.0005  max mem: 19734
Epoch: [1]  [1170/1251]  eta: 0:00:38  lr: 0.000001  loss: 4.6367 (4.8725)  time: 0.4522  data: 0.0006  max mem: 19734
Epoch: [1]  [1180/1251]  eta: 0:00:33  lr: 0.000001  loss: 4.7801 (4.8714)  time: 0.4545  data: 0.0006  max mem: 19734
Epoch: [1]  [1190/1251]  eta: 0:00:28  lr: 0.000001  loss: 4.9060 (4.8705)  time: 0.4529  data: 0.0008  max mem: 19734
Epoch: [1]  [1200/1251]  eta: 0:00:23  lr: 0.000001  loss: 4.9060 (4.8710)  time: 0.4481  data: 0.0007  max mem: 19734
Epoch: [1]  [1210/1251]  eta: 0:00:19  lr: 0.000001  loss: 5.0551 (4.8686)  time: 0.4459  data: 0.0002  max mem: 19734
Epoch: [1]  [1220/1251]  eta: 0:00:14  lr: 0.000001  loss: 5.0121 (4.8660)  time: 0.4464  data: 0.0002  max mem: 19734
Epoch: [1]  [1230/1251]  eta: 0:00:09  lr: 0.000001  loss: 4.3483 (4.8628)  time: 0.4453  data: 0.0002  max mem: 19734
Epoch: [1]  [1240/1251]  eta: 0:00:05  lr: 0.000001  loss: 4.8595 (4.8639)  time: 0.4530  data: 0.0002  max mem: 19734
loss info: cls_loss=4.4471, ratio_loss=0.2632, pruning_loss=0.2763, mse_loss=1.2045
Epoch: [1]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 4.8389 (4.8595)  time: 0.4720  data: 0.0002  max mem: 19734
Epoch: [1] Total time: 0:09:47 (0.4699 s / it)
Averaged stats: lr: 0.000001  loss: 4.8389 (4.8640)
Test:  [  0/261]  eta: 1:59:07  loss: 1.7077 (1.7077)  acc1: 64.0625 (64.0625)  acc5: 86.4583 (86.4583)  time: 27.3859  data: 27.3118  max mem: 19734
Test:  [ 10/261]  eta: 0:10:51  loss: 1.6642 (1.6386)  acc1: 67.7083 (66.0985)  acc5: 89.0625 (87.4527)  time: 2.5944  data: 2.4934  max mem: 19734
Test:  [ 20/261]  eta: 0:05:52  loss: 1.7558 (1.8203)  acc1: 61.9792 (61.7064)  acc5: 85.4167 (84.3998)  time: 0.1653  data: 0.0098  max mem: 19734
Test:  [ 30/261]  eta: 0:04:07  loss: 1.7383 (1.7376)  acc1: 61.9792 (63.9953)  acc5: 82.8125 (85.0134)  time: 0.2315  data: 0.0084  max mem: 19734
Test:  [ 40/261]  eta: 0:04:14  loss: 1.6098 (1.7479)  acc1: 66.6667 (63.4273)  acc5: 87.5000 (85.0864)  time: 0.8251  data: 0.5986  max mem: 19734
Test:  [ 50/261]  eta: 0:03:24  loss: 1.8742 (1.8277)  acc1: 58.3333 (61.1418)  acc5: 82.2917 (84.2933)  time: 0.8089  data: 0.6046  max mem: 19734
Test:  [ 60/261]  eta: 0:02:51  loss: 2.0509 (1.8685)  acc1: 52.0833 (59.8702)  acc5: 81.7708 (83.9993)  time: 0.2456  data: 0.0183  max mem: 19734
Test:  [ 70/261]  eta: 0:02:27  loss: 2.0196 (1.8767)  acc1: 54.1667 (59.3970)  acc5: 83.8542 (84.1256)  time: 0.2674  data: 0.0493  max mem: 19734
Test:  [ 80/261]  eta: 0:02:09  loss: 1.7789 (1.8502)  acc1: 60.4167 (60.1145)  acc5: 84.8958 (84.5358)  time: 0.2922  data: 0.0476  max mem: 19734
Test:  [ 90/261]  eta: 0:01:53  loss: 1.6731 (1.8136)  acc1: 64.0625 (60.9261)  acc5: 87.5000 (85.0561)  time: 0.2868  data: 0.0178  max mem: 19734
Test:  [100/261]  eta: 0:01:38  loss: 1.4969 (1.8012)  acc1: 68.2292 (61.2108)  acc5: 85.9375 (85.1021)  time: 0.2037  data: 0.0181  max mem: 19734
Test:  [110/261]  eta: 0:01:27  loss: 1.6681 (1.8104)  acc1: 64.5833 (61.0642)  acc5: 83.8542 (84.7738)  time: 0.2006  data: 0.0112  max mem: 19734
Test:  [120/261]  eta: 0:01:17  loss: 2.0090 (1.8346)  acc1: 57.8125 (60.6491)  acc5: 80.2083 (84.3277)  time: 0.2187  data: 0.0101  max mem: 19734
Test:  [130/261]  eta: 0:01:12  loss: 2.0608 (1.8655)  acc1: 53.1250 (60.1304)  acc5: 79.1667 (83.8104)  time: 0.4131  data: 0.2300  max mem: 19734
Test:  [140/261]  eta: 0:01:03  loss: 2.0834 (1.8761)  acc1: 53.6458 (59.7518)  acc5: 79.1667 (83.6695)  time: 0.3674  data: 0.2288  max mem: 19734
Test:  [150/261]  eta: 0:00:57  loss: 1.8585 (1.8724)  acc1: 60.9375 (59.9855)  acc5: 82.2917 (83.5748)  time: 0.2439  data: 0.1015  max mem: 19734
Test:  [160/261]  eta: 0:00:49  loss: 1.8230 (1.8916)  acc1: 60.9375 (59.7503)  acc5: 80.7292 (83.2363)  time: 0.2832  data: 0.1012  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 2.1561 (1.9173)  acc1: 53.1250 (59.2806)  acc5: 77.0833 (82.8369)  time: 0.3761  data: 0.2441  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 2.1810 (1.9312)  acc1: 53.6458 (59.0355)  acc5: 77.0833 (82.6226)  time: 0.3254  data: 0.2466  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 2.1810 (1.9411)  acc1: 54.1667 (58.8760)  acc5: 78.6458 (82.3898)  time: 0.0714  data: 0.0067  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 2.1558 (1.9486)  acc1: 55.2083 (58.7453)  acc5: 78.6458 (82.2424)  time: 0.0667  data: 0.0051  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 2.1071 (1.9542)  acc1: 55.7292 (58.7036)  acc5: 78.6458 (82.1115)  time: 0.0666  data: 0.0049  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 2.2326 (1.9786)  acc1: 53.6458 (58.1684)  acc5: 75.5208 (81.7048)  time: 0.0617  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 2.2272 (1.9868)  acc1: 50.5208 (57.9748)  acc5: 75.0000 (81.5318)  time: 0.0616  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 2.0486 (1.9857)  acc1: 54.6875 (57.9011)  acc5: 80.2083 (81.5850)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.7743 (1.9661)  acc1: 60.9375 (58.2815)  acc5: 85.4167 (81.8642)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.5945 (1.9545)  acc1: 63.5417 (58.5340)  acc5: 88.0208 (82.0620)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:31 (0.3493 s / it)
* Acc@1 58.534 Acc@5 82.062 loss 1.954
Accuracy of the network on the 50000 test images: 58.5%
Max accuracy: 61.23%
Epoch: [2]  [   0/1251]  eta: 6:01:46  lr: 0.000005  loss: 4.1875 (4.1875)  time: 17.3516  data: 9.7663  max mem: 19734
Epoch: [2]  [  10/1251]  eta: 0:46:31  lr: 0.000005  loss: 4.2175 (4.5117)  time: 2.2491  data: 0.9922  max mem: 19734
Epoch: [2]  [  20/1251]  eta: 0:28:36  lr: 0.000005  loss: 4.6754 (4.6236)  time: 0.5969  data: 0.0576  max mem: 19734
Epoch: [2]  [  30/1251]  eta: 0:22:13  lr: 0.000005  loss: 4.9766 (4.6985)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [2]  [  40/1251]  eta: 0:18:54  lr: 0.000005  loss: 4.9586 (4.6705)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [2]  [  50/1251]  eta: 0:16:52  lr: 0.000005  loss: 4.9586 (4.7041)  time: 0.4564  data: 0.0005  max mem: 19734
Epoch: [2]  [  60/1251]  eta: 0:15:29  lr: 0.000005  loss: 4.9752 (4.7396)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [2]  [  70/1251]  eta: 0:14:28  lr: 0.000005  loss: 4.9346 (4.7307)  time: 0.4615  data: 0.0004  max mem: 19734
Epoch: [2]  [  80/1251]  eta: 0:13:41  lr: 0.000005  loss: 4.6267 (4.7145)  time: 0.4589  data: 0.0005  max mem: 19734
Epoch: [2]  [  90/1251]  eta: 0:13:03  lr: 0.000005  loss: 4.5928 (4.6744)  time: 0.4591  data: 0.0005  max mem: 19734
loss info: cls_loss=4.4482, ratio_loss=0.2338, pruning_loss=0.2683, mse_loss=1.2275
Epoch: [2]  [ 100/1251]  eta: 0:12:33  lr: 0.000005  loss: 4.6152 (4.6645)  time: 0.4678  data: 0.0005  max mem: 19734
Epoch: [2]  [ 110/1251]  eta: 0:12:07  lr: 0.000005  loss: 4.3626 (4.6086)  time: 0.4671  data: 0.0005  max mem: 19734
Epoch: [2]  [ 120/1251]  eta: 0:11:43  lr: 0.000005  loss: 4.3869 (4.6084)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [2]  [ 130/1251]  eta: 0:11:25  lr: 0.000005  loss: 4.5629 (4.5940)  time: 0.4662  data: 0.0005  max mem: 19734
Epoch: [2]  [ 140/1251]  eta: 0:11:08  lr: 0.000005  loss: 4.5629 (4.5913)  time: 0.4781  data: 0.0005  max mem: 19734
Epoch: [2]  [ 150/1251]  eta: 0:10:52  lr: 0.000005  loss: 4.5622 (4.5608)  time: 0.4689  data: 0.0006  max mem: 19734
Epoch: [2]  [ 160/1251]  eta: 0:10:36  lr: 0.000005  loss: 4.5291 (4.5588)  time: 0.4577  data: 0.0005  max mem: 19734
Epoch: [2]  [ 170/1251]  eta: 0:10:23  lr: 0.000005  loss: 4.5091 (4.5447)  time: 0.4567  data: 0.0005  max mem: 19734
Epoch: [2]  [ 180/1251]  eta: 0:10:10  lr: 0.000005  loss: 4.5829 (4.5434)  time: 0.4587  data: 0.0005  max mem: 19734
Epoch: [2]  [ 190/1251]  eta: 0:09:58  lr: 0.000005  loss: 4.6944 (4.5363)  time: 0.4556  data: 0.0005  max mem: 19734
loss info: cls_loss=4.2439, ratio_loss=0.1724, pruning_loss=0.2599, mse_loss=1.1991
Epoch: [2]  [ 200/1251]  eta: 0:09:46  lr: 0.000005  loss: 4.4594 (4.5348)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [2]  [ 210/1251]  eta: 0:09:36  lr: 0.000005  loss: 4.5240 (4.5323)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [2]  [ 220/1251]  eta: 0:09:26  lr: 0.000005  loss: 4.4183 (4.5158)  time: 0.4545  data: 0.0005  max mem: 19734
Epoch: [2]  [ 230/1251]  eta: 0:09:16  lr: 0.000005  loss: 4.4183 (4.5164)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [2]  [ 240/1251]  eta: 0:09:07  lr: 0.000005  loss: 4.2106 (4.4894)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [2]  [ 250/1251]  eta: 0:08:59  lr: 0.000005  loss: 3.9562 (4.4735)  time: 0.4673  data: 0.0005  max mem: 19734
Epoch: [2]  [ 260/1251]  eta: 0:08:50  lr: 0.000005  loss: 4.2230 (4.4588)  time: 0.4672  data: 0.0005  max mem: 19734
Epoch: [2]  [ 270/1251]  eta: 0:08:42  lr: 0.000005  loss: 4.4983 (4.4595)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [2]  [ 280/1251]  eta: 0:08:36  lr: 0.000005  loss: 4.4429 (4.4426)  time: 0.4859  data: 0.0005  max mem: 19734
Epoch: [2]  [ 290/1251]  eta: 0:08:28  lr: 0.000005  loss: 3.7030 (4.4141)  time: 0.4852  data: 0.0005  max mem: 19734
loss info: cls_loss=4.0273, ratio_loss=0.1066, pruning_loss=0.2584, mse_loss=1.1988
Epoch: [2]  [ 300/1251]  eta: 0:08:20  lr: 0.000005  loss: 3.7394 (4.4007)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [2]  [ 310/1251]  eta: 0:08:13  lr: 0.000005  loss: 4.2084 (4.3888)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [2]  [ 320/1251]  eta: 0:08:06  lr: 0.000005  loss: 4.1251 (4.3807)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [2]  [ 330/1251]  eta: 0:07:58  lr: 0.000005  loss: 3.9106 (4.3669)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [2]  [ 340/1251]  eta: 0:07:51  lr: 0.000005  loss: 3.6497 (4.3521)  time: 0.4510  data: 0.0006  max mem: 19734
Epoch: [2]  [ 350/1251]  eta: 0:07:45  lr: 0.000005  loss: 3.7066 (4.3363)  time: 0.4510  data: 0.0006  max mem: 19734
Epoch: [2]  [ 360/1251]  eta: 0:07:38  lr: 0.000005  loss: 3.6062 (4.3058)  time: 0.4521  data: 0.0007  max mem: 19734
Epoch: [2]  [ 370/1251]  eta: 0:07:31  lr: 0.000005  loss: 3.4387 (4.2924)  time: 0.4516  data: 0.0006  max mem: 19734
Epoch: [2]  [ 380/1251]  eta: 0:07:25  lr: 0.000005  loss: 4.0033 (4.2822)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [2]  [ 390/1251]  eta: 0:07:18  lr: 0.000005  loss: 3.9101 (4.2664)  time: 0.4526  data: 0.0005  max mem: 19734
loss info: cls_loss=3.7544, ratio_loss=0.0523, pruning_loss=0.2559, mse_loss=1.2186
Epoch: [2]  [ 400/1251]  eta: 0:07:12  lr: 0.000005  loss: 3.8932 (4.2575)  time: 0.4584  data: 0.0004  max mem: 19734
Epoch: [2]  [ 410/1251]  eta: 0:07:06  lr: 0.000005  loss: 3.9636 (4.2445)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [2]  [ 420/1251]  eta: 0:07:00  lr: 0.000005  loss: 3.9636 (4.2377)  time: 0.4614  data: 0.0004  max mem: 19734
Epoch: [2]  [ 430/1251]  eta: 0:06:55  lr: 0.000005  loss: 4.0943 (4.2341)  time: 0.4817  data: 0.0004  max mem: 19734
Epoch: [2]  [ 440/1251]  eta: 0:06:49  lr: 0.000005  loss: 4.1570 (4.2309)  time: 0.4723  data: 0.0004  max mem: 19734
Epoch: [2]  [ 450/1251]  eta: 0:06:43  lr: 0.000005  loss: 3.8681 (4.2158)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [2]  [ 460/1251]  eta: 0:06:37  lr: 0.000005  loss: 3.6832 (4.2043)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [2]  [ 470/1251]  eta: 0:06:31  lr: 0.000005  loss: 3.8942 (4.2006)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [2]  [ 480/1251]  eta: 0:06:25  lr: 0.000005  loss: 3.9796 (4.1900)  time: 0.4526  data: 0.0006  max mem: 19734
Epoch: [2]  [ 490/1251]  eta: 0:06:19  lr: 0.000005  loss: 4.0354 (4.1890)  time: 0.4522  data: 0.0005  max mem: 19734
loss info: cls_loss=3.7972, ratio_loss=0.0218, pruning_loss=0.2482, mse_loss=1.1890
Epoch: [2]  [ 500/1251]  eta: 0:06:14  lr: 0.000005  loss: 4.1347 (4.1808)  time: 0.4516  data: 0.0005  max mem: 19734
Epoch: [2]  [ 510/1251]  eta: 0:06:08  lr: 0.000005  loss: 3.9370 (4.1761)  time: 0.4508  data: 0.0006  max mem: 19734
Epoch: [2]  [ 520/1251]  eta: 0:06:02  lr: 0.000005  loss: 3.8742 (4.1679)  time: 0.4511  data: 0.0005  max mem: 19734
Epoch: [2]  [ 530/1251]  eta: 0:05:57  lr: 0.000005  loss: 3.8712 (4.1576)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [2]  [ 540/1251]  eta: 0:05:52  lr: 0.000005  loss: 3.9496 (4.1522)  time: 0.4600  data: 0.0004  max mem: 19734
Epoch: [2]  [ 550/1251]  eta: 0:05:46  lr: 0.000005  loss: 4.0429 (4.1479)  time: 0.4610  data: 0.0004  max mem: 19734
Epoch: [2]  [ 560/1251]  eta: 0:05:41  lr: 0.000005  loss: 3.8498 (4.1391)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [2]  [ 570/1251]  eta: 0:05:36  lr: 0.000005  loss: 3.7376 (4.1326)  time: 0.4811  data: 0.0005  max mem: 19734
Epoch: [2]  [ 580/1251]  eta: 0:05:30  lr: 0.000005  loss: 3.7376 (4.1226)  time: 0.4804  data: 0.0005  max mem: 19734
Epoch: [2]  [ 590/1251]  eta: 0:05:25  lr: 0.000005  loss: 3.7948 (4.1165)  time: 0.4521  data: 0.0005  max mem: 19734
loss info: cls_loss=3.7226, ratio_loss=0.0100, pruning_loss=0.2483, mse_loss=1.2211
Epoch: [2]  [ 600/1251]  eta: 0:05:20  lr: 0.000005  loss: 3.9580 (4.1112)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [2]  [ 610/1251]  eta: 0:05:14  lr: 0.000005  loss: 3.8329 (4.1051)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [2]  [ 620/1251]  eta: 0:05:09  lr: 0.000005  loss: 3.8050 (4.1015)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [2]  [ 630/1251]  eta: 0:05:04  lr: 0.000005  loss: 3.8661 (4.0977)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [2]  [ 640/1251]  eta: 0:04:59  lr: 0.000005  loss: 3.4841 (4.0836)  time: 0.4524  data: 0.0005  max mem: 19734
Epoch: [2]  [ 650/1251]  eta: 0:04:53  lr: 0.000005  loss: 3.4400 (4.0776)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [2]  [ 660/1251]  eta: 0:04:48  lr: 0.000005  loss: 3.8477 (4.0714)  time: 0.4535  data: 0.0006  max mem: 19734
Epoch: [2]  [ 670/1251]  eta: 0:04:43  lr: 0.000005  loss: 3.6053 (4.0661)  time: 0.4525  data: 0.0006  max mem: 19734
Epoch: [2]  [ 680/1251]  eta: 0:04:38  lr: 0.000005  loss: 3.7774 (4.0609)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [2]  [ 690/1251]  eta: 0:04:33  lr: 0.000005  loss: 3.7511 (4.0539)  time: 0.4633  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6213, ratio_loss=0.0055, pruning_loss=0.2454, mse_loss=1.1921
Epoch: [2]  [ 700/1251]  eta: 0:04:28  lr: 0.000005  loss: 3.8262 (4.0511)  time: 0.4641  data: 0.0006  max mem: 19734
Epoch: [2]  [ 710/1251]  eta: 0:04:23  lr: 0.000005  loss: 3.9009 (4.0488)  time: 0.4755  data: 0.0004  max mem: 19734
Epoch: [2]  [ 720/1251]  eta: 0:04:18  lr: 0.000005  loss: 3.8182 (4.0427)  time: 0.4814  data: 0.0004  max mem: 19734
Epoch: [2]  [ 730/1251]  eta: 0:04:13  lr: 0.000005  loss: 3.8346 (4.0422)  time: 0.4588  data: 0.0004  max mem: 19734
Epoch: [2]  [ 740/1251]  eta: 0:04:08  lr: 0.000005  loss: 4.1318 (4.0422)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [2]  [ 750/1251]  eta: 0:04:03  lr: 0.000005  loss: 4.1318 (4.0383)  time: 0.4524  data: 0.0005  max mem: 19734
Epoch: [2]  [ 760/1251]  eta: 0:03:57  lr: 0.000005  loss: 3.6976 (4.0365)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [2]  [ 770/1251]  eta: 0:03:52  lr: 0.000005  loss: 3.6696 (4.0296)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [2]  [ 780/1251]  eta: 0:03:47  lr: 0.000005  loss: 3.4294 (4.0254)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [2]  [ 790/1251]  eta: 0:03:42  lr: 0.000005  loss: 3.6712 (4.0222)  time: 0.4535  data: 0.0005  max mem: 19734
loss info: cls_loss=3.7599, ratio_loss=0.0043, pruning_loss=0.2369, mse_loss=1.1086
Epoch: [2]  [ 800/1251]  eta: 0:03:37  lr: 0.000005  loss: 3.9786 (4.0197)  time: 0.4517  data: 0.0005  max mem: 19734
Epoch: [2]  [ 810/1251]  eta: 0:03:32  lr: 0.000005  loss: 3.9786 (4.0171)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [2]  [ 820/1251]  eta: 0:03:27  lr: 0.000005  loss: 3.9807 (4.0137)  time: 0.4542  data: 0.0011  max mem: 19734
Epoch: [2]  [ 830/1251]  eta: 0:03:22  lr: 0.000005  loss: 3.9517 (4.0071)  time: 0.4536  data: 0.0010  max mem: 19734
Epoch: [2]  [ 840/1251]  eta: 0:03:18  lr: 0.000005  loss: 3.4717 (4.0007)  time: 0.4684  data: 0.0004  max mem: 19734
Epoch: [2]  [ 850/1251]  eta: 0:03:13  lr: 0.000005  loss: 3.8267 (3.9989)  time: 0.4697  data: 0.0007  max mem: 19734
Epoch: [2]  [ 860/1251]  eta: 0:03:08  lr: 0.000005  loss: 3.8889 (3.9942)  time: 0.4813  data: 0.0008  max mem: 19734
Epoch: [2]  [ 870/1251]  eta: 0:03:03  lr: 0.000005  loss: 3.6016 (3.9895)  time: 0.4793  data: 0.0006  max mem: 19734
Epoch: [2]  [ 880/1251]  eta: 0:02:58  lr: 0.000005  loss: 3.8512 (3.9881)  time: 0.4508  data: 0.0006  max mem: 19734
Epoch: [2]  [ 890/1251]  eta: 0:02:53  lr: 0.000005  loss: 3.9691 (3.9866)  time: 0.4506  data: 0.0006  max mem: 19734
loss info: cls_loss=3.6295, ratio_loss=0.0040, pruning_loss=0.2443, mse_loss=1.1421
Epoch: [2]  [ 900/1251]  eta: 0:02:48  lr: 0.000005  loss: 3.7106 (3.9820)  time: 0.4514  data: 0.0005  max mem: 19734
Epoch: [2]  [ 910/1251]  eta: 0:02:43  lr: 0.000005  loss: 3.7106 (3.9795)  time: 0.4517  data: 0.0005  max mem: 19734
Epoch: [2]  [ 920/1251]  eta: 0:02:38  lr: 0.000005  loss: 4.0077 (3.9784)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [2]  [ 930/1251]  eta: 0:02:34  lr: 0.000005  loss: 3.8117 (3.9734)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [2]  [ 940/1251]  eta: 0:02:29  lr: 0.000005  loss: 3.4697 (3.9684)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [2]  [ 950/1251]  eta: 0:02:24  lr: 0.000005  loss: 3.4697 (3.9648)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [2]  [ 960/1251]  eta: 0:02:19  lr: 0.000005  loss: 3.8621 (3.9623)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [2]  [ 970/1251]  eta: 0:02:14  lr: 0.000005  loss: 3.4558 (3.9559)  time: 0.4549  data: 0.0006  max mem: 19734
Epoch: [2]  [ 980/1251]  eta: 0:02:09  lr: 0.000005  loss: 3.4769 (3.9518)  time: 0.4538  data: 0.0006  max mem: 19734
Epoch: [2]  [ 990/1251]  eta: 0:02:04  lr: 0.000005  loss: 3.6090 (3.9502)  time: 0.4636  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5663, ratio_loss=0.0036, pruning_loss=0.2421, mse_loss=1.1704
Epoch: [2]  [1000/1251]  eta: 0:02:00  lr: 0.000005  loss: 3.6856 (3.9461)  time: 0.4873  data: 0.0005  max mem: 19734
Epoch: [2]  [1010/1251]  eta: 0:01:55  lr: 0.000005  loss: 3.7267 (3.9445)  time: 0.4760  data: 0.0004  max mem: 19734
Epoch: [2]  [1020/1251]  eta: 0:01:50  lr: 0.000005  loss: 3.5873 (3.9395)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [2]  [1030/1251]  eta: 0:01:45  lr: 0.000005  loss: 3.6681 (3.9379)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [2]  [1040/1251]  eta: 0:01:40  lr: 0.000005  loss: 3.6629 (3.9335)  time: 0.4522  data: 0.0005  max mem: 19734
Epoch: [2]  [1050/1251]  eta: 0:01:35  lr: 0.000005  loss: 3.6342 (3.9311)  time: 0.4513  data: 0.0007  max mem: 19734
Epoch: [2]  [1060/1251]  eta: 0:01:31  lr: 0.000005  loss: 3.9266 (3.9311)  time: 0.4522  data: 0.0006  max mem: 19734
Epoch: [2]  [1070/1251]  eta: 0:01:26  lr: 0.000005  loss: 3.9315 (3.9292)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [2]  [1080/1251]  eta: 0:01:21  lr: 0.000005  loss: 3.7399 (3.9277)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [2]  [1090/1251]  eta: 0:01:16  lr: 0.000005  loss: 3.7756 (3.9262)  time: 0.4546  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6614, ratio_loss=0.0037, pruning_loss=0.2371, mse_loss=1.1018
Epoch: [2]  [1100/1251]  eta: 0:01:11  lr: 0.000005  loss: 3.7756 (3.9223)  time: 0.4539  data: 0.0006  max mem: 19734
Epoch: [2]  [1110/1251]  eta: 0:01:07  lr: 0.000005  loss: 3.8252 (3.9222)  time: 0.4515  data: 0.0006  max mem: 19734
Epoch: [2]  [1120/1251]  eta: 0:01:02  lr: 0.000005  loss: 3.8930 (3.9208)  time: 0.4511  data: 0.0004  max mem: 19734
Epoch: [2]  [1130/1251]  eta: 0:00:57  lr: 0.000005  loss: 3.8397 (3.9210)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [2]  [1140/1251]  eta: 0:00:52  lr: 0.000005  loss: 3.8825 (3.9186)  time: 0.4587  data: 0.0004  max mem: 19734
Epoch: [2]  [1150/1251]  eta: 0:00:48  lr: 0.000005  loss: 3.8456 (3.9183)  time: 0.4779  data: 0.0006  max mem: 19734
Epoch: [2]  [1160/1251]  eta: 0:00:43  lr: 0.000005  loss: 3.9003 (3.9162)  time: 0.4725  data: 0.0006  max mem: 19734
Epoch: [2]  [1170/1251]  eta: 0:00:38  lr: 0.000005  loss: 4.0348 (3.9160)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [2]  [1180/1251]  eta: 0:00:33  lr: 0.000005  loss: 3.7871 (3.9145)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [2]  [1190/1251]  eta: 0:00:28  lr: 0.000005  loss: 3.7112 (3.9125)  time: 0.4514  data: 0.0008  max mem: 19734
loss info: cls_loss=3.6999, ratio_loss=0.0036, pruning_loss=0.2381, mse_loss=1.1759
Epoch: [2]  [1200/1251]  eta: 0:00:24  lr: 0.000005  loss: 3.7607 (3.9101)  time: 0.4477  data: 0.0006  max mem: 19734
Epoch: [2]  [1210/1251]  eta: 0:00:19  lr: 0.000005  loss: 3.7633 (3.9079)  time: 0.4444  data: 0.0002  max mem: 19734
Epoch: [2]  [1220/1251]  eta: 0:00:14  lr: 0.000005  loss: 3.8930 (3.9076)  time: 0.4444  data: 0.0002  max mem: 19734
Epoch: [2]  [1230/1251]  eta: 0:00:09  lr: 0.000005  loss: 3.9344 (3.9070)  time: 0.4443  data: 0.0002  max mem: 19734
Epoch: [2]  [1240/1251]  eta: 0:00:05  lr: 0.000005  loss: 3.9097 (3.9060)  time: 0.4456  data: 0.0002  max mem: 19734
Epoch: [2]  [1250/1251]  eta: 0:00:00  lr: 0.000005  loss: 3.8346 (3.9031)  time: 0.4460  data: 0.0002  max mem: 19734
Epoch: [2] Total time: 0:09:53 (0.4741 s / it)
Averaged stats: lr: 0.000005  loss: 3.8346 (3.8829)
Test:  [  0/261]  eta: 1:07:41  loss: 1.1684 (1.1684)  acc1: 77.0833 (77.0833)  acc5: 92.1875 (92.1875)  time: 15.5606  data: 15.3930  max mem: 19734
Test:  [ 10/261]  eta: 0:11:22  loss: 1.1684 (1.1872)  acc1: 77.0833 (74.8580)  acc5: 93.7500 (92.3295)  time: 2.7181  data: 2.4889  max mem: 19734
Test:  [ 20/261]  eta: 0:06:03  loss: 1.3573 (1.3700)  acc1: 69.2708 (69.3452)  acc5: 90.1042 (89.9306)  time: 0.8037  data: 0.6052  max mem: 19734
Test:  [ 30/261]  eta: 0:04:14  loss: 1.2328 (1.2665)  acc1: 72.3958 (72.8999)  acc5: 89.5833 (90.5746)  time: 0.2102  data: 0.0109  max mem: 19734
Test:  [ 40/261]  eta: 0:03:40  loss: 0.9857 (1.2386)  acc1: 81.7708 (73.6916)  acc5: 92.7083 (90.8156)  time: 0.4659  data: 0.2760  max mem: 19734
Test:  [ 50/261]  eta: 0:02:55  loss: 1.5576 (1.3474)  acc1: 63.5417 (70.4861)  acc5: 86.9792 (89.6752)  time: 0.4129  data: 0.2778  max mem: 19734
Test:  [ 60/261]  eta: 0:02:24  loss: 1.7245 (1.4029)  acc1: 59.3750 (68.6049)  acc5: 85.9375 (89.1650)  time: 0.1469  data: 0.0173  max mem: 19734
Test:  [ 70/261]  eta: 0:02:02  loss: 1.6025 (1.4068)  acc1: 60.4167 (67.9871)  acc5: 88.0208 (89.2972)  time: 0.1542  data: 0.0150  max mem: 19734
Test:  [ 80/261]  eta: 0:01:49  loss: 1.3270 (1.3877)  acc1: 67.1875 (68.4542)  acc5: 91.6667 (89.6734)  time: 0.2653  data: 0.1239  max mem: 19734
Test:  [ 90/261]  eta: 0:01:35  loss: 1.2659 (1.3567)  acc1: 71.3542 (69.2365)  acc5: 91.6667 (90.0469)  time: 0.2775  data: 0.1460  max mem: 19734
Test:  [100/261]  eta: 0:01:31  loss: 1.1823 (1.3556)  acc1: 73.4375 (69.2296)  acc5: 91.1458 (90.1506)  time: 0.4102  data: 0.2717  max mem: 19734
Test:  [110/261]  eta: 0:01:20  loss: 1.2912 (1.3687)  acc1: 68.7500 (69.1770)  acc5: 89.5833 (89.9587)  time: 0.3981  data: 0.2521  max mem: 19734
Test:  [120/261]  eta: 0:01:10  loss: 1.6864 (1.4012)  acc1: 63.0208 (68.5864)  acc5: 84.8958 (89.4628)  time: 0.1575  data: 0.0173  max mem: 19734
Test:  [130/261]  eta: 0:01:03  loss: 1.7684 (1.4404)  acc1: 60.4167 (67.8475)  acc5: 82.2917 (88.9154)  time: 0.2198  data: 0.0709  max mem: 19734
Test:  [140/261]  eta: 0:00:58  loss: 1.8216 (1.4614)  acc1: 57.8125 (67.3537)  acc5: 84.3750 (88.6340)  time: 0.3955  data: 0.2512  max mem: 19734
Test:  [150/261]  eta: 0:00:51  loss: 1.5203 (1.4613)  acc1: 66.1458 (67.5083)  acc5: 85.9375 (88.5762)  time: 0.3494  data: 0.1979  max mem: 19734
Test:  [160/261]  eta: 0:00:45  loss: 1.3997 (1.4814)  acc1: 70.3125 (67.2166)  acc5: 88.0208 (88.2311)  time: 0.1841  data: 0.0177  max mem: 19734
Test:  [170/261]  eta: 0:00:40  loss: 1.8394 (1.5099)  acc1: 58.3333 (66.5936)  acc5: 81.2500 (87.8533)  time: 0.2736  data: 0.0990  max mem: 19734
Test:  [180/261]  eta: 0:00:34  loss: 1.8052 (1.5247)  acc1: 58.3333 (66.2926)  acc5: 82.8125 (87.6813)  time: 0.3017  data: 0.1177  max mem: 19734
Test:  [190/261]  eta: 0:00:29  loss: 1.7440 (1.5322)  acc1: 61.9792 (66.1895)  acc5: 84.3750 (87.5436)  time: 0.2208  data: 0.0712  max mem: 19734
Test:  [200/261]  eta: 0:00:24  loss: 1.6870 (1.5417)  acc1: 64.0625 (66.0085)  acc5: 83.3333 (87.3445)  time: 0.1948  data: 0.0915  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.8105 (1.5522)  acc1: 63.0208 (65.8595)  acc5: 83.3333 (87.1643)  time: 0.1579  data: 0.0847  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.8608 (1.5712)  acc1: 59.3750 (65.4153)  acc5: 81.2500 (86.8990)  time: 0.0969  data: 0.0353  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.8166 (1.5805)  acc1: 56.2500 (65.2237)  acc5: 82.8125 (86.7875)  time: 0.0617  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.7668 (1.5859)  acc1: 61.9792 (65.1301)  acc5: 84.8958 (86.7782)  time: 0.0616  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.4049 (1.5713)  acc1: 68.2292 (65.4299)  acc5: 89.0625 (86.9729)  time: 0.0616  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.2212 (1.5659)  acc1: 71.8750 (65.5740)  acc5: 90.6250 (87.0760)  time: 0.0599  data: 0.0002  max mem: 19734
Test: Total time: 0:01:26 (0.3320 s / it)
* Acc@1 65.574 Acc@5 87.076 loss 1.566
Accuracy of the network on the 50000 test images: 65.6%
Max accuracy: 65.57%
Epoch: [3]  [   0/1251]  eta: 2:39:49  lr: 0.000009  loss: 3.5006 (3.5006)  time: 7.6657  data: 7.1937  max mem: 19734
Epoch: [3]  [  10/1251]  eta: 0:24:52  lr: 0.000009  loss: 4.0096 (3.9066)  time: 1.2027  data: 0.6833  max mem: 19734
Epoch: [3]  [  20/1251]  eta: 0:17:20  lr: 0.000009  loss: 3.9113 (3.7440)  time: 0.5044  data: 0.0163  max mem: 19734
Epoch: [3]  [  30/1251]  eta: 0:15:01  lr: 0.000009  loss: 3.7347 (3.7101)  time: 0.4829  data: 0.0004  max mem: 19734
Epoch: [3]  [  40/1251]  eta: 0:13:40  lr: 0.000009  loss: 3.7347 (3.7217)  time: 0.5008  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6931, ratio_loss=0.0036, pruning_loss=0.2319, mse_loss=1.1323
Epoch: [3]  [  50/1251]  eta: 0:12:41  lr: 0.000009  loss: 3.6071 (3.7261)  time: 0.4727  data: 0.0005  max mem: 19734
Epoch: [3]  [  60/1251]  eta: 0:12:01  lr: 0.000009  loss: 3.4791 (3.6739)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [3]  [  70/1251]  eta: 0:11:30  lr: 0.000009  loss: 3.7132 (3.6857)  time: 0.4573  data: 0.0004  max mem: 19734
Epoch: [3]  [  80/1251]  eta: 0:11:05  lr: 0.000009  loss: 3.9144 (3.6929)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [3]  [  90/1251]  eta: 0:10:45  lr: 0.000009  loss: 3.8503 (3.6635)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [3]  [ 100/1251]  eta: 0:10:28  lr: 0.000009  loss: 3.8537 (3.6877)  time: 0.4554  data: 0.0005  max mem: 19734
Epoch: [3]  [ 110/1251]  eta: 0:10:13  lr: 0.000009  loss: 3.7994 (3.6720)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [3]  [ 120/1251]  eta: 0:10:00  lr: 0.000009  loss: 3.6231 (3.6669)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [3]  [ 130/1251]  eta: 0:09:48  lr: 0.000009  loss: 3.5003 (3.6470)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [3]  [ 140/1251]  eta: 0:09:39  lr: 0.000009  loss: 3.4920 (3.6439)  time: 0.4674  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5251, ratio_loss=0.0037, pruning_loss=0.2389, mse_loss=1.1475
Epoch: [3]  [ 150/1251]  eta: 0:09:29  lr: 0.000009  loss: 3.7310 (3.6304)  time: 0.4671  data: 0.0006  max mem: 19734
Epoch: [3]  [ 160/1251]  eta: 0:09:20  lr: 0.000009  loss: 3.5656 (3.6251)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [3]  [ 170/1251]  eta: 0:09:13  lr: 0.000009  loss: 3.5656 (3.6245)  time: 0.4667  data: 0.0005  max mem: 19734
Epoch: [3]  [ 180/1251]  eta: 0:09:07  lr: 0.000009  loss: 3.8205 (3.6233)  time: 0.4921  data: 0.0005  max mem: 19734
Epoch: [3]  [ 190/1251]  eta: 0:08:59  lr: 0.000009  loss: 3.2896 (3.6122)  time: 0.4821  data: 0.0004  max mem: 19734
Epoch: [3]  [ 200/1251]  eta: 0:08:51  lr: 0.000009  loss: 3.3575 (3.6079)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [3]  [ 210/1251]  eta: 0:08:43  lr: 0.000009  loss: 3.6609 (3.6156)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [3]  [ 220/1251]  eta: 0:08:36  lr: 0.000009  loss: 3.8555 (3.6271)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [3]  [ 230/1251]  eta: 0:08:29  lr: 0.000009  loss: 3.8394 (3.6312)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [3]  [ 240/1251]  eta: 0:08:22  lr: 0.000009  loss: 3.8571 (3.6401)  time: 0.4527  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5814, ratio_loss=0.0039, pruning_loss=0.2326, mse_loss=1.0861
Epoch: [3]  [ 250/1251]  eta: 0:08:15  lr: 0.000009  loss: 3.8749 (3.6332)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [3]  [ 260/1251]  eta: 0:08:09  lr: 0.000009  loss: 3.5464 (3.6259)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [3]  [ 270/1251]  eta: 0:08:02  lr: 0.000009  loss: 3.6046 (3.6304)  time: 0.4515  data: 0.0004  max mem: 19734
Epoch: [3]  [ 280/1251]  eta: 0:07:56  lr: 0.000009  loss: 3.6786 (3.6269)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [3]  [ 290/1251]  eta: 0:07:50  lr: 0.000009  loss: 3.7905 (3.6376)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [3]  [ 300/1251]  eta: 0:07:44  lr: 0.000009  loss: 3.8734 (3.6448)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [3]  [ 310/1251]  eta: 0:07:38  lr: 0.000009  loss: 3.8725 (3.6496)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [3]  [ 320/1251]  eta: 0:07:35  lr: 0.000009  loss: 3.4259 (3.6350)  time: 0.4901  data: 0.0003  max mem: 19734
Epoch: [3]  [ 330/1251]  eta: 0:07:29  lr: 0.000009  loss: 3.1376 (3.6244)  time: 0.4906  data: 0.0003  max mem: 19734
Epoch: [3]  [ 340/1251]  eta: 0:07:23  lr: 0.000009  loss: 3.4110 (3.6263)  time: 0.4544  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5672, ratio_loss=0.0037, pruning_loss=0.2367, mse_loss=1.1214
Epoch: [3]  [ 350/1251]  eta: 0:07:17  lr: 0.000009  loss: 3.6003 (3.6219)  time: 0.4551  data: 0.0006  max mem: 19734
Epoch: [3]  [ 360/1251]  eta: 0:07:12  lr: 0.000009  loss: 3.6653 (3.6229)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [3]  [ 370/1251]  eta: 0:07:06  lr: 0.000009  loss: 3.6653 (3.6219)  time: 0.4540  data: 0.0003  max mem: 19734
Epoch: [3]  [ 380/1251]  eta: 0:07:01  lr: 0.000009  loss: 3.4647 (3.6186)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [3]  [ 390/1251]  eta: 0:06:55  lr: 0.000009  loss: 3.6237 (3.6211)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [3]  [ 400/1251]  eta: 0:06:50  lr: 0.000009  loss: 3.7075 (3.6215)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [3]  [ 410/1251]  eta: 0:06:44  lr: 0.000009  loss: 3.7141 (3.6218)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [3]  [ 420/1251]  eta: 0:06:39  lr: 0.000009  loss: 3.8596 (3.6282)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [3]  [ 430/1251]  eta: 0:06:34  lr: 0.000009  loss: 3.9325 (3.6328)  time: 0.4638  data: 0.0005  max mem: 19734
Epoch: [3]  [ 440/1251]  eta: 0:06:29  lr: 0.000009  loss: 3.5715 (3.6228)  time: 0.4646  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5787, ratio_loss=0.0039, pruning_loss=0.2319, mse_loss=1.0986
Epoch: [3]  [ 450/1251]  eta: 0:06:23  lr: 0.000009  loss: 3.4511 (3.6254)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [3]  [ 460/1251]  eta: 0:06:18  lr: 0.000009  loss: 3.8290 (3.6290)  time: 0.4637  data: 0.0003  max mem: 19734
Epoch: [3]  [ 470/1251]  eta: 0:06:14  lr: 0.000009  loss: 3.8397 (3.6299)  time: 0.4861  data: 0.0004  max mem: 19734
Epoch: [3]  [ 480/1251]  eta: 0:06:09  lr: 0.000009  loss: 3.7759 (3.6240)  time: 0.4757  data: 0.0004  max mem: 19734
Epoch: [3]  [ 490/1251]  eta: 0:06:04  lr: 0.000009  loss: 3.7407 (3.6265)  time: 0.4542  data: 0.0003  max mem: 19734
Epoch: [3]  [ 500/1251]  eta: 0:05:58  lr: 0.000009  loss: 3.7546 (3.6272)  time: 0.4513  data: 0.0003  max mem: 19734
Epoch: [3]  [ 510/1251]  eta: 0:05:53  lr: 0.000009  loss: 3.5195 (3.6245)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [3]  [ 520/1251]  eta: 0:05:48  lr: 0.000009  loss: 3.5323 (3.6242)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [3]  [ 530/1251]  eta: 0:05:43  lr: 0.000009  loss: 3.6655 (3.6216)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [3]  [ 540/1251]  eta: 0:05:38  lr: 0.000009  loss: 3.6824 (3.6227)  time: 0.4502  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5588, ratio_loss=0.0039, pruning_loss=0.2340, mse_loss=1.1686
Epoch: [3]  [ 550/1251]  eta: 0:05:33  lr: 0.000009  loss: 3.7447 (3.6180)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [3]  [ 560/1251]  eta: 0:05:28  lr: 0.000009  loss: 3.7693 (3.6195)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [3]  [ 570/1251]  eta: 0:05:23  lr: 0.000009  loss: 3.8189 (3.6254)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [3]  [ 580/1251]  eta: 0:05:18  lr: 0.000009  loss: 3.7511 (3.6209)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [3]  [ 590/1251]  eta: 0:05:13  lr: 0.000009  loss: 3.4606 (3.6169)  time: 0.4583  data: 0.0005  max mem: 19734
Epoch: [3]  [ 600/1251]  eta: 0:05:08  lr: 0.000009  loss: 3.8693 (3.6175)  time: 0.4507  data: 0.0004  max mem: 19734
Epoch: [3]  [ 610/1251]  eta: 0:05:04  lr: 0.000009  loss: 3.8110 (3.6160)  time: 0.4902  data: 0.0004  max mem: 19734
Epoch: [3]  [ 620/1251]  eta: 0:04:59  lr: 0.000009  loss: 3.8226 (3.6204)  time: 0.4900  data: 0.0005  max mem: 19734
Epoch: [3]  [ 630/1251]  eta: 0:04:54  lr: 0.000009  loss: 3.8226 (3.6182)  time: 0.4519  data: 0.0005  max mem: 19734
Epoch: [3]  [ 640/1251]  eta: 0:04:49  lr: 0.000009  loss: 3.7810 (3.6203)  time: 0.4523  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5802, ratio_loss=0.0040, pruning_loss=0.2328, mse_loss=1.1557
Epoch: [3]  [ 650/1251]  eta: 0:04:44  lr: 0.000009  loss: 3.9245 (3.6245)  time: 0.4508  data: 0.0004  max mem: 19734
Epoch: [3]  [ 660/1251]  eta: 0:04:39  lr: 0.000009  loss: 3.9350 (3.6243)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [3]  [ 670/1251]  eta: 0:04:34  lr: 0.000009  loss: 3.8074 (3.6252)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [3]  [ 680/1251]  eta: 0:04:29  lr: 0.000009  loss: 3.8074 (3.6245)  time: 0.4516  data: 0.0007  max mem: 19734
Epoch: [3]  [ 690/1251]  eta: 0:04:24  lr: 0.000009  loss: 3.7149 (3.6247)  time: 0.4506  data: 0.0008  max mem: 19734
Epoch: [3]  [ 700/1251]  eta: 0:04:19  lr: 0.000009  loss: 3.9104 (3.6237)  time: 0.4509  data: 0.0006  max mem: 19734
Epoch: [3]  [ 710/1251]  eta: 0:04:15  lr: 0.000009  loss: 3.8635 (3.6224)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [3]  [ 720/1251]  eta: 0:04:10  lr: 0.000009  loss: 3.9088 (3.6287)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [3]  [ 730/1251]  eta: 0:04:05  lr: 0.000009  loss: 3.8884 (3.6287)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [3]  [ 740/1251]  eta: 0:04:00  lr: 0.000009  loss: 3.5085 (3.6240)  time: 0.4600  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5812, ratio_loss=0.0039, pruning_loss=0.2311, mse_loss=1.1275
Epoch: [3]  [ 750/1251]  eta: 0:03:56  lr: 0.000009  loss: 3.3822 (3.6235)  time: 0.4800  data: 0.0005  max mem: 19734
Epoch: [3]  [ 760/1251]  eta: 0:03:51  lr: 0.000009  loss: 3.8382 (3.6259)  time: 0.4932  data: 0.0003  max mem: 19734
Epoch: [3]  [ 770/1251]  eta: 0:03:46  lr: 0.000009  loss: 3.8382 (3.6255)  time: 0.4666  data: 0.0004  max mem: 19734
Epoch: [3]  [ 780/1251]  eta: 0:03:41  lr: 0.000009  loss: 3.7185 (3.6253)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [3]  [ 790/1251]  eta: 0:03:37  lr: 0.000009  loss: 3.7185 (3.6247)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [3]  [ 800/1251]  eta: 0:03:32  lr: 0.000009  loss: 3.5810 (3.6239)  time: 0.4509  data: 0.0003  max mem: 19734
Epoch: [3]  [ 810/1251]  eta: 0:03:27  lr: 0.000009  loss: 3.7318 (3.6260)  time: 0.4509  data: 0.0003  max mem: 19734
Epoch: [3]  [ 820/1251]  eta: 0:03:22  lr: 0.000009  loss: 3.7794 (3.6264)  time: 0.4507  data: 0.0004  max mem: 19734
Epoch: [3]  [ 830/1251]  eta: 0:03:17  lr: 0.000009  loss: 3.7794 (3.6271)  time: 0.4510  data: 0.0004  max mem: 19734
Epoch: [3]  [ 840/1251]  eta: 0:03:12  lr: 0.000009  loss: 3.9001 (3.6298)  time: 0.4514  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6193, ratio_loss=0.0040, pruning_loss=0.2290, mse_loss=1.1101
Epoch: [3]  [ 850/1251]  eta: 0:03:08  lr: 0.000009  loss: 3.7696 (3.6306)  time: 0.4504  data: 0.0005  max mem: 19734
Epoch: [3]  [ 860/1251]  eta: 0:03:03  lr: 0.000009  loss: 3.6385 (3.6284)  time: 0.4512  data: 0.0005  max mem: 19734
Epoch: [3]  [ 870/1251]  eta: 0:02:58  lr: 0.000009  loss: 3.5527 (3.6299)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [3]  [ 880/1251]  eta: 0:02:53  lr: 0.000009  loss: 3.7914 (3.6298)  time: 0.4618  data: 0.0005  max mem: 19734
Epoch: [3]  [ 890/1251]  eta: 0:02:49  lr: 0.000009  loss: 3.7914 (3.6315)  time: 0.4721  data: 0.0005  max mem: 19734
Epoch: [3]  [ 900/1251]  eta: 0:02:44  lr: 0.000009  loss: 3.7863 (3.6324)  time: 0.4802  data: 0.0005  max mem: 19734
Epoch: [3]  [ 910/1251]  eta: 0:02:39  lr: 0.000009  loss: 3.6717 (3.6333)  time: 0.4689  data: 0.0007  max mem: 19734
Epoch: [3]  [ 920/1251]  eta: 0:02:35  lr: 0.000009  loss: 3.6717 (3.6345)  time: 0.4505  data: 0.0006  max mem: 19734
Epoch: [3]  [ 930/1251]  eta: 0:02:30  lr: 0.000009  loss: 3.7962 (3.6355)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [3]  [ 940/1251]  eta: 0:02:25  lr: 0.000009  loss: 3.9761 (3.6378)  time: 0.4524  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6820, ratio_loss=0.0038, pruning_loss=0.2240, mse_loss=1.0532
Epoch: [3]  [ 950/1251]  eta: 0:02:20  lr: 0.000009  loss: 3.8748 (3.6401)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [3]  [ 960/1251]  eta: 0:02:16  lr: 0.000009  loss: 3.6370 (3.6392)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [3]  [ 970/1251]  eta: 0:02:11  lr: 0.000009  loss: 3.8243 (3.6407)  time: 0.4512  data: 0.0006  max mem: 19734
Epoch: [3]  [ 980/1251]  eta: 0:02:06  lr: 0.000009  loss: 3.8371 (3.6388)  time: 0.4515  data: 0.0004  max mem: 19734
Epoch: [3]  [ 990/1251]  eta: 0:02:02  lr: 0.000009  loss: 3.7216 (3.6394)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [3]  [1000/1251]  eta: 0:01:57  lr: 0.000009  loss: 3.8359 (3.6398)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [3]  [1010/1251]  eta: 0:01:52  lr: 0.000009  loss: 3.8420 (3.6401)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [3]  [1020/1251]  eta: 0:01:47  lr: 0.000009  loss: 3.7248 (3.6369)  time: 0.4636  data: 0.0005  max mem: 19734
Epoch: [3]  [1030/1251]  eta: 0:01:43  lr: 0.000009  loss: 3.6032 (3.6365)  time: 0.4633  data: 0.0005  max mem: 19734
Epoch: [3]  [1040/1251]  eta: 0:01:38  lr: 0.000009  loss: 3.7625 (3.6369)  time: 0.4747  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5609, ratio_loss=0.0039, pruning_loss=0.2295, mse_loss=1.1159
Epoch: [3]  [1050/1251]  eta: 0:01:33  lr: 0.000009  loss: 3.8340 (3.6372)  time: 0.4848  data: 0.0004  max mem: 19734
Epoch: [3]  [1060/1251]  eta: 0:01:29  lr: 0.000009  loss: 3.6154 (3.6369)  time: 0.4625  data: 0.0006  max mem: 19734
Epoch: [3]  [1070/1251]  eta: 0:01:24  lr: 0.000009  loss: 3.4815 (3.6346)  time: 0.4520  data: 0.0005  max mem: 19734
Epoch: [3]  [1080/1251]  eta: 0:01:19  lr: 0.000009  loss: 3.7248 (3.6353)  time: 0.4501  data: 0.0004  max mem: 19734
Epoch: [3]  [1090/1251]  eta: 0:01:15  lr: 0.000009  loss: 3.7359 (3.6354)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [3]  [1100/1251]  eta: 0:01:10  lr: 0.000009  loss: 3.6527 (3.6333)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [3]  [1110/1251]  eta: 0:01:05  lr: 0.000009  loss: 3.3791 (3.6296)  time: 0.4511  data: 0.0004  max mem: 19734
Epoch: [3]  [1120/1251]  eta: 0:01:01  lr: 0.000009  loss: 3.3791 (3.6279)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [3]  [1130/1251]  eta: 0:00:56  lr: 0.000009  loss: 3.5589 (3.6273)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [3]  [1140/1251]  eta: 0:00:51  lr: 0.000009  loss: 3.7021 (3.6283)  time: 0.4522  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4825, ratio_loss=0.0041, pruning_loss=0.2305, mse_loss=1.1194
Epoch: [3]  [1150/1251]  eta: 0:00:47  lr: 0.000009  loss: 3.6705 (3.6271)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [3]  [1160/1251]  eta: 0:00:42  lr: 0.000009  loss: 3.4519 (3.6268)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [3]  [1170/1251]  eta: 0:00:37  lr: 0.000009  loss: 3.7551 (3.6274)  time: 0.4576  data: 0.0006  max mem: 19734
Epoch: [3]  [1180/1251]  eta: 0:00:33  lr: 0.000009  loss: 3.8195 (3.6275)  time: 0.4568  data: 0.0006  max mem: 19734
Epoch: [3]  [1190/1251]  eta: 0:00:28  lr: 0.000009  loss: 3.8220 (3.6285)  time: 0.4770  data: 0.0006  max mem: 19734
Epoch: [3]  [1200/1251]  eta: 0:00:23  lr: 0.000009  loss: 3.7841 (3.6292)  time: 0.4738  data: 0.0005  max mem: 19734
Epoch: [3]  [1210/1251]  eta: 0:00:19  lr: 0.000009  loss: 3.6097 (3.6268)  time: 0.4443  data: 0.0001  max mem: 19734
Epoch: [3]  [1220/1251]  eta: 0:00:14  lr: 0.000009  loss: 3.6097 (3.6281)  time: 0.4435  data: 0.0001  max mem: 19734
Epoch: [3]  [1230/1251]  eta: 0:00:09  lr: 0.000009  loss: 3.9835 (3.6313)  time: 0.4431  data: 0.0001  max mem: 19734
Epoch: [3]  [1240/1251]  eta: 0:00:05  lr: 0.000009  loss: 3.9481 (3.6307)  time: 0.4437  data: 0.0001  max mem: 19734
loss info: cls_loss=3.6477, ratio_loss=0.0040, pruning_loss=0.2252, mse_loss=1.0630
Epoch: [3]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 3.8358 (3.6326)  time: 0.4437  data: 0.0001  max mem: 19734
Epoch: [3] Total time: 0:09:42 (0.4658 s / it)
Averaged stats: lr: 0.000009  loss: 3.8358 (3.6368)
Test:  [  0/261]  eta: 2:33:25  loss: 1.0812 (1.0812)  acc1: 79.6875 (79.6875)  acc5: 93.7500 (93.7500)  time: 35.2706  data: 35.1386  max mem: 19734
Test:  [ 10/261]  eta: 0:14:10  loss: 1.0679 (1.0923)  acc1: 83.3333 (81.1553)  acc5: 93.7500 (93.5606)  time: 3.3877  data: 3.2142  max mem: 19734
Test:  [ 20/261]  eta: 0:07:37  loss: 1.2350 (1.2028)  acc1: 74.4792 (76.6121)  acc5: 92.7083 (92.8819)  time: 0.2285  data: 0.0231  max mem: 19734
Test:  [ 30/261]  eta: 0:05:18  loss: 1.0816 (1.1199)  acc1: 80.7292 (79.5363)  acc5: 93.7500 (93.4308)  time: 0.2716  data: 0.0190  max mem: 19734
Test:  [ 40/261]  eta: 0:04:02  loss: 0.8808 (1.0949)  acc1: 85.4167 (80.1956)  acc5: 94.2708 (93.5340)  time: 0.2604  data: 0.0133  max mem: 19734
Test:  [ 50/261]  eta: 0:03:11  loss: 1.3247 (1.1746)  acc1: 72.9167 (77.7471)  acc5: 92.1875 (92.8615)  time: 0.1839  data: 0.0107  max mem: 19734
Test:  [ 60/261]  eta: 0:02:37  loss: 1.3630 (1.1891)  acc1: 71.3542 (76.8272)  acc5: 91.6667 (92.9730)  time: 0.1331  data: 0.0094  max mem: 19734
Test:  [ 70/261]  eta: 0:02:16  loss: 1.2870 (1.1892)  acc1: 71.3542 (76.2911)  acc5: 93.7500 (93.2365)  time: 0.2240  data: 0.0927  max mem: 19734
Test:  [ 80/261]  eta: 0:01:56  loss: 1.1516 (1.1912)  acc1: 77.6042 (76.5368)  acc5: 95.3125 (93.4414)  time: 0.2251  data: 0.0927  max mem: 19734
Test:  [ 90/261]  eta: 0:01:45  loss: 1.1855 (1.1819)  acc1: 79.1667 (76.8658)  acc5: 94.2708 (93.5096)  time: 0.2671  data: 0.1380  max mem: 19734
Test:  [100/261]  eta: 0:01:35  loss: 1.2007 (1.1876)  acc1: 79.1667 (76.8152)  acc5: 93.2292 (93.5283)  time: 0.3921  data: 0.2688  max mem: 19734
Test:  [110/261]  eta: 0:01:28  loss: 1.3302 (1.2170)  acc1: 73.4375 (76.3889)  acc5: 92.1875 (93.1353)  time: 0.4365  data: 0.2954  max mem: 19734
Test:  [120/261]  eta: 0:01:18  loss: 1.5959 (1.2573)  acc1: 67.1875 (75.4649)  acc5: 86.9792 (92.5060)  time: 0.3498  data: 0.1671  max mem: 19734
Test:  [130/261]  eta: 0:01:11  loss: 1.7375 (1.3093)  acc1: 62.5000 (74.4911)  acc5: 83.8542 (91.7979)  time: 0.3369  data: 0.1869  max mem: 19734
Test:  [140/261]  eta: 0:01:06  loss: 1.6828 (1.3379)  acc1: 63.0208 (73.8586)  acc5: 85.9375 (91.4487)  time: 0.5391  data: 0.3609  max mem: 19734
Test:  [150/261]  eta: 0:01:01  loss: 1.5914 (1.3511)  acc1: 68.2292 (73.7514)  acc5: 87.5000 (91.1596)  time: 0.6302  data: 0.4106  max mem: 19734
Test:  [160/261]  eta: 0:00:53  loss: 1.4718 (1.3751)  acc1: 71.3542 (73.3825)  acc5: 87.5000 (90.8256)  time: 0.3683  data: 0.2351  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.9080 (1.4105)  acc1: 59.8958 (72.5116)  acc5: 84.3750 (90.4027)  time: 0.0913  data: 0.0113  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.8500 (1.4297)  acc1: 60.9375 (72.1369)  acc5: 84.8958 (90.1905)  time: 0.0825  data: 0.0070  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.7907 (1.4472)  acc1: 63.5417 (71.8586)  acc5: 85.9375 (89.9896)  time: 0.0689  data: 0.0022  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.7680 (1.4634)  acc1: 66.1458 (71.5096)  acc5: 85.9375 (89.7569)  time: 0.0637  data: 0.0019  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.8008 (1.4835)  acc1: 65.1042 (71.1789)  acc5: 84.3750 (89.4969)  time: 0.0635  data: 0.0018  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 2.0031 (1.5034)  acc1: 60.9375 (70.6896)  acc5: 82.8125 (89.2652)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.8749 (1.5171)  acc1: 59.8958 (70.3553)  acc5: 83.8542 (89.0896)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.8260 (1.5256)  acc1: 61.9792 (70.1310)  acc5: 85.9375 (89.0128)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.4286 (1.5137)  acc1: 70.3125 (70.3540)  acc5: 89.5833 (89.1538)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.2983 (1.5103)  acc1: 72.3958 (70.4120)  acc5: 93.2292 (89.2520)  time: 0.0596  data: 0.0001  max mem: 19734
Test: Total time: 0:01:31 (0.3525 s / it)
* Acc@1 70.412 Acc@5 89.253 loss 1.510
Accuracy of the network on the 50000 test images: 70.4%
Max accuracy: 70.41%
Epoch: [4]  [   0/1251]  eta: 5:26:26  lr: 0.000012  loss: 4.2965 (4.2965)  time: 15.6567  data: 15.1049  max mem: 19734
Epoch: [4]  [  10/1251]  eta: 0:40:05  lr: 0.000012  loss: 3.8088 (3.7197)  time: 1.9387  data: 1.3934  max mem: 19734
Epoch: [4]  [  20/1251]  eta: 0:25:19  lr: 0.000012  loss: 3.7896 (3.6317)  time: 0.5136  data: 0.0113  max mem: 19734
Epoch: [4]  [  30/1251]  eta: 0:20:04  lr: 0.000012  loss: 3.5969 (3.5280)  time: 0.4621  data: 0.0005  max mem: 19734
Epoch: [4]  [  40/1251]  eta: 0:17:18  lr: 0.000012  loss: 3.6619 (3.5760)  time: 0.4622  data: 0.0005  max mem: 19734
Epoch: [4]  [  50/1251]  eta: 0:15:36  lr: 0.000012  loss: 3.7003 (3.5660)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [4]  [  60/1251]  eta: 0:14:26  lr: 0.000012  loss: 3.8004 (3.5942)  time: 0.4610  data: 0.0004  max mem: 19734
Epoch: [4]  [  70/1251]  eta: 0:13:47  lr: 0.000012  loss: 3.7721 (3.5649)  time: 0.4994  data: 0.0006  max mem: 19734
Epoch: [4]  [  80/1251]  eta: 0:13:09  lr: 0.000012  loss: 3.5985 (3.5502)  time: 0.5097  data: 0.0006  max mem: 19734
Epoch: [4]  [  90/1251]  eta: 0:12:34  lr: 0.000012  loss: 3.5264 (3.5337)  time: 0.4706  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4864, ratio_loss=0.0041, pruning_loss=0.2298, mse_loss=1.0887
Epoch: [4]  [ 100/1251]  eta: 0:12:06  lr: 0.000012  loss: 3.6279 (3.5555)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [4]  [ 110/1251]  eta: 0:11:42  lr: 0.000012  loss: 3.6928 (3.5530)  time: 0.4593  data: 0.0004  max mem: 19734
Epoch: [4]  [ 120/1251]  eta: 0:11:21  lr: 0.000012  loss: 3.3792 (3.5484)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [4]  [ 130/1251]  eta: 0:11:03  lr: 0.000012  loss: 3.6093 (3.5635)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [4]  [ 140/1251]  eta: 0:10:47  lr: 0.000012  loss: 3.8966 (3.5840)  time: 0.4603  data: 0.0004  max mem: 19734
Epoch: [4]  [ 150/1251]  eta: 0:10:33  lr: 0.000012  loss: 3.8097 (3.5783)  time: 0.4669  data: 0.0004  max mem: 19734
Epoch: [4]  [ 160/1251]  eta: 0:10:19  lr: 0.000012  loss: 3.6836 (3.5959)  time: 0.4633  data: 0.0004  max mem: 19734
Epoch: [4]  [ 170/1251]  eta: 0:10:06  lr: 0.000012  loss: 3.7089 (3.5878)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [4]  [ 180/1251]  eta: 0:09:54  lr: 0.000012  loss: 3.7489 (3.5953)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [4]  [ 190/1251]  eta: 0:09:43  lr: 0.000012  loss: 3.6458 (3.5833)  time: 0.4537  data: 0.0007  max mem: 19734
loss info: cls_loss=3.5978, ratio_loss=0.0041, pruning_loss=0.2241, mse_loss=1.0818
Epoch: [4]  [ 200/1251]  eta: 0:09:32  lr: 0.000012  loss: 3.6905 (3.5951)  time: 0.4550  data: 0.0007  max mem: 19734
Epoch: [4]  [ 210/1251]  eta: 0:09:25  lr: 0.000012  loss: 3.9977 (3.6063)  time: 0.4775  data: 0.0005  max mem: 19734
Epoch: [4]  [ 220/1251]  eta: 0:09:18  lr: 0.000012  loss: 3.9243 (3.6109)  time: 0.5107  data: 0.0005  max mem: 19734
Epoch: [4]  [ 230/1251]  eta: 0:09:09  lr: 0.000012  loss: 3.8116 (3.6051)  time: 0.4904  data: 0.0004  max mem: 19734
Epoch: [4]  [ 240/1251]  eta: 0:09:00  lr: 0.000012  loss: 3.9468 (3.6231)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [4]  [ 250/1251]  eta: 0:08:52  lr: 0.000012  loss: 3.9819 (3.6258)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [4]  [ 260/1251]  eta: 0:08:44  lr: 0.000012  loss: 3.8148 (3.6272)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [4]  [ 270/1251]  eta: 0:08:36  lr: 0.000012  loss: 3.8148 (3.6285)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [4]  [ 280/1251]  eta: 0:08:28  lr: 0.000012  loss: 3.9445 (3.6340)  time: 0.4536  data: 0.0006  max mem: 19734
Epoch: [4]  [ 290/1251]  eta: 0:08:20  lr: 0.000012  loss: 3.9279 (3.6310)  time: 0.4542  data: 0.0007  max mem: 19734
loss info: cls_loss=3.6830, ratio_loss=0.0041, pruning_loss=0.2224, mse_loss=1.0234
Epoch: [4]  [ 300/1251]  eta: 0:08:14  lr: 0.000012  loss: 3.7068 (3.6389)  time: 0.4645  data: 0.0006  max mem: 19734
Epoch: [4]  [ 310/1251]  eta: 0:08:07  lr: 0.000012  loss: 3.7068 (3.6304)  time: 0.4649  data: 0.0005  max mem: 19734
Epoch: [4]  [ 320/1251]  eta: 0:08:00  lr: 0.000012  loss: 3.4823 (3.6256)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [4]  [ 330/1251]  eta: 0:07:53  lr: 0.000012  loss: 3.4245 (3.6175)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [4]  [ 340/1251]  eta: 0:07:46  lr: 0.000012  loss: 3.8117 (3.6218)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [4]  [ 350/1251]  eta: 0:07:40  lr: 0.000012  loss: 3.9175 (3.6253)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [4]  [ 360/1251]  eta: 0:07:35  lr: 0.000012  loss: 3.7871 (3.6231)  time: 0.4886  data: 0.0004  max mem: 19734
Epoch: [4]  [ 370/1251]  eta: 0:07:29  lr: 0.000012  loss: 3.5022 (3.6219)  time: 0.5026  data: 0.0004  max mem: 19734
Epoch: [4]  [ 380/1251]  eta: 0:07:23  lr: 0.000012  loss: 3.5579 (3.6185)  time: 0.4714  data: 0.0005  max mem: 19734
Epoch: [4]  [ 390/1251]  eta: 0:07:16  lr: 0.000012  loss: 3.6776 (3.6164)  time: 0.4580  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5293, ratio_loss=0.0043, pruning_loss=0.2287, mse_loss=1.0761
Epoch: [4]  [ 400/1251]  eta: 0:07:10  lr: 0.000012  loss: 3.7900 (3.6200)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [4]  [ 410/1251]  eta: 0:07:04  lr: 0.000012  loss: 3.7677 (3.6222)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [4]  [ 420/1251]  eta: 0:06:58  lr: 0.000012  loss: 3.6632 (3.6174)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [4]  [ 430/1251]  eta: 0:06:52  lr: 0.000012  loss: 3.7179 (3.6177)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [4]  [ 440/1251]  eta: 0:06:46  lr: 0.000012  loss: 3.5333 (3.6155)  time: 0.4614  data: 0.0006  max mem: 19734
Epoch: [4]  [ 450/1251]  eta: 0:06:41  lr: 0.000012  loss: 3.5333 (3.6161)  time: 0.4610  data: 0.0006  max mem: 19734
Epoch: [4]  [ 460/1251]  eta: 0:06:35  lr: 0.000012  loss: 3.8177 (3.6194)  time: 0.4581  data: 0.0007  max mem: 19734
Epoch: [4]  [ 470/1251]  eta: 0:06:29  lr: 0.000012  loss: 3.9808 (3.6229)  time: 0.4595  data: 0.0006  max mem: 19734
Epoch: [4]  [ 480/1251]  eta: 0:06:24  lr: 0.000012  loss: 3.5238 (3.6166)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [4]  [ 490/1251]  eta: 0:06:18  lr: 0.000012  loss: 3.6211 (3.6203)  time: 0.4555  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5636, ratio_loss=0.0043, pruning_loss=0.2258, mse_loss=1.0897
Epoch: [4]  [ 500/1251]  eta: 0:06:13  lr: 0.000012  loss: 3.7367 (3.6207)  time: 0.4783  data: 0.0004  max mem: 19734
Epoch: [4]  [ 510/1251]  eta: 0:06:08  lr: 0.000012  loss: 3.4353 (3.6170)  time: 0.4950  data: 0.0004  max mem: 19734
Epoch: [4]  [ 520/1251]  eta: 0:06:02  lr: 0.000012  loss: 3.6795 (3.6195)  time: 0.4709  data: 0.0004  max mem: 19734
Epoch: [4]  [ 530/1251]  eta: 0:05:57  lr: 0.000012  loss: 3.5077 (3.6119)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [4]  [ 540/1251]  eta: 0:05:51  lr: 0.000012  loss: 3.5102 (3.6108)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [4]  [ 550/1251]  eta: 0:05:46  lr: 0.000012  loss: 3.7785 (3.6088)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [4]  [ 560/1251]  eta: 0:05:40  lr: 0.000012  loss: 3.7354 (3.6100)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [4]  [ 570/1251]  eta: 0:05:35  lr: 0.000012  loss: 3.5416 (3.6067)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [4]  [ 580/1251]  eta: 0:05:30  lr: 0.000012  loss: 3.6711 (3.6077)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [4]  [ 590/1251]  eta: 0:05:24  lr: 0.000012  loss: 3.8370 (3.6119)  time: 0.4600  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5335, ratio_loss=0.0044, pruning_loss=0.2246, mse_loss=1.0011
Epoch: [4]  [ 600/1251]  eta: 0:05:19  lr: 0.000012  loss: 3.8370 (3.6088)  time: 0.4592  data: 0.0004  max mem: 19734
Epoch: [4]  [ 610/1251]  eta: 0:05:14  lr: 0.000012  loss: 3.6378 (3.6083)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [4]  [ 620/1251]  eta: 0:05:08  lr: 0.000012  loss: 3.7435 (3.6086)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [4]  [ 630/1251]  eta: 0:05:03  lr: 0.000012  loss: 3.9012 (3.6141)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [4]  [ 640/1251]  eta: 0:04:58  lr: 0.000012  loss: 3.9012 (3.6132)  time: 0.4648  data: 0.0008  max mem: 19734
Epoch: [4]  [ 650/1251]  eta: 0:04:54  lr: 0.000012  loss: 3.6618 (3.6126)  time: 0.4968  data: 0.0009  max mem: 19734
Epoch: [4]  [ 660/1251]  eta: 0:04:48  lr: 0.000012  loss: 3.4592 (3.6105)  time: 0.4918  data: 0.0004  max mem: 19734
Epoch: [4]  [ 670/1251]  eta: 0:04:43  lr: 0.000012  loss: 3.4592 (3.6090)  time: 0.4600  data: 0.0006  max mem: 19734
Epoch: [4]  [ 680/1251]  eta: 0:04:38  lr: 0.000012  loss: 3.5443 (3.6107)  time: 0.4530  data: 0.0006  max mem: 19734
Epoch: [4]  [ 690/1251]  eta: 0:04:33  lr: 0.000012  loss: 3.7111 (3.6108)  time: 0.4536  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5610, ratio_loss=0.0042, pruning_loss=0.2226, mse_loss=1.0262
Epoch: [4]  [ 700/1251]  eta: 0:04:28  lr: 0.000012  loss: 3.8192 (3.6126)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [4]  [ 710/1251]  eta: 0:04:23  lr: 0.000012  loss: 3.8192 (3.6104)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [4]  [ 720/1251]  eta: 0:04:18  lr: 0.000012  loss: 3.3789 (3.6064)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [4]  [ 730/1251]  eta: 0:04:12  lr: 0.000012  loss: 3.4268 (3.6051)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [4]  [ 740/1251]  eta: 0:04:07  lr: 0.000012  loss: 3.5536 (3.6039)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [4]  [ 750/1251]  eta: 0:04:02  lr: 0.000012  loss: 3.6548 (3.6059)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [4]  [ 760/1251]  eta: 0:03:57  lr: 0.000012  loss: 3.9413 (3.6082)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [4]  [ 770/1251]  eta: 0:03:52  lr: 0.000012  loss: 3.6546 (3.6073)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [4]  [ 780/1251]  eta: 0:03:47  lr: 0.000012  loss: 3.6546 (3.6066)  time: 0.4535  data: 0.0006  max mem: 19734
Epoch: [4]  [ 790/1251]  eta: 0:03:43  lr: 0.000012  loss: 3.5341 (3.6036)  time: 0.4769  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5046, ratio_loss=0.0046, pruning_loss=0.2214, mse_loss=1.0394
Epoch: [4]  [ 800/1251]  eta: 0:03:38  lr: 0.000012  loss: 3.5341 (3.6039)  time: 0.4955  data: 0.0006  max mem: 19734
Epoch: [4]  [ 810/1251]  eta: 0:03:33  lr: 0.000012  loss: 3.6354 (3.6037)  time: 0.4704  data: 0.0006  max mem: 19734
Epoch: [4]  [ 820/1251]  eta: 0:03:28  lr: 0.000012  loss: 3.6692 (3.6056)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [4]  [ 830/1251]  eta: 0:03:23  lr: 0.000012  loss: 3.7803 (3.6067)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [4]  [ 840/1251]  eta: 0:03:18  lr: 0.000012  loss: 3.7419 (3.6068)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [4]  [ 850/1251]  eta: 0:03:13  lr: 0.000012  loss: 3.6001 (3.6053)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [4]  [ 860/1251]  eta: 0:03:08  lr: 0.000012  loss: 3.8515 (3.6097)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [4]  [ 870/1251]  eta: 0:03:03  lr: 0.000012  loss: 3.9675 (3.6133)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [4]  [ 880/1251]  eta: 0:02:58  lr: 0.000012  loss: 3.8903 (3.6136)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [4]  [ 890/1251]  eta: 0:02:53  lr: 0.000012  loss: 3.6509 (3.6119)  time: 0.4654  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5887, ratio_loss=0.0043, pruning_loss=0.2186, mse_loss=1.0235
Epoch: [4]  [ 900/1251]  eta: 0:02:48  lr: 0.000012  loss: 3.3146 (3.6091)  time: 0.4677  data: 0.0005  max mem: 19734
Epoch: [4]  [ 910/1251]  eta: 0:02:43  lr: 0.000012  loss: 3.7014 (3.6114)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [4]  [ 920/1251]  eta: 0:02:38  lr: 0.000012  loss: 3.9237 (3.6122)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [4]  [ 930/1251]  eta: 0:02:34  lr: 0.000012  loss: 3.9060 (3.6127)  time: 0.4641  data: 0.0004  max mem: 19734
Epoch: [4]  [ 940/1251]  eta: 0:02:29  lr: 0.000012  loss: 3.7926 (3.6137)  time: 0.4916  data: 0.0004  max mem: 19734
Epoch: [4]  [ 950/1251]  eta: 0:02:24  lr: 0.000012  loss: 3.7438 (3.6130)  time: 0.4903  data: 0.0004  max mem: 19734
Epoch: [4]  [ 960/1251]  eta: 0:02:19  lr: 0.000012  loss: 3.7383 (3.6115)  time: 0.4634  data: 0.0004  max mem: 19734
Epoch: [4]  [ 970/1251]  eta: 0:02:14  lr: 0.000012  loss: 3.7836 (3.6123)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [4]  [ 980/1251]  eta: 0:02:09  lr: 0.000012  loss: 3.7622 (3.6128)  time: 0.4510  data: 0.0004  max mem: 19734
Epoch: [4]  [ 990/1251]  eta: 0:02:05  lr: 0.000012  loss: 3.5953 (3.6123)  time: 0.4533  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6152, ratio_loss=0.0045, pruning_loss=0.2183, mse_loss=1.0440
Epoch: [4]  [1000/1251]  eta: 0:02:00  lr: 0.000012  loss: 3.7381 (3.6130)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [4]  [1010/1251]  eta: 0:01:55  lr: 0.000012  loss: 3.8075 (3.6149)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [4]  [1020/1251]  eta: 0:01:50  lr: 0.000012  loss: 3.9004 (3.6140)  time: 0.4553  data: 0.0005  max mem: 19734
Epoch: [4]  [1030/1251]  eta: 0:01:45  lr: 0.000012  loss: 3.2628 (3.6107)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [4]  [1040/1251]  eta: 0:01:40  lr: 0.000012  loss: 3.4945 (3.6101)  time: 0.4613  data: 0.0004  max mem: 19734
Epoch: [4]  [1050/1251]  eta: 0:01:36  lr: 0.000012  loss: 3.5444 (3.6098)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [4]  [1060/1251]  eta: 0:01:31  lr: 0.000012  loss: 3.8119 (3.6106)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [4]  [1070/1251]  eta: 0:01:26  lr: 0.000012  loss: 3.8159 (3.6120)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [4]  [1080/1251]  eta: 0:01:21  lr: 0.000012  loss: 3.7023 (3.6120)  time: 0.4814  data: 0.0004  max mem: 19734
Epoch: [4]  [1090/1251]  eta: 0:01:16  lr: 0.000012  loss: 3.7107 (3.6121)  time: 0.5011  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5594, ratio_loss=0.0045, pruning_loss=0.2180, mse_loss=1.0226
Epoch: [4]  [1100/1251]  eta: 0:01:12  lr: 0.000012  loss: 3.6006 (3.6123)  time: 0.4720  data: 0.0004  max mem: 19734
Epoch: [4]  [1110/1251]  eta: 0:01:07  lr: 0.000012  loss: 3.3641 (3.6091)  time: 0.4517  data: 0.0005  max mem: 19734
Epoch: [4]  [1120/1251]  eta: 0:01:02  lr: 0.000012  loss: 3.4081 (3.6085)  time: 0.4516  data: 0.0005  max mem: 19734
Epoch: [4]  [1130/1251]  eta: 0:00:57  lr: 0.000012  loss: 3.5694 (3.6079)  time: 0.4539  data: 0.0007  max mem: 19734
Epoch: [4]  [1140/1251]  eta: 0:00:52  lr: 0.000012  loss: 3.3281 (3.6045)  time: 0.4554  data: 0.0007  max mem: 19734
Epoch: [4]  [1150/1251]  eta: 0:00:48  lr: 0.000012  loss: 3.3665 (3.6045)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [4]  [1160/1251]  eta: 0:00:43  lr: 0.000012  loss: 3.5772 (3.6033)  time: 0.4545  data: 0.0005  max mem: 19734
Epoch: [4]  [1170/1251]  eta: 0:00:38  lr: 0.000012  loss: 3.6707 (3.6050)  time: 0.4519  data: 0.0005  max mem: 19734
Epoch: [4]  [1180/1251]  eta: 0:00:33  lr: 0.000012  loss: 3.7110 (3.6045)  time: 0.4513  data: 0.0005  max mem: 19734
Epoch: [4]  [1190/1251]  eta: 0:00:29  lr: 0.000012  loss: 3.6616 (3.6041)  time: 0.4616  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4818, ratio_loss=0.0047, pruning_loss=0.2221, mse_loss=1.0150
Epoch: [4]  [1200/1251]  eta: 0:00:24  lr: 0.000012  loss: 3.6663 (3.6052)  time: 0.4577  data: 0.0005  max mem: 19734
Epoch: [4]  [1210/1251]  eta: 0:00:19  lr: 0.000012  loss: 3.3784 (3.6011)  time: 0.4440  data: 0.0002  max mem: 19734
Epoch: [4]  [1220/1251]  eta: 0:00:14  lr: 0.000012  loss: 3.3839 (3.6014)  time: 0.4474  data: 0.0002  max mem: 19734
Epoch: [4]  [1230/1251]  eta: 0:00:09  lr: 0.000012  loss: 3.5862 (3.6000)  time: 0.4673  data: 0.0002  max mem: 19734
Epoch: [4]  [1240/1251]  eta: 0:00:05  lr: 0.000012  loss: 3.8608 (3.6013)  time: 0.4710  data: 0.0002  max mem: 19734
Epoch: [4]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 3.8806 (3.6036)  time: 0.4524  data: 0.0002  max mem: 19734
Epoch: [4] Total time: 0:09:55 (0.4758 s / it)
Averaged stats: lr: 0.000012  loss: 3.8806 (3.6012)
Test:  [  0/261]  eta: 0:54:34  loss: 1.1867 (1.1867)  acc1: 78.6458 (78.6458)  acc5: 91.6667 (91.6667)  time: 12.5446  data: 12.3931  max mem: 19734
Test:  [ 10/261]  eta: 0:11:27  loss: 1.1867 (1.2226)  acc1: 80.2083 (80.0189)  acc5: 93.7500 (92.4242)  time: 2.7382  data: 2.6008  max mem: 19734
Test:  [ 20/261]  eta: 0:06:17  loss: 1.2842 (1.3024)  acc1: 76.5625 (75.8929)  acc5: 91.1458 (92.0139)  time: 1.0156  data: 0.8199  max mem: 19734
Test:  [ 30/261]  eta: 0:04:30  loss: 1.1930 (1.2364)  acc1: 79.6875 (78.6794)  acc5: 92.1875 (92.5403)  time: 0.3099  data: 0.0174  max mem: 19734
Test:  [ 40/261]  eta: 0:03:29  loss: 1.1197 (1.2144)  acc1: 84.3750 (79.2937)  acc5: 92.7083 (92.5178)  time: 0.3007  data: 0.0197  max mem: 19734
Test:  [ 50/261]  eta: 0:02:51  loss: 1.3656 (1.2791)  acc1: 73.9583 (77.2774)  acc5: 90.6250 (91.8709)  time: 0.2526  data: 0.0163  max mem: 19734
Test:  [ 60/261]  eta: 0:02:26  loss: 1.4028 (1.2897)  acc1: 71.8750 (76.6223)  acc5: 90.6250 (91.8887)  time: 0.2728  data: 0.0144  max mem: 19734
Test:  [ 70/261]  eta: 0:02:07  loss: 1.2782 (1.2877)  acc1: 72.3958 (76.0270)  acc5: 92.1875 (92.1655)  time: 0.3046  data: 0.0186  max mem: 19734
Test:  [ 80/261]  eta: 0:01:50  loss: 1.3160 (1.3064)  acc1: 75.0000 (76.1638)  acc5: 93.2292 (92.2068)  time: 0.2508  data: 0.0145  max mem: 19734
Test:  [ 90/261]  eta: 0:01:37  loss: 1.3696 (1.3012)  acc1: 76.5625 (76.4538)  acc5: 92.1875 (92.2676)  time: 0.2216  data: 0.0798  max mem: 19734
Test:  [100/261]  eta: 0:01:37  loss: 1.2776 (1.3022)  acc1: 76.5625 (76.4284)  acc5: 92.1875 (92.2855)  time: 0.5949  data: 0.4658  max mem: 19734
Test:  [110/261]  eta: 0:01:25  loss: 1.4147 (1.3292)  acc1: 74.4792 (76.0182)  acc5: 90.6250 (91.9482)  time: 0.5538  data: 0.4012  max mem: 19734
Test:  [120/261]  eta: 0:01:16  loss: 1.6271 (1.3612)  acc1: 68.2292 (75.1636)  acc5: 88.0208 (91.4170)  time: 0.2049  data: 0.0205  max mem: 19734
Test:  [130/261]  eta: 0:01:07  loss: 1.7573 (1.4032)  acc1: 64.0625 (74.2525)  acc5: 83.8542 (90.8357)  time: 0.2481  data: 0.0140  max mem: 19734
Test:  [140/261]  eta: 0:01:01  loss: 1.7567 (1.4247)  acc1: 64.0625 (73.5890)  acc5: 86.4583 (90.6176)  time: 0.2877  data: 0.0396  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: 1.6219 (1.4293)  acc1: 69.2708 (73.6134)  acc5: 88.0208 (90.4456)  time: 0.2780  data: 0.0413  max mem: 19734
Test:  [160/261]  eta: 0:00:46  loss: 1.5331 (1.4481)  acc1: 73.9583 (73.2402)  acc5: 86.4583 (90.1333)  time: 0.1878  data: 0.0090  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: 1.9207 (1.4799)  acc1: 59.3750 (72.3715)  acc5: 83.8542 (89.7722)  time: 0.4292  data: 0.2908  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: 2.0282 (1.5007)  acc1: 59.3750 (71.9527)  acc5: 83.8542 (89.5085)  time: 0.4988  data: 0.3679  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: 1.8439 (1.5164)  acc1: 62.5000 (71.6787)  acc5: 85.4167 (89.3106)  time: 0.2003  data: 0.0882  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.7657 (1.5293)  acc1: 65.6250 (71.3645)  acc5: 85.4167 (89.0832)  time: 0.1779  data: 0.0843  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.6927 (1.5431)  acc1: 65.6250 (71.0826)  acc5: 84.8958 (88.8724)  time: 0.1458  data: 0.0773  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.8552 (1.5615)  acc1: 61.4583 (70.6259)  acc5: 84.8958 (88.6666)  time: 0.0616  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.9247 (1.5736)  acc1: 59.3750 (70.2877)  acc5: 82.8125 (88.4740)  time: 0.0630  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.8100 (1.5819)  acc1: 59.3750 (70.0099)  acc5: 84.8958 (88.3926)  time: 0.0629  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.5206 (1.5720)  acc1: 71.3542 (70.2482)  acc5: 90.6250 (88.5479)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.4882 (1.5700)  acc1: 72.9167 (70.3380)  acc5: 92.1875 (88.6140)  time: 0.0596  data: 0.0001  max mem: 19734
Test: Total time: 0:01:32 (0.3540 s / it)
* Acc@1 70.338 Acc@5 88.614 loss 1.570
Accuracy of the network on the 50000 test images: 70.3%
Max accuracy: 70.41%
Epoch: [5]  [   0/1251]  eta: 6:06:08  lr: 0.000016  loss: 3.6885 (3.6885)  time: 17.5607  data: 17.0317  max mem: 19734
Epoch: [5]  [  10/1251]  eta: 0:42:43  lr: 0.000016  loss: 3.8746 (3.8401)  time: 2.0658  data: 1.5488  max mem: 19734
Epoch: [5]  [  20/1251]  eta: 0:26:46  lr: 0.000016  loss: 3.8421 (3.7496)  time: 0.4925  data: 0.0005  max mem: 19734
Epoch: [5]  [  30/1251]  eta: 0:21:06  lr: 0.000016  loss: 3.7457 (3.6954)  time: 0.4710  data: 0.0006  max mem: 19734
Epoch: [5]  [  40/1251]  eta: 0:18:07  lr: 0.000016  loss: 3.7457 (3.7536)  time: 0.4707  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6095, ratio_loss=0.0045, pruning_loss=0.2165, mse_loss=1.0049
Epoch: [5]  [  50/1251]  eta: 0:16:16  lr: 0.000016  loss: 3.8658 (3.7700)  time: 0.4656  data: 0.0005  max mem: 19734
Epoch: [5]  [  60/1251]  eta: 0:15:04  lr: 0.000016  loss: 3.8658 (3.7601)  time: 0.4740  data: 0.0005  max mem: 19734
Epoch: [5]  [  70/1251]  eta: 0:14:07  lr: 0.000016  loss: 3.8515 (3.7570)  time: 0.4743  data: 0.0005  max mem: 19734
Epoch: [5]  [  80/1251]  eta: 0:13:22  lr: 0.000016  loss: 3.8314 (3.7344)  time: 0.4615  data: 0.0004  max mem: 19734
Epoch: [5]  [  90/1251]  eta: 0:12:47  lr: 0.000016  loss: 3.6333 (3.7260)  time: 0.4609  data: 0.0004  max mem: 19734
Epoch: [5]  [ 100/1251]  eta: 0:12:17  lr: 0.000016  loss: 3.5428 (3.6849)  time: 0.4610  data: 0.0004  max mem: 19734
Epoch: [5]  [ 110/1251]  eta: 0:11:59  lr: 0.000016  loss: 3.6655 (3.6847)  time: 0.4917  data: 0.0005  max mem: 19734
Epoch: [5]  [ 120/1251]  eta: 0:11:43  lr: 0.000016  loss: 3.8280 (3.6890)  time: 0.5257  data: 0.0004  max mem: 19734
Epoch: [5]  [ 130/1251]  eta: 0:11:23  lr: 0.000016  loss: 3.7927 (3.6830)  time: 0.4933  data: 0.0005  max mem: 19734
Epoch: [5]  [ 140/1251]  eta: 0:11:05  lr: 0.000016  loss: 3.5491 (3.6752)  time: 0.4602  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5966, ratio_loss=0.0048, pruning_loss=0.2149, mse_loss=0.9671
Epoch: [5]  [ 150/1251]  eta: 0:10:49  lr: 0.000016  loss: 3.7163 (3.6743)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [5]  [ 160/1251]  eta: 0:10:34  lr: 0.000016  loss: 3.7509 (3.6714)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [5]  [ 170/1251]  eta: 0:10:20  lr: 0.000016  loss: 3.7786 (3.6753)  time: 0.4577  data: 0.0006  max mem: 19734
Epoch: [5]  [ 180/1251]  eta: 0:10:08  lr: 0.000016  loss: 3.4931 (3.6587)  time: 0.4559  data: 0.0007  max mem: 19734
Epoch: [5]  [ 190/1251]  eta: 0:09:56  lr: 0.000016  loss: 3.4306 (3.6507)  time: 0.4565  data: 0.0006  max mem: 19734
Epoch: [5]  [ 200/1251]  eta: 0:09:45  lr: 0.000016  loss: 3.6525 (3.6380)  time: 0.4577  data: 0.0005  max mem: 19734
Epoch: [5]  [ 210/1251]  eta: 0:09:35  lr: 0.000016  loss: 3.5851 (3.6225)  time: 0.4665  data: 0.0005  max mem: 19734
Epoch: [5]  [ 220/1251]  eta: 0:09:25  lr: 0.000016  loss: 3.2835 (3.6106)  time: 0.4674  data: 0.0004  max mem: 19734
Epoch: [5]  [ 230/1251]  eta: 0:09:16  lr: 0.000016  loss: 3.4976 (3.6050)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [5]  [ 240/1251]  eta: 0:09:07  lr: 0.000016  loss: 3.6695 (3.6029)  time: 0.4576  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4173, ratio_loss=0.0047, pruning_loss=0.2192, mse_loss=1.0113
Epoch: [5]  [ 250/1251]  eta: 0:08:59  lr: 0.000016  loss: 3.7244 (3.5956)  time: 0.4645  data: 0.0006  max mem: 19734
Epoch: [5]  [ 260/1251]  eta: 0:08:53  lr: 0.000016  loss: 3.7522 (3.6013)  time: 0.5038  data: 0.0008  max mem: 19734
Epoch: [5]  [ 270/1251]  eta: 0:08:45  lr: 0.000016  loss: 3.7198 (3.5985)  time: 0.5055  data: 0.0007  max mem: 19734
Epoch: [5]  [ 280/1251]  eta: 0:08:37  lr: 0.000016  loss: 3.7243 (3.5988)  time: 0.4687  data: 0.0005  max mem: 19734
Epoch: [5]  [ 290/1251]  eta: 0:08:30  lr: 0.000016  loss: 3.7396 (3.6038)  time: 0.4599  data: 0.0005  max mem: 19734
Epoch: [5]  [ 300/1251]  eta: 0:08:22  lr: 0.000016  loss: 3.6835 (3.6031)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [5]  [ 310/1251]  eta: 0:08:15  lr: 0.000016  loss: 3.6291 (3.6030)  time: 0.4571  data: 0.0005  max mem: 19734
Epoch: [5]  [ 320/1251]  eta: 0:08:07  lr: 0.000016  loss: 3.6824 (3.6027)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [5]  [ 330/1251]  eta: 0:08:00  lr: 0.000016  loss: 3.6455 (3.5991)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [5]  [ 340/1251]  eta: 0:07:53  lr: 0.000016  loss: 3.7080 (3.6025)  time: 0.4544  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5979, ratio_loss=0.0051, pruning_loss=0.2123, mse_loss=0.9551
Epoch: [5]  [ 350/1251]  eta: 0:07:46  lr: 0.000016  loss: 3.6160 (3.5945)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [5]  [ 360/1251]  eta: 0:07:40  lr: 0.000016  loss: 3.5939 (3.5936)  time: 0.4642  data: 0.0004  max mem: 19734
Epoch: [5]  [ 370/1251]  eta: 0:07:33  lr: 0.000016  loss: 3.6873 (3.6013)  time: 0.4663  data: 0.0004  max mem: 19734
Epoch: [5]  [ 380/1251]  eta: 0:07:27  lr: 0.000016  loss: 3.7682 (3.5974)  time: 0.4586  data: 0.0005  max mem: 19734
Epoch: [5]  [ 390/1251]  eta: 0:07:21  lr: 0.000016  loss: 3.6471 (3.5977)  time: 0.4593  data: 0.0005  max mem: 19734
Epoch: [5]  [ 400/1251]  eta: 0:07:16  lr: 0.000016  loss: 3.4838 (3.5915)  time: 0.4912  data: 0.0005  max mem: 19734
Epoch: [5]  [ 410/1251]  eta: 0:07:11  lr: 0.000016  loss: 3.4838 (3.5902)  time: 0.5169  data: 0.0005  max mem: 19734
Epoch: [5]  [ 420/1251]  eta: 0:07:04  lr: 0.000016  loss: 3.5527 (3.5840)  time: 0.4826  data: 0.0004  max mem: 19734
Epoch: [5]  [ 430/1251]  eta: 0:06:58  lr: 0.000016  loss: 3.2699 (3.5771)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [5]  [ 440/1251]  eta: 0:06:52  lr: 0.000016  loss: 3.4786 (3.5745)  time: 0.4535  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4489, ratio_loss=0.0052, pruning_loss=0.2178, mse_loss=1.0132
Epoch: [5]  [ 450/1251]  eta: 0:06:46  lr: 0.000016  loss: 3.7873 (3.5805)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [5]  [ 460/1251]  eta: 0:06:40  lr: 0.000016  loss: 3.7432 (3.5829)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [5]  [ 470/1251]  eta: 0:06:34  lr: 0.000016  loss: 3.7365 (3.5879)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [5]  [ 480/1251]  eta: 0:06:28  lr: 0.000016  loss: 3.8212 (3.5880)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [5]  [ 490/1251]  eta: 0:06:23  lr: 0.000016  loss: 3.4484 (3.5819)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [5]  [ 500/1251]  eta: 0:06:17  lr: 0.000016  loss: 3.4484 (3.5808)  time: 0.4705  data: 0.0004  max mem: 19734
Epoch: [5]  [ 510/1251]  eta: 0:06:12  lr: 0.000016  loss: 3.5423 (3.5771)  time: 0.4696  data: 0.0004  max mem: 19734
Epoch: [5]  [ 520/1251]  eta: 0:06:06  lr: 0.000016  loss: 3.3731 (3.5787)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [5]  [ 530/1251]  eta: 0:06:00  lr: 0.000016  loss: 3.8358 (3.5796)  time: 0.4594  data: 0.0004  max mem: 19734
Epoch: [5]  [ 540/1251]  eta: 0:05:55  lr: 0.000016  loss: 3.8138 (3.5847)  time: 0.4779  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5434, ratio_loss=0.0052, pruning_loss=0.2152, mse_loss=0.9874
Epoch: [5]  [ 550/1251]  eta: 0:05:51  lr: 0.000016  loss: 3.7363 (3.5780)  time: 0.5109  data: 0.0005  max mem: 19734
Epoch: [5]  [ 560/1251]  eta: 0:05:45  lr: 0.000016  loss: 3.6598 (3.5784)  time: 0.5039  data: 0.0004  max mem: 19734
Epoch: [5]  [ 570/1251]  eta: 0:05:40  lr: 0.000016  loss: 3.6010 (3.5787)  time: 0.4720  data: 0.0004  max mem: 19734
Epoch: [5]  [ 580/1251]  eta: 0:05:34  lr: 0.000016  loss: 3.5107 (3.5759)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [5]  [ 590/1251]  eta: 0:05:29  lr: 0.000016  loss: 3.6179 (3.5779)  time: 0.4577  data: 0.0004  max mem: 19734
Epoch: [5]  [ 600/1251]  eta: 0:05:24  lr: 0.000016  loss: 3.4031 (3.5711)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [5]  [ 610/1251]  eta: 0:05:18  lr: 0.000016  loss: 3.2133 (3.5705)  time: 0.4586  data: 0.0005  max mem: 19734
Epoch: [5]  [ 620/1251]  eta: 0:05:13  lr: 0.000016  loss: 3.5318 (3.5736)  time: 0.4575  data: 0.0005  max mem: 19734
Epoch: [5]  [ 630/1251]  eta: 0:05:07  lr: 0.000016  loss: 3.7944 (3.5724)  time: 0.4571  data: 0.0005  max mem: 19734
Epoch: [5]  [ 640/1251]  eta: 0:05:02  lr: 0.000016  loss: 3.7455 (3.5720)  time: 0.4578  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4766, ratio_loss=0.0054, pruning_loss=0.2171, mse_loss=0.9221
Epoch: [5]  [ 650/1251]  eta: 0:04:57  lr: 0.000016  loss: 3.5881 (3.5666)  time: 0.4691  data: 0.0004  max mem: 19734
Epoch: [5]  [ 660/1251]  eta: 0:04:52  lr: 0.000016  loss: 3.3290 (3.5674)  time: 0.4673  data: 0.0004  max mem: 19734
Epoch: [5]  [ 670/1251]  eta: 0:04:46  lr: 0.000016  loss: 3.7341 (3.5658)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [5]  [ 680/1251]  eta: 0:04:41  lr: 0.000016  loss: 3.6153 (3.5692)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [5]  [ 690/1251]  eta: 0:04:36  lr: 0.000016  loss: 3.5027 (3.5644)  time: 0.4864  data: 0.0005  max mem: 19734
Epoch: [5]  [ 700/1251]  eta: 0:04:32  lr: 0.000016  loss: 3.2831 (3.5606)  time: 0.5167  data: 0.0005  max mem: 19734
Epoch: [5]  [ 710/1251]  eta: 0:04:26  lr: 0.000016  loss: 3.5634 (3.5613)  time: 0.4877  data: 0.0005  max mem: 19734
Epoch: [5]  [ 720/1251]  eta: 0:04:21  lr: 0.000016  loss: 3.6246 (3.5597)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [5]  [ 730/1251]  eta: 0:04:16  lr: 0.000016  loss: 3.4769 (3.5593)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [5]  [ 740/1251]  eta: 0:04:11  lr: 0.000016  loss: 3.5530 (3.5572)  time: 0.4570  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4087, ratio_loss=0.0054, pruning_loss=0.2174, mse_loss=0.9351
Epoch: [5]  [ 750/1251]  eta: 0:04:06  lr: 0.000016  loss: 3.1232 (3.5514)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [5]  [ 760/1251]  eta: 0:04:01  lr: 0.000016  loss: 3.4322 (3.5531)  time: 0.4612  data: 0.0004  max mem: 19734
Epoch: [5]  [ 770/1251]  eta: 0:03:56  lr: 0.000016  loss: 3.7639 (3.5525)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [5]  [ 780/1251]  eta: 0:03:50  lr: 0.000016  loss: 3.7086 (3.5537)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [5]  [ 790/1251]  eta: 0:03:45  lr: 0.000016  loss: 3.5828 (3.5499)  time: 0.4611  data: 0.0004  max mem: 19734
Epoch: [5]  [ 800/1251]  eta: 0:03:40  lr: 0.000016  loss: 3.5828 (3.5486)  time: 0.4725  data: 0.0006  max mem: 19734
Epoch: [5]  [ 810/1251]  eta: 0:03:35  lr: 0.000016  loss: 3.7911 (3.5518)  time: 0.4685  data: 0.0006  max mem: 19734
Epoch: [5]  [ 820/1251]  eta: 0:03:30  lr: 0.000016  loss: 3.6956 (3.5501)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [5]  [ 830/1251]  eta: 0:03:25  lr: 0.000016  loss: 3.2022 (3.5472)  time: 0.4793  data: 0.0005  max mem: 19734
Epoch: [5]  [ 840/1251]  eta: 0:03:21  lr: 0.000016  loss: 3.4896 (3.5486)  time: 0.5035  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4687, ratio_loss=0.0055, pruning_loss=0.2175, mse_loss=0.9845
Epoch: [5]  [ 850/1251]  eta: 0:03:16  lr: 0.000016  loss: 3.6246 (3.5493)  time: 0.4909  data: 0.0004  max mem: 19734
Epoch: [5]  [ 860/1251]  eta: 0:03:11  lr: 0.000016  loss: 3.4956 (3.5478)  time: 0.4662  data: 0.0004  max mem: 19734
Epoch: [5]  [ 870/1251]  eta: 0:03:06  lr: 0.000016  loss: 3.4956 (3.5487)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [5]  [ 880/1251]  eta: 0:03:01  lr: 0.000016  loss: 3.6432 (3.5495)  time: 0.4614  data: 0.0004  max mem: 19734
Epoch: [5]  [ 890/1251]  eta: 0:02:56  lr: 0.000016  loss: 3.6432 (3.5500)  time: 0.4614  data: 0.0004  max mem: 19734
Epoch: [5]  [ 900/1251]  eta: 0:02:51  lr: 0.000016  loss: 3.9734 (3.5550)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [5]  [ 910/1251]  eta: 0:02:46  lr: 0.000016  loss: 3.9692 (3.5563)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [5]  [ 920/1251]  eta: 0:02:41  lr: 0.000016  loss: 3.9053 (3.5596)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [5]  [ 930/1251]  eta: 0:02:36  lr: 0.000016  loss: 3.7666 (3.5556)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [5]  [ 940/1251]  eta: 0:02:31  lr: 0.000016  loss: 3.4546 (3.5553)  time: 0.4565  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5580, ratio_loss=0.0056, pruning_loss=0.2121, mse_loss=0.9135
Epoch: [5]  [ 950/1251]  eta: 0:02:26  lr: 0.000016  loss: 3.4725 (3.5551)  time: 0.4668  data: 0.0005  max mem: 19734
Epoch: [5]  [ 960/1251]  eta: 0:02:21  lr: 0.000016  loss: 3.5796 (3.5540)  time: 0.4682  data: 0.0004  max mem: 19734
Epoch: [5]  [ 970/1251]  eta: 0:02:16  lr: 0.000016  loss: 3.5950 (3.5554)  time: 0.4689  data: 0.0004  max mem: 19734
Epoch: [5]  [ 980/1251]  eta: 0:02:11  lr: 0.000016  loss: 3.7367 (3.5556)  time: 0.4914  data: 0.0004  max mem: 19734
Epoch: [5]  [ 990/1251]  eta: 0:02:06  lr: 0.000016  loss: 3.7350 (3.5569)  time: 0.5015  data: 0.0004  max mem: 19734
Epoch: [5]  [1000/1251]  eta: 0:02:01  lr: 0.000016  loss: 3.7350 (3.5564)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [5]  [1010/1251]  eta: 0:01:57  lr: 0.000016  loss: 3.7908 (3.5594)  time: 0.4561  data: 0.0004  max mem: 19734
Epoch: [5]  [1020/1251]  eta: 0:01:52  lr: 0.000016  loss: 3.7908 (3.5594)  time: 0.4565  data: 0.0005  max mem: 19734
Epoch: [5]  [1030/1251]  eta: 0:01:47  lr: 0.000016  loss: 3.1927 (3.5563)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [5]  [1040/1251]  eta: 0:01:42  lr: 0.000016  loss: 3.4717 (3.5563)  time: 0.4538  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5129, ratio_loss=0.0057, pruning_loss=0.2092, mse_loss=0.8986
Epoch: [5]  [1050/1251]  eta: 0:01:37  lr: 0.000016  loss: 3.5761 (3.5573)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [5]  [1060/1251]  eta: 0:01:32  lr: 0.000016  loss: 3.5761 (3.5574)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [5]  [1070/1251]  eta: 0:01:27  lr: 0.000016  loss: 3.4850 (3.5566)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [5]  [1080/1251]  eta: 0:01:22  lr: 0.000016  loss: 3.4293 (3.5546)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [5]  [1090/1251]  eta: 0:01:17  lr: 0.000016  loss: 3.6458 (3.5575)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [5]  [1100/1251]  eta: 0:01:12  lr: 0.000016  loss: 3.6999 (3.5575)  time: 0.4637  data: 0.0004  max mem: 19734
Epoch: [5]  [1110/1251]  eta: 0:01:08  lr: 0.000016  loss: 3.7406 (3.5577)  time: 0.4646  data: 0.0004  max mem: 19734
Epoch: [5]  [1120/1251]  eta: 0:01:03  lr: 0.000016  loss: 3.7154 (3.5566)  time: 0.4643  data: 0.0004  max mem: 19734
Epoch: [5]  [1130/1251]  eta: 0:00:58  lr: 0.000016  loss: 3.5517 (3.5565)  time: 0.5068  data: 0.0004  max mem: 19734
Epoch: [5]  [1140/1251]  eta: 0:00:53  lr: 0.000016  loss: 3.6557 (3.5570)  time: 0.5106  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5033, ratio_loss=0.0061, pruning_loss=0.2117, mse_loss=0.8772
Epoch: [5]  [1150/1251]  eta: 0:00:48  lr: 0.000016  loss: 3.4908 (3.5550)  time: 0.4711  data: 0.0004  max mem: 19734
Epoch: [5]  [1160/1251]  eta: 0:00:43  lr: 0.000016  loss: 3.5396 (3.5548)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [5]  [1170/1251]  eta: 0:00:39  lr: 0.000016  loss: 3.6934 (3.5565)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [5]  [1180/1251]  eta: 0:00:34  lr: 0.000016  loss: 3.6034 (3.5548)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [5]  [1190/1251]  eta: 0:00:29  lr: 0.000016  loss: 3.3673 (3.5522)  time: 0.4552  data: 0.0007  max mem: 19734
Epoch: [5]  [1200/1251]  eta: 0:00:24  lr: 0.000016  loss: 3.5642 (3.5524)  time: 0.4515  data: 0.0006  max mem: 19734
Epoch: [5]  [1210/1251]  eta: 0:00:19  lr: 0.000016  loss: 3.7444 (3.5536)  time: 0.4500  data: 0.0002  max mem: 19734
Epoch: [5]  [1220/1251]  eta: 0:00:14  lr: 0.000016  loss: 3.8708 (3.5547)  time: 0.4507  data: 0.0001  max mem: 19734
Epoch: [5]  [1230/1251]  eta: 0:00:10  lr: 0.000016  loss: 3.5012 (3.5545)  time: 0.4493  data: 0.0001  max mem: 19734
Epoch: [5]  [1240/1251]  eta: 0:00:05  lr: 0.000016  loss: 3.4431 (3.5530)  time: 0.4508  data: 0.0001  max mem: 19734
loss info: cls_loss=3.4815, ratio_loss=0.0061, pruning_loss=0.2105, mse_loss=0.9021
Epoch: [5]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.6014 (3.5526)  time: 0.4612  data: 0.0001  max mem: 19734
Epoch: [5] Total time: 0:10:02 (0.4816 s / it)
Averaged stats: lr: 0.000016  loss: 3.6014 (3.5599)
Test:  [  0/261]  eta: 2:29:32  loss: 1.1573 (1.1573)  acc1: 78.6458 (78.6458)  acc5: 93.2292 (93.2292)  time: 34.3764  data: 33.9675  max mem: 19734
Test:  [ 10/261]  eta: 0:14:02  loss: 0.9717 (1.0689)  acc1: 82.8125 (81.3447)  acc5: 93.2292 (93.7027)  time: 3.3582  data: 3.0955  max mem: 19734
Test:  [ 20/261]  eta: 0:07:38  loss: 1.1746 (1.1868)  acc1: 76.5625 (76.7609)  acc5: 92.1875 (92.6587)  time: 0.2775  data: 0.0087  max mem: 19734
Test:  [ 30/261]  eta: 0:05:13  loss: 1.1584 (1.1121)  acc1: 80.2083 (79.3851)  acc5: 92.7083 (93.2460)  time: 0.2571  data: 0.0121  max mem: 19734
Test:  [ 40/261]  eta: 0:04:14  loss: 0.9032 (1.0747)  acc1: 84.8958 (80.2846)  acc5: 93.2292 (93.4324)  time: 0.3653  data: 0.1237  max mem: 19734
Test:  [ 50/261]  eta: 0:03:29  loss: 1.1827 (1.1374)  acc1: 74.4792 (78.2373)  acc5: 92.7083 (92.7594)  time: 0.4220  data: 0.1198  max mem: 19734
Test:  [ 60/261]  eta: 0:02:56  loss: 1.2426 (1.1517)  acc1: 72.3958 (77.5102)  acc5: 91.6667 (92.8279)  time: 0.3205  data: 0.0101  max mem: 19734
Test:  [ 70/261]  eta: 0:02:30  loss: 1.2118 (1.1588)  acc1: 72.9167 (76.9513)  acc5: 93.7500 (92.9798)  time: 0.2730  data: 0.0136  max mem: 19734
Test:  [ 80/261]  eta: 0:02:21  loss: 1.1807 (1.1699)  acc1: 75.5208 (77.1541)  acc5: 93.7500 (93.1070)  time: 0.4738  data: 0.2072  max mem: 19734
Test:  [ 90/261]  eta: 0:02:00  loss: 1.2130 (1.1579)  acc1: 80.2083 (77.5641)  acc5: 93.2292 (93.2063)  time: 0.4074  data: 0.2035  max mem: 19734
Test:  [100/261]  eta: 0:01:44  loss: 1.0510 (1.1546)  acc1: 80.2083 (77.6867)  acc5: 93.2292 (93.2550)  time: 0.1304  data: 0.0388  max mem: 19734
Test:  [110/261]  eta: 0:01:41  loss: 1.2041 (1.1792)  acc1: 75.5208 (77.2945)  acc5: 92.1875 (92.9617)  time: 0.5318  data: 0.4065  max mem: 19734
Test:  [120/261]  eta: 0:01:28  loss: 1.5324 (1.2189)  acc1: 67.1875 (76.3645)  acc5: 88.0208 (92.3855)  time: 0.5162  data: 0.3813  max mem: 19734
Test:  [130/261]  eta: 0:01:17  loss: 1.7300 (1.2634)  acc1: 65.1042 (75.4214)  acc5: 84.3750 (91.7541)  time: 0.1492  data: 0.0161  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: 1.5926 (1.2872)  acc1: 66.1458 (74.7821)  acc5: 86.4583 (91.5226)  time: 0.1594  data: 0.0278  max mem: 19734
Test:  [150/261]  eta: 0:01:01  loss: 1.4328 (1.2899)  acc1: 69.7917 (74.8172)  acc5: 89.5833 (91.3735)  time: 0.3221  data: 0.2199  max mem: 19734
Test:  [160/261]  eta: 0:00:53  loss: 1.3525 (1.3112)  acc1: 74.4792 (74.3756)  acc5: 89.5833 (91.0617)  time: 0.3087  data: 0.2054  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: 1.6937 (1.3445)  acc1: 60.9375 (73.5228)  acc5: 84.8958 (90.6920)  time: 0.1385  data: 0.0157  max mem: 19734
Test:  [180/261]  eta: 0:00:40  loss: 1.8621 (1.3656)  acc1: 60.9375 (73.0577)  acc5: 84.8958 (90.4437)  time: 0.2123  data: 0.1024  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: 1.7149 (1.3809)  acc1: 62.5000 (72.7531)  acc5: 85.9375 (90.2351)  time: 0.1661  data: 0.0945  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.7126 (1.3974)  acc1: 67.7083 (72.4243)  acc5: 85.4167 (89.9720)  time: 0.0622  data: 0.0008  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: 1.6525 (1.4114)  acc1: 66.1458 (72.1342)  acc5: 85.9375 (89.7734)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: 1.8226 (1.4307)  acc1: 63.0208 (71.6299)  acc5: 84.8958 (89.5527)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.7427 (1.4411)  acc1: 59.8958 (71.3181)  acc5: 85.9375 (89.4323)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.6887 (1.4520)  acc1: 60.9375 (70.9997)  acc5: 87.5000 (89.3305)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.4501 (1.4439)  acc1: 69.7917 (71.2463)  acc5: 91.6667 (89.5003)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.3629 (1.4442)  acc1: 73.9583 (71.2700)  acc5: 94.2708 (89.5680)  time: 0.0603  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3633 s / it)
* Acc@1 71.270 Acc@5 89.568 loss 1.444
Accuracy of the network on the 50000 test images: 71.3%
Max accuracy: 71.27%
Epoch: [6]  [   0/1251]  eta: 5:05:29  lr: 0.000020  loss: 2.8505 (2.8505)  time: 14.6516  data: 14.1657  max mem: 19734
Epoch: [6]  [  10/1251]  eta: 0:40:21  lr: 0.000020  loss: 3.3845 (3.4022)  time: 1.9509  data: 1.2883  max mem: 19734
Epoch: [6]  [  20/1251]  eta: 0:26:02  lr: 0.000020  loss: 3.4665 (3.4162)  time: 0.5999  data: 0.0005  max mem: 19734
Epoch: [6]  [  30/1251]  eta: 0:20:40  lr: 0.000020  loss: 3.5692 (3.4347)  time: 0.5019  data: 0.0005  max mem: 19734
Epoch: [6]  [  40/1251]  eta: 0:17:46  lr: 0.000020  loss: 3.6425 (3.4863)  time: 0.4734  data: 0.0005  max mem: 19734
Epoch: [6]  [  50/1251]  eta: 0:15:59  lr: 0.000020  loss: 3.6543 (3.4490)  time: 0.4623  data: 0.0005  max mem: 19734
Epoch: [6]  [  60/1251]  eta: 0:14:47  lr: 0.000020  loss: 3.1133 (3.3905)  time: 0.4677  data: 0.0006  max mem: 19734
Epoch: [6]  [  70/1251]  eta: 0:13:54  lr: 0.000020  loss: 3.4241 (3.4158)  time: 0.4709  data: 0.0006  max mem: 19734
Epoch: [6]  [  80/1251]  eta: 0:13:16  lr: 0.000020  loss: 3.6247 (3.4500)  time: 0.4804  data: 0.0005  max mem: 19734
Epoch: [6]  [  90/1251]  eta: 0:12:41  lr: 0.000020  loss: 3.6247 (3.4495)  time: 0.4765  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4148, ratio_loss=0.0063, pruning_loss=0.2096, mse_loss=0.8928
Epoch: [6]  [ 100/1251]  eta: 0:12:12  lr: 0.000020  loss: 3.7049 (3.4663)  time: 0.4607  data: 0.0004  max mem: 19734
Epoch: [6]  [ 110/1251]  eta: 0:11:48  lr: 0.000020  loss: 3.6974 (3.4520)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [6]  [ 120/1251]  eta: 0:11:26  lr: 0.000020  loss: 3.5996 (3.4600)  time: 0.4578  data: 0.0005  max mem: 19734
Epoch: [6]  [ 130/1251]  eta: 0:11:07  lr: 0.000020  loss: 3.6803 (3.4718)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [6]  [ 140/1251]  eta: 0:10:50  lr: 0.000020  loss: 3.5460 (3.4767)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [6]  [ 150/1251]  eta: 0:10:38  lr: 0.000020  loss: 3.5460 (3.4914)  time: 0.4798  data: 0.0004  max mem: 19734
Epoch: [6]  [ 160/1251]  eta: 0:10:27  lr: 0.000020  loss: 3.8067 (3.5037)  time: 0.4984  data: 0.0004  max mem: 19734
Epoch: [6]  [ 170/1251]  eta: 0:10:16  lr: 0.000020  loss: 3.6431 (3.4948)  time: 0.4926  data: 0.0008  max mem: 19734
Epoch: [6]  [ 180/1251]  eta: 0:10:03  lr: 0.000020  loss: 3.4929 (3.4992)  time: 0.4759  data: 0.0009  max mem: 19734
Epoch: [6]  [ 190/1251]  eta: 0:09:52  lr: 0.000020  loss: 3.6470 (3.5089)  time: 0.4595  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5322, ratio_loss=0.0069, pruning_loss=0.2050, mse_loss=0.8820
Epoch: [6]  [ 200/1251]  eta: 0:09:41  lr: 0.000020  loss: 3.7878 (3.5240)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [6]  [ 210/1251]  eta: 0:09:31  lr: 0.000020  loss: 3.6131 (3.5209)  time: 0.4581  data: 0.0005  max mem: 19734
Epoch: [6]  [ 220/1251]  eta: 0:09:22  lr: 0.000020  loss: 3.2582 (3.4975)  time: 0.4715  data: 0.0005  max mem: 19734
Epoch: [6]  [ 230/1251]  eta: 0:09:13  lr: 0.000020  loss: 3.3793 (3.5009)  time: 0.4739  data: 0.0005  max mem: 19734
Epoch: [6]  [ 240/1251]  eta: 0:09:04  lr: 0.000020  loss: 3.5316 (3.5024)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [6]  [ 250/1251]  eta: 0:08:56  lr: 0.000020  loss: 3.4632 (3.4985)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [6]  [ 260/1251]  eta: 0:08:47  lr: 0.000020  loss: 3.4760 (3.5001)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [6]  [ 270/1251]  eta: 0:08:39  lr: 0.000020  loss: 3.6791 (3.5027)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [6]  [ 280/1251]  eta: 0:08:32  lr: 0.000020  loss: 3.3785 (3.5017)  time: 0.4583  data: 0.0004  max mem: 19734
Epoch: [6]  [ 290/1251]  eta: 0:08:25  lr: 0.000020  loss: 3.5142 (3.5046)  time: 0.4701  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4236, ratio_loss=0.0067, pruning_loss=0.2088, mse_loss=0.8466
Epoch: [6]  [ 300/1251]  eta: 0:08:19  lr: 0.000020  loss: 3.5222 (3.5058)  time: 0.4931  data: 0.0005  max mem: 19734
Epoch: [6]  [ 310/1251]  eta: 0:08:14  lr: 0.000020  loss: 3.6824 (3.5074)  time: 0.5152  data: 0.0005  max mem: 19734
Epoch: [6]  [ 320/1251]  eta: 0:08:07  lr: 0.000020  loss: 3.6378 (3.5019)  time: 0.4936  data: 0.0005  max mem: 19734
Epoch: [6]  [ 330/1251]  eta: 0:08:00  lr: 0.000020  loss: 3.6378 (3.5111)  time: 0.4611  data: 0.0005  max mem: 19734
Epoch: [6]  [ 340/1251]  eta: 0:07:53  lr: 0.000020  loss: 3.8424 (3.5118)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [6]  [ 350/1251]  eta: 0:07:46  lr: 0.000020  loss: 3.6617 (3.5064)  time: 0.4591  data: 0.0005  max mem: 19734
Epoch: [6]  [ 360/1251]  eta: 0:07:39  lr: 0.000020  loss: 3.7553 (3.5124)  time: 0.4602  data: 0.0007  max mem: 19734
Epoch: [6]  [ 370/1251]  eta: 0:07:33  lr: 0.000020  loss: 3.8255 (3.5169)  time: 0.4650  data: 0.0006  max mem: 19734
Epoch: [6]  [ 380/1251]  eta: 0:07:27  lr: 0.000020  loss: 3.7063 (3.5137)  time: 0.4631  data: 0.0004  max mem: 19734
Epoch: [6]  [ 390/1251]  eta: 0:07:20  lr: 0.000020  loss: 3.7410 (3.5175)  time: 0.4557  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5333, ratio_loss=0.0070, pruning_loss=0.2031, mse_loss=0.8321
Epoch: [6]  [ 400/1251]  eta: 0:07:14  lr: 0.000020  loss: 3.8693 (3.5241)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [6]  [ 410/1251]  eta: 0:07:08  lr: 0.000020  loss: 3.4307 (3.5194)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [6]  [ 420/1251]  eta: 0:07:02  lr: 0.000020  loss: 3.4196 (3.5229)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [6]  [ 430/1251]  eta: 0:06:55  lr: 0.000020  loss: 3.6070 (3.5214)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [6]  [ 440/1251]  eta: 0:06:50  lr: 0.000020  loss: 3.6123 (3.5246)  time: 0.4720  data: 0.0004  max mem: 19734
Epoch: [6]  [ 450/1251]  eta: 0:06:45  lr: 0.000020  loss: 3.5892 (3.5190)  time: 0.5008  data: 0.0004  max mem: 19734
Epoch: [6]  [ 460/1251]  eta: 0:06:40  lr: 0.000020  loss: 3.4641 (3.5164)  time: 0.4961  data: 0.0007  max mem: 19734
Epoch: [6]  [ 470/1251]  eta: 0:06:34  lr: 0.000020  loss: 3.5994 (3.5160)  time: 0.4655  data: 0.0007  max mem: 19734
Epoch: [6]  [ 480/1251]  eta: 0:06:28  lr: 0.000020  loss: 3.3914 (3.5140)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [6]  [ 490/1251]  eta: 0:06:22  lr: 0.000020  loss: 3.3333 (3.5124)  time: 0.4534  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4395, ratio_loss=0.0071, pruning_loss=0.2041, mse_loss=0.8742
Epoch: [6]  [ 500/1251]  eta: 0:06:16  lr: 0.000020  loss: 3.5456 (3.5130)  time: 0.4527  data: 0.0007  max mem: 19734
Epoch: [6]  [ 510/1251]  eta: 0:06:10  lr: 0.000020  loss: 3.4578 (3.5098)  time: 0.4530  data: 0.0006  max mem: 19734
Epoch: [6]  [ 520/1251]  eta: 0:06:05  lr: 0.000020  loss: 3.3046 (3.5075)  time: 0.4646  data: 0.0004  max mem: 19734
Epoch: [6]  [ 530/1251]  eta: 0:05:59  lr: 0.000020  loss: 3.5429 (3.5108)  time: 0.4652  data: 0.0004  max mem: 19734
Epoch: [6]  [ 540/1251]  eta: 0:05:54  lr: 0.000020  loss: 3.7359 (3.5103)  time: 0.4560  data: 0.0006  max mem: 19734
Epoch: [6]  [ 550/1251]  eta: 0:05:48  lr: 0.000020  loss: 3.3652 (3.5032)  time: 0.4582  data: 0.0007  max mem: 19734
Epoch: [6]  [ 560/1251]  eta: 0:05:43  lr: 0.000020  loss: 3.5847 (3.5028)  time: 0.4573  data: 0.0004  max mem: 19734
Epoch: [6]  [ 570/1251]  eta: 0:05:38  lr: 0.000020  loss: 3.7961 (3.5072)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [6]  [ 580/1251]  eta: 0:05:33  lr: 0.000020  loss: 3.7891 (3.5086)  time: 0.4798  data: 0.0004  max mem: 19734
Epoch: [6]  [ 590/1251]  eta: 0:05:28  lr: 0.000020  loss: 3.7891 (3.5148)  time: 0.4916  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4777, ratio_loss=0.0072, pruning_loss=0.2013, mse_loss=0.8372
Epoch: [6]  [ 600/1251]  eta: 0:05:23  lr: 0.000020  loss: 3.6751 (3.5110)  time: 0.4979  data: 0.0004  max mem: 19734
Epoch: [6]  [ 610/1251]  eta: 0:05:17  lr: 0.000020  loss: 3.3233 (3.5068)  time: 0.4827  data: 0.0005  max mem: 19734
Epoch: [6]  [ 620/1251]  eta: 0:05:12  lr: 0.000020  loss: 3.5525 (3.5102)  time: 0.4520  data: 0.0005  max mem: 19734
Epoch: [6]  [ 630/1251]  eta: 0:05:07  lr: 0.000020  loss: 3.5525 (3.5067)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [6]  [ 640/1251]  eta: 0:05:01  lr: 0.000020  loss: 3.5235 (3.5083)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [6]  [ 650/1251]  eta: 0:04:56  lr: 0.000020  loss: 3.6188 (3.5069)  time: 0.4514  data: 0.0005  max mem: 19734
Epoch: [6]  [ 660/1251]  eta: 0:04:51  lr: 0.000020  loss: 3.3980 (3.5047)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [6]  [ 670/1251]  eta: 0:04:46  lr: 0.000020  loss: 3.5897 (3.5064)  time: 0.4699  data: 0.0005  max mem: 19734
Epoch: [6]  [ 680/1251]  eta: 0:04:40  lr: 0.000020  loss: 3.6033 (3.5092)  time: 0.4692  data: 0.0005  max mem: 19734
Epoch: [6]  [ 690/1251]  eta: 0:04:35  lr: 0.000020  loss: 3.5437 (3.5071)  time: 0.4591  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4353, ratio_loss=0.0075, pruning_loss=0.2004, mse_loss=0.8292
Epoch: [6]  [ 700/1251]  eta: 0:04:30  lr: 0.000020  loss: 3.5737 (3.5107)  time: 0.4576  data: 0.0006  max mem: 19734
Epoch: [6]  [ 710/1251]  eta: 0:04:25  lr: 0.000020  loss: 3.8007 (3.5098)  time: 0.4572  data: 0.0006  max mem: 19734
Epoch: [6]  [ 720/1251]  eta: 0:04:20  lr: 0.000020  loss: 3.1860 (3.5070)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [6]  [ 730/1251]  eta: 0:04:15  lr: 0.000020  loss: 3.0206 (3.5048)  time: 0.4718  data: 0.0005  max mem: 19734
Epoch: [6]  [ 740/1251]  eta: 0:04:10  lr: 0.000020  loss: 3.7041 (3.5056)  time: 0.4894  data: 0.0005  max mem: 19734
Epoch: [6]  [ 750/1251]  eta: 0:04:05  lr: 0.000020  loss: 3.6785 (3.5057)  time: 0.4889  data: 0.0005  max mem: 19734
Epoch: [6]  [ 760/1251]  eta: 0:04:00  lr: 0.000020  loss: 3.6785 (3.5094)  time: 0.4692  data: 0.0005  max mem: 19734
Epoch: [6]  [ 770/1251]  eta: 0:03:55  lr: 0.000020  loss: 3.7366 (3.5110)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [6]  [ 780/1251]  eta: 0:03:50  lr: 0.000020  loss: 3.5671 (3.5121)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [6]  [ 790/1251]  eta: 0:03:44  lr: 0.000020  loss: 3.4536 (3.5119)  time: 0.4525  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4966, ratio_loss=0.0086, pruning_loss=0.1956, mse_loss=0.8009
Epoch: [6]  [ 800/1251]  eta: 0:03:39  lr: 0.000020  loss: 3.4136 (3.5094)  time: 0.4542  data: 0.0006  max mem: 19734
Epoch: [6]  [ 810/1251]  eta: 0:03:35  lr: 0.000020  loss: 3.3163 (3.5083)  time: 0.4719  data: 0.0006  max mem: 19734
Epoch: [6]  [ 820/1251]  eta: 0:03:29  lr: 0.000020  loss: 3.5597 (3.5101)  time: 0.4720  data: 0.0006  max mem: 19734
Epoch: [6]  [ 830/1251]  eta: 0:03:24  lr: 0.000020  loss: 3.7509 (3.5116)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [6]  [ 840/1251]  eta: 0:03:19  lr: 0.000020  loss: 3.5622 (3.5093)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [6]  [ 850/1251]  eta: 0:03:14  lr: 0.000020  loss: 3.4730 (3.5101)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [6]  [ 860/1251]  eta: 0:03:09  lr: 0.000020  loss: 3.7261 (3.5109)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [6]  [ 870/1251]  eta: 0:03:05  lr: 0.000020  loss: 3.7137 (3.5117)  time: 0.4639  data: 0.0004  max mem: 19734
Epoch: [6]  [ 880/1251]  eta: 0:03:00  lr: 0.000020  loss: 3.5969 (3.5138)  time: 0.4799  data: 0.0004  max mem: 19734
Epoch: [6]  [ 890/1251]  eta: 0:02:55  lr: 0.000020  loss: 3.5506 (3.5158)  time: 0.4884  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5142, ratio_loss=0.0077, pruning_loss=0.1945, mse_loss=0.7844
Epoch: [6]  [ 900/1251]  eta: 0:02:50  lr: 0.000020  loss: 3.5506 (3.5168)  time: 0.4763  data: 0.0004  max mem: 19734
Epoch: [6]  [ 910/1251]  eta: 0:02:45  lr: 0.000020  loss: 3.4662 (3.5152)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [6]  [ 920/1251]  eta: 0:02:40  lr: 0.000020  loss: 3.3711 (3.5147)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [6]  [ 930/1251]  eta: 0:02:35  lr: 0.000020  loss: 3.4942 (3.5138)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [6]  [ 940/1251]  eta: 0:02:30  lr: 0.000020  loss: 3.7103 (3.5152)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [6]  [ 950/1251]  eta: 0:02:25  lr: 0.000020  loss: 3.7298 (3.5182)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [6]  [ 960/1251]  eta: 0:02:20  lr: 0.000020  loss: 3.6611 (3.5190)  time: 0.4657  data: 0.0004  max mem: 19734
Epoch: [6]  [ 970/1251]  eta: 0:02:15  lr: 0.000020  loss: 3.5223 (3.5170)  time: 0.4624  data: 0.0005  max mem: 19734
Epoch: [6]  [ 980/1251]  eta: 0:02:10  lr: 0.000020  loss: 3.2558 (3.5150)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [6]  [ 990/1251]  eta: 0:02:06  lr: 0.000020  loss: 3.6780 (3.5194)  time: 0.4548  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5041, ratio_loss=0.0084, pruning_loss=0.1932, mse_loss=0.8133
Epoch: [6]  [1000/1251]  eta: 0:02:01  lr: 0.000020  loss: 3.6903 (3.5203)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [6]  [1010/1251]  eta: 0:01:56  lr: 0.000020  loss: 3.6291 (3.5197)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [6]  [1020/1251]  eta: 0:01:51  lr: 0.000020  loss: 3.7175 (3.5200)  time: 0.4685  data: 0.0004  max mem: 19734
Epoch: [6]  [1030/1251]  eta: 0:01:46  lr: 0.000020  loss: 3.6683 (3.5214)  time: 0.4927  data: 0.0005  max mem: 19734
Epoch: [6]  [1040/1251]  eta: 0:01:41  lr: 0.000020  loss: 3.6683 (3.5234)  time: 0.4892  data: 0.0005  max mem: 19734
Epoch: [6]  [1050/1251]  eta: 0:01:36  lr: 0.000020  loss: 3.8052 (3.5242)  time: 0.4682  data: 0.0004  max mem: 19734
Epoch: [6]  [1060/1251]  eta: 0:01:32  lr: 0.000020  loss: 3.6728 (3.5242)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [6]  [1070/1251]  eta: 0:01:27  lr: 0.000020  loss: 3.5759 (3.5233)  time: 0.4551  data: 0.0008  max mem: 19734
Epoch: [6]  [1080/1251]  eta: 0:01:22  lr: 0.000020  loss: 3.4962 (3.5218)  time: 0.4549  data: 0.0009  max mem: 19734
Epoch: [6]  [1090/1251]  eta: 0:01:17  lr: 0.000020  loss: 3.8317 (3.5238)  time: 0.4565  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5334, ratio_loss=0.0079, pruning_loss=0.1898, mse_loss=0.7846
Epoch: [6]  [1100/1251]  eta: 0:01:12  lr: 0.000020  loss: 3.8775 (3.5251)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [6]  [1110/1251]  eta: 0:01:07  lr: 0.000020  loss: 3.6144 (3.5251)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [6]  [1120/1251]  eta: 0:01:02  lr: 0.000020  loss: 3.6144 (3.5257)  time: 0.4661  data: 0.0004  max mem: 19734
Epoch: [6]  [1130/1251]  eta: 0:00:58  lr: 0.000020  loss: 3.6834 (3.5247)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [6]  [1140/1251]  eta: 0:00:53  lr: 0.000020  loss: 3.6743 (3.5263)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [6]  [1150/1251]  eta: 0:00:48  lr: 0.000020  loss: 3.6697 (3.5241)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [6]  [1160/1251]  eta: 0:00:43  lr: 0.000020  loss: 3.5478 (3.5244)  time: 0.4613  data: 0.0006  max mem: 19734
Epoch: [6]  [1170/1251]  eta: 0:00:38  lr: 0.000020  loss: 3.6216 (3.5237)  time: 0.4839  data: 0.0006  max mem: 19734
Epoch: [6]  [1180/1251]  eta: 0:00:34  lr: 0.000020  loss: 3.4882 (3.5213)  time: 0.5054  data: 0.0005  max mem: 19734
Epoch: [6]  [1190/1251]  eta: 0:00:29  lr: 0.000020  loss: 3.1416 (3.5195)  time: 0.4860  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4327, ratio_loss=0.0080, pruning_loss=0.1922, mse_loss=0.7986
Epoch: [6]  [1200/1251]  eta: 0:00:24  lr: 0.000020  loss: 3.4487 (3.5201)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [6]  [1210/1251]  eta: 0:00:19  lr: 0.000020  loss: 3.4287 (3.5196)  time: 0.4457  data: 0.0001  max mem: 19734
Epoch: [6]  [1220/1251]  eta: 0:00:14  lr: 0.000020  loss: 3.4862 (3.5203)  time: 0.4451  data: 0.0001  max mem: 19734
Epoch: [6]  [1230/1251]  eta: 0:00:10  lr: 0.000020  loss: 3.7978 (3.5218)  time: 0.4471  data: 0.0001  max mem: 19734
Epoch: [6]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 3.7978 (3.5230)  time: 0.4482  data: 0.0001  max mem: 19734
Epoch: [6]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.6952 (3.5245)  time: 0.4486  data: 0.0001  max mem: 19734
Epoch: [6] Total time: 0:09:59 (0.4792 s / it)
Averaged stats: lr: 0.000020  loss: 3.6952 (3.5320)
Test:  [  0/261]  eta: 1:15:18  loss: 1.0393 (1.0393)  acc1: 81.2500 (81.2500)  acc5: 92.1875 (92.1875)  time: 17.3137  data: 17.1039  max mem: 19734
Test:  [ 10/261]  eta: 0:13:16  loss: 0.7818 (0.9042)  acc1: 83.8542 (82.5284)  acc5: 95.8333 (95.1231)  time: 3.1747  data: 3.0787  max mem: 19734
Test:  [ 20/261]  eta: 0:07:07  loss: 1.0615 (1.0480)  acc1: 78.6458 (77.6786)  acc5: 92.1875 (93.7500)  time: 0.9972  data: 0.8428  max mem: 19734
Test:  [ 30/261]  eta: 0:04:56  loss: 1.0254 (0.9751)  acc1: 79.6875 (80.4940)  acc5: 92.7083 (94.1364)  time: 0.2432  data: 0.0068  max mem: 19734
Test:  [ 40/261]  eta: 0:04:22  loss: 0.7402 (0.9421)  acc1: 87.5000 (81.4151)  acc5: 94.2708 (94.4233)  time: 0.5675  data: 0.3467  max mem: 19734
Test:  [ 50/261]  eta: 0:03:28  loss: 1.0758 (1.0088)  acc1: 75.5208 (79.4016)  acc5: 92.7083 (93.7806)  time: 0.5292  data: 0.3535  max mem: 19734
Test:  [ 60/261]  eta: 0:02:50  loss: 1.1601 (1.0248)  acc1: 72.3958 (78.7910)  acc5: 92.7083 (93.8268)  time: 0.1577  data: 0.0164  max mem: 19734
Test:  [ 70/261]  eta: 0:02:24  loss: 1.0996 (1.0250)  acc1: 74.4792 (78.2644)  acc5: 94.7917 (94.0948)  time: 0.1600  data: 0.0156  max mem: 19734
Test:  [ 80/261]  eta: 0:02:08  loss: 1.0640 (1.0345)  acc1: 76.0417 (78.4529)  acc5: 95.8333 (94.1744)  time: 0.2774  data: 0.1059  max mem: 19734
Test:  [ 90/261]  eta: 0:01:50  loss: 0.9914 (1.0208)  acc1: 81.7708 (78.8519)  acc5: 93.7500 (94.3052)  time: 0.2726  data: 0.1321  max mem: 19734
Test:  [100/261]  eta: 0:01:48  loss: 0.9330 (1.0220)  acc1: 81.7708 (78.8521)  acc5: 93.7500 (94.3327)  time: 0.5280  data: 0.4316  max mem: 19734
Test:  [110/261]  eta: 0:01:34  loss: 1.0768 (1.0470)  acc1: 75.5208 (78.3971)  acc5: 92.7083 (94.0034)  time: 0.5395  data: 0.4031  max mem: 19734
Test:  [120/261]  eta: 0:01:23  loss: 1.4016 (1.0881)  acc1: 70.8333 (77.5396)  acc5: 89.0625 (93.4659)  time: 0.2017  data: 0.0119  max mem: 19734
Test:  [130/261]  eta: 0:01:17  loss: 1.5524 (1.1370)  acc1: 65.6250 (76.5386)  acc5: 85.9375 (92.8236)  time: 0.3943  data: 0.2084  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: 1.5207 (1.1650)  acc1: 66.1458 (75.8459)  acc5: 88.5417 (92.5458)  time: 0.3393  data: 0.2046  max mem: 19734
Test:  [150/261]  eta: 0:00:58  loss: 1.3737 (1.1714)  acc1: 70.8333 (75.8313)  acc5: 89.5833 (92.3876)  time: 0.1127  data: 0.0058  max mem: 19734
Test:  [160/261]  eta: 0:00:50  loss: 1.2422 (1.1928)  acc1: 74.4792 (75.4044)  acc5: 90.1042 (92.0840)  time: 0.0988  data: 0.0056  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: 1.7030 (1.2264)  acc1: 61.4583 (74.5675)  acc5: 85.9375 (91.6849)  time: 0.1333  data: 0.0571  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: 1.7512 (1.2491)  acc1: 61.4583 (74.0878)  acc5: 85.9375 (91.4192)  time: 0.1764  data: 0.0901  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.6215 (1.2650)  acc1: 64.5833 (73.7375)  acc5: 89.0625 (91.2495)  time: 0.1748  data: 0.0828  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.5588 (1.2816)  acc1: 68.2292 (73.4012)  acc5: 86.4583 (90.9981)  time: 0.1918  data: 0.1106  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.6128 (1.2965)  acc1: 68.2292 (73.1265)  acc5: 86.4583 (90.7731)  time: 0.1315  data: 0.0672  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.6142 (1.3147)  acc1: 65.6250 (72.7163)  acc5: 86.4583 (90.5732)  time: 0.0692  data: 0.0046  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.6279 (1.3270)  acc1: 63.5417 (72.4161)  acc5: 86.9792 (90.4108)  time: 0.0972  data: 0.0328  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.5850 (1.3371)  acc1: 63.5417 (72.1387)  acc5: 88.5417 (90.3376)  time: 0.0945  data: 0.0328  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.3759 (1.3323)  acc1: 70.3125 (72.3045)  acc5: 91.1458 (90.4465)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.2678 (1.3329)  acc1: 74.4792 (72.3260)  acc5: 93.7500 (90.4900)  time: 0.0601  data: 0.0002  max mem: 19734
Test: Total time: 0:01:32 (0.3554 s / it)
* Acc@1 72.326 Acc@5 90.490 loss 1.333
Accuracy of the network on the 50000 test images: 72.3%
Max accuracy: 72.33%
Epoch: [7]  [   0/1251]  eta: 5:09:01  lr: 0.000020  loss: 3.6468 (3.6468)  time: 14.8212  data: 14.3799  max mem: 19734
Epoch: [7]  [  10/1251]  eta: 0:37:56  lr: 0.000020  loss: 3.6468 (3.7188)  time: 1.8343  data: 1.3077  max mem: 19734
Epoch: [7]  [  20/1251]  eta: 0:24:17  lr: 0.000020  loss: 3.5962 (3.6451)  time: 0.5018  data: 0.0004  max mem: 19734
Epoch: [7]  [  30/1251]  eta: 0:19:18  lr: 0.000020  loss: 3.6968 (3.6294)  time: 0.4619  data: 0.0005  max mem: 19734
Epoch: [7]  [  40/1251]  eta: 0:16:43  lr: 0.000020  loss: 3.6968 (3.6339)  time: 0.4560  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5831, ratio_loss=0.0086, pruning_loss=0.1883, mse_loss=0.8025
Epoch: [7]  [  50/1251]  eta: 0:15:19  lr: 0.000020  loss: 3.5176 (3.5594)  time: 0.4812  data: 0.0004  max mem: 19734
Epoch: [7]  [  60/1251]  eta: 0:14:23  lr: 0.000020  loss: 3.5378 (3.5493)  time: 0.5115  data: 0.0005  max mem: 19734
Epoch: [7]  [  70/1251]  eta: 0:13:36  lr: 0.000020  loss: 3.5528 (3.5574)  time: 0.5010  data: 0.0005  max mem: 19734
Epoch: [7]  [  80/1251]  eta: 0:12:56  lr: 0.000020  loss: 3.6417 (3.5615)  time: 0.4745  data: 0.0006  max mem: 19734
Epoch: [7]  [  90/1251]  eta: 0:12:27  lr: 0.000020  loss: 3.6741 (3.5660)  time: 0.4756  data: 0.0006  max mem: 19734
Epoch: [7]  [ 100/1251]  eta: 0:11:59  lr: 0.000020  loss: 3.5911 (3.5570)  time: 0.4736  data: 0.0004  max mem: 19734
Epoch: [7]  [ 110/1251]  eta: 0:11:36  lr: 0.000020  loss: 3.3534 (3.5507)  time: 0.4587  data: 0.0005  max mem: 19734
Epoch: [7]  [ 120/1251]  eta: 0:11:15  lr: 0.000020  loss: 3.5602 (3.5437)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [7]  [ 130/1251]  eta: 0:10:57  lr: 0.000020  loss: 3.7622 (3.5422)  time: 0.4574  data: 0.0005  max mem: 19734
Epoch: [7]  [ 140/1251]  eta: 0:10:41  lr: 0.000020  loss: 3.6149 (3.5294)  time: 0.4566  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4333, ratio_loss=0.0086, pruning_loss=0.1905, mse_loss=0.8079
Epoch: [7]  [ 150/1251]  eta: 0:10:27  lr: 0.000020  loss: 3.4588 (3.5198)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [7]  [ 160/1251]  eta: 0:10:13  lr: 0.000020  loss: 3.5804 (3.5273)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [7]  [ 170/1251]  eta: 0:10:01  lr: 0.000020  loss: 3.5804 (3.5351)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [7]  [ 180/1251]  eta: 0:09:50  lr: 0.000020  loss: 3.7107 (3.5493)  time: 0.4598  data: 0.0005  max mem: 19734
Epoch: [7]  [ 190/1251]  eta: 0:09:41  lr: 0.000020  loss: 3.7421 (3.5399)  time: 0.4707  data: 0.0005  max mem: 19734
Epoch: [7]  [ 200/1251]  eta: 0:09:31  lr: 0.000020  loss: 3.2790 (3.5325)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [7]  [ 210/1251]  eta: 0:09:24  lr: 0.000020  loss: 3.7681 (3.5510)  time: 0.4936  data: 0.0005  max mem: 19734
Epoch: [7]  [ 220/1251]  eta: 0:09:15  lr: 0.000020  loss: 3.7792 (3.5449)  time: 0.4835  data: 0.0007  max mem: 19734
Epoch: [7]  [ 230/1251]  eta: 0:09:06  lr: 0.000020  loss: 3.6257 (3.5485)  time: 0.4556  data: 0.0006  max mem: 19734
Epoch: [7]  [ 240/1251]  eta: 0:08:58  lr: 0.000020  loss: 3.4103 (3.5371)  time: 0.4691  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5237, ratio_loss=0.0082, pruning_loss=0.1846, mse_loss=0.7683
Epoch: [7]  [ 250/1251]  eta: 0:08:50  lr: 0.000020  loss: 3.2613 (3.5195)  time: 0.4695  data: 0.0005  max mem: 19734
Epoch: [7]  [ 260/1251]  eta: 0:08:42  lr: 0.000020  loss: 3.3618 (3.5198)  time: 0.4554  data: 0.0005  max mem: 19734
Epoch: [7]  [ 270/1251]  eta: 0:08:34  lr: 0.000020  loss: 3.6335 (3.5266)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [7]  [ 280/1251]  eta: 0:08:26  lr: 0.000020  loss: 3.6335 (3.5232)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [7]  [ 290/1251]  eta: 0:08:19  lr: 0.000020  loss: 3.1412 (3.5109)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [7]  [ 300/1251]  eta: 0:08:12  lr: 0.000020  loss: 3.3456 (3.5094)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [7]  [ 310/1251]  eta: 0:08:05  lr: 0.000020  loss: 3.5765 (3.5057)  time: 0.4556  data: 0.0006  max mem: 19734
Epoch: [7]  [ 320/1251]  eta: 0:07:58  lr: 0.000020  loss: 3.5495 (3.5073)  time: 0.4582  data: 0.0006  max mem: 19734
Epoch: [7]  [ 330/1251]  eta: 0:07:51  lr: 0.000020  loss: 3.4065 (3.5043)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [7]  [ 340/1251]  eta: 0:07:45  lr: 0.000020  loss: 3.4124 (3.5007)  time: 0.4739  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3769, ratio_loss=0.0087, pruning_loss=0.1865, mse_loss=0.7759
Epoch: [7]  [ 350/1251]  eta: 0:07:40  lr: 0.000020  loss: 3.5340 (3.5082)  time: 0.4918  data: 0.0005  max mem: 19734
Epoch: [7]  [ 360/1251]  eta: 0:07:34  lr: 0.000020  loss: 3.6079 (3.5030)  time: 0.4882  data: 0.0004  max mem: 19734
Epoch: [7]  [ 370/1251]  eta: 0:07:28  lr: 0.000020  loss: 3.3943 (3.5019)  time: 0.4703  data: 0.0005  max mem: 19734
Epoch: [7]  [ 380/1251]  eta: 0:07:22  lr: 0.000020  loss: 3.3691 (3.4999)  time: 0.4616  data: 0.0004  max mem: 19734
Epoch: [7]  [ 390/1251]  eta: 0:07:16  lr: 0.000020  loss: 3.5355 (3.5013)  time: 0.4640  data: 0.0004  max mem: 19734
Epoch: [7]  [ 400/1251]  eta: 0:07:09  lr: 0.000020  loss: 3.3691 (3.4969)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [7]  [ 410/1251]  eta: 0:07:03  lr: 0.000020  loss: 3.3691 (3.5000)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [7]  [ 420/1251]  eta: 0:06:57  lr: 0.000020  loss: 3.6271 (3.4941)  time: 0.4586  data: 0.0004  max mem: 19734
Epoch: [7]  [ 430/1251]  eta: 0:06:52  lr: 0.000020  loss: 3.3156 (3.4913)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [7]  [ 440/1251]  eta: 0:06:46  lr: 0.000020  loss: 3.3931 (3.4941)  time: 0.4558  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4322, ratio_loss=0.0083, pruning_loss=0.1885, mse_loss=0.7989
Epoch: [7]  [ 450/1251]  eta: 0:06:40  lr: 0.000020  loss: 3.6679 (3.4914)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [7]  [ 460/1251]  eta: 0:06:34  lr: 0.000020  loss: 3.7221 (3.4978)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [7]  [ 470/1251]  eta: 0:06:29  lr: 0.000020  loss: 3.7221 (3.4980)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [7]  [ 480/1251]  eta: 0:06:23  lr: 0.000020  loss: 3.3426 (3.4939)  time: 0.4699  data: 0.0005  max mem: 19734
Epoch: [7]  [ 490/1251]  eta: 0:06:18  lr: 0.000020  loss: 3.3477 (3.4914)  time: 0.4734  data: 0.0004  max mem: 19734
Epoch: [7]  [ 500/1251]  eta: 0:06:13  lr: 0.000020  loss: 3.5919 (3.4948)  time: 0.4773  data: 0.0004  max mem: 19734
Epoch: [7]  [ 510/1251]  eta: 0:06:07  lr: 0.000020  loss: 3.6990 (3.4989)  time: 0.4696  data: 0.0007  max mem: 19734
Epoch: [7]  [ 520/1251]  eta: 0:06:02  lr: 0.000020  loss: 3.6841 (3.4993)  time: 0.4542  data: 0.0007  max mem: 19734
Epoch: [7]  [ 530/1251]  eta: 0:05:56  lr: 0.000020  loss: 3.6062 (3.4989)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [7]  [ 540/1251]  eta: 0:05:51  lr: 0.000020  loss: 3.3108 (3.4948)  time: 0.4648  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4576, ratio_loss=0.0082, pruning_loss=0.1835, mse_loss=0.7757
Epoch: [7]  [ 550/1251]  eta: 0:05:45  lr: 0.000020  loss: 3.5216 (3.4972)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [7]  [ 560/1251]  eta: 0:05:40  lr: 0.000020  loss: 3.5989 (3.4946)  time: 0.4533  data: 0.0007  max mem: 19734
Epoch: [7]  [ 570/1251]  eta: 0:05:35  lr: 0.000020  loss: 3.3940 (3.4913)  time: 0.4555  data: 0.0007  max mem: 19734
Epoch: [7]  [ 580/1251]  eta: 0:05:29  lr: 0.000020  loss: 3.3988 (3.4898)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [7]  [ 590/1251]  eta: 0:05:24  lr: 0.000020  loss: 3.6142 (3.4891)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [7]  [ 600/1251]  eta: 0:05:19  lr: 0.000020  loss: 3.5634 (3.4879)  time: 0.4604  data: 0.0005  max mem: 19734
Epoch: [7]  [ 610/1251]  eta: 0:05:13  lr: 0.000020  loss: 3.7913 (3.4933)  time: 0.4577  data: 0.0004  max mem: 19734
Epoch: [7]  [ 620/1251]  eta: 0:05:08  lr: 0.000020  loss: 3.6971 (3.4892)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [7]  [ 630/1251]  eta: 0:05:03  lr: 0.000020  loss: 3.3773 (3.4899)  time: 0.4750  data: 0.0005  max mem: 19734
Epoch: [7]  [ 640/1251]  eta: 0:04:58  lr: 0.000020  loss: 3.7509 (3.4931)  time: 0.4894  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4370, ratio_loss=0.0087, pruning_loss=0.1813, mse_loss=0.7643
Epoch: [7]  [ 650/1251]  eta: 0:04:54  lr: 0.000020  loss: 3.8068 (3.4937)  time: 0.4908  data: 0.0006  max mem: 19734
Epoch: [7]  [ 660/1251]  eta: 0:04:48  lr: 0.000020  loss: 3.8068 (3.4973)  time: 0.4752  data: 0.0004  max mem: 19734
Epoch: [7]  [ 670/1251]  eta: 0:04:43  lr: 0.000020  loss: 3.8757 (3.5012)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [7]  [ 680/1251]  eta: 0:04:38  lr: 0.000020  loss: 3.7544 (3.5021)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [7]  [ 690/1251]  eta: 0:04:33  lr: 0.000020  loss: 3.7438 (3.5044)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [7]  [ 700/1251]  eta: 0:04:28  lr: 0.000020  loss: 3.5182 (3.5022)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [7]  [ 710/1251]  eta: 0:04:23  lr: 0.000020  loss: 3.5268 (3.5032)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [7]  [ 720/1251]  eta: 0:04:18  lr: 0.000020  loss: 3.6480 (3.5038)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [7]  [ 730/1251]  eta: 0:04:13  lr: 0.000020  loss: 3.8070 (3.5079)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [7]  [ 740/1251]  eta: 0:04:07  lr: 0.000020  loss: 3.7916 (3.5068)  time: 0.4551  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5579, ratio_loss=0.0085, pruning_loss=0.1777, mse_loss=0.7412
Epoch: [7]  [ 750/1251]  eta: 0:04:02  lr: 0.000020  loss: 3.1087 (3.4992)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [7]  [ 760/1251]  eta: 0:03:57  lr: 0.000020  loss: 3.0112 (3.4951)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [7]  [ 770/1251]  eta: 0:03:53  lr: 0.000020  loss: 3.5596 (3.4986)  time: 0.4734  data: 0.0004  max mem: 19734
Epoch: [7]  [ 780/1251]  eta: 0:03:48  lr: 0.000020  loss: 3.7654 (3.5009)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [7]  [ 790/1251]  eta: 0:03:43  lr: 0.000020  loss: 3.4672 (3.5002)  time: 0.4910  data: 0.0004  max mem: 19734
Epoch: [7]  [ 800/1251]  eta: 0:03:38  lr: 0.000020  loss: 3.3941 (3.4998)  time: 0.4871  data: 0.0004  max mem: 19734
Epoch: [7]  [ 810/1251]  eta: 0:03:33  lr: 0.000020  loss: 3.6180 (3.4996)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [7]  [ 820/1251]  eta: 0:03:28  lr: 0.000020  loss: 3.4461 (3.4992)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [7]  [ 830/1251]  eta: 0:03:23  lr: 0.000020  loss: 3.5347 (3.5000)  time: 0.4681  data: 0.0005  max mem: 19734
Epoch: [7]  [ 840/1251]  eta: 0:03:18  lr: 0.000020  loss: 3.5467 (3.4976)  time: 0.4672  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3988, ratio_loss=0.0087, pruning_loss=0.1817, mse_loss=0.7250
Epoch: [7]  [ 850/1251]  eta: 0:03:13  lr: 0.000020  loss: 3.3646 (3.4971)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [7]  [ 860/1251]  eta: 0:03:08  lr: 0.000020  loss: 3.4830 (3.4959)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [7]  [ 870/1251]  eta: 0:03:03  lr: 0.000020  loss: 3.6089 (3.4976)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [7]  [ 880/1251]  eta: 0:02:58  lr: 0.000020  loss: 3.6012 (3.4971)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [7]  [ 890/1251]  eta: 0:02:53  lr: 0.000020  loss: 3.3984 (3.4949)  time: 0.4526  data: 0.0007  max mem: 19734
Epoch: [7]  [ 900/1251]  eta: 0:02:48  lr: 0.000020  loss: 3.4291 (3.4962)  time: 0.4534  data: 0.0007  max mem: 19734
Epoch: [7]  [ 910/1251]  eta: 0:02:44  lr: 0.000020  loss: 3.7072 (3.4991)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [7]  [ 920/1251]  eta: 0:02:39  lr: 0.000020  loss: 3.7184 (3.5014)  time: 0.4762  data: 0.0004  max mem: 19734
Epoch: [7]  [ 930/1251]  eta: 0:02:34  lr: 0.000020  loss: 3.6817 (3.5029)  time: 0.5119  data: 0.0004  max mem: 19734
Epoch: [7]  [ 940/1251]  eta: 0:02:29  lr: 0.000020  loss: 3.6208 (3.5043)  time: 0.4919  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5268, ratio_loss=0.0082, pruning_loss=0.1758, mse_loss=0.7623
Epoch: [7]  [ 950/1251]  eta: 0:02:24  lr: 0.000020  loss: 3.6208 (3.5059)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [7]  [ 960/1251]  eta: 0:02:19  lr: 0.000020  loss: 3.5903 (3.5064)  time: 0.4583  data: 0.0005  max mem: 19734
Epoch: [7]  [ 970/1251]  eta: 0:02:15  lr: 0.000020  loss: 3.5794 (3.5047)  time: 0.4597  data: 0.0005  max mem: 19734
Epoch: [7]  [ 980/1251]  eta: 0:02:10  lr: 0.000020  loss: 3.5794 (3.5048)  time: 0.4678  data: 0.0004  max mem: 19734
Epoch: [7]  [ 990/1251]  eta: 0:02:05  lr: 0.000020  loss: 3.6353 (3.5051)  time: 0.4640  data: 0.0004  max mem: 19734
Epoch: [7]  [1000/1251]  eta: 0:02:00  lr: 0.000020  loss: 3.4981 (3.5038)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [7]  [1010/1251]  eta: 0:01:55  lr: 0.000020  loss: 3.4876 (3.5035)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [7]  [1020/1251]  eta: 0:01:50  lr: 0.000020  loss: 3.5264 (3.5018)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [7]  [1030/1251]  eta: 0:01:45  lr: 0.000020  loss: 3.4546 (3.5022)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [7]  [1040/1251]  eta: 0:01:41  lr: 0.000020  loss: 3.5147 (3.5026)  time: 0.4540  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4468, ratio_loss=0.0091, pruning_loss=0.1754, mse_loss=0.7772
Epoch: [7]  [1050/1251]  eta: 0:01:36  lr: 0.000020  loss: 3.5971 (3.5035)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [7]  [1060/1251]  eta: 0:01:31  lr: 0.000020  loss: 3.4937 (3.5009)  time: 0.4871  data: 0.0004  max mem: 19734
Epoch: [7]  [1070/1251]  eta: 0:01:26  lr: 0.000020  loss: 3.5655 (3.5023)  time: 0.5057  data: 0.0006  max mem: 19734
Epoch: [7]  [1080/1251]  eta: 0:01:22  lr: 0.000020  loss: 3.5367 (3.5001)  time: 0.4962  data: 0.0009  max mem: 19734
Epoch: [7]  [1090/1251]  eta: 0:01:17  lr: 0.000020  loss: 3.3504 (3.4989)  time: 0.4797  data: 0.0007  max mem: 19734
Epoch: [7]  [1100/1251]  eta: 0:01:12  lr: 0.000020  loss: 3.3575 (3.4974)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [7]  [1110/1251]  eta: 0:01:07  lr: 0.000020  loss: 3.5486 (3.4975)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [7]  [1120/1251]  eta: 0:01:02  lr: 0.000020  loss: 3.5689 (3.4974)  time: 0.4648  data: 0.0004  max mem: 19734
Epoch: [7]  [1130/1251]  eta: 0:00:57  lr: 0.000020  loss: 3.6268 (3.4972)  time: 0.4608  data: 0.0004  max mem: 19734
Epoch: [7]  [1140/1251]  eta: 0:00:53  lr: 0.000020  loss: 3.5463 (3.4957)  time: 0.4502  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3719, ratio_loss=0.0083, pruning_loss=0.1766, mse_loss=0.7775
Epoch: [7]  [1150/1251]  eta: 0:00:48  lr: 0.000020  loss: 3.3633 (3.4946)  time: 0.4516  data: 0.0005  max mem: 19734
Epoch: [7]  [1160/1251]  eta: 0:00:43  lr: 0.000020  loss: 3.7101 (3.4967)  time: 0.4520  data: 0.0005  max mem: 19734
Epoch: [7]  [1170/1251]  eta: 0:00:38  lr: 0.000020  loss: 3.7382 (3.4966)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [7]  [1180/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.5606 (3.4958)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [7]  [1190/1251]  eta: 0:00:29  lr: 0.000020  loss: 3.5688 (3.4967)  time: 0.4571  data: 0.0007  max mem: 19734
Epoch: [7]  [1200/1251]  eta: 0:00:24  lr: 0.000020  loss: 3.5157 (3.4939)  time: 0.4555  data: 0.0005  max mem: 19734
Epoch: [7]  [1210/1251]  eta: 0:00:19  lr: 0.000020  loss: 3.2096 (3.4939)  time: 0.4636  data: 0.0002  max mem: 19734
Epoch: [7]  [1220/1251]  eta: 0:00:14  lr: 0.000020  loss: 3.5222 (3.4933)  time: 0.4714  data: 0.0002  max mem: 19734
Epoch: [7]  [1230/1251]  eta: 0:00:10  lr: 0.000020  loss: 3.5222 (3.4932)  time: 0.4774  data: 0.0002  max mem: 19734
Epoch: [7]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 3.4886 (3.4921)  time: 0.4678  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4272, ratio_loss=0.0090, pruning_loss=0.1742, mse_loss=0.7609
Epoch: [7]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.5166 (3.4931)  time: 0.4501  data: 0.0002  max mem: 19734
Epoch: [7] Total time: 0:09:57 (0.4775 s / it)
Averaged stats: lr: 0.000020  loss: 3.5166 (3.4895)
Test:  [  0/261]  eta: 2:10:12  loss: 0.9331 (0.9331)  acc1: 80.2083 (80.2083)  acc5: 92.7083 (92.7083)  time: 29.9339  data: 29.6731  max mem: 19734
Test:  [ 10/261]  eta: 0:12:51  loss: 0.7685 (0.8364)  acc1: 83.8542 (82.8599)  acc5: 95.3125 (95.6439)  time: 3.0721  data: 2.8065  max mem: 19734
Test:  [ 20/261]  eta: 0:06:55  loss: 0.9955 (0.9808)  acc1: 78.6458 (78.3482)  acc5: 93.2292 (94.4196)  time: 0.3138  data: 0.0726  max mem: 19734
Test:  [ 30/261]  eta: 0:04:46  loss: 0.8382 (0.8992)  acc1: 82.2917 (81.1492)  acc5: 93.7500 (94.9429)  time: 0.2355  data: 0.0216  max mem: 19734
Test:  [ 40/261]  eta: 0:04:16  loss: 0.6625 (0.8743)  acc1: 87.5000 (81.9106)  acc5: 96.3542 (95.1220)  time: 0.5657  data: 0.3713  max mem: 19734
Test:  [ 50/261]  eta: 0:03:26  loss: 1.0134 (0.9432)  acc1: 76.5625 (79.8918)  acc5: 93.7500 (94.5159)  time: 0.5653  data: 0.3699  max mem: 19734
Test:  [ 60/261]  eta: 0:02:50  loss: 1.0772 (0.9556)  acc1: 75.5208 (79.3887)  acc5: 93.7500 (94.5355)  time: 0.2068  data: 0.0149  max mem: 19734
Test:  [ 70/261]  eta: 0:02:25  loss: 1.0610 (0.9537)  acc1: 76.5625 (79.0640)  acc5: 95.3125 (94.7770)  time: 0.2191  data: 0.0129  max mem: 19734
Test:  [ 80/261]  eta: 0:02:08  loss: 0.9272 (0.9611)  acc1: 79.1667 (79.1602)  acc5: 95.8333 (94.8431)  time: 0.2830  data: 0.0843  max mem: 19734
Test:  [ 90/261]  eta: 0:01:50  loss: 0.9019 (0.9465)  acc1: 82.2917 (79.5559)  acc5: 95.3125 (94.9462)  time: 0.2402  data: 0.0869  max mem: 19734
Test:  [100/261]  eta: 0:01:42  loss: 0.8776 (0.9471)  acc1: 82.2917 (79.5586)  acc5: 95.3125 (94.9567)  time: 0.3510  data: 0.1973  max mem: 19734
Test:  [110/261]  eta: 0:01:29  loss: 0.9957 (0.9732)  acc1: 77.6042 (79.0494)  acc5: 93.7500 (94.5852)  time: 0.3509  data: 0.1955  max mem: 19734
Test:  [120/261]  eta: 0:01:18  loss: 1.3069 (1.0175)  acc1: 68.7500 (78.0475)  acc5: 88.5417 (94.0255)  time: 0.1674  data: 0.0111  max mem: 19734
Test:  [130/261]  eta: 0:01:12  loss: 1.5584 (1.0659)  acc1: 67.7083 (77.0913)  acc5: 86.4583 (93.3922)  time: 0.3019  data: 0.1431  max mem: 19734
Test:  [140/261]  eta: 0:01:05  loss: 1.4459 (1.0930)  acc1: 67.1875 (76.4221)  acc5: 88.5417 (93.1442)  time: 0.4094  data: 0.2797  max mem: 19734
Test:  [150/261]  eta: 0:00:57  loss: 1.3259 (1.0980)  acc1: 70.8333 (76.4349)  acc5: 90.1042 (92.9636)  time: 0.3333  data: 0.2313  max mem: 19734
Test:  [160/261]  eta: 0:00:50  loss: 1.1282 (1.1208)  acc1: 77.0833 (76.0093)  acc5: 91.1458 (92.6566)  time: 0.2011  data: 0.0918  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: 1.4767 (1.1538)  acc1: 63.5417 (75.1614)  acc5: 86.4583 (92.2850)  time: 0.3923  data: 0.2773  max mem: 19734
Test:  [180/261]  eta: 0:00:40  loss: 1.6284 (1.1739)  acc1: 63.5417 (74.7151)  acc5: 86.4583 (92.0724)  time: 0.4993  data: 0.3673  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: 1.4997 (1.1877)  acc1: 67.1875 (74.4710)  acc5: 89.5833 (91.8985)  time: 0.2172  data: 0.0965  max mem: 19734
Test:  [200/261]  eta: 0:00:28  loss: 1.5218 (1.2046)  acc1: 69.2708 (74.1345)  acc5: 88.0208 (91.6900)  time: 0.1415  data: 0.0573  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: 1.5659 (1.2198)  acc1: 67.7083 (73.8645)  acc5: 86.9792 (91.4643)  time: 0.1214  data: 0.0525  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: 1.6096 (1.2390)  acc1: 65.6250 (73.4422)  acc5: 85.9375 (91.2707)  time: 0.0621  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.5286 (1.2494)  acc1: 64.5833 (73.2120)  acc5: 88.0208 (91.1210)  time: 0.0618  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.4668 (1.2588)  acc1: 65.1042 (72.9750)  acc5: 89.0625 (91.0335)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.2756 (1.2532)  acc1: 74.4792 (73.1553)  acc5: 92.1875 (91.1417)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.1210 (1.2519)  acc1: 76.5625 (73.1900)  acc5: 94.7917 (91.2080)  time: 0.0598  data: 0.0001  max mem: 19734
Test: Total time: 0:01:36 (0.3703 s / it)
* Acc@1 73.190 Acc@5 91.208 loss 1.252
Accuracy of the network on the 50000 test images: 73.2%
Max accuracy: 73.19%
Epoch: [8]  [   0/1251]  eta: 4:23:22  lr: 0.000020  loss: 3.6407 (3.6407)  time: 12.6316  data: 12.1914  max mem: 19734
Epoch: [8]  [  10/1251]  eta: 0:34:31  lr: 0.000020  loss: 3.7315 (3.7119)  time: 1.6695  data: 1.1088  max mem: 19734
Epoch: [8]  [  20/1251]  eta: 0:22:24  lr: 0.000020  loss: 3.7315 (3.6960)  time: 0.5149  data: 0.0006  max mem: 19734
Epoch: [8]  [  30/1251]  eta: 0:18:03  lr: 0.000020  loss: 3.7603 (3.7093)  time: 0.4577  data: 0.0007  max mem: 19734
Epoch: [8]  [  40/1251]  eta: 0:15:48  lr: 0.000020  loss: 3.6960 (3.5946)  time: 0.4595  data: 0.0006  max mem: 19734
Epoch: [8]  [  50/1251]  eta: 0:14:25  lr: 0.000020  loss: 3.6960 (3.6322)  time: 0.4614  data: 0.0005  max mem: 19734
Epoch: [8]  [  60/1251]  eta: 0:13:27  lr: 0.000020  loss: 3.7387 (3.6161)  time: 0.4612  data: 0.0005  max mem: 19734
Epoch: [8]  [  70/1251]  eta: 0:12:45  lr: 0.000020  loss: 3.5814 (3.6175)  time: 0.4632  data: 0.0005  max mem: 19734
Epoch: [8]  [  80/1251]  eta: 0:12:12  lr: 0.000020  loss: 3.6732 (3.6082)  time: 0.4674  data: 0.0006  max mem: 19734
Epoch: [8]  [  90/1251]  eta: 0:11:46  lr: 0.000020  loss: 3.6732 (3.6057)  time: 0.4679  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5649, ratio_loss=0.0082, pruning_loss=0.1695, mse_loss=0.7307
Epoch: [8]  [ 100/1251]  eta: 0:11:35  lr: 0.000020  loss: 3.5545 (3.5842)  time: 0.5185  data: 0.0005  max mem: 19734
Epoch: [8]  [ 110/1251]  eta: 0:11:20  lr: 0.000020  loss: 3.6418 (3.5950)  time: 0.5411  data: 0.0006  max mem: 19734
Epoch: [8]  [ 120/1251]  eta: 0:11:04  lr: 0.000020  loss: 3.6418 (3.5856)  time: 0.4990  data: 0.0007  max mem: 19734
Epoch: [8]  [ 130/1251]  eta: 0:10:47  lr: 0.000020  loss: 3.5785 (3.5839)  time: 0.4727  data: 0.0007  max mem: 19734
Epoch: [8]  [ 140/1251]  eta: 0:10:32  lr: 0.000020  loss: 3.5470 (3.5624)  time: 0.4617  data: 0.0007  max mem: 19734
Epoch: [8]  [ 150/1251]  eta: 0:10:18  lr: 0.000020  loss: 3.1954 (3.5504)  time: 0.4624  data: 0.0005  max mem: 19734
Epoch: [8]  [ 160/1251]  eta: 0:10:06  lr: 0.000020  loss: 3.5139 (3.5576)  time: 0.4610  data: 0.0005  max mem: 19734
Epoch: [8]  [ 170/1251]  eta: 0:09:54  lr: 0.000020  loss: 3.6609 (3.5646)  time: 0.4613  data: 0.0006  max mem: 19734
Epoch: [8]  [ 180/1251]  eta: 0:09:44  lr: 0.000020  loss: 3.7435 (3.5528)  time: 0.4607  data: 0.0006  max mem: 19734
Epoch: [8]  [ 190/1251]  eta: 0:09:33  lr: 0.000020  loss: 3.7356 (3.5617)  time: 0.4574  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4761, ratio_loss=0.0082, pruning_loss=0.1739, mse_loss=0.7332
Epoch: [8]  [ 200/1251]  eta: 0:09:24  lr: 0.000020  loss: 3.6994 (3.5654)  time: 0.4586  data: 0.0005  max mem: 19734
Epoch: [8]  [ 210/1251]  eta: 0:09:14  lr: 0.000020  loss: 3.5130 (3.5530)  time: 0.4596  data: 0.0006  max mem: 19734
Epoch: [8]  [ 220/1251]  eta: 0:09:05  lr: 0.000020  loss: 3.1852 (3.5389)  time: 0.4578  data: 0.0006  max mem: 19734
Epoch: [8]  [ 230/1251]  eta: 0:08:57  lr: 0.000020  loss: 3.2841 (3.5371)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [8]  [ 240/1251]  eta: 0:08:51  lr: 0.000020  loss: 3.6774 (3.5464)  time: 0.4865  data: 0.0005  max mem: 19734
Epoch: [8]  [ 250/1251]  eta: 0:08:44  lr: 0.000020  loss: 3.8476 (3.5576)  time: 0.4972  data: 0.0005  max mem: 19734
Epoch: [8]  [ 260/1251]  eta: 0:08:39  lr: 0.000020  loss: 3.8102 (3.5592)  time: 0.5032  data: 0.0006  max mem: 19734
Epoch: [8]  [ 270/1251]  eta: 0:08:31  lr: 0.000020  loss: 3.5508 (3.5513)  time: 0.4909  data: 0.0006  max mem: 19734
Epoch: [8]  [ 280/1251]  eta: 0:08:24  lr: 0.000020  loss: 3.5416 (3.5552)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [8]  [ 290/1251]  eta: 0:08:17  lr: 0.000020  loss: 3.5725 (3.5541)  time: 0.4557  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5128, ratio_loss=0.0084, pruning_loss=0.1700, mse_loss=0.7245
Epoch: [8]  [ 300/1251]  eta: 0:08:10  lr: 0.000020  loss: 3.6045 (3.5610)  time: 0.4585  data: 0.0005  max mem: 19734
Epoch: [8]  [ 310/1251]  eta: 0:08:03  lr: 0.000020  loss: 3.4499 (3.5518)  time: 0.4588  data: 0.0005  max mem: 19734
Epoch: [8]  [ 320/1251]  eta: 0:07:56  lr: 0.000020  loss: 3.3172 (3.5476)  time: 0.4579  data: 0.0006  max mem: 19734
Epoch: [8]  [ 330/1251]  eta: 0:07:49  lr: 0.000020  loss: 3.6607 (3.5514)  time: 0.4568  data: 0.0006  max mem: 19734
Epoch: [8]  [ 340/1251]  eta: 0:07:43  lr: 0.000020  loss: 3.6607 (3.5507)  time: 0.4550  data: 0.0006  max mem: 19734
Epoch: [8]  [ 350/1251]  eta: 0:07:36  lr: 0.000020  loss: 3.6976 (3.5516)  time: 0.4541  data: 0.0006  max mem: 19734
Epoch: [8]  [ 360/1251]  eta: 0:07:30  lr: 0.000020  loss: 3.6976 (3.5528)  time: 0.4553  data: 0.0005  max mem: 19734
Epoch: [8]  [ 370/1251]  eta: 0:07:24  lr: 0.000020  loss: 3.5178 (3.5419)  time: 0.4546  data: 0.0006  max mem: 19734
Epoch: [8]  [ 380/1251]  eta: 0:07:18  lr: 0.000020  loss: 3.1106 (3.5371)  time: 0.4636  data: 0.0006  max mem: 19734
Epoch: [8]  [ 390/1251]  eta: 0:07:13  lr: 0.000020  loss: 3.4404 (3.5351)  time: 0.4969  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4377, ratio_loss=0.0082, pruning_loss=0.1694, mse_loss=0.7054
Epoch: [8]  [ 400/1251]  eta: 0:07:07  lr: 0.000020  loss: 3.3928 (3.5280)  time: 0.4958  data: 0.0005  max mem: 19734
Epoch: [8]  [ 410/1251]  eta: 0:07:02  lr: 0.000020  loss: 3.3928 (3.5276)  time: 0.4846  data: 0.0006  max mem: 19734
Epoch: [8]  [ 420/1251]  eta: 0:06:56  lr: 0.000020  loss: 3.3331 (3.5202)  time: 0.4762  data: 0.0006  max mem: 19734
Epoch: [8]  [ 430/1251]  eta: 0:06:50  lr: 0.000020  loss: 3.3331 (3.5167)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [8]  [ 440/1251]  eta: 0:06:44  lr: 0.000020  loss: 3.3965 (3.5186)  time: 0.4523  data: 0.0007  max mem: 19734
Epoch: [8]  [ 450/1251]  eta: 0:06:39  lr: 0.000020  loss: 3.5622 (3.5172)  time: 0.4529  data: 0.0007  max mem: 19734
Epoch: [8]  [ 460/1251]  eta: 0:06:33  lr: 0.000020  loss: 3.6527 (3.5165)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [8]  [ 470/1251]  eta: 0:06:27  lr: 0.000020  loss: 3.7276 (3.5201)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [8]  [ 480/1251]  eta: 0:06:22  lr: 0.000020  loss: 3.5922 (3.5172)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [8]  [ 490/1251]  eta: 0:06:16  lr: 0.000020  loss: 3.5288 (3.5184)  time: 0.4550  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4187, ratio_loss=0.0080, pruning_loss=0.1729, mse_loss=0.7211
Epoch: [8]  [ 500/1251]  eta: 0:06:10  lr: 0.000020  loss: 3.6555 (3.5210)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [8]  [ 510/1251]  eta: 0:06:05  lr: 0.000020  loss: 3.5467 (3.5204)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [8]  [ 520/1251]  eta: 0:06:00  lr: 0.000020  loss: 3.5418 (3.5224)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [8]  [ 530/1251]  eta: 0:05:55  lr: 0.000020  loss: 3.2661 (3.5153)  time: 0.4834  data: 0.0005  max mem: 19734
Epoch: [8]  [ 540/1251]  eta: 0:05:50  lr: 0.000020  loss: 3.1255 (3.5174)  time: 0.5105  data: 0.0005  max mem: 19734
Epoch: [8]  [ 550/1251]  eta: 0:05:45  lr: 0.000020  loss: 3.7157 (3.5152)  time: 0.4924  data: 0.0005  max mem: 19734
Epoch: [8]  [ 560/1251]  eta: 0:05:40  lr: 0.000020  loss: 3.1164 (3.5063)  time: 0.4731  data: 0.0004  max mem: 19734
Epoch: [8]  [ 570/1251]  eta: 0:05:34  lr: 0.000020  loss: 3.1164 (3.5064)  time: 0.4640  data: 0.0004  max mem: 19734
Epoch: [8]  [ 580/1251]  eta: 0:05:29  lr: 0.000020  loss: 3.7096 (3.5073)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [8]  [ 590/1251]  eta: 0:05:24  lr: 0.000020  loss: 3.4436 (3.5037)  time: 0.4569  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3955, ratio_loss=0.0076, pruning_loss=0.1716, mse_loss=0.7176
Epoch: [8]  [ 600/1251]  eta: 0:05:19  lr: 0.000020  loss: 3.4436 (3.5027)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [8]  [ 610/1251]  eta: 0:05:13  lr: 0.000020  loss: 3.4620 (3.5018)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [8]  [ 620/1251]  eta: 0:05:08  lr: 0.000020  loss: 3.4620 (3.5005)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [8]  [ 630/1251]  eta: 0:05:03  lr: 0.000020  loss: 3.6599 (3.5061)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [8]  [ 640/1251]  eta: 0:04:58  lr: 0.000020  loss: 3.6251 (3.5033)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [8]  [ 650/1251]  eta: 0:04:52  lr: 0.000020  loss: 3.3371 (3.5015)  time: 0.4513  data: 0.0004  max mem: 19734
Epoch: [8]  [ 660/1251]  eta: 0:04:47  lr: 0.000020  loss: 3.4252 (3.5031)  time: 0.4503  data: 0.0004  max mem: 19734
Epoch: [8]  [ 670/1251]  eta: 0:04:42  lr: 0.000020  loss: 3.5536 (3.5027)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [8]  [ 680/1251]  eta: 0:04:37  lr: 0.000020  loss: 3.7173 (3.5069)  time: 0.4922  data: 0.0004  max mem: 19734
Epoch: [8]  [ 690/1251]  eta: 0:04:33  lr: 0.000020  loss: 3.7110 (3.5070)  time: 0.4994  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4844, ratio_loss=0.0081, pruning_loss=0.1678, mse_loss=0.7293
Epoch: [8]  [ 700/1251]  eta: 0:04:28  lr: 0.000020  loss: 3.5951 (3.5089)  time: 0.4783  data: 0.0004  max mem: 19734
Epoch: [8]  [ 710/1251]  eta: 0:04:23  lr: 0.000020  loss: 3.6579 (3.5064)  time: 0.4757  data: 0.0004  max mem: 19734
Epoch: [8]  [ 720/1251]  eta: 0:04:18  lr: 0.000020  loss: 3.6579 (3.5053)  time: 0.4658  data: 0.0005  max mem: 19734
Epoch: [8]  [ 730/1251]  eta: 0:04:13  lr: 0.000020  loss: 3.7075 (3.5067)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [8]  [ 740/1251]  eta: 0:04:08  lr: 0.000020  loss: 3.7073 (3.5074)  time: 0.4623  data: 0.0004  max mem: 19734
Epoch: [8]  [ 750/1251]  eta: 0:04:02  lr: 0.000020  loss: 3.5516 (3.5075)  time: 0.4615  data: 0.0004  max mem: 19734
Epoch: [8]  [ 760/1251]  eta: 0:03:57  lr: 0.000020  loss: 3.4361 (3.5055)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [8]  [ 770/1251]  eta: 0:03:52  lr: 0.000020  loss: 3.3379 (3.5034)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [8]  [ 780/1251]  eta: 0:03:47  lr: 0.000020  loss: 3.5270 (3.5049)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [8]  [ 790/1251]  eta: 0:03:42  lr: 0.000020  loss: 3.4757 (3.5006)  time: 0.4533  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4300, ratio_loss=0.0082, pruning_loss=0.1675, mse_loss=0.6950
Epoch: [8]  [ 800/1251]  eta: 0:03:37  lr: 0.000020  loss: 3.4443 (3.5018)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [8]  [ 810/1251]  eta: 0:03:32  lr: 0.000020  loss: 3.7635 (3.5012)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [8]  [ 820/1251]  eta: 0:03:28  lr: 0.000020  loss: 3.4271 (3.4977)  time: 0.5001  data: 0.0007  max mem: 19734
Epoch: [8]  [ 830/1251]  eta: 0:03:23  lr: 0.000020  loss: 3.4802 (3.4985)  time: 0.5121  data: 0.0008  max mem: 19734
Epoch: [8]  [ 840/1251]  eta: 0:03:18  lr: 0.000020  loss: 3.6393 (3.4982)  time: 0.4759  data: 0.0004  max mem: 19734
Epoch: [8]  [ 850/1251]  eta: 0:03:13  lr: 0.000020  loss: 3.7524 (3.4979)  time: 0.4613  data: 0.0004  max mem: 19734
Epoch: [8]  [ 860/1251]  eta: 0:03:08  lr: 0.000020  loss: 3.6878 (3.4988)  time: 0.4615  data: 0.0004  max mem: 19734
Epoch: [8]  [ 870/1251]  eta: 0:03:03  lr: 0.000020  loss: 3.6569 (3.4996)  time: 0.4631  data: 0.0004  max mem: 19734
Epoch: [8]  [ 880/1251]  eta: 0:02:58  lr: 0.000020  loss: 3.6028 (3.4989)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [8]  [ 890/1251]  eta: 0:02:53  lr: 0.000020  loss: 3.4516 (3.4972)  time: 0.4517  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4185, ratio_loss=0.0078, pruning_loss=0.1684, mse_loss=0.7162
Epoch: [8]  [ 900/1251]  eta: 0:02:48  lr: 0.000020  loss: 3.6204 (3.4984)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [8]  [ 910/1251]  eta: 0:02:44  lr: 0.000020  loss: 3.7290 (3.4957)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [8]  [ 920/1251]  eta: 0:02:39  lr: 0.000020  loss: 3.1884 (3.4935)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [8]  [ 930/1251]  eta: 0:02:34  lr: 0.000020  loss: 3.2771 (3.4921)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [8]  [ 940/1251]  eta: 0:02:29  lr: 0.000020  loss: 3.5558 (3.4940)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [8]  [ 950/1251]  eta: 0:02:24  lr: 0.000020  loss: 3.5900 (3.4923)  time: 0.4512  data: 0.0004  max mem: 19734
Epoch: [8]  [ 960/1251]  eta: 0:02:19  lr: 0.000020  loss: 3.6362 (3.4945)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [8]  [ 970/1251]  eta: 0:02:15  lr: 0.000020  loss: 3.6297 (3.4938)  time: 0.5057  data: 0.0005  max mem: 19734
Epoch: [8]  [ 980/1251]  eta: 0:02:10  lr: 0.000020  loss: 3.5650 (3.4931)  time: 0.5177  data: 0.0004  max mem: 19734
Epoch: [8]  [ 990/1251]  eta: 0:02:05  lr: 0.000020  loss: 3.5645 (3.4931)  time: 0.4829  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4329, ratio_loss=0.0080, pruning_loss=0.1658, mse_loss=0.7123
Epoch: [8]  [1000/1251]  eta: 0:02:00  lr: 0.000020  loss: 3.6487 (3.4962)  time: 0.4608  data: 0.0004  max mem: 19734
Epoch: [8]  [1010/1251]  eta: 0:01:55  lr: 0.000020  loss: 3.8112 (3.4975)  time: 0.4662  data: 0.0004  max mem: 19734
Epoch: [8]  [1020/1251]  eta: 0:01:50  lr: 0.000020  loss: 3.5817 (3.4978)  time: 0.4639  data: 0.0005  max mem: 19734
Epoch: [8]  [1030/1251]  eta: 0:01:46  lr: 0.000020  loss: 3.5591 (3.4979)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [8]  [1040/1251]  eta: 0:01:41  lr: 0.000020  loss: 3.4993 (3.4984)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [8]  [1050/1251]  eta: 0:01:36  lr: 0.000020  loss: 3.4984 (3.4960)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [8]  [1060/1251]  eta: 0:01:31  lr: 0.000020  loss: 3.5793 (3.4967)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [8]  [1070/1251]  eta: 0:01:26  lr: 0.000020  loss: 3.6224 (3.4959)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [8]  [1080/1251]  eta: 0:01:21  lr: 0.000020  loss: 3.2747 (3.4949)  time: 0.4513  data: 0.0004  max mem: 19734
Epoch: [8]  [1090/1251]  eta: 0:01:16  lr: 0.000020  loss: 3.6581 (3.4967)  time: 0.4519  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4908, ratio_loss=0.0081, pruning_loss=0.1643, mse_loss=0.7332
Epoch: [8]  [1100/1251]  eta: 0:01:12  lr: 0.000020  loss: 3.6183 (3.4943)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [8]  [1110/1251]  eta: 0:01:07  lr: 0.000020  loss: 3.4344 (3.4945)  time: 0.4785  data: 0.0005  max mem: 19734
Epoch: [8]  [1120/1251]  eta: 0:01:02  lr: 0.000020  loss: 3.5088 (3.4949)  time: 0.4979  data: 0.0005  max mem: 19734
Epoch: [8]  [1130/1251]  eta: 0:00:57  lr: 0.000020  loss: 3.5244 (3.4938)  time: 0.4876  data: 0.0005  max mem: 19734
Epoch: [8]  [1140/1251]  eta: 0:00:53  lr: 0.000020  loss: 3.5505 (3.4949)  time: 0.4685  data: 0.0005  max mem: 19734
Epoch: [8]  [1150/1251]  eta: 0:00:48  lr: 0.000020  loss: 3.5510 (3.4955)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [8]  [1160/1251]  eta: 0:00:43  lr: 0.000020  loss: 3.4662 (3.4949)  time: 0.4662  data: 0.0005  max mem: 19734
Epoch: [8]  [1170/1251]  eta: 0:00:38  lr: 0.000020  loss: 3.4477 (3.4934)  time: 0.4643  data: 0.0005  max mem: 19734
Epoch: [8]  [1180/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.4477 (3.4913)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [8]  [1190/1251]  eta: 0:00:29  lr: 0.000020  loss: 3.6165 (3.4910)  time: 0.4537  data: 0.0008  max mem: 19734
loss info: cls_loss=3.4013, ratio_loss=0.0084, pruning_loss=0.1660, mse_loss=0.6968
Epoch: [8]  [1200/1251]  eta: 0:00:24  lr: 0.000020  loss: 3.6630 (3.4915)  time: 0.4510  data: 0.0006  max mem: 19734
Epoch: [8]  [1210/1251]  eta: 0:00:19  lr: 0.000020  loss: 3.7537 (3.4924)  time: 0.4474  data: 0.0002  max mem: 19734
Epoch: [8]  [1220/1251]  eta: 0:00:14  lr: 0.000020  loss: 3.4604 (3.4912)  time: 0.4473  data: 0.0002  max mem: 19734
Epoch: [8]  [1230/1251]  eta: 0:00:10  lr: 0.000020  loss: 3.4285 (3.4916)  time: 0.4467  data: 0.0002  max mem: 19734
Epoch: [8]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 3.7089 (3.4908)  time: 0.4451  data: 0.0001  max mem: 19734
Epoch: [8]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.7855 (3.4923)  time: 0.4460  data: 0.0001  max mem: 19734
Epoch: [8] Total time: 0:09:56 (0.4765 s / it)
Averaged stats: lr: 0.000020  loss: 3.7855 (3.4817)
Test:  [  0/261]  eta: 0:50:22  loss: 0.7867 (0.7867)  acc1: 83.8542 (83.8542)  acc5: 94.7917 (94.7917)  time: 11.5808  data: 11.4414  max mem: 19734
Test:  [ 10/261]  eta: 0:09:50  loss: 0.7545 (0.8132)  acc1: 83.8542 (83.3333)  acc5: 96.3542 (95.5019)  time: 2.3514  data: 2.2300  max mem: 19734
Test:  [ 20/261]  eta: 0:05:14  loss: 1.0091 (0.9653)  acc1: 78.1250 (78.5962)  acc5: 92.7083 (94.0476)  time: 0.7919  data: 0.6619  max mem: 19734
Test:  [ 30/261]  eta: 0:03:32  loss: 0.8711 (0.8880)  acc1: 80.7292 (81.4012)  acc5: 94.2708 (94.5229)  time: 0.1333  data: 0.0142  max mem: 19734
Test:  [ 40/261]  eta: 0:03:13  loss: 0.6670 (0.8541)  acc1: 86.9792 (82.3552)  acc5: 95.8333 (94.8806)  time: 0.4251  data: 0.3001  max mem: 19734
Test:  [ 50/261]  eta: 0:02:36  loss: 0.9643 (0.9165)  acc1: 77.0833 (80.5862)  acc5: 94.2708 (94.4649)  time: 0.4706  data: 0.2989  max mem: 19734
Test:  [ 60/261]  eta: 0:02:09  loss: 1.0540 (0.9274)  acc1: 76.0417 (80.1230)  acc5: 93.7500 (94.4928)  time: 0.1716  data: 0.0108  max mem: 19734
Test:  [ 70/261]  eta: 0:02:12  loss: 1.0116 (0.9295)  acc1: 78.1250 (79.5481)  acc5: 95.3125 (94.6890)  time: 0.5741  data: 0.4183  max mem: 19734
Test:  [ 80/261]  eta: 0:01:57  loss: 0.9356 (0.9366)  acc1: 79.1667 (79.6425)  acc5: 95.3125 (94.7595)  time: 0.6581  data: 0.4497  max mem: 19734
Test:  [ 90/261]  eta: 0:01:41  loss: 0.9013 (0.9223)  acc1: 81.2500 (80.0710)  acc5: 95.3125 (94.8832)  time: 0.2362  data: 0.0453  max mem: 19734
Test:  [100/261]  eta: 0:01:39  loss: 0.8610 (0.9247)  acc1: 82.2917 (80.0175)  acc5: 95.3125 (94.9361)  time: 0.4920  data: 0.3549  max mem: 19734
Test:  [110/261]  eta: 0:01:26  loss: 0.9738 (0.9492)  acc1: 75.5208 (79.5186)  acc5: 93.7500 (94.6462)  time: 0.4827  data: 0.3547  max mem: 19734
Test:  [120/261]  eta: 0:01:15  loss: 1.3189 (0.9921)  acc1: 69.2708 (78.5899)  acc5: 89.0625 (94.0771)  time: 0.1332  data: 0.0140  max mem: 19734
Test:  [130/261]  eta: 0:01:11  loss: 1.5335 (1.0407)  acc1: 67.1875 (77.5883)  acc5: 86.9792 (93.4637)  time: 0.3866  data: 0.2558  max mem: 19734
Test:  [140/261]  eta: 0:01:03  loss: 1.3669 (1.0682)  acc1: 67.1875 (76.9134)  acc5: 88.5417 (93.2255)  time: 0.4300  data: 0.2583  max mem: 19734
Test:  [150/261]  eta: 0:00:55  loss: 1.3237 (1.0753)  acc1: 71.3542 (76.8453)  acc5: 90.6250 (93.0326)  time: 0.1671  data: 0.0144  max mem: 19734
Test:  [160/261]  eta: 0:00:47  loss: 1.1878 (1.0989)  acc1: 77.0833 (76.4460)  acc5: 90.6250 (92.6922)  time: 0.1322  data: 0.0118  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: 1.5054 (1.1308)  acc1: 64.5833 (75.6122)  acc5: 86.4583 (92.3124)  time: 0.2187  data: 0.1059  max mem: 19734
Test:  [180/261]  eta: 0:00:35  loss: 1.5962 (1.1490)  acc1: 63.0208 (75.1727)  acc5: 86.9792 (92.1300)  time: 0.1932  data: 0.1042  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: 1.4489 (1.1628)  acc1: 67.7083 (74.8882)  acc5: 89.5833 (91.9475)  time: 0.2221  data: 0.1446  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.4489 (1.1796)  acc1: 69.2708 (74.5388)  acc5: 87.5000 (91.7159)  time: 0.2529  data: 0.1805  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.4822 (1.1938)  acc1: 67.7083 (74.2694)  acc5: 86.9792 (91.5235)  time: 0.1302  data: 0.0644  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4985 (1.2127)  acc1: 66.1458 (73.7839)  acc5: 88.0208 (91.3414)  time: 0.0863  data: 0.0249  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.5358 (1.2231)  acc1: 63.5417 (73.5051)  acc5: 88.5417 (91.2112)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.4529 (1.2332)  acc1: 64.5833 (73.2408)  acc5: 89.0625 (91.1307)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.2347 (1.2264)  acc1: 72.9167 (73.4064)  acc5: 93.2292 (91.2496)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.1079 (1.2250)  acc1: 73.9583 (73.4400)  acc5: 94.2708 (91.3220)  time: 0.0599  data: 0.0002  max mem: 19734
Test: Total time: 0:01:29 (0.3431 s / it)
* Acc@1 73.440 Acc@5 91.322 loss 1.225
Accuracy of the network on the 50000 test images: 73.4%
Max accuracy: 73.44%
Epoch: [9]  [   0/1251]  eta: 4:03:37  lr: 0.000020  loss: 2.7351 (2.7351)  time: 11.6845  data: 11.0949  max mem: 19734
Epoch: [9]  [  10/1251]  eta: 0:33:33  lr: 0.000020  loss: 3.4297 (3.3621)  time: 1.6223  data: 1.0092  max mem: 19734
Epoch: [9]  [  20/1251]  eta: 0:22:10  lr: 0.000020  loss: 3.2991 (3.2792)  time: 0.5511  data: 0.0005  max mem: 19734
Epoch: [9]  [  30/1251]  eta: 0:17:54  lr: 0.000020  loss: 3.4790 (3.3871)  time: 0.4713  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4362, ratio_loss=0.0078, pruning_loss=0.1642, mse_loss=0.6902
Epoch: [9]  [  40/1251]  eta: 0:15:42  lr: 0.000020  loss: 3.7249 (3.4226)  time: 0.4594  data: 0.0004  max mem: 19734
Epoch: [9]  [  50/1251]  eta: 0:14:20  lr: 0.000020  loss: 3.7166 (3.4629)  time: 0.4628  data: 0.0005  max mem: 19734
Epoch: [9]  [  60/1251]  eta: 0:13:23  lr: 0.000020  loss: 3.5751 (3.4538)  time: 0.4620  data: 0.0005  max mem: 19734
Epoch: [9]  [  70/1251]  eta: 0:12:41  lr: 0.000020  loss: 3.4974 (3.4818)  time: 0.4638  data: 0.0004  max mem: 19734
Epoch: [9]  [  80/1251]  eta: 0:12:08  lr: 0.000020  loss: 3.5499 (3.4864)  time: 0.4631  data: 0.0003  max mem: 19734
Epoch: [9]  [  90/1251]  eta: 0:11:41  lr: 0.000020  loss: 3.6566 (3.5178)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [9]  [ 100/1251]  eta: 0:11:19  lr: 0.000020  loss: 3.8362 (3.5208)  time: 0.4607  data: 0.0005  max mem: 19734
Epoch: [9]  [ 110/1251]  eta: 0:11:00  lr: 0.000020  loss: 3.6628 (3.5196)  time: 0.4612  data: 0.0006  max mem: 19734
Epoch: [9]  [ 120/1251]  eta: 0:10:43  lr: 0.000020  loss: 3.6463 (3.5248)  time: 0.4601  data: 0.0005  max mem: 19734
Epoch: [9]  [ 130/1251]  eta: 0:10:28  lr: 0.000020  loss: 3.5403 (3.5166)  time: 0.4586  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5227, ratio_loss=0.0076, pruning_loss=0.1622, mse_loss=0.7284
Epoch: [9]  [ 140/1251]  eta: 0:10:18  lr: 0.000020  loss: 3.5584 (3.5191)  time: 0.4817  data: 0.0006  max mem: 19734
Epoch: [9]  [ 150/1251]  eta: 0:10:09  lr: 0.000020  loss: 3.3296 (3.4920)  time: 0.5128  data: 0.0005  max mem: 19734
Epoch: [9]  [ 160/1251]  eta: 0:09:59  lr: 0.000020  loss: 3.7054 (3.5222)  time: 0.4990  data: 0.0004  max mem: 19734
Epoch: [9]  [ 170/1251]  eta: 0:09:48  lr: 0.000020  loss: 3.7832 (3.5235)  time: 0.4684  data: 0.0004  max mem: 19734
Epoch: [9]  [ 180/1251]  eta: 0:09:37  lr: 0.000020  loss: 3.5963 (3.5267)  time: 0.4594  data: 0.0005  max mem: 19734
Epoch: [9]  [ 190/1251]  eta: 0:09:27  lr: 0.000020  loss: 3.7343 (3.5285)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [9]  [ 200/1251]  eta: 0:09:18  lr: 0.000020  loss: 3.7343 (3.5323)  time: 0.4602  data: 0.0004  max mem: 19734
Epoch: [9]  [ 210/1251]  eta: 0:09:09  lr: 0.000020  loss: 3.6189 (3.5327)  time: 0.4614  data: 0.0004  max mem: 19734
Epoch: [9]  [ 220/1251]  eta: 0:09:01  lr: 0.000020  loss: 3.5796 (3.5298)  time: 0.4612  data: 0.0005  max mem: 19734
Epoch: [9]  [ 230/1251]  eta: 0:08:53  lr: 0.000020  loss: 3.7938 (3.5431)  time: 0.4579  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5655, ratio_loss=0.0076, pruning_loss=0.1602, mse_loss=0.7167
Epoch: [9]  [ 240/1251]  eta: 0:08:44  lr: 0.000020  loss: 3.8007 (3.5531)  time: 0.4550  data: 0.0006  max mem: 19734
Epoch: [9]  [ 250/1251]  eta: 0:08:37  lr: 0.000020  loss: 3.7901 (3.5571)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [9]  [ 260/1251]  eta: 0:08:29  lr: 0.000020  loss: 3.5955 (3.5535)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [9]  [ 270/1251]  eta: 0:08:22  lr: 0.000020  loss: 3.5047 (3.5452)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [9]  [ 280/1251]  eta: 0:08:15  lr: 0.000020  loss: 3.6144 (3.5481)  time: 0.4650  data: 0.0005  max mem: 19734
Epoch: [9]  [ 290/1251]  eta: 0:08:11  lr: 0.000020  loss: 3.6587 (3.5535)  time: 0.4979  data: 0.0005  max mem: 19734
Epoch: [9]  [ 300/1251]  eta: 0:08:04  lr: 0.000020  loss: 3.4037 (3.5457)  time: 0.4933  data: 0.0005  max mem: 19734
Epoch: [9]  [ 310/1251]  eta: 0:07:58  lr: 0.000020  loss: 3.2295 (3.5441)  time: 0.4723  data: 0.0005  max mem: 19734
Epoch: [9]  [ 320/1251]  eta: 0:07:51  lr: 0.000020  loss: 3.5605 (3.5398)  time: 0.4667  data: 0.0004  max mem: 19734
Epoch: [9]  [ 330/1251]  eta: 0:07:45  lr: 0.000020  loss: 3.5133 (3.5324)  time: 0.4561  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4412, ratio_loss=0.0078, pruning_loss=0.1634, mse_loss=0.7164
Epoch: [9]  [ 340/1251]  eta: 0:07:39  lr: 0.000020  loss: 3.5153 (3.5308)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [9]  [ 350/1251]  eta: 0:07:32  lr: 0.000020  loss: 3.6380 (3.5280)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [9]  [ 360/1251]  eta: 0:07:26  lr: 0.000020  loss: 3.5431 (3.5273)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [9]  [ 370/1251]  eta: 0:07:20  lr: 0.000020  loss: 3.2189 (3.5188)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [9]  [ 380/1251]  eta: 0:07:14  lr: 0.000020  loss: 3.2941 (3.5205)  time: 0.4561  data: 0.0008  max mem: 19734
Epoch: [9]  [ 390/1251]  eta: 0:07:08  lr: 0.000020  loss: 3.6498 (3.5180)  time: 0.4539  data: 0.0007  max mem: 19734
Epoch: [9]  [ 400/1251]  eta: 0:07:02  lr: 0.000020  loss: 3.6498 (3.5170)  time: 0.4532  data: 0.0008  max mem: 19734
Epoch: [9]  [ 410/1251]  eta: 0:06:56  lr: 0.000020  loss: 3.4064 (3.5152)  time: 0.4524  data: 0.0008  max mem: 19734
Epoch: [9]  [ 420/1251]  eta: 0:06:50  lr: 0.000020  loss: 3.4415 (3.5184)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [9]  [ 430/1251]  eta: 0:06:45  lr: 0.000020  loss: 3.5289 (3.5149)  time: 0.4738  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4400, ratio_loss=0.0078, pruning_loss=0.1615, mse_loss=0.6848
Epoch: [9]  [ 440/1251]  eta: 0:06:42  lr: 0.000020  loss: 3.6027 (3.5181)  time: 0.5204  data: 0.0004  max mem: 19734
Epoch: [9]  [ 450/1251]  eta: 0:06:36  lr: 0.000020  loss: 3.6027 (3.5177)  time: 0.5138  data: 0.0004  max mem: 19734
Epoch: [9]  [ 460/1251]  eta: 0:06:31  lr: 0.000020  loss: 3.5643 (3.5091)  time: 0.4664  data: 0.0004  max mem: 19734
Epoch: [9]  [ 470/1251]  eta: 0:06:25  lr: 0.000020  loss: 3.2554 (3.5076)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [9]  [ 480/1251]  eta: 0:06:19  lr: 0.000020  loss: 3.4410 (3.5061)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [9]  [ 490/1251]  eta: 0:06:14  lr: 0.000020  loss: 3.3140 (3.5015)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [9]  [ 500/1251]  eta: 0:06:08  lr: 0.000020  loss: 3.3955 (3.5011)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [9]  [ 510/1251]  eta: 0:06:03  lr: 0.000020  loss: 3.5885 (3.5029)  time: 0.4532  data: 0.0008  max mem: 19734
Epoch: [9]  [ 520/1251]  eta: 0:05:57  lr: 0.000020  loss: 3.5685 (3.5030)  time: 0.4550  data: 0.0007  max mem: 19734
Epoch: [9]  [ 530/1251]  eta: 0:05:52  lr: 0.000020  loss: 3.5010 (3.5020)  time: 0.4561  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3749, ratio_loss=0.0077, pruning_loss=0.1622, mse_loss=0.7409
Epoch: [9]  [ 540/1251]  eta: 0:05:47  lr: 0.000020  loss: 3.3989 (3.4984)  time: 0.4543  data: 0.0005  max mem: 19734
Epoch: [9]  [ 550/1251]  eta: 0:05:41  lr: 0.000020  loss: 3.4467 (3.5000)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [9]  [ 560/1251]  eta: 0:05:36  lr: 0.000020  loss: 3.6510 (3.5005)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [9]  [ 570/1251]  eta: 0:05:31  lr: 0.000020  loss: 3.5859 (3.4987)  time: 0.4640  data: 0.0007  max mem: 19734
Epoch: [9]  [ 580/1251]  eta: 0:05:27  lr: 0.000020  loss: 3.5398 (3.4970)  time: 0.4982  data: 0.0008  max mem: 19734
Epoch: [9]  [ 590/1251]  eta: 0:05:22  lr: 0.000020  loss: 3.5839 (3.4976)  time: 0.5198  data: 0.0006  max mem: 19734
Epoch: [9]  [ 600/1251]  eta: 0:05:17  lr: 0.000020  loss: 3.4324 (3.4944)  time: 0.4961  data: 0.0006  max mem: 19734
Epoch: [9]  [ 610/1251]  eta: 0:05:12  lr: 0.000020  loss: 3.4570 (3.4931)  time: 0.4677  data: 0.0004  max mem: 19734
Epoch: [9]  [ 620/1251]  eta: 0:05:07  lr: 0.000020  loss: 3.4927 (3.4934)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [9]  [ 630/1251]  eta: 0:05:02  lr: 0.000020  loss: 3.1322 (3.4897)  time: 0.4547  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4334, ratio_loss=0.0072, pruning_loss=0.1594, mse_loss=0.6432
Epoch: [9]  [ 640/1251]  eta: 0:04:56  lr: 0.000020  loss: 3.6551 (3.4935)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [9]  [ 650/1251]  eta: 0:04:51  lr: 0.000020  loss: 3.6551 (3.4942)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [9]  [ 660/1251]  eta: 0:04:46  lr: 0.000020  loss: 3.5398 (3.4935)  time: 0.4529  data: 0.0003  max mem: 19734
Epoch: [9]  [ 670/1251]  eta: 0:04:41  lr: 0.000020  loss: 3.5047 (3.4940)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [9]  [ 680/1251]  eta: 0:04:36  lr: 0.000020  loss: 3.4059 (3.4915)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [9]  [ 690/1251]  eta: 0:04:31  lr: 0.000020  loss: 3.4921 (3.4923)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [9]  [ 700/1251]  eta: 0:04:26  lr: 0.000020  loss: 3.5816 (3.4901)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [9]  [ 710/1251]  eta: 0:04:21  lr: 0.000020  loss: 3.4924 (3.4898)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [9]  [ 720/1251]  eta: 0:04:16  lr: 0.000020  loss: 3.4924 (3.4893)  time: 0.4711  data: 0.0004  max mem: 19734
Epoch: [9]  [ 730/1251]  eta: 0:04:11  lr: 0.000020  loss: 3.3879 (3.4861)  time: 0.5028  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3953, ratio_loss=0.0075, pruning_loss=0.1613, mse_loss=0.7232
Epoch: [9]  [ 740/1251]  eta: 0:04:06  lr: 0.000020  loss: 3.3879 (3.4850)  time: 0.4979  data: 0.0004  max mem: 19734
Epoch: [9]  [ 750/1251]  eta: 0:04:01  lr: 0.000020  loss: 3.7868 (3.4865)  time: 0.4654  data: 0.0004  max mem: 19734
Epoch: [9]  [ 760/1251]  eta: 0:03:56  lr: 0.000020  loss: 3.6220 (3.4866)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [9]  [ 770/1251]  eta: 0:03:51  lr: 0.000020  loss: 3.3947 (3.4858)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [9]  [ 780/1251]  eta: 0:03:46  lr: 0.000020  loss: 3.5784 (3.4865)  time: 0.4545  data: 0.0007  max mem: 19734
Epoch: [9]  [ 790/1251]  eta: 0:03:41  lr: 0.000020  loss: 3.6678 (3.4878)  time: 0.4554  data: 0.0006  max mem: 19734
Epoch: [9]  [ 800/1251]  eta: 0:03:36  lr: 0.000020  loss: 3.7014 (3.4888)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [9]  [ 810/1251]  eta: 0:03:31  lr: 0.000020  loss: 3.6550 (3.4911)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [9]  [ 820/1251]  eta: 0:03:27  lr: 0.000020  loss: 3.6550 (3.4921)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [9]  [ 830/1251]  eta: 0:03:22  lr: 0.000020  loss: 3.5061 (3.4908)  time: 0.4546  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4608, ratio_loss=0.0073, pruning_loss=0.1597, mse_loss=0.7051
Epoch: [9]  [ 840/1251]  eta: 0:03:17  lr: 0.000020  loss: 3.3501 (3.4863)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [9]  [ 850/1251]  eta: 0:03:12  lr: 0.000020  loss: 3.3307 (3.4842)  time: 0.4559  data: 0.0003  max mem: 19734
Epoch: [9]  [ 860/1251]  eta: 0:03:07  lr: 0.000020  loss: 3.3307 (3.4834)  time: 0.4660  data: 0.0004  max mem: 19734
Epoch: [9]  [ 870/1251]  eta: 0:03:02  lr: 0.000020  loss: 3.3273 (3.4822)  time: 0.4999  data: 0.0004  max mem: 19734
Epoch: [9]  [ 880/1251]  eta: 0:02:58  lr: 0.000020  loss: 3.3273 (3.4786)  time: 0.4978  data: 0.0004  max mem: 19734
Epoch: [9]  [ 890/1251]  eta: 0:02:53  lr: 0.000020  loss: 3.2079 (3.4794)  time: 0.4813  data: 0.0004  max mem: 19734
Epoch: [9]  [ 900/1251]  eta: 0:02:48  lr: 0.000020  loss: 3.6854 (3.4790)  time: 0.4707  data: 0.0004  max mem: 19734
Epoch: [9]  [ 910/1251]  eta: 0:02:43  lr: 0.000020  loss: 3.7249 (3.4813)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [9]  [ 920/1251]  eta: 0:02:38  lr: 0.000020  loss: 3.5443 (3.4787)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [9]  [ 930/1251]  eta: 0:02:33  lr: 0.000020  loss: 3.2479 (3.4770)  time: 0.4523  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3270, ratio_loss=0.0075, pruning_loss=0.1629, mse_loss=0.6855
Epoch: [9]  [ 940/1251]  eta: 0:02:28  lr: 0.000020  loss: 3.2059 (3.4731)  time: 0.4522  data: 0.0003  max mem: 19734
Epoch: [9]  [ 950/1251]  eta: 0:02:23  lr: 0.000020  loss: 3.2477 (3.4720)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [9]  [ 960/1251]  eta: 0:02:19  lr: 0.000020  loss: 3.4718 (3.4710)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [9]  [ 970/1251]  eta: 0:02:14  lr: 0.000020  loss: 3.4664 (3.4716)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [9]  [ 980/1251]  eta: 0:02:09  lr: 0.000020  loss: 3.5610 (3.4726)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [9]  [ 990/1251]  eta: 0:02:04  lr: 0.000020  loss: 3.6683 (3.4722)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [9]  [1000/1251]  eta: 0:01:59  lr: 0.000020  loss: 3.4908 (3.4732)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [9]  [1010/1251]  eta: 0:01:55  lr: 0.000020  loss: 3.7735 (3.4743)  time: 0.4790  data: 0.0003  max mem: 19734
Epoch: [9]  [1020/1251]  eta: 0:01:50  lr: 0.000020  loss: 3.4760 (3.4726)  time: 0.5140  data: 0.0003  max mem: 19734
Epoch: [9]  [1030/1251]  eta: 0:01:45  lr: 0.000020  loss: 3.4760 (3.4743)  time: 0.4974  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4625, ratio_loss=0.0075, pruning_loss=0.1566, mse_loss=0.6636
Epoch: [9]  [1040/1251]  eta: 0:01:40  lr: 0.000020  loss: 3.6326 (3.4753)  time: 0.4729  data: 0.0004  max mem: 19734
Epoch: [9]  [1050/1251]  eta: 0:01:35  lr: 0.000020  loss: 3.5447 (3.4755)  time: 0.4645  data: 0.0006  max mem: 19734
Epoch: [9]  [1060/1251]  eta: 0:01:31  lr: 0.000020  loss: 3.5447 (3.4764)  time: 0.4519  data: 0.0005  max mem: 19734
Epoch: [9]  [1070/1251]  eta: 0:01:26  lr: 0.000020  loss: 3.6921 (3.4777)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [9]  [1080/1251]  eta: 0:01:21  lr: 0.000020  loss: 3.6426 (3.4760)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [9]  [1090/1251]  eta: 0:01:16  lr: 0.000020  loss: 3.5669 (3.4765)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [9]  [1100/1251]  eta: 0:01:11  lr: 0.000020  loss: 3.5438 (3.4757)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [9]  [1110/1251]  eta: 0:01:07  lr: 0.000020  loss: 3.3295 (3.4714)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [9]  [1120/1251]  eta: 0:01:02  lr: 0.000020  loss: 3.3295 (3.4713)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [9]  [1130/1251]  eta: 0:00:57  lr: 0.000020  loss: 3.6145 (3.4707)  time: 0.4513  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3723, ratio_loss=0.0072, pruning_loss=0.1586, mse_loss=0.7113
Epoch: [9]  [1140/1251]  eta: 0:00:52  lr: 0.000020  loss: 3.4500 (3.4694)  time: 0.4508  data: 0.0004  max mem: 19734
Epoch: [9]  [1150/1251]  eta: 0:00:48  lr: 0.000020  loss: 3.4500 (3.4703)  time: 0.4679  data: 0.0004  max mem: 19734
Epoch: [9]  [1160/1251]  eta: 0:00:43  lr: 0.000020  loss: 3.4274 (3.4693)  time: 0.4910  data: 0.0003  max mem: 19734
Epoch: [9]  [1170/1251]  eta: 0:00:38  lr: 0.000020  loss: 3.3989 (3.4676)  time: 0.4846  data: 0.0003  max mem: 19734
Epoch: [9]  [1180/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.1520 (3.4647)  time: 0.4781  data: 0.0004  max mem: 19734
Epoch: [9]  [1190/1251]  eta: 0:00:29  lr: 0.000020  loss: 3.2791 (3.4645)  time: 0.4784  data: 0.0007  max mem: 19734
Epoch: [9]  [1200/1251]  eta: 0:00:24  lr: 0.000020  loss: 3.4724 (3.4622)  time: 0.4596  data: 0.0006  max mem: 19734
Epoch: [9]  [1210/1251]  eta: 0:00:19  lr: 0.000020  loss: 3.6064 (3.4641)  time: 0.4457  data: 0.0002  max mem: 19734
Epoch: [9]  [1220/1251]  eta: 0:00:14  lr: 0.000020  loss: 3.5954 (3.4637)  time: 0.4463  data: 0.0002  max mem: 19734
Epoch: [9]  [1230/1251]  eta: 0:00:09  lr: 0.000020  loss: 3.4382 (3.4628)  time: 0.4474  data: 0.0002  max mem: 19734
loss info: cls_loss=3.3449, ratio_loss=0.0075, pruning_loss=0.1585, mse_loss=0.6740
Epoch: [9]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 3.4039 (3.4621)  time: 0.4482  data: 0.0002  max mem: 19734
Epoch: [9]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.6447 (3.4630)  time: 0.4485  data: 0.0002  max mem: 19734
Epoch: [9] Total time: 0:09:54 (0.4750 s / it)
Averaged stats: lr: 0.000020  loss: 3.6447 (3.4848)
Test:  [  0/261]  eta: 1:14:16  loss: 0.7871 (0.7871)  acc1: 81.7708 (81.7708)  acc5: 94.2708 (94.2708)  time: 17.0758  data: 16.9046  max mem: 19734
Test:  [ 10/261]  eta: 0:08:18  loss: 0.7595 (0.8016)  acc1: 84.8958 (83.0966)  acc5: 95.3125 (95.5019)  time: 1.9865  data: 1.8070  max mem: 19734
Test:  [ 20/261]  eta: 0:04:24  loss: 1.0049 (0.9547)  acc1: 79.1667 (78.6706)  acc5: 92.7083 (94.1964)  time: 0.2983  data: 0.1552  max mem: 19734
Test:  [ 30/261]  eta: 0:03:00  loss: 0.8317 (0.8722)  acc1: 83.8542 (81.5860)  acc5: 94.2708 (94.8421)  time: 0.1204  data: 0.0155  max mem: 19734
Test:  [ 40/261]  eta: 0:02:58  loss: 0.6461 (0.8397)  acc1: 86.9792 (82.4695)  acc5: 96.8750 (95.1982)  time: 0.5037  data: 0.3673  max mem: 19734
Test:  [ 50/261]  eta: 0:02:22  loss: 0.9453 (0.9044)  acc1: 77.0833 (80.6168)  acc5: 95.8333 (94.7815)  time: 0.5038  data: 0.3629  max mem: 19734
Test:  [ 60/261]  eta: 0:01:58  loss: 1.0916 (0.9211)  acc1: 76.0417 (80.0290)  acc5: 93.2292 (94.7148)  time: 0.1466  data: 0.0162  max mem: 19734
Test:  [ 70/261]  eta: 0:02:08  loss: 1.0283 (0.9215)  acc1: 77.0833 (79.5481)  acc5: 94.7917 (94.9164)  time: 0.6681  data: 0.5105  max mem: 19734
Test:  [ 80/261]  eta: 0:01:50  loss: 0.8954 (0.9254)  acc1: 78.6458 (79.7004)  acc5: 95.8333 (95.0039)  time: 0.6693  data: 0.5063  max mem: 19734
Test:  [ 90/261]  eta: 0:01:35  loss: 0.8737 (0.9100)  acc1: 82.2917 (80.0996)  acc5: 95.3125 (95.1007)  time: 0.1627  data: 0.0149  max mem: 19734
Test:  [100/261]  eta: 0:01:33  loss: 0.8737 (0.9118)  acc1: 81.2500 (79.9918)  acc5: 94.7917 (95.1372)  time: 0.4727  data: 0.3501  max mem: 19734
Test:  [110/261]  eta: 0:01:22  loss: 0.9640 (0.9355)  acc1: 76.5625 (79.4998)  acc5: 94.2708 (94.8714)  time: 0.4916  data: 0.3517  max mem: 19734
Test:  [120/261]  eta: 0:01:12  loss: 1.3016 (0.9756)  acc1: 69.2708 (78.5942)  acc5: 89.5833 (94.3526)  time: 0.1528  data: 0.0138  max mem: 19734
Test:  [130/261]  eta: 0:01:03  loss: 1.4081 (1.0231)  acc1: 67.1875 (77.6598)  acc5: 86.4583 (93.6983)  time: 0.1175  data: 0.0121  max mem: 19734
Test:  [140/261]  eta: 0:01:02  loss: 1.4057 (1.0510)  acc1: 67.7083 (76.9319)  acc5: 88.5417 (93.4471)  time: 0.5456  data: 0.4260  max mem: 19734
Test:  [150/261]  eta: 0:00:55  loss: 1.2879 (1.0567)  acc1: 70.3125 (76.9247)  acc5: 90.6250 (93.2809)  time: 0.6152  data: 0.4273  max mem: 19734
Test:  [160/261]  eta: 0:00:48  loss: 1.1354 (1.0786)  acc1: 76.5625 (76.5302)  acc5: 91.1458 (92.9639)  time: 0.2064  data: 0.0175  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.3463 (1.1105)  acc1: 65.1042 (75.7249)  acc5: 85.9375 (92.5835)  time: 0.5349  data: 0.4111  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.5555 (1.1284)  acc1: 64.5833 (75.3194)  acc5: 85.9375 (92.3976)  time: 0.5103  data: 0.4075  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4251 (1.1425)  acc1: 67.7083 (75.0355)  acc5: 89.5833 (92.2339)  time: 0.0953  data: 0.0091  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.4251 (1.1591)  acc1: 68.2292 (74.6554)  acc5: 89.5833 (92.0035)  time: 0.0868  data: 0.0057  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.4828 (1.1738)  acc1: 67.1875 (74.3755)  acc5: 86.9792 (91.7777)  time: 0.0773  data: 0.0019  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.5367 (1.1924)  acc1: 65.6250 (73.9371)  acc5: 85.9375 (91.5606)  time: 0.0639  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.5164 (1.2027)  acc1: 64.0625 (73.6720)  acc5: 86.9792 (91.4299)  time: 0.0843  data: 0.0219  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.4418 (1.2114)  acc1: 66.1458 (73.4548)  acc5: 89.5833 (91.3663)  time: 0.0840  data: 0.0219  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.1651 (1.2046)  acc1: 74.4792 (73.6035)  acc5: 93.2292 (91.4820)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0427 (1.2039)  acc1: 75.0000 (73.6620)  acc5: 94.2708 (91.5380)  time: 0.0598  data: 0.0002  max mem: 19734
Test: Total time: 0:01:33 (0.3576 s / it)
* Acc@1 73.662 Acc@5 91.538 loss 1.204
Accuracy of the network on the 50000 test images: 73.7%
Max accuracy: 73.66%
Epoch: [10]  [   0/1251]  eta: 3:56:05  lr: 0.000020  loss: 3.8883 (3.8883)  time: 11.3230  data: 10.8723  max mem: 19734
Epoch: [10]  [  10/1251]  eta: 0:31:41  lr: 0.000020  loss: 3.7921 (3.7394)  time: 1.5324  data: 0.9894  max mem: 19734
Epoch: [10]  [  20/1251]  eta: 0:21:08  lr: 0.000020  loss: 3.7360 (3.6473)  time: 0.5154  data: 0.0007  max mem: 19734
Epoch: [10]  [  30/1251]  eta: 0:17:12  lr: 0.000020  loss: 3.5046 (3.5457)  time: 0.4674  data: 0.0004  max mem: 19734
Epoch: [10]  [  40/1251]  eta: 0:15:24  lr: 0.000020  loss: 3.3924 (3.5140)  time: 0.4826  data: 0.0004  max mem: 19734
Epoch: [10]  [  50/1251]  eta: 0:14:22  lr: 0.000020  loss: 3.5314 (3.4835)  time: 0.5209  data: 0.0005  max mem: 19734
Epoch: [10]  [  60/1251]  eta: 0:13:31  lr: 0.000020  loss: 3.5314 (3.4564)  time: 0.5132  data: 0.0005  max mem: 19734
Epoch: [10]  [  70/1251]  eta: 0:12:53  lr: 0.000020  loss: 3.6848 (3.4886)  time: 0.4936  data: 0.0006  max mem: 19734
Epoch: [10]  [  80/1251]  eta: 0:12:19  lr: 0.000020  loss: 3.8122 (3.5184)  time: 0.4813  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4769, ratio_loss=0.0071, pruning_loss=0.1562, mse_loss=0.6809
Epoch: [10]  [  90/1251]  eta: 0:11:52  lr: 0.000020  loss: 3.6524 (3.5062)  time: 0.4665  data: 0.0005  max mem: 19734
Epoch: [10]  [ 100/1251]  eta: 0:11:28  lr: 0.000020  loss: 3.3422 (3.4865)  time: 0.4637  data: 0.0004  max mem: 19734
Epoch: [10]  [ 110/1251]  eta: 0:11:08  lr: 0.000020  loss: 3.5831 (3.4872)  time: 0.4616  data: 0.0004  max mem: 19734
Epoch: [10]  [ 120/1251]  eta: 0:10:50  lr: 0.000020  loss: 3.5850 (3.4998)  time: 0.4595  data: 0.0007  max mem: 19734
Epoch: [10]  [ 130/1251]  eta: 0:10:35  lr: 0.000020  loss: 3.5413 (3.4959)  time: 0.4584  data: 0.0007  max mem: 19734
Epoch: [10]  [ 140/1251]  eta: 0:10:21  lr: 0.000020  loss: 3.6315 (3.4966)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [10]  [ 150/1251]  eta: 0:10:07  lr: 0.000020  loss: 3.5586 (3.4932)  time: 0.4577  data: 0.0004  max mem: 19734
Epoch: [10]  [ 160/1251]  eta: 0:09:56  lr: 0.000020  loss: 3.5507 (3.4997)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [10]  [ 170/1251]  eta: 0:09:45  lr: 0.000020  loss: 3.6868 (3.5069)  time: 0.4641  data: 0.0005  max mem: 19734
Epoch: [10]  [ 180/1251]  eta: 0:09:35  lr: 0.000020  loss: 3.6899 (3.5111)  time: 0.4626  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4807, ratio_loss=0.0078, pruning_loss=0.1560, mse_loss=0.7078
Epoch: [10]  [ 190/1251]  eta: 0:09:30  lr: 0.000020  loss: 3.7372 (3.5116)  time: 0.4990  data: 0.0007  max mem: 19734
Epoch: [10]  [ 200/1251]  eta: 0:09:22  lr: 0.000020  loss: 3.6576 (3.5058)  time: 0.5216  data: 0.0009  max mem: 19734
Epoch: [10]  [ 210/1251]  eta: 0:09:14  lr: 0.000020  loss: 3.6414 (3.5125)  time: 0.4907  data: 0.0008  max mem: 19734
Epoch: [10]  [ 220/1251]  eta: 0:09:06  lr: 0.000020  loss: 3.7364 (3.5139)  time: 0.4696  data: 0.0005  max mem: 19734
Epoch: [10]  [ 230/1251]  eta: 0:08:57  lr: 0.000020  loss: 3.7364 (3.5152)  time: 0.4613  data: 0.0004  max mem: 19734
Epoch: [10]  [ 240/1251]  eta: 0:08:49  lr: 0.000020  loss: 3.0833 (3.5005)  time: 0.4607  data: 0.0004  max mem: 19734
Epoch: [10]  [ 250/1251]  eta: 0:08:41  lr: 0.000020  loss: 3.2795 (3.5034)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [10]  [ 260/1251]  eta: 0:08:33  lr: 0.000020  loss: 3.6688 (3.5134)  time: 0.4551  data: 0.0010  max mem: 19734
Epoch: [10]  [ 270/1251]  eta: 0:08:26  lr: 0.000020  loss: 3.5776 (3.5050)  time: 0.4555  data: 0.0010  max mem: 19734
Epoch: [10]  [ 280/1251]  eta: 0:08:19  lr: 0.000020  loss: 3.2995 (3.4975)  time: 0.4542  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4293, ratio_loss=0.0071, pruning_loss=0.1566, mse_loss=0.6826
Epoch: [10]  [ 290/1251]  eta: 0:08:12  lr: 0.000020  loss: 3.3321 (3.4933)  time: 0.4541  data: 0.0006  max mem: 19734
Epoch: [10]  [ 300/1251]  eta: 0:08:05  lr: 0.000020  loss: 3.3495 (3.4919)  time: 0.4539  data: 0.0006  max mem: 19734
Epoch: [10]  [ 310/1251]  eta: 0:07:58  lr: 0.000020  loss: 3.7447 (3.4947)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [10]  [ 320/1251]  eta: 0:07:52  lr: 0.000020  loss: 3.6433 (3.4930)  time: 0.4636  data: 0.0005  max mem: 19734
Epoch: [10]  [ 330/1251]  eta: 0:07:46  lr: 0.000020  loss: 3.4917 (3.4876)  time: 0.4749  data: 0.0005  max mem: 19734
Epoch: [10]  [ 340/1251]  eta: 0:07:41  lr: 0.000020  loss: 3.5744 (3.4928)  time: 0.5009  data: 0.0005  max mem: 19734
Epoch: [10]  [ 350/1251]  eta: 0:07:35  lr: 0.000020  loss: 3.5744 (3.4857)  time: 0.4944  data: 0.0005  max mem: 19734
Epoch: [10]  [ 360/1251]  eta: 0:07:29  lr: 0.000020  loss: 3.4861 (3.4889)  time: 0.4737  data: 0.0004  max mem: 19734
Epoch: [10]  [ 370/1251]  eta: 0:07:23  lr: 0.000020  loss: 3.4861 (3.4888)  time: 0.4699  data: 0.0005  max mem: 19734
Epoch: [10]  [ 380/1251]  eta: 0:07:17  lr: 0.000020  loss: 3.4996 (3.4874)  time: 0.4578  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4384, ratio_loss=0.0070, pruning_loss=0.1558, mse_loss=0.6957
Epoch: [10]  [ 390/1251]  eta: 0:07:11  lr: 0.000020  loss: 3.6889 (3.4897)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [10]  [ 400/1251]  eta: 0:07:05  lr: 0.000020  loss: 3.7115 (3.4938)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [10]  [ 410/1251]  eta: 0:06:59  lr: 0.000020  loss: 3.7065 (3.4983)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [10]  [ 420/1251]  eta: 0:06:53  lr: 0.000020  loss: 3.5670 (3.4966)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [10]  [ 430/1251]  eta: 0:06:47  lr: 0.000020  loss: 3.5670 (3.4966)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [10]  [ 440/1251]  eta: 0:06:42  lr: 0.000020  loss: 3.6041 (3.4942)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [10]  [ 450/1251]  eta: 0:06:36  lr: 0.000020  loss: 3.5631 (3.4977)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [10]  [ 460/1251]  eta: 0:06:30  lr: 0.000020  loss: 3.6819 (3.5005)  time: 0.4587  data: 0.0005  max mem: 19734
Epoch: [10]  [ 470/1251]  eta: 0:06:25  lr: 0.000020  loss: 3.6312 (3.4964)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [10]  [ 480/1251]  eta: 0:06:20  lr: 0.000020  loss: 3.3773 (3.4888)  time: 0.4783  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4486, ratio_loss=0.0071, pruning_loss=0.1561, mse_loss=0.6632
Epoch: [10]  [ 490/1251]  eta: 0:06:15  lr: 0.000020  loss: 3.3030 (3.4848)  time: 0.4908  data: 0.0004  max mem: 19734
Epoch: [10]  [ 500/1251]  eta: 0:06:10  lr: 0.000020  loss: 3.7011 (3.4880)  time: 0.4748  data: 0.0005  max mem: 19734
Epoch: [10]  [ 510/1251]  eta: 0:06:04  lr: 0.000020  loss: 3.8668 (3.4947)  time: 0.4617  data: 0.0005  max mem: 19734
Epoch: [10]  [ 520/1251]  eta: 0:05:59  lr: 0.000020  loss: 3.7993 (3.4964)  time: 0.4521  data: 0.0007  max mem: 19734
Epoch: [10]  [ 530/1251]  eta: 0:05:53  lr: 0.000020  loss: 3.7141 (3.4983)  time: 0.4542  data: 0.0007  max mem: 19734
Epoch: [10]  [ 540/1251]  eta: 0:05:48  lr: 0.000020  loss: 3.7411 (3.5008)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [10]  [ 550/1251]  eta: 0:05:43  lr: 0.000020  loss: 3.6364 (3.4991)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [10]  [ 560/1251]  eta: 0:05:37  lr: 0.000020  loss: 3.3307 (3.4969)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [10]  [ 570/1251]  eta: 0:05:32  lr: 0.000020  loss: 3.1885 (3.4909)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [10]  [ 580/1251]  eta: 0:05:27  lr: 0.000020  loss: 3.6408 (3.4939)  time: 0.4537  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5023, ratio_loss=0.0076, pruning_loss=0.1546, mse_loss=0.6486
Epoch: [10]  [ 590/1251]  eta: 0:05:21  lr: 0.000020  loss: 3.8772 (3.4966)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [10]  [ 600/1251]  eta: 0:05:16  lr: 0.000020  loss: 3.6341 (3.4975)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [10]  [ 610/1251]  eta: 0:05:11  lr: 0.000020  loss: 3.4212 (3.4938)  time: 0.4638  data: 0.0006  max mem: 19734
Epoch: [10]  [ 620/1251]  eta: 0:05:06  lr: 0.000020  loss: 3.4117 (3.4923)  time: 0.4728  data: 0.0006  max mem: 19734
Epoch: [10]  [ 630/1251]  eta: 0:05:02  lr: 0.000020  loss: 3.5366 (3.4927)  time: 0.5028  data: 0.0005  max mem: 19734
Epoch: [10]  [ 640/1251]  eta: 0:04:57  lr: 0.000020  loss: 3.7486 (3.4976)  time: 0.4922  data: 0.0004  max mem: 19734
Epoch: [10]  [ 650/1251]  eta: 0:04:52  lr: 0.000020  loss: 3.7011 (3.4953)  time: 0.4641  data: 0.0004  max mem: 19734
Epoch: [10]  [ 660/1251]  eta: 0:04:46  lr: 0.000020  loss: 3.3744 (3.4915)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [10]  [ 670/1251]  eta: 0:04:41  lr: 0.000020  loss: 3.5464 (3.4948)  time: 0.4517  data: 0.0005  max mem: 19734
Epoch: [10]  [ 680/1251]  eta: 0:04:36  lr: 0.000020  loss: 3.5396 (3.4923)  time: 0.4520  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4409, ratio_loss=0.0075, pruning_loss=0.1544, mse_loss=0.6595
Epoch: [10]  [ 690/1251]  eta: 0:04:31  lr: 0.000020  loss: 3.4524 (3.4930)  time: 0.4537  data: 0.0006  max mem: 19734
Epoch: [10]  [ 700/1251]  eta: 0:04:26  lr: 0.000020  loss: 3.6480 (3.4956)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [10]  [ 710/1251]  eta: 0:04:21  lr: 0.000020  loss: 3.6526 (3.4972)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [10]  [ 720/1251]  eta: 0:04:16  lr: 0.000020  loss: 3.6414 (3.4985)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [10]  [ 730/1251]  eta: 0:04:11  lr: 0.000020  loss: 3.5187 (3.4957)  time: 0.4523  data: 0.0006  max mem: 19734
Epoch: [10]  [ 740/1251]  eta: 0:04:06  lr: 0.000020  loss: 3.4752 (3.4926)  time: 0.4545  data: 0.0006  max mem: 19734
Epoch: [10]  [ 750/1251]  eta: 0:04:01  lr: 0.000020  loss: 3.5436 (3.4923)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [10]  [ 760/1251]  eta: 0:03:56  lr: 0.000020  loss: 3.4294 (3.4899)  time: 0.4622  data: 0.0005  max mem: 19734
Epoch: [10]  [ 770/1251]  eta: 0:03:51  lr: 0.000020  loss: 3.3284 (3.4881)  time: 0.4963  data: 0.0005  max mem: 19734
Epoch: [10]  [ 780/1251]  eta: 0:03:47  lr: 0.000020  loss: 3.4444 (3.4892)  time: 0.5086  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4303, ratio_loss=0.0076, pruning_loss=0.1547, mse_loss=0.6500
Epoch: [10]  [ 790/1251]  eta: 0:03:42  lr: 0.000020  loss: 3.6875 (3.4882)  time: 0.4898  data: 0.0006  max mem: 19734
Epoch: [10]  [ 800/1251]  eta: 0:03:37  lr: 0.000020  loss: 3.0924 (3.4824)  time: 0.4674  data: 0.0005  max mem: 19734
Epoch: [10]  [ 810/1251]  eta: 0:03:32  lr: 0.000020  loss: 3.0149 (3.4810)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [10]  [ 820/1251]  eta: 0:03:27  lr: 0.000020  loss: 3.4440 (3.4806)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [10]  [ 830/1251]  eta: 0:03:22  lr: 0.000020  loss: 3.4939 (3.4802)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [10]  [ 840/1251]  eta: 0:03:17  lr: 0.000020  loss: 3.6900 (3.4810)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [10]  [ 850/1251]  eta: 0:03:12  lr: 0.000020  loss: 3.6900 (3.4816)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [10]  [ 860/1251]  eta: 0:03:07  lr: 0.000020  loss: 3.7069 (3.4835)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [10]  [ 870/1251]  eta: 0:03:02  lr: 0.000020  loss: 3.7604 (3.4876)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [10]  [ 880/1251]  eta: 0:02:57  lr: 0.000020  loss: 3.6170 (3.4863)  time: 0.4540  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3880, ratio_loss=0.0070, pruning_loss=0.1548, mse_loss=0.6613
Epoch: [10]  [ 890/1251]  eta: 0:02:52  lr: 0.000020  loss: 3.1420 (3.4804)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [10]  [ 900/1251]  eta: 0:02:48  lr: 0.000020  loss: 2.6975 (3.4756)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [10]  [ 910/1251]  eta: 0:02:43  lr: 0.000020  loss: 3.2291 (3.4754)  time: 0.4946  data: 0.0004  max mem: 19734
Epoch: [10]  [ 920/1251]  eta: 0:02:38  lr: 0.000020  loss: 3.4949 (3.4763)  time: 0.5254  data: 0.0004  max mem: 19734
Epoch: [10]  [ 930/1251]  eta: 0:02:33  lr: 0.000020  loss: 3.7304 (3.4811)  time: 0.4843  data: 0.0007  max mem: 19734
Epoch: [10]  [ 940/1251]  eta: 0:02:29  lr: 0.000020  loss: 3.7944 (3.4824)  time: 0.4637  data: 0.0007  max mem: 19734
Epoch: [10]  [ 950/1251]  eta: 0:02:24  lr: 0.000020  loss: 3.4972 (3.4814)  time: 0.4648  data: 0.0004  max mem: 19734
Epoch: [10]  [ 960/1251]  eta: 0:02:19  lr: 0.000020  loss: 3.6093 (3.4827)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [10]  [ 970/1251]  eta: 0:02:14  lr: 0.000020  loss: 3.5934 (3.4812)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [10]  [ 980/1251]  eta: 0:02:09  lr: 0.000020  loss: 3.4540 (3.4801)  time: 0.4538  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4269, ratio_loss=0.0071, pruning_loss=0.1531, mse_loss=0.6791
Epoch: [10]  [ 990/1251]  eta: 0:02:04  lr: 0.000020  loss: 3.5010 (3.4794)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [10]  [1000/1251]  eta: 0:01:59  lr: 0.000020  loss: 3.5010 (3.4793)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [10]  [1010/1251]  eta: 0:01:55  lr: 0.000020  loss: 3.2368 (3.4751)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [10]  [1020/1251]  eta: 0:01:50  lr: 0.000020  loss: 3.3316 (3.4738)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [10]  [1030/1251]  eta: 0:01:45  lr: 0.000020  loss: 3.4963 (3.4739)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [10]  [1040/1251]  eta: 0:01:40  lr: 0.000020  loss: 3.5091 (3.4729)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [10]  [1050/1251]  eta: 0:01:35  lr: 0.000020  loss: 3.5091 (3.4732)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [10]  [1060/1251]  eta: 0:01:31  lr: 0.000020  loss: 3.4063 (3.4707)  time: 0.4971  data: 0.0004  max mem: 19734
Epoch: [10]  [1070/1251]  eta: 0:01:26  lr: 0.000020  loss: 3.2008 (3.4704)  time: 0.5193  data: 0.0004  max mem: 19734
Epoch: [10]  [1080/1251]  eta: 0:01:21  lr: 0.000020  loss: 3.5745 (3.4710)  time: 0.4858  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3388, ratio_loss=0.0075, pruning_loss=0.1550, mse_loss=0.6556
Epoch: [10]  [1090/1251]  eta: 0:01:16  lr: 0.000020  loss: 3.1468 (3.4686)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [10]  [1100/1251]  eta: 0:01:12  lr: 0.000020  loss: 3.1381 (3.4677)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [10]  [1110/1251]  eta: 0:01:07  lr: 0.000020  loss: 3.4125 (3.4675)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [10]  [1120/1251]  eta: 0:01:02  lr: 0.000020  loss: 3.7863 (3.4699)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [10]  [1130/1251]  eta: 0:00:57  lr: 0.000020  loss: 3.5772 (3.4659)  time: 0.4520  data: 0.0005  max mem: 19734
Epoch: [10]  [1140/1251]  eta: 0:00:52  lr: 0.000020  loss: 2.9924 (3.4627)  time: 0.4534  data: 0.0007  max mem: 19734
Epoch: [10]  [1150/1251]  eta: 0:00:48  lr: 0.000020  loss: 3.0057 (3.4606)  time: 0.4522  data: 0.0008  max mem: 19734
Epoch: [10]  [1160/1251]  eta: 0:00:43  lr: 0.000020  loss: 3.5449 (3.4616)  time: 0.4518  data: 0.0006  max mem: 19734
Epoch: [10]  [1170/1251]  eta: 0:00:38  lr: 0.000020  loss: 3.7097 (3.4630)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [10]  [1180/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.5532 (3.4615)  time: 0.4537  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3422, ratio_loss=0.0072, pruning_loss=0.1543, mse_loss=0.6922
Epoch: [10]  [1190/1251]  eta: 0:00:28  lr: 0.000020  loss: 3.3572 (3.4610)  time: 0.4516  data: 0.0008  max mem: 19734
Epoch: [10]  [1200/1251]  eta: 0:00:24  lr: 0.000020  loss: 3.5934 (3.4617)  time: 0.4700  data: 0.0007  max mem: 19734
Epoch: [10]  [1210/1251]  eta: 0:00:19  lr: 0.000020  loss: 3.6489 (3.4624)  time: 0.4804  data: 0.0001  max mem: 19734
Epoch: [10]  [1220/1251]  eta: 0:00:14  lr: 0.000020  loss: 3.4273 (3.4614)  time: 0.4629  data: 0.0001  max mem: 19734
Epoch: [10]  [1230/1251]  eta: 0:00:09  lr: 0.000020  loss: 3.4260 (3.4616)  time: 0.4551  data: 0.0001  max mem: 19734
Epoch: [10]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 3.5212 (3.4634)  time: 0.4512  data: 0.0001  max mem: 19734
Epoch: [10]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.6637 (3.4644)  time: 0.4456  data: 0.0001  max mem: 19734
Epoch: [10] Total time: 0:09:54 (0.4750 s / it)
Averaged stats: lr: 0.000020  loss: 3.6637 (3.4732)
Test:  [  0/261]  eta: 0:54:32  loss: 0.8193 (0.8193)  acc1: 82.2917 (82.2917)  acc5: 94.7917 (94.7917)  time: 12.5381  data: 12.3819  max mem: 19734
Test:  [ 10/261]  eta: 0:09:33  loss: 0.7737 (0.8143)  acc1: 86.4583 (83.3333)  acc5: 95.3125 (95.5492)  time: 2.2868  data: 2.1352  max mem: 19734
Test:  [ 20/261]  eta: 0:05:00  loss: 1.0411 (0.9674)  acc1: 76.5625 (79.0427)  acc5: 92.7083 (94.1468)  time: 0.6803  data: 0.5621  max mem: 19734
Test:  [ 30/261]  eta: 0:03:22  loss: 0.8516 (0.8821)  acc1: 83.3333 (81.7204)  acc5: 93.7500 (94.7077)  time: 0.1031  data: 0.0143  max mem: 19734
Test:  [ 40/261]  eta: 0:02:53  loss: 0.6444 (0.8527)  acc1: 86.4583 (82.5203)  acc5: 96.3542 (95.0838)  time: 0.2996  data: 0.2006  max mem: 19734
Test:  [ 50/261]  eta: 0:02:18  loss: 0.9494 (0.9152)  acc1: 77.6042 (80.5964)  acc5: 94.2708 (94.6283)  time: 0.3161  data: 0.1989  max mem: 19734
Test:  [ 60/261]  eta: 0:01:52  loss: 1.0624 (0.9285)  acc1: 75.5208 (80.1144)  acc5: 93.7500 (94.6124)  time: 0.1076  data: 0.0112  max mem: 19734
Test:  [ 70/261]  eta: 0:01:52  loss: 0.9963 (0.9292)  acc1: 75.5208 (79.6215)  acc5: 94.7917 (94.8137)  time: 0.4193  data: 0.3391  max mem: 19734
Test:  [ 80/261]  eta: 0:01:43  loss: 0.8627 (0.9333)  acc1: 78.1250 (79.6875)  acc5: 95.8333 (94.9203)  time: 0.5982  data: 0.5000  max mem: 19734
Test:  [ 90/261]  eta: 0:01:32  loss: 0.8501 (0.9168)  acc1: 83.3333 (80.1168)  acc5: 95.8333 (95.0836)  time: 0.3674  data: 0.2600  max mem: 19734
Test:  [100/261]  eta: 0:01:22  loss: 0.8501 (0.9187)  acc1: 83.3333 (80.0794)  acc5: 95.8333 (95.1475)  time: 0.2778  data: 0.1649  max mem: 19734
Test:  [110/261]  eta: 0:01:13  loss: 0.9316 (0.9425)  acc1: 76.0417 (79.6030)  acc5: 93.7500 (94.8386)  time: 0.2213  data: 0.1191  max mem: 19734
Test:  [120/261]  eta: 0:01:09  loss: 1.2669 (0.9829)  acc1: 69.7917 (78.6631)  acc5: 89.5833 (94.2924)  time: 0.3959  data: 0.2967  max mem: 19734
Test:  [130/261]  eta: 0:01:01  loss: 1.3839 (1.0282)  acc1: 67.1875 (77.6598)  acc5: 86.4583 (93.6625)  time: 0.3725  data: 0.2560  max mem: 19734
Test:  [140/261]  eta: 0:00:54  loss: 1.3949 (1.0558)  acc1: 67.7083 (76.9577)  acc5: 88.5417 (93.3843)  time: 0.1778  data: 0.0585  max mem: 19734
Test:  [150/261]  eta: 0:00:52  loss: 1.2780 (1.0611)  acc1: 70.3125 (76.9523)  acc5: 90.6250 (93.2637)  time: 0.4838  data: 0.3534  max mem: 19734
Test:  [160/261]  eta: 0:00:45  loss: 1.0957 (1.0814)  acc1: 77.6042 (76.6046)  acc5: 91.6667 (92.9218)  time: 0.4413  data: 0.3115  max mem: 19734
Test:  [170/261]  eta: 0:00:39  loss: 1.4001 (1.1125)  acc1: 64.0625 (75.7645)  acc5: 86.4583 (92.5378)  time: 0.1554  data: 0.0148  max mem: 19734
Test:  [180/261]  eta: 0:00:35  loss: 1.5344 (1.1312)  acc1: 63.5417 (75.3511)  acc5: 86.9792 (92.3199)  time: 0.3532  data: 0.2127  max mem: 19734
Test:  [190/261]  eta: 0:00:29  loss: 1.4635 (1.1454)  acc1: 66.1458 (75.0791)  acc5: 89.5833 (92.1575)  time: 0.3019  data: 0.2103  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.4136 (1.1612)  acc1: 69.7917 (74.7694)  acc5: 88.0208 (91.9077)  time: 0.1625  data: 0.0920  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.4873 (1.1755)  acc1: 68.2292 (74.5137)  acc5: 86.9792 (91.6691)  time: 0.1589  data: 0.0890  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.5294 (1.1946)  acc1: 66.1458 (74.0644)  acc5: 87.5000 (91.4640)  time: 0.2532  data: 0.1791  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.5435 (1.2046)  acc1: 65.1042 (73.8028)  acc5: 88.5417 (91.3623)  time: 0.2504  data: 0.1791  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.4413 (1.2133)  acc1: 65.6250 (73.5650)  acc5: 90.1042 (91.2928)  time: 0.0645  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.1270 (1.2063)  acc1: 75.5208 (73.7363)  acc5: 92.1875 (91.4280)  time: 0.0617  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0611 (1.2055)  acc1: 77.0833 (73.7780)  acc5: 94.7917 (91.4980)  time: 0.0602  data: 0.0001  max mem: 19734
Test: Total time: 0:01:30 (0.3455 s / it)
* Acc@1 73.778 Acc@5 91.498 loss 1.205
Accuracy of the network on the 50000 test images: 73.8%
Max accuracy: 73.78%
Epoch: [11]  [   0/1251]  eta: 4:21:47  lr: 0.000020  loss: 3.8516 (3.8516)  time: 12.5563  data: 12.0868  max mem: 19734
Epoch: [11]  [  10/1251]  eta: 0:33:30  lr: 0.000020  loss: 3.4942 (3.5477)  time: 1.6197  data: 1.1216  max mem: 19734
Epoch: [11]  [  20/1251]  eta: 0:21:54  lr: 0.000020  loss: 3.8002 (3.6533)  time: 0.4931  data: 0.0128  max mem: 19734
Epoch: [11]  [  30/1251]  eta: 0:17:46  lr: 0.000020  loss: 3.6984 (3.5238)  time: 0.4630  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4875, ratio_loss=0.0073, pruning_loss=0.1510, mse_loss=0.6319
Epoch: [11]  [  40/1251]  eta: 0:15:41  lr: 0.000020  loss: 3.2863 (3.5044)  time: 0.4728  data: 0.0004  max mem: 19734
Epoch: [11]  [  50/1251]  eta: 0:14:18  lr: 0.000020  loss: 3.2863 (3.4594)  time: 0.4696  data: 0.0004  max mem: 19734
Epoch: [11]  [  60/1251]  eta: 0:13:21  lr: 0.000020  loss: 3.4220 (3.4461)  time: 0.4599  data: 0.0004  max mem: 19734
Epoch: [11]  [  70/1251]  eta: 0:12:40  lr: 0.000020  loss: 3.4220 (3.4252)  time: 0.4620  data: 0.0004  max mem: 19734
Epoch: [11]  [  80/1251]  eta: 0:12:10  lr: 0.000020  loss: 3.5457 (3.4626)  time: 0.4745  data: 0.0004  max mem: 19734
Epoch: [11]  [  90/1251]  eta: 0:11:48  lr: 0.000020  loss: 3.7416 (3.4559)  time: 0.4912  data: 0.0004  max mem: 19734
Epoch: [11]  [ 100/1251]  eta: 0:11:30  lr: 0.000020  loss: 3.4914 (3.4654)  time: 0.5009  data: 0.0004  max mem: 19734
Epoch: [11]  [ 110/1251]  eta: 0:11:13  lr: 0.000020  loss: 3.6365 (3.4684)  time: 0.4987  data: 0.0004  max mem: 19734
Epoch: [11]  [ 120/1251]  eta: 0:10:55  lr: 0.000020  loss: 3.4052 (3.4460)  time: 0.4759  data: 0.0004  max mem: 19734
Epoch: [11]  [ 130/1251]  eta: 0:10:39  lr: 0.000020  loss: 3.5516 (3.4592)  time: 0.4634  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3931, ratio_loss=0.0070, pruning_loss=0.1528, mse_loss=0.6482
Epoch: [11]  [ 140/1251]  eta: 0:10:25  lr: 0.000020  loss: 3.6044 (3.4524)  time: 0.4628  data: 0.0007  max mem: 19734
Epoch: [11]  [ 150/1251]  eta: 0:10:11  lr: 0.000020  loss: 3.5614 (3.4569)  time: 0.4573  data: 0.0004  max mem: 19734
Epoch: [11]  [ 160/1251]  eta: 0:10:00  lr: 0.000020  loss: 3.4951 (3.4436)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [11]  [ 170/1251]  eta: 0:09:48  lr: 0.000020  loss: 3.4431 (3.4420)  time: 0.4591  data: 0.0005  max mem: 19734
Epoch: [11]  [ 180/1251]  eta: 0:09:38  lr: 0.000020  loss: 3.5412 (3.4405)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [11]  [ 190/1251]  eta: 0:09:29  lr: 0.000020  loss: 3.4146 (3.4285)  time: 0.4711  data: 0.0004  max mem: 19734
Epoch: [11]  [ 200/1251]  eta: 0:09:20  lr: 0.000020  loss: 3.3256 (3.4229)  time: 0.4706  data: 0.0004  max mem: 19734
Epoch: [11]  [ 210/1251]  eta: 0:09:10  lr: 0.000020  loss: 3.5049 (3.4256)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [11]  [ 220/1251]  eta: 0:09:02  lr: 0.000020  loss: 3.5837 (3.4259)  time: 0.4577  data: 0.0004  max mem: 19734
Epoch: [11]  [ 230/1251]  eta: 0:08:56  lr: 0.000020  loss: 3.7575 (3.4401)  time: 0.4875  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4144, ratio_loss=0.0069, pruning_loss=0.1527, mse_loss=0.6799
Epoch: [11]  [ 240/1251]  eta: 0:08:49  lr: 0.000020  loss: 3.7575 (3.4515)  time: 0.4985  data: 0.0004  max mem: 19734
Epoch: [11]  [ 250/1251]  eta: 0:08:44  lr: 0.000020  loss: 3.6824 (3.4585)  time: 0.5002  data: 0.0004  max mem: 19734
Epoch: [11]  [ 260/1251]  eta: 0:08:36  lr: 0.000020  loss: 3.5879 (3.4571)  time: 0.4883  data: 0.0004  max mem: 19734
Epoch: [11]  [ 270/1251]  eta: 0:08:28  lr: 0.000020  loss: 3.5724 (3.4568)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [11]  [ 280/1251]  eta: 0:08:21  lr: 0.000020  loss: 3.3937 (3.4539)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [11]  [ 290/1251]  eta: 0:08:14  lr: 0.000020  loss: 3.3937 (3.4582)  time: 0.4558  data: 0.0006  max mem: 19734
Epoch: [11]  [ 300/1251]  eta: 0:08:07  lr: 0.000020  loss: 3.5056 (3.4551)  time: 0.4550  data: 0.0006  max mem: 19734
Epoch: [11]  [ 310/1251]  eta: 0:08:00  lr: 0.000020  loss: 3.7136 (3.4664)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [11]  [ 320/1251]  eta: 0:07:53  lr: 0.000020  loss: 3.7136 (3.4726)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [11]  [ 330/1251]  eta: 0:07:46  lr: 0.000020  loss: 3.6641 (3.4782)  time: 0.4537  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5193, ratio_loss=0.0072, pruning_loss=0.1487, mse_loss=0.6500
Epoch: [11]  [ 340/1251]  eta: 0:07:40  lr: 0.000020  loss: 3.6372 (3.4780)  time: 0.4609  data: 0.0005  max mem: 19734
Epoch: [11]  [ 350/1251]  eta: 0:07:34  lr: 0.000020  loss: 3.2640 (3.4702)  time: 0.4604  data: 0.0004  max mem: 19734
Epoch: [11]  [ 360/1251]  eta: 0:07:28  lr: 0.000020  loss: 3.2640 (3.4697)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [11]  [ 370/1251]  eta: 0:07:22  lr: 0.000020  loss: 3.5425 (3.4706)  time: 0.4623  data: 0.0006  max mem: 19734
Epoch: [11]  [ 380/1251]  eta: 0:07:16  lr: 0.000020  loss: 3.6220 (3.4729)  time: 0.4684  data: 0.0006  max mem: 19734
Epoch: [11]  [ 390/1251]  eta: 0:07:11  lr: 0.000020  loss: 3.6816 (3.4775)  time: 0.4880  data: 0.0004  max mem: 19734
Epoch: [11]  [ 400/1251]  eta: 0:07:06  lr: 0.000020  loss: 3.6455 (3.4766)  time: 0.4924  data: 0.0004  max mem: 19734
Epoch: [11]  [ 410/1251]  eta: 0:07:00  lr: 0.000020  loss: 3.5623 (3.4740)  time: 0.4640  data: 0.0004  max mem: 19734
Epoch: [11]  [ 420/1251]  eta: 0:06:54  lr: 0.000020  loss: 3.1980 (3.4710)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [11]  [ 430/1251]  eta: 0:06:48  lr: 0.000020  loss: 3.6816 (3.4777)  time: 0.4532  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4515, ratio_loss=0.0071, pruning_loss=0.1508, mse_loss=0.6445
Epoch: [11]  [ 440/1251]  eta: 0:06:42  lr: 0.000020  loss: 3.5589 (3.4797)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [11]  [ 450/1251]  eta: 0:06:36  lr: 0.000020  loss: 3.3081 (3.4694)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [11]  [ 460/1251]  eta: 0:06:31  lr: 0.000020  loss: 3.3026 (3.4658)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [11]  [ 470/1251]  eta: 0:06:25  lr: 0.000020  loss: 3.5763 (3.4633)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [11]  [ 480/1251]  eta: 0:06:19  lr: 0.000020  loss: 3.5681 (3.4641)  time: 0.4535  data: 0.0004  max mem: 19734
Epoch: [11]  [ 490/1251]  eta: 0:06:14  lr: 0.000020  loss: 3.5681 (3.4656)  time: 0.4639  data: 0.0004  max mem: 19734
Epoch: [11]  [ 500/1251]  eta: 0:06:09  lr: 0.000020  loss: 3.4235 (3.4638)  time: 0.4633  data: 0.0004  max mem: 19734
Epoch: [11]  [ 510/1251]  eta: 0:06:03  lr: 0.000020  loss: 3.4042 (3.4625)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [11]  [ 520/1251]  eta: 0:05:59  lr: 0.000020  loss: 3.2457 (3.4609)  time: 0.4838  data: 0.0005  max mem: 19734
Epoch: [11]  [ 530/1251]  eta: 0:05:54  lr: 0.000020  loss: 3.2457 (3.4622)  time: 0.4941  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3430, ratio_loss=0.0065, pruning_loss=0.1529, mse_loss=0.6547
Epoch: [11]  [ 540/1251]  eta: 0:05:49  lr: 0.000020  loss: 3.4487 (3.4616)  time: 0.4943  data: 0.0004  max mem: 19734
Epoch: [11]  [ 550/1251]  eta: 0:05:44  lr: 0.000020  loss: 3.3626 (3.4583)  time: 0.4857  data: 0.0004  max mem: 19734
Epoch: [11]  [ 560/1251]  eta: 0:05:38  lr: 0.000020  loss: 3.6358 (3.4607)  time: 0.4600  data: 0.0005  max mem: 19734
Epoch: [11]  [ 570/1251]  eta: 0:05:33  lr: 0.000020  loss: 3.6866 (3.4610)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [11]  [ 580/1251]  eta: 0:05:28  lr: 0.000020  loss: 3.6596 (3.4628)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [11]  [ 590/1251]  eta: 0:05:22  lr: 0.000020  loss: 3.6596 (3.4636)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [11]  [ 600/1251]  eta: 0:05:17  lr: 0.000020  loss: 3.6207 (3.4632)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [11]  [ 610/1251]  eta: 0:05:12  lr: 0.000020  loss: 3.7636 (3.4676)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [11]  [ 620/1251]  eta: 0:05:07  lr: 0.000020  loss: 3.7346 (3.4681)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [11]  [ 630/1251]  eta: 0:05:02  lr: 0.000020  loss: 3.5396 (3.4678)  time: 0.4551  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4910, ratio_loss=0.0068, pruning_loss=0.1495, mse_loss=0.6539
Epoch: [11]  [ 640/1251]  eta: 0:04:57  lr: 0.000020  loss: 3.4983 (3.4713)  time: 0.4618  data: 0.0004  max mem: 19734
Epoch: [11]  [ 650/1251]  eta: 0:04:51  lr: 0.000020  loss: 3.6982 (3.4734)  time: 0.4625  data: 0.0004  max mem: 19734
Epoch: [11]  [ 660/1251]  eta: 0:04:46  lr: 0.000020  loss: 3.6279 (3.4708)  time: 0.4624  data: 0.0005  max mem: 19734
Epoch: [11]  [ 670/1251]  eta: 0:04:42  lr: 0.000020  loss: 3.6279 (3.4746)  time: 0.4898  data: 0.0004  max mem: 19734
Epoch: [11]  [ 680/1251]  eta: 0:04:37  lr: 0.000020  loss: 3.6333 (3.4742)  time: 0.5136  data: 0.0005  max mem: 19734
Epoch: [11]  [ 690/1251]  eta: 0:04:32  lr: 0.000020  loss: 3.5564 (3.4750)  time: 0.4855  data: 0.0005  max mem: 19734
Epoch: [11]  [ 700/1251]  eta: 0:04:27  lr: 0.000020  loss: 3.7569 (3.4765)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [11]  [ 710/1251]  eta: 0:04:22  lr: 0.000020  loss: 3.7603 (3.4805)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [11]  [ 720/1251]  eta: 0:04:17  lr: 0.000020  loss: 3.7206 (3.4804)  time: 0.4565  data: 0.0005  max mem: 19734
Epoch: [11]  [ 730/1251]  eta: 0:04:12  lr: 0.000020  loss: 3.7126 (3.4859)  time: 0.4555  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5310, ratio_loss=0.0071, pruning_loss=0.1473, mse_loss=0.6411
Epoch: [11]  [ 740/1251]  eta: 0:04:07  lr: 0.000020  loss: 3.7001 (3.4849)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [11]  [ 750/1251]  eta: 0:04:02  lr: 0.000020  loss: 3.2983 (3.4831)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [11]  [ 760/1251]  eta: 0:03:57  lr: 0.000020  loss: 3.6262 (3.4868)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [11]  [ 770/1251]  eta: 0:03:52  lr: 0.000020  loss: 3.8156 (3.4868)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [11]  [ 780/1251]  eta: 0:03:47  lr: 0.000020  loss: 3.6242 (3.4853)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [11]  [ 790/1251]  eta: 0:03:42  lr: 0.000020  loss: 3.6280 (3.4859)  time: 0.4665  data: 0.0004  max mem: 19734
Epoch: [11]  [ 800/1251]  eta: 0:03:37  lr: 0.000020  loss: 3.5498 (3.4864)  time: 0.4638  data: 0.0004  max mem: 19734
Epoch: [11]  [ 810/1251]  eta: 0:03:32  lr: 0.000020  loss: 3.4936 (3.4835)  time: 0.4807  data: 0.0004  max mem: 19734
Epoch: [11]  [ 820/1251]  eta: 0:03:27  lr: 0.000020  loss: 3.5302 (3.4846)  time: 0.4979  data: 0.0004  max mem: 19734
Epoch: [11]  [ 830/1251]  eta: 0:03:23  lr: 0.000020  loss: 3.6559 (3.4852)  time: 0.4877  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4736, ratio_loss=0.0070, pruning_loss=0.1482, mse_loss=0.6255
Epoch: [11]  [ 840/1251]  eta: 0:03:18  lr: 0.000020  loss: 3.6559 (3.4864)  time: 0.4688  data: 0.0004  max mem: 19734
Epoch: [11]  [ 850/1251]  eta: 0:03:13  lr: 0.000020  loss: 3.7477 (3.4904)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [11]  [ 860/1251]  eta: 0:03:08  lr: 0.000020  loss: 3.7382 (3.4916)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [11]  [ 870/1251]  eta: 0:03:03  lr: 0.000020  loss: 3.5480 (3.4909)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [11]  [ 880/1251]  eta: 0:02:58  lr: 0.000020  loss: 3.4558 (3.4870)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [11]  [ 890/1251]  eta: 0:02:53  lr: 0.000020  loss: 3.4351 (3.4850)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [11]  [ 900/1251]  eta: 0:02:48  lr: 0.000020  loss: 3.4253 (3.4853)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [11]  [ 910/1251]  eta: 0:02:43  lr: 0.000020  loss: 3.4403 (3.4826)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [11]  [ 920/1251]  eta: 0:02:38  lr: 0.000020  loss: 3.4403 (3.4806)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [11]  [ 930/1251]  eta: 0:02:33  lr: 0.000020  loss: 3.1750 (3.4780)  time: 0.4553  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3154, ratio_loss=0.0068, pruning_loss=0.1533, mse_loss=0.6440
Epoch: [11]  [ 940/1251]  eta: 0:02:29  lr: 0.000020  loss: 3.0097 (3.4728)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [11]  [ 950/1251]  eta: 0:02:24  lr: 0.000020  loss: 3.4163 (3.4723)  time: 0.4760  data: 0.0005  max mem: 19734
Epoch: [11]  [ 960/1251]  eta: 0:02:19  lr: 0.000020  loss: 3.4163 (3.4700)  time: 0.4885  data: 0.0006  max mem: 19734
Epoch: [11]  [ 970/1251]  eta: 0:02:14  lr: 0.000020  loss: 3.5017 (3.4709)  time: 0.4985  data: 0.0006  max mem: 19734
Epoch: [11]  [ 980/1251]  eta: 0:02:09  lr: 0.000020  loss: 3.6332 (3.4711)  time: 0.4851  data: 0.0004  max mem: 19734
Epoch: [11]  [ 990/1251]  eta: 0:02:05  lr: 0.000020  loss: 3.6326 (3.4724)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [11]  [1000/1251]  eta: 0:02:00  lr: 0.000020  loss: 3.5712 (3.4709)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [11]  [1010/1251]  eta: 0:01:55  lr: 0.000020  loss: 3.2480 (3.4682)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [11]  [1020/1251]  eta: 0:01:50  lr: 0.000020  loss: 3.3066 (3.4680)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [11]  [1030/1251]  eta: 0:01:45  lr: 0.000020  loss: 3.6742 (3.4708)  time: 0.4553  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4408, ratio_loss=0.0068, pruning_loss=0.1506, mse_loss=0.6411
Epoch: [11]  [1040/1251]  eta: 0:01:40  lr: 0.000020  loss: 3.6750 (3.4723)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [11]  [1050/1251]  eta: 0:01:36  lr: 0.000020  loss: 3.5052 (3.4713)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [11]  [1060/1251]  eta: 0:01:31  lr: 0.000020  loss: 3.3129 (3.4694)  time: 0.4517  data: 0.0005  max mem: 19734
Epoch: [11]  [1070/1251]  eta: 0:01:26  lr: 0.000020  loss: 3.2258 (3.4678)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [11]  [1080/1251]  eta: 0:01:21  lr: 0.000020  loss: 3.4911 (3.4700)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [11]  [1090/1251]  eta: 0:01:16  lr: 0.000020  loss: 3.6453 (3.4709)  time: 0.4624  data: 0.0004  max mem: 19734
Epoch: [11]  [1100/1251]  eta: 0:01:12  lr: 0.000020  loss: 3.6209 (3.4709)  time: 0.4860  data: 0.0004  max mem: 19734
Epoch: [11]  [1110/1251]  eta: 0:01:07  lr: 0.000020  loss: 3.5414 (3.4705)  time: 0.4987  data: 0.0004  max mem: 19734
Epoch: [11]  [1120/1251]  eta: 0:01:02  lr: 0.000020  loss: 3.5587 (3.4710)  time: 0.4920  data: 0.0004  max mem: 19734
Epoch: [11]  [1130/1251]  eta: 0:00:57  lr: 0.000020  loss: 3.5984 (3.4717)  time: 0.4738  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4274, ratio_loss=0.0075, pruning_loss=0.1491, mse_loss=0.6447
Epoch: [11]  [1140/1251]  eta: 0:00:52  lr: 0.000020  loss: 3.6135 (3.4707)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [11]  [1150/1251]  eta: 0:00:48  lr: 0.000020  loss: 3.6789 (3.4713)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [11]  [1160/1251]  eta: 0:00:43  lr: 0.000020  loss: 3.6933 (3.4722)  time: 0.4545  data: 0.0005  max mem: 19734
Epoch: [11]  [1170/1251]  eta: 0:00:38  lr: 0.000020  loss: 3.4092 (3.4693)  time: 0.4552  data: 0.0005  max mem: 19734
Epoch: [11]  [1180/1251]  eta: 0:00:33  lr: 0.000020  loss: 3.5175 (3.4698)  time: 0.4551  data: 0.0007  max mem: 19734
Epoch: [11]  [1190/1251]  eta: 0:00:29  lr: 0.000020  loss: 3.6776 (3.4707)  time: 0.4530  data: 0.0010  max mem: 19734
Epoch: [11]  [1200/1251]  eta: 0:00:24  lr: 0.000020  loss: 3.5531 (3.4702)  time: 0.4478  data: 0.0006  max mem: 19734
Epoch: [11]  [1210/1251]  eta: 0:00:19  lr: 0.000020  loss: 3.4237 (3.4687)  time: 0.4455  data: 0.0002  max mem: 19734
Epoch: [11]  [1220/1251]  eta: 0:00:14  lr: 0.000020  loss: 3.3867 (3.4691)  time: 0.4461  data: 0.0002  max mem: 19734
Epoch: [11]  [1230/1251]  eta: 0:00:09  lr: 0.000020  loss: 3.6895 (3.4693)  time: 0.4514  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4261, ratio_loss=0.0071, pruning_loss=0.1476, mse_loss=0.6287
Epoch: [11]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 3.7258 (3.4708)  time: 0.4504  data: 0.0001  max mem: 19734
Epoch: [11]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 3.7258 (3.4712)  time: 0.4670  data: 0.0002  max mem: 19734
Epoch: [11] Total time: 0:09:55 (0.4759 s / it)
Averaged stats: lr: 0.000020  loss: 3.7258 (3.4540)
Test:  [  0/261]  eta: 1:36:57  loss: 0.6943 (0.6943)  acc1: 83.8542 (83.8542)  acc5: 95.8333 (95.8333)  time: 22.2885  data: 22.0636  max mem: 19734
Test:  [ 10/261]  eta: 0:11:49  loss: 0.6943 (0.7733)  acc1: 83.8542 (83.6174)  acc5: 96.3542 (95.7386)  time: 2.8274  data: 2.5949  max mem: 19734
Test:  [ 20/261]  eta: 0:06:28  loss: 0.9785 (0.9326)  acc1: 78.6458 (78.8690)  acc5: 94.2708 (94.3452)  time: 0.5763  data: 0.3325  max mem: 19734
Test:  [ 30/261]  eta: 0:04:29  loss: 0.8307 (0.8551)  acc1: 82.8125 (81.5692)  acc5: 94.7917 (94.9597)  time: 0.2521  data: 0.0162  max mem: 19734
Test:  [ 40/261]  eta: 0:04:03  loss: 0.6029 (0.8251)  acc1: 86.9792 (82.4695)  acc5: 96.8750 (95.2617)  time: 0.5683  data: 0.3685  max mem: 19734
Test:  [ 50/261]  eta: 0:03:14  loss: 0.9199 (0.8844)  acc1: 78.6458 (80.6066)  acc5: 94.2708 (94.8836)  time: 0.5481  data: 0.3649  max mem: 19734
Test:  [ 60/261]  eta: 0:02:41  loss: 1.0007 (0.8944)  acc1: 77.0833 (80.1571)  acc5: 94.2708 (94.9027)  time: 0.1918  data: 0.0127  max mem: 19734
Test:  [ 70/261]  eta: 0:02:16  loss: 0.9747 (0.8979)  acc1: 77.6042 (79.6508)  acc5: 95.8333 (95.1291)  time: 0.1840  data: 0.0178  max mem: 19734
Test:  [ 80/261]  eta: 0:01:59  loss: 0.8568 (0.9012)  acc1: 78.1250 (79.8032)  acc5: 96.3542 (95.1903)  time: 0.2164  data: 0.0929  max mem: 19734
Test:  [ 90/261]  eta: 0:01:43  loss: 0.8452 (0.8862)  acc1: 83.3333 (80.2484)  acc5: 96.3542 (95.3068)  time: 0.2117  data: 0.1011  max mem: 19734
Test:  [100/261]  eta: 0:01:39  loss: 0.8389 (0.8877)  acc1: 83.3333 (80.1877)  acc5: 95.3125 (95.3692)  time: 0.4409  data: 0.3062  max mem: 19734
Test:  [110/261]  eta: 0:01:25  loss: 0.8927 (0.9120)  acc1: 76.0417 (79.6781)  acc5: 94.2708 (95.0826)  time: 0.4082  data: 0.2977  max mem: 19734
Test:  [120/261]  eta: 0:01:16  loss: 1.2423 (0.9539)  acc1: 71.3542 (78.7664)  acc5: 90.1042 (94.4947)  time: 0.1624  data: 0.0170  max mem: 19734
Test:  [130/261]  eta: 0:01:08  loss: 1.4506 (1.0021)  acc1: 66.6667 (77.8149)  acc5: 86.9792 (93.8295)  time: 0.2463  data: 0.0402  max mem: 19734
Test:  [140/261]  eta: 0:01:05  loss: 1.3492 (1.0290)  acc1: 68.7500 (77.2015)  acc5: 88.5417 (93.5505)  time: 0.5747  data: 0.3944  max mem: 19734
Test:  [150/261]  eta: 0:00:58  loss: 1.2987 (1.0345)  acc1: 71.3542 (77.1799)  acc5: 91.1458 (93.4396)  time: 0.5609  data: 0.3696  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: 1.1067 (1.0562)  acc1: 78.1250 (76.8116)  acc5: 91.6667 (93.1386)  time: 0.2255  data: 0.0156  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.3787 (1.0874)  acc1: 65.1042 (76.0447)  acc5: 86.4583 (92.7875)  time: 0.3192  data: 0.1597  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.5253 (1.1048)  acc1: 64.5833 (75.6359)  acc5: 88.0208 (92.5990)  time: 0.2840  data: 0.1584  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4423 (1.1184)  acc1: 66.1458 (75.3981)  acc5: 90.1042 (92.4438)  time: 0.1436  data: 0.0309  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.4404 (1.1358)  acc1: 70.8333 (75.0596)  acc5: 88.5417 (92.1979)  time: 0.1277  data: 0.0362  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: 1.4404 (1.1500)  acc1: 70.3125 (74.7729)  acc5: 87.5000 (91.9925)  time: 0.1365  data: 0.0622  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: 1.5124 (1.1693)  acc1: 66.1458 (74.3024)  acc5: 88.0208 (91.7798)  time: 0.1139  data: 0.0499  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.5203 (1.1793)  acc1: 66.1458 (74.0192)  acc5: 88.5417 (91.6937)  time: 0.0616  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.4063 (1.1893)  acc1: 67.7083 (73.7811)  acc5: 91.1458 (91.6278)  time: 0.0684  data: 0.0069  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.1921 (1.1834)  acc1: 74.4792 (73.9397)  acc5: 92.7083 (91.7476)  time: 0.0684  data: 0.0069  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0273 (1.1828)  acc1: 76.5625 (73.9860)  acc5: 94.7917 (91.8140)  time: 0.0597  data: 0.0001  max mem: 19734
Test: Total time: 0:01:34 (0.3633 s / it)
* Acc@1 73.986 Acc@5 91.814 loss 1.183
Accuracy of the network on the 50000 test images: 74.0%
Max accuracy: 73.99%
Epoch: [12]  [   0/1251]  eta: 5:05:30  lr: 0.000019  loss: 4.3398 (4.3398)  time: 14.6531  data: 14.2054  max mem: 19734
Epoch: [12]  [  10/1251]  eta: 0:39:49  lr: 0.000019  loss: 3.3724 (3.4145)  time: 1.9258  data: 1.2919  max mem: 19734
Epoch: [12]  [  20/1251]  eta: 0:25:07  lr: 0.000019  loss: 3.4952 (3.3983)  time: 0.5532  data: 0.0006  max mem: 19734
Epoch: [12]  [  30/1251]  eta: 0:19:54  lr: 0.000019  loss: 3.5738 (3.4701)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [12]  [  40/1251]  eta: 0:17:10  lr: 0.000019  loss: 3.5125 (3.4327)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [12]  [  50/1251]  eta: 0:15:29  lr: 0.000019  loss: 3.1167 (3.3995)  time: 0.4583  data: 0.0004  max mem: 19734
Epoch: [12]  [  60/1251]  eta: 0:14:24  lr: 0.000019  loss: 3.1056 (3.3590)  time: 0.4697  data: 0.0004  max mem: 19734
Epoch: [12]  [  70/1251]  eta: 0:13:34  lr: 0.000019  loss: 3.5258 (3.4128)  time: 0.4730  data: 0.0004  max mem: 19734
Epoch: [12]  [  80/1251]  eta: 0:12:55  lr: 0.000019  loss: 3.6600 (3.4339)  time: 0.4672  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4197, ratio_loss=0.0065, pruning_loss=0.1493, mse_loss=0.6108
Epoch: [12]  [  90/1251]  eta: 0:12:23  lr: 0.000019  loss: 3.5571 (3.4302)  time: 0.4655  data: 0.0004  max mem: 19734
Epoch: [12]  [ 100/1251]  eta: 0:11:56  lr: 0.000019  loss: 3.5498 (3.4256)  time: 0.4632  data: 0.0004  max mem: 19734
Epoch: [12]  [ 110/1251]  eta: 0:11:34  lr: 0.000019  loss: 3.5498 (3.4371)  time: 0.4636  data: 0.0005  max mem: 19734
Epoch: [12]  [ 120/1251]  eta: 0:11:14  lr: 0.000019  loss: 3.3987 (3.4228)  time: 0.4616  data: 0.0005  max mem: 19734
Epoch: [12]  [ 130/1251]  eta: 0:11:01  lr: 0.000019  loss: 3.3666 (3.4341)  time: 0.4912  data: 0.0005  max mem: 19734
Epoch: [12]  [ 140/1251]  eta: 0:10:45  lr: 0.000019  loss: 3.8069 (3.4605)  time: 0.4900  data: 0.0005  max mem: 19734
Epoch: [12]  [ 150/1251]  eta: 0:10:35  lr: 0.000019  loss: 3.5756 (3.4485)  time: 0.4908  data: 0.0005  max mem: 19734
Epoch: [12]  [ 160/1251]  eta: 0:10:22  lr: 0.000019  loss: 3.5756 (3.4508)  time: 0.4933  data: 0.0005  max mem: 19734
Epoch: [12]  [ 170/1251]  eta: 0:10:09  lr: 0.000019  loss: 3.6551 (3.4638)  time: 0.4629  data: 0.0005  max mem: 19734
Epoch: [12]  [ 180/1251]  eta: 0:09:57  lr: 0.000019  loss: 3.7016 (3.4549)  time: 0.4601  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4516, ratio_loss=0.0068, pruning_loss=0.1461, mse_loss=0.6343
Epoch: [12]  [ 190/1251]  eta: 0:09:46  lr: 0.000019  loss: 3.3963 (3.4529)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [12]  [ 200/1251]  eta: 0:09:36  lr: 0.000019  loss: 3.5523 (3.4573)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [12]  [ 210/1251]  eta: 0:09:26  lr: 0.000019  loss: 3.5523 (3.4564)  time: 0.4669  data: 0.0005  max mem: 19734
Epoch: [12]  [ 220/1251]  eta: 0:09:17  lr: 0.000019  loss: 3.4397 (3.4577)  time: 0.4664  data: 0.0005  max mem: 19734
Epoch: [12]  [ 230/1251]  eta: 0:09:08  lr: 0.000019  loss: 3.3946 (3.4503)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [12]  [ 240/1251]  eta: 0:08:59  lr: 0.000019  loss: 3.5553 (3.4600)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [12]  [ 250/1251]  eta: 0:08:51  lr: 0.000019  loss: 3.5881 (3.4597)  time: 0.4565  data: 0.0005  max mem: 19734
Epoch: [12]  [ 260/1251]  eta: 0:08:42  lr: 0.000019  loss: 3.5307 (3.4608)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [12]  [ 270/1251]  eta: 0:08:36  lr: 0.000019  loss: 3.7676 (3.4744)  time: 0.4730  data: 0.0004  max mem: 19734
Epoch: [12]  [ 280/1251]  eta: 0:08:30  lr: 0.000019  loss: 3.7118 (3.4770)  time: 0.4955  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4896, ratio_loss=0.0071, pruning_loss=0.1460, mse_loss=0.6655
Epoch: [12]  [ 290/1251]  eta: 0:08:23  lr: 0.000019  loss: 3.6088 (3.4757)  time: 0.4894  data: 0.0005  max mem: 19734
Epoch: [12]  [ 300/1251]  eta: 0:08:17  lr: 0.000019  loss: 3.6597 (3.4735)  time: 0.4844  data: 0.0004  max mem: 19734
Epoch: [12]  [ 310/1251]  eta: 0:08:09  lr: 0.000019  loss: 3.5239 (3.4657)  time: 0.4716  data: 0.0004  max mem: 19734
Epoch: [12]  [ 320/1251]  eta: 0:08:02  lr: 0.000019  loss: 3.3255 (3.4571)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [12]  [ 330/1251]  eta: 0:07:55  lr: 0.000019  loss: 3.3700 (3.4554)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [12]  [ 340/1251]  eta: 0:07:48  lr: 0.000019  loss: 3.4090 (3.4588)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [12]  [ 350/1251]  eta: 0:07:42  lr: 0.000019  loss: 3.6600 (3.4607)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [12]  [ 360/1251]  eta: 0:07:36  lr: 0.000019  loss: 3.6357 (3.4634)  time: 0.4692  data: 0.0005  max mem: 19734
Epoch: [12]  [ 370/1251]  eta: 0:07:29  lr: 0.000019  loss: 3.6357 (3.4683)  time: 0.4700  data: 0.0005  max mem: 19734
Epoch: [12]  [ 380/1251]  eta: 0:07:23  lr: 0.000019  loss: 3.3528 (3.4562)  time: 0.4560  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3619, ratio_loss=0.0069, pruning_loss=0.1493, mse_loss=0.6367
Epoch: [12]  [ 390/1251]  eta: 0:07:17  lr: 0.000019  loss: 3.0991 (3.4562)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [12]  [ 400/1251]  eta: 0:07:11  lr: 0.000019  loss: 3.5094 (3.4572)  time: 0.4575  data: 0.0005  max mem: 19734
Epoch: [12]  [ 410/1251]  eta: 0:07:05  lr: 0.000019  loss: 3.5094 (3.4577)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [12]  [ 420/1251]  eta: 0:07:00  lr: 0.000019  loss: 3.2948 (3.4532)  time: 0.4949  data: 0.0004  max mem: 19734
Epoch: [12]  [ 430/1251]  eta: 0:06:54  lr: 0.000019  loss: 3.4929 (3.4555)  time: 0.4961  data: 0.0005  max mem: 19734
Epoch: [12]  [ 440/1251]  eta: 0:06:49  lr: 0.000019  loss: 3.5514 (3.4547)  time: 0.4931  data: 0.0005  max mem: 19734
Epoch: [12]  [ 450/1251]  eta: 0:06:44  lr: 0.000019  loss: 3.4792 (3.4534)  time: 0.4907  data: 0.0005  max mem: 19734
Epoch: [12]  [ 460/1251]  eta: 0:06:38  lr: 0.000019  loss: 3.2281 (3.4472)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [12]  [ 470/1251]  eta: 0:06:32  lr: 0.000019  loss: 3.3549 (3.4458)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [12]  [ 480/1251]  eta: 0:06:26  lr: 0.000019  loss: 3.6264 (3.4542)  time: 0.4561  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4159, ratio_loss=0.0062, pruning_loss=0.1490, mse_loss=0.6227
Epoch: [12]  [ 490/1251]  eta: 0:06:20  lr: 0.000019  loss: 3.7507 (3.4548)  time: 0.4571  data: 0.0005  max mem: 19734
Epoch: [12]  [ 500/1251]  eta: 0:06:15  lr: 0.000019  loss: 3.6677 (3.4557)  time: 0.4630  data: 0.0006  max mem: 19734
Epoch: [12]  [ 510/1251]  eta: 0:06:09  lr: 0.000019  loss: 3.7489 (3.4599)  time: 0.4616  data: 0.0005  max mem: 19734
Epoch: [12]  [ 520/1251]  eta: 0:06:04  lr: 0.000019  loss: 3.5044 (3.4573)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [12]  [ 530/1251]  eta: 0:05:58  lr: 0.000019  loss: 3.6406 (3.4601)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [12]  [ 540/1251]  eta: 0:05:53  lr: 0.000019  loss: 3.6804 (3.4621)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [12]  [ 550/1251]  eta: 0:05:47  lr: 0.000019  loss: 3.5993 (3.4625)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [12]  [ 560/1251]  eta: 0:05:42  lr: 0.000019  loss: 3.6154 (3.4663)  time: 0.4734  data: 0.0004  max mem: 19734
Epoch: [12]  [ 570/1251]  eta: 0:05:37  lr: 0.000019  loss: 3.6760 (3.4639)  time: 0.4885  data: 0.0004  max mem: 19734
Epoch: [12]  [ 580/1251]  eta: 0:05:32  lr: 0.000019  loss: 3.1026 (3.4609)  time: 0.4780  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4524, ratio_loss=0.0067, pruning_loss=0.1478, mse_loss=0.6374
Epoch: [12]  [ 590/1251]  eta: 0:05:27  lr: 0.000019  loss: 3.4218 (3.4597)  time: 0.4814  data: 0.0006  max mem: 19734
Epoch: [12]  [ 600/1251]  eta: 0:05:21  lr: 0.000019  loss: 3.5501 (3.4619)  time: 0.4732  data: 0.0006  max mem: 19734
Epoch: [12]  [ 610/1251]  eta: 0:05:16  lr: 0.000019  loss: 3.5782 (3.4649)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [12]  [ 620/1251]  eta: 0:05:11  lr: 0.000019  loss: 3.5782 (3.4656)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [12]  [ 630/1251]  eta: 0:05:05  lr: 0.000019  loss: 3.2512 (3.4629)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [12]  [ 640/1251]  eta: 0:05:00  lr: 0.000019  loss: 3.1801 (3.4610)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [12]  [ 650/1251]  eta: 0:04:55  lr: 0.000019  loss: 3.6518 (3.4640)  time: 0.4618  data: 0.0005  max mem: 19734
Epoch: [12]  [ 660/1251]  eta: 0:04:50  lr: 0.000019  loss: 3.6732 (3.4644)  time: 0.4644  data: 0.0004  max mem: 19734
Epoch: [12]  [ 670/1251]  eta: 0:04:45  lr: 0.000019  loss: 3.5257 (3.4638)  time: 0.4610  data: 0.0005  max mem: 19734
Epoch: [12]  [ 680/1251]  eta: 0:04:39  lr: 0.000019  loss: 3.4448 (3.4624)  time: 0.4583  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4331, ratio_loss=0.0066, pruning_loss=0.1459, mse_loss=0.6076
Epoch: [12]  [ 690/1251]  eta: 0:04:34  lr: 0.000019  loss: 3.5689 (3.4629)  time: 0.4554  data: 0.0006  max mem: 19734
Epoch: [12]  [ 700/1251]  eta: 0:04:29  lr: 0.000019  loss: 3.5676 (3.4624)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [12]  [ 710/1251]  eta: 0:04:24  lr: 0.000019  loss: 3.5340 (3.4631)  time: 0.4918  data: 0.0004  max mem: 19734
Epoch: [12]  [ 720/1251]  eta: 0:04:19  lr: 0.000019  loss: 3.5724 (3.4625)  time: 0.4939  data: 0.0004  max mem: 19734
Epoch: [12]  [ 730/1251]  eta: 0:04:14  lr: 0.000019  loss: 3.6296 (3.4622)  time: 0.4715  data: 0.0004  max mem: 19734
Epoch: [12]  [ 740/1251]  eta: 0:04:09  lr: 0.000019  loss: 3.6296 (3.4628)  time: 0.4711  data: 0.0004  max mem: 19734
Epoch: [12]  [ 750/1251]  eta: 0:04:04  lr: 0.000019  loss: 3.5716 (3.4628)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [12]  [ 760/1251]  eta: 0:03:59  lr: 0.000019  loss: 3.5716 (3.4613)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [12]  [ 770/1251]  eta: 0:03:54  lr: 0.000019  loss: 3.2691 (3.4577)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [12]  [ 780/1251]  eta: 0:03:49  lr: 0.000019  loss: 3.4053 (3.4593)  time: 0.4536  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4155, ratio_loss=0.0067, pruning_loss=0.1466, mse_loss=0.6253
Epoch: [12]  [ 790/1251]  eta: 0:03:44  lr: 0.000019  loss: 3.6035 (3.4593)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [12]  [ 800/1251]  eta: 0:03:39  lr: 0.000019  loss: 3.4491 (3.4579)  time: 0.4627  data: 0.0005  max mem: 19734
Epoch: [12]  [ 810/1251]  eta: 0:03:34  lr: 0.000019  loss: 3.6374 (3.4611)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [12]  [ 820/1251]  eta: 0:03:29  lr: 0.000019  loss: 3.7227 (3.4622)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [12]  [ 830/1251]  eta: 0:03:24  lr: 0.000019  loss: 3.5651 (3.4639)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [12]  [ 840/1251]  eta: 0:03:19  lr: 0.000019  loss: 3.5651 (3.4624)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [12]  [ 850/1251]  eta: 0:03:14  lr: 0.000019  loss: 3.6284 (3.4644)  time: 0.4823  data: 0.0004  max mem: 19734
Epoch: [12]  [ 860/1251]  eta: 0:03:09  lr: 0.000019  loss: 3.7033 (3.4656)  time: 0.4942  data: 0.0004  max mem: 19734
Epoch: [12]  [ 870/1251]  eta: 0:03:04  lr: 0.000019  loss: 3.8241 (3.4696)  time: 0.4836  data: 0.0004  max mem: 19734
Epoch: [12]  [ 880/1251]  eta: 0:02:59  lr: 0.000019  loss: 3.7997 (3.4715)  time: 0.4783  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5353, ratio_loss=0.0065, pruning_loss=0.1441, mse_loss=0.6209
Epoch: [12]  [ 890/1251]  eta: 0:02:54  lr: 0.000019  loss: 3.6884 (3.4698)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [12]  [ 900/1251]  eta: 0:02:49  lr: 0.000019  loss: 3.2259 (3.4686)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [12]  [ 910/1251]  eta: 0:02:45  lr: 0.000019  loss: 3.5559 (3.4682)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [12]  [ 920/1251]  eta: 0:02:40  lr: 0.000019  loss: 3.0802 (3.4618)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [12]  [ 930/1251]  eta: 0:02:35  lr: 0.000019  loss: 3.2691 (3.4619)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [12]  [ 940/1251]  eta: 0:02:30  lr: 0.000019  loss: 3.3533 (3.4590)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [12]  [ 950/1251]  eta: 0:02:25  lr: 0.000019  loss: 3.2923 (3.4582)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [12]  [ 960/1251]  eta: 0:02:20  lr: 0.000019  loss: 3.5285 (3.4568)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [12]  [ 970/1251]  eta: 0:02:15  lr: 0.000019  loss: 3.5285 (3.4554)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [12]  [ 980/1251]  eta: 0:02:10  lr: 0.000019  loss: 3.1606 (3.4536)  time: 0.4548  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2756, ratio_loss=0.0066, pruning_loss=0.1501, mse_loss=0.6333
Epoch: [12]  [ 990/1251]  eta: 0:02:05  lr: 0.000019  loss: 3.5745 (3.4557)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [12]  [1000/1251]  eta: 0:02:01  lr: 0.000019  loss: 3.5745 (3.4536)  time: 0.4909  data: 0.0004  max mem: 19734
Epoch: [12]  [1010/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.5104 (3.4544)  time: 0.5021  data: 0.0004  max mem: 19734
Epoch: [12]  [1020/1251]  eta: 0:01:51  lr: 0.000019  loss: 3.4776 (3.4530)  time: 0.4763  data: 0.0003  max mem: 19734
Epoch: [12]  [1030/1251]  eta: 0:01:46  lr: 0.000019  loss: 3.4229 (3.4518)  time: 0.4643  data: 0.0004  max mem: 19734
Epoch: [12]  [1040/1251]  eta: 0:01:41  lr: 0.000019  loss: 3.4409 (3.4512)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [12]  [1050/1251]  eta: 0:01:36  lr: 0.000019  loss: 3.3648 (3.4501)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [12]  [1060/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.5066 (3.4502)  time: 0.4506  data: 0.0004  max mem: 19734
Epoch: [12]  [1070/1251]  eta: 0:01:26  lr: 0.000019  loss: 3.7248 (3.4523)  time: 0.4508  data: 0.0004  max mem: 19734
Epoch: [12]  [1080/1251]  eta: 0:01:22  lr: 0.000019  loss: 3.6258 (3.4512)  time: 0.4523  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3655, ratio_loss=0.0069, pruning_loss=0.1473, mse_loss=0.6193
Epoch: [12]  [1090/1251]  eta: 0:01:17  lr: 0.000019  loss: 3.3900 (3.4490)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [12]  [1100/1251]  eta: 0:01:12  lr: 0.000019  loss: 3.3765 (3.4494)  time: 0.4610  data: 0.0004  max mem: 19734
Epoch: [12]  [1110/1251]  eta: 0:01:07  lr: 0.000019  loss: 3.3765 (3.4493)  time: 0.4626  data: 0.0004  max mem: 19734
Epoch: [12]  [1120/1251]  eta: 0:01:02  lr: 0.000019  loss: 3.4451 (3.4498)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [12]  [1130/1251]  eta: 0:00:57  lr: 0.000019  loss: 3.5953 (3.4507)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [12]  [1140/1251]  eta: 0:00:53  lr: 0.000019  loss: 3.6330 (3.4524)  time: 0.4729  data: 0.0004  max mem: 19734
Epoch: [12]  [1150/1251]  eta: 0:00:48  lr: 0.000019  loss: 3.6274 (3.4528)  time: 0.4925  data: 0.0004  max mem: 19734
Epoch: [12]  [1160/1251]  eta: 0:00:43  lr: 0.000019  loss: 3.6274 (3.4528)  time: 0.4974  data: 0.0004  max mem: 19734
Epoch: [12]  [1170/1251]  eta: 0:00:38  lr: 0.000019  loss: 3.4494 (3.4503)  time: 0.4792  data: 0.0004  max mem: 19734
Epoch: [12]  [1180/1251]  eta: 0:00:34  lr: 0.000019  loss: 3.4403 (3.4510)  time: 0.4555  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4386, ratio_loss=0.0063, pruning_loss=0.1475, mse_loss=0.6332
Epoch: [12]  [1190/1251]  eta: 0:00:29  lr: 0.000019  loss: 3.7398 (3.4526)  time: 0.4548  data: 0.0007  max mem: 19734
Epoch: [12]  [1200/1251]  eta: 0:00:24  lr: 0.000019  loss: 3.7510 (3.4538)  time: 0.4499  data: 0.0006  max mem: 19734
Epoch: [12]  [1210/1251]  eta: 0:00:19  lr: 0.000019  loss: 3.7873 (3.4535)  time: 0.4472  data: 0.0002  max mem: 19734
Epoch: [12]  [1220/1251]  eta: 0:00:14  lr: 0.000019  loss: 3.5461 (3.4545)  time: 0.4466  data: 0.0002  max mem: 19734
Epoch: [12]  [1230/1251]  eta: 0:00:10  lr: 0.000019  loss: 3.4790 (3.4519)  time: 0.4449  data: 0.0002  max mem: 19734
Epoch: [12]  [1240/1251]  eta: 0:00:05  lr: 0.000019  loss: 3.1247 (3.4516)  time: 0.4536  data: 0.0002  max mem: 19734
Epoch: [12]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.5275 (3.4524)  time: 0.4527  data: 0.0002  max mem: 19734
Epoch: [12] Total time: 0:09:58 (0.4782 s / it)
Averaged stats: lr: 0.000019  loss: 3.5275 (3.4656)
Test:  [  0/261]  eta: 1:33:27  loss: 0.7224 (0.7224)  acc1: 83.3333 (83.3333)  acc5: 95.8333 (95.8333)  time: 21.4834  data: 21.3616  max mem: 19734
Test:  [ 10/261]  eta: 0:13:47  loss: 0.7224 (0.7688)  acc1: 83.3333 (83.6648)  acc5: 96.3542 (95.7386)  time: 3.2957  data: 3.1404  max mem: 19734
Test:  [ 20/261]  eta: 0:07:17  loss: 0.9625 (0.9282)  acc1: 79.6875 (79.3403)  acc5: 93.2292 (94.3204)  time: 0.8306  data: 0.6654  max mem: 19734
Test:  [ 30/261]  eta: 0:04:58  loss: 0.8127 (0.8464)  acc1: 80.7292 (81.9892)  acc5: 93.7500 (94.8253)  time: 0.1915  data: 0.0166  max mem: 19734
Test:  [ 40/261]  eta: 0:03:48  loss: 0.5891 (0.8157)  acc1: 88.5417 (82.8379)  acc5: 96.3542 (95.2109)  time: 0.2180  data: 0.0161  max mem: 19734
Test:  [ 50/261]  eta: 0:03:06  loss: 0.9109 (0.8705)  acc1: 78.1250 (81.0968)  acc5: 94.7917 (94.8734)  time: 0.2444  data: 0.0103  max mem: 19734
Test:  [ 60/261]  eta: 0:02:40  loss: 1.0282 (0.8839)  acc1: 75.0000 (80.4816)  acc5: 94.2708 (94.8856)  time: 0.3163  data: 0.0156  max mem: 19734
Test:  [ 70/261]  eta: 0:02:16  loss: 1.0161 (0.8881)  acc1: 76.0417 (79.9002)  acc5: 95.3125 (95.0851)  time: 0.2875  data: 0.0139  max mem: 19734
Test:  [ 80/261]  eta: 0:02:15  loss: 0.8330 (0.8900)  acc1: 80.2083 (80.0926)  acc5: 96.3542 (95.2096)  time: 0.5908  data: 0.3764  max mem: 19734
Test:  [ 90/261]  eta: 0:01:57  loss: 0.8255 (0.8759)  acc1: 82.8125 (80.4430)  acc5: 96.3542 (95.3354)  time: 0.5998  data: 0.3830  max mem: 19734
Test:  [100/261]  eta: 0:01:41  loss: 0.8140 (0.8777)  acc1: 82.8125 (80.4043)  acc5: 96.3542 (95.3847)  time: 0.1675  data: 0.0283  max mem: 19734
Test:  [110/261]  eta: 0:01:34  loss: 0.9025 (0.9005)  acc1: 77.6042 (79.9315)  acc5: 94.2708 (95.0873)  time: 0.3523  data: 0.2659  max mem: 19734
Test:  [120/261]  eta: 0:01:23  loss: 1.2391 (0.9428)  acc1: 70.3125 (78.9084)  acc5: 90.1042 (94.5334)  time: 0.3791  data: 0.2543  max mem: 19734
Test:  [130/261]  eta: 0:01:13  loss: 1.4503 (0.9899)  acc1: 67.1875 (77.8865)  acc5: 86.9792 (93.8971)  time: 0.1846  data: 0.0136  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: 1.3999 (1.0173)  acc1: 68.2292 (77.2015)  acc5: 89.5833 (93.6170)  time: 0.1799  data: 0.0126  max mem: 19734
Test:  [150/261]  eta: 0:00:56  loss: 1.2422 (1.0226)  acc1: 71.8750 (77.1730)  acc5: 90.6250 (93.4810)  time: 0.1840  data: 0.0438  max mem: 19734
Test:  [160/261]  eta: 0:00:49  loss: 1.0328 (1.0447)  acc1: 77.0833 (76.7469)  acc5: 91.6667 (93.1806)  time: 0.2255  data: 0.0839  max mem: 19734
Test:  [170/261]  eta: 0:00:44  loss: 1.3307 (1.0751)  acc1: 64.5833 (75.9929)  acc5: 86.9792 (92.8332)  time: 0.3049  data: 0.1588  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4768 (1.0919)  acc1: 64.5833 (75.5784)  acc5: 87.5000 (92.6450)  time: 0.2876  data: 0.1845  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4346 (1.1047)  acc1: 66.6667 (75.3163)  acc5: 90.1042 (92.4793)  time: 0.1920  data: 0.1233  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.4023 (1.1221)  acc1: 70.8333 (75.0104)  acc5: 89.0625 (92.2316)  time: 0.1311  data: 0.0513  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.4023 (1.1381)  acc1: 69.2708 (74.6816)  acc5: 87.5000 (92.0024)  time: 0.1292  data: 0.0511  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4840 (1.1575)  acc1: 66.6667 (74.1799)  acc5: 87.5000 (91.8104)  time: 0.1131  data: 0.0510  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.5169 (1.1671)  acc1: 66.1458 (73.9696)  acc5: 88.5417 (91.6915)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.4389 (1.1763)  acc1: 66.6667 (73.7465)  acc5: 90.6250 (91.6299)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.1493 (1.1695)  acc1: 75.0000 (73.9189)  acc5: 92.1875 (91.7351)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0139 (1.1692)  acc1: 76.0417 (73.9220)  acc5: 95.3125 (91.7860)  time: 0.0596  data: 0.0002  max mem: 19734
Test: Total time: 0:01:32 (0.3561 s / it)
* Acc@1 73.922 Acc@5 91.786 loss 1.169
Accuracy of the network on the 50000 test images: 73.9%
Max accuracy: 73.99%
Epoch: [13]  [   0/1251]  eta: 6:01:43  lr: 0.000019  loss: 2.4776 (2.4776)  time: 17.3489  data: 10.7592  max mem: 19734
Epoch: [13]  [  10/1251]  eta: 0:43:07  lr: 0.000019  loss: 3.5339 (3.1980)  time: 2.0848  data: 1.0396  max mem: 19734
Epoch: [13]  [  20/1251]  eta: 0:26:52  lr: 0.000019  loss: 3.5339 (3.3566)  time: 0.5082  data: 0.0341  max mem: 19734
Epoch: [13]  [  30/1251]  eta: 0:21:18  lr: 0.000019  loss: 3.3849 (3.3392)  time: 0.4768  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4018, ratio_loss=0.0068, pruning_loss=0.1485, mse_loss=0.6126
Epoch: [13]  [  40/1251]  eta: 0:18:25  lr: 0.000019  loss: 3.3207 (3.3758)  time: 0.4964  data: 0.0004  max mem: 19734
Epoch: [13]  [  50/1251]  eta: 0:16:42  lr: 0.000019  loss: 3.4493 (3.3961)  time: 0.5046  data: 0.0004  max mem: 19734
Epoch: [13]  [  60/1251]  eta: 0:15:21  lr: 0.000019  loss: 3.5322 (3.4248)  time: 0.4880  data: 0.0004  max mem: 19734
Epoch: [13]  [  70/1251]  eta: 0:14:21  lr: 0.000019  loss: 3.5322 (3.4243)  time: 0.4609  data: 0.0005  max mem: 19734
Epoch: [13]  [  80/1251]  eta: 0:13:35  lr: 0.000019  loss: 3.6376 (3.4327)  time: 0.4603  data: 0.0004  max mem: 19734
Epoch: [13]  [  90/1251]  eta: 0:12:58  lr: 0.000019  loss: 3.2816 (3.4160)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [13]  [ 100/1251]  eta: 0:12:29  lr: 0.000019  loss: 3.2816 (3.4147)  time: 0.4699  data: 0.0005  max mem: 19734
Epoch: [13]  [ 110/1251]  eta: 0:12:03  lr: 0.000019  loss: 3.6463 (3.4136)  time: 0.4712  data: 0.0005  max mem: 19734
Epoch: [13]  [ 120/1251]  eta: 0:11:41  lr: 0.000019  loss: 3.5341 (3.4042)  time: 0.4597  data: 0.0005  max mem: 19734
Epoch: [13]  [ 130/1251]  eta: 0:11:20  lr: 0.000019  loss: 3.5691 (3.4149)  time: 0.4582  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4029, ratio_loss=0.0072, pruning_loss=0.1475, mse_loss=0.6183
Epoch: [13]  [ 140/1251]  eta: 0:11:03  lr: 0.000019  loss: 3.6666 (3.4147)  time: 0.4578  data: 0.0007  max mem: 19734
Epoch: [13]  [ 150/1251]  eta: 0:10:46  lr: 0.000019  loss: 3.3635 (3.4162)  time: 0.4564  data: 0.0005  max mem: 19734
Epoch: [13]  [ 160/1251]  eta: 0:10:31  lr: 0.000019  loss: 3.2393 (3.4070)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [13]  [ 170/1251]  eta: 0:10:18  lr: 0.000019  loss: 3.2383 (3.4049)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [13]  [ 180/1251]  eta: 0:10:09  lr: 0.000019  loss: 3.6525 (3.4299)  time: 0.4910  data: 0.0005  max mem: 19734
Epoch: [13]  [ 190/1251]  eta: 0:10:00  lr: 0.000019  loss: 3.6525 (3.4170)  time: 0.5120  data: 0.0005  max mem: 19734
Epoch: [13]  [ 200/1251]  eta: 0:09:49  lr: 0.000019  loss: 3.0505 (3.4118)  time: 0.4876  data: 0.0004  max mem: 19734
Epoch: [13]  [ 210/1251]  eta: 0:09:39  lr: 0.000019  loss: 3.5877 (3.4178)  time: 0.4670  data: 0.0004  max mem: 19734
Epoch: [13]  [ 220/1251]  eta: 0:09:28  lr: 0.000019  loss: 3.4964 (3.4150)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [13]  [ 230/1251]  eta: 0:09:19  lr: 0.000019  loss: 3.3620 (3.4158)  time: 0.4560  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3968, ratio_loss=0.0066, pruning_loss=0.1464, mse_loss=0.6179
Epoch: [13]  [ 240/1251]  eta: 0:09:09  lr: 0.000019  loss: 3.5636 (3.4265)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [13]  [ 250/1251]  eta: 0:09:01  lr: 0.000019  loss: 3.7510 (3.4393)  time: 0.4633  data: 0.0005  max mem: 19734
Epoch: [13]  [ 260/1251]  eta: 0:08:52  lr: 0.000019  loss: 3.7207 (3.4377)  time: 0.4622  data: 0.0005  max mem: 19734
Epoch: [13]  [ 270/1251]  eta: 0:08:44  lr: 0.000019  loss: 3.2940 (3.4334)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [13]  [ 280/1251]  eta: 0:08:36  lr: 0.000019  loss: 3.1654 (3.4277)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [13]  [ 290/1251]  eta: 0:08:28  lr: 0.000019  loss: 3.4350 (3.4256)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [13]  [ 300/1251]  eta: 0:08:20  lr: 0.000019  loss: 3.4138 (3.4214)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [13]  [ 310/1251]  eta: 0:08:13  lr: 0.000019  loss: 3.3601 (3.4209)  time: 0.4564  data: 0.0005  max mem: 19734
Epoch: [13]  [ 320/1251]  eta: 0:08:07  lr: 0.000019  loss: 3.7543 (3.4268)  time: 0.4799  data: 0.0005  max mem: 19734
Epoch: [13]  [ 330/1251]  eta: 0:08:01  lr: 0.000019  loss: 3.8086 (3.4342)  time: 0.4981  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4124, ratio_loss=0.0069, pruning_loss=0.1463, mse_loss=0.6032
Epoch: [13]  [ 340/1251]  eta: 0:07:55  lr: 0.000019  loss: 3.4323 (3.4281)  time: 0.4935  data: 0.0004  max mem: 19734
Epoch: [13]  [ 350/1251]  eta: 0:07:48  lr: 0.000019  loss: 3.3269 (3.4270)  time: 0.4763  data: 0.0004  max mem: 19734
Epoch: [13]  [ 360/1251]  eta: 0:07:41  lr: 0.000019  loss: 3.5832 (3.4253)  time: 0.4590  data: 0.0005  max mem: 19734
Epoch: [13]  [ 370/1251]  eta: 0:07:35  lr: 0.000019  loss: 3.6967 (3.4323)  time: 0.4586  data: 0.0005  max mem: 19734
Epoch: [13]  [ 380/1251]  eta: 0:07:28  lr: 0.000019  loss: 3.6954 (3.4335)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [13]  [ 390/1251]  eta: 0:07:22  lr: 0.000019  loss: 3.6287 (3.4299)  time: 0.4658  data: 0.0005  max mem: 19734
Epoch: [13]  [ 400/1251]  eta: 0:07:16  lr: 0.000019  loss: 3.5993 (3.4331)  time: 0.4635  data: 0.0006  max mem: 19734
Epoch: [13]  [ 410/1251]  eta: 0:07:09  lr: 0.000019  loss: 3.5212 (3.4307)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [13]  [ 420/1251]  eta: 0:07:03  lr: 0.000019  loss: 3.4396 (3.4307)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [13]  [ 430/1251]  eta: 0:06:57  lr: 0.000019  loss: 3.4171 (3.4334)  time: 0.4551  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4394, ratio_loss=0.0064, pruning_loss=0.1451, mse_loss=0.5948
Epoch: [13]  [ 440/1251]  eta: 0:06:51  lr: 0.000019  loss: 3.5660 (3.4372)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [13]  [ 450/1251]  eta: 0:06:45  lr: 0.000019  loss: 3.7186 (3.4398)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [13]  [ 460/1251]  eta: 0:06:39  lr: 0.000019  loss: 3.4817 (3.4333)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [13]  [ 470/1251]  eta: 0:06:34  lr: 0.000019  loss: 3.4237 (3.4335)  time: 0.4965  data: 0.0004  max mem: 19734
Epoch: [13]  [ 480/1251]  eta: 0:06:29  lr: 0.000019  loss: 3.6412 (3.4367)  time: 0.5126  data: 0.0004  max mem: 19734
Epoch: [13]  [ 490/1251]  eta: 0:06:23  lr: 0.000019  loss: 3.6567 (3.4407)  time: 0.4697  data: 0.0005  max mem: 19734
Epoch: [13]  [ 500/1251]  eta: 0:06:17  lr: 0.000019  loss: 3.6156 (3.4445)  time: 0.4546  data: 0.0006  max mem: 19734
Epoch: [13]  [ 510/1251]  eta: 0:06:12  lr: 0.000019  loss: 3.7046 (3.4508)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [13]  [ 520/1251]  eta: 0:06:06  lr: 0.000019  loss: 3.7046 (3.4519)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [13]  [ 530/1251]  eta: 0:06:00  lr: 0.000019  loss: 3.2483 (3.4449)  time: 0.4560  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4417, ratio_loss=0.0071, pruning_loss=0.1432, mse_loss=0.6109
Epoch: [13]  [ 540/1251]  eta: 0:05:55  lr: 0.000019  loss: 3.0732 (3.4431)  time: 0.4613  data: 0.0004  max mem: 19734
Epoch: [13]  [ 550/1251]  eta: 0:05:49  lr: 0.000019  loss: 3.4313 (3.4405)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [13]  [ 560/1251]  eta: 0:05:44  lr: 0.000019  loss: 3.2874 (3.4398)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [13]  [ 570/1251]  eta: 0:05:38  lr: 0.000019  loss: 3.5966 (3.4435)  time: 0.4592  data: 0.0005  max mem: 19734
Epoch: [13]  [ 580/1251]  eta: 0:05:33  lr: 0.000019  loss: 3.5966 (3.4463)  time: 0.4613  data: 0.0005  max mem: 19734
Epoch: [13]  [ 590/1251]  eta: 0:05:28  lr: 0.000019  loss: 3.5612 (3.4478)  time: 0.4614  data: 0.0005  max mem: 19734
Epoch: [13]  [ 600/1251]  eta: 0:05:22  lr: 0.000019  loss: 3.5612 (3.4453)  time: 0.4603  data: 0.0004  max mem: 19734
Epoch: [13]  [ 610/1251]  eta: 0:05:17  lr: 0.000019  loss: 3.6823 (3.4480)  time: 0.4665  data: 0.0004  max mem: 19734
Epoch: [13]  [ 620/1251]  eta: 0:05:13  lr: 0.000019  loss: 3.6120 (3.4505)  time: 0.5075  data: 0.0005  max mem: 19734
Epoch: [13]  [ 630/1251]  eta: 0:05:08  lr: 0.000019  loss: 3.4583 (3.4480)  time: 0.5092  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4131, ratio_loss=0.0068, pruning_loss=0.1470, mse_loss=0.5938
Epoch: [13]  [ 640/1251]  eta: 0:05:02  lr: 0.000019  loss: 3.2646 (3.4452)  time: 0.4653  data: 0.0004  max mem: 19734
Epoch: [13]  [ 650/1251]  eta: 0:04:57  lr: 0.000019  loss: 3.2646 (3.4418)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [13]  [ 660/1251]  eta: 0:04:51  lr: 0.000019  loss: 3.5497 (3.4441)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [13]  [ 670/1251]  eta: 0:04:46  lr: 0.000019  loss: 3.5089 (3.4411)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [13]  [ 680/1251]  eta: 0:04:41  lr: 0.000019  loss: 3.4031 (3.4397)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [13]  [ 690/1251]  eta: 0:04:36  lr: 0.000019  loss: 3.6937 (3.4436)  time: 0.4640  data: 0.0006  max mem: 19734
Epoch: [13]  [ 700/1251]  eta: 0:04:31  lr: 0.000019  loss: 3.7217 (3.4438)  time: 0.4637  data: 0.0005  max mem: 19734
Epoch: [13]  [ 710/1251]  eta: 0:04:25  lr: 0.000019  loss: 3.4538 (3.4432)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [13]  [ 720/1251]  eta: 0:04:20  lr: 0.000019  loss: 3.4538 (3.4415)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [13]  [ 730/1251]  eta: 0:04:15  lr: 0.000019  loss: 3.5858 (3.4413)  time: 0.4546  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4054, ratio_loss=0.0064, pruning_loss=0.1458, mse_loss=0.6016
Epoch: [13]  [ 740/1251]  eta: 0:04:10  lr: 0.000019  loss: 3.7321 (3.4434)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [13]  [ 750/1251]  eta: 0:04:05  lr: 0.000019  loss: 3.6260 (3.4414)  time: 0.4740  data: 0.0005  max mem: 19734
Epoch: [13]  [ 760/1251]  eta: 0:04:00  lr: 0.000019  loss: 3.1445 (3.4376)  time: 0.4895  data: 0.0005  max mem: 19734
Epoch: [13]  [ 770/1251]  eta: 0:03:55  lr: 0.000019  loss: 3.1036 (3.4356)  time: 0.4977  data: 0.0004  max mem: 19734
Epoch: [13]  [ 780/1251]  eta: 0:03:50  lr: 0.000019  loss: 3.4332 (3.4361)  time: 0.4811  data: 0.0005  max mem: 19734
Epoch: [13]  [ 790/1251]  eta: 0:03:45  lr: 0.000019  loss: 3.3839 (3.4343)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [13]  [ 800/1251]  eta: 0:03:40  lr: 0.000019  loss: 3.2629 (3.4358)  time: 0.4513  data: 0.0004  max mem: 19734
Epoch: [13]  [ 810/1251]  eta: 0:03:35  lr: 0.000019  loss: 3.6433 (3.4348)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [13]  [ 820/1251]  eta: 0:03:30  lr: 0.000019  loss: 3.6528 (3.4367)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [13]  [ 830/1251]  eta: 0:03:25  lr: 0.000019  loss: 3.6702 (3.4380)  time: 0.4531  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3884, ratio_loss=0.0063, pruning_loss=0.1460, mse_loss=0.5991
Epoch: [13]  [ 840/1251]  eta: 0:03:20  lr: 0.000019  loss: 3.7354 (3.4411)  time: 0.4639  data: 0.0006  max mem: 19734
Epoch: [13]  [ 850/1251]  eta: 0:03:15  lr: 0.000019  loss: 3.6117 (3.4401)  time: 0.4653  data: 0.0008  max mem: 19734
Epoch: [13]  [ 860/1251]  eta: 0:03:10  lr: 0.000019  loss: 3.6117 (3.4428)  time: 0.4576  data: 0.0006  max mem: 19734
Epoch: [13]  [ 870/1251]  eta: 0:03:05  lr: 0.000019  loss: 3.6289 (3.4414)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [13]  [ 880/1251]  eta: 0:03:00  lr: 0.000019  loss: 3.4329 (3.4392)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [13]  [ 890/1251]  eta: 0:02:55  lr: 0.000019  loss: 3.5633 (3.4406)  time: 0.4587  data: 0.0004  max mem: 19734
Epoch: [13]  [ 900/1251]  eta: 0:02:50  lr: 0.000019  loss: 3.7711 (3.4424)  time: 0.4813  data: 0.0004  max mem: 19734
Epoch: [13]  [ 910/1251]  eta: 0:02:45  lr: 0.000019  loss: 3.7694 (3.4436)  time: 0.5130  data: 0.0004  max mem: 19734
Epoch: [13]  [ 920/1251]  eta: 0:02:40  lr: 0.000019  loss: 3.3050 (3.4404)  time: 0.5043  data: 0.0004  max mem: 19734
Epoch: [13]  [ 930/1251]  eta: 0:02:36  lr: 0.000019  loss: 2.9931 (3.4366)  time: 0.4733  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3827, ratio_loss=0.0068, pruning_loss=0.1438, mse_loss=0.5955
Epoch: [13]  [ 940/1251]  eta: 0:02:31  lr: 0.000019  loss: 3.3388 (3.4377)  time: 0.4558  data: 0.0006  max mem: 19734
Epoch: [13]  [ 950/1251]  eta: 0:02:26  lr: 0.000019  loss: 3.4608 (3.4374)  time: 0.4534  data: 0.0006  max mem: 19734
Epoch: [13]  [ 960/1251]  eta: 0:02:21  lr: 0.000019  loss: 3.4608 (3.4367)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [13]  [ 970/1251]  eta: 0:02:16  lr: 0.000019  loss: 3.2836 (3.4346)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [13]  [ 980/1251]  eta: 0:02:11  lr: 0.000019  loss: 3.5278 (3.4369)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [13]  [ 990/1251]  eta: 0:02:06  lr: 0.000019  loss: 3.5880 (3.4369)  time: 0.4629  data: 0.0004  max mem: 19734
Epoch: [13]  [1000/1251]  eta: 0:02:01  lr: 0.000019  loss: 3.5880 (3.4383)  time: 0.4637  data: 0.0004  max mem: 19734
Epoch: [13]  [1010/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.5909 (3.4380)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [13]  [1020/1251]  eta: 0:01:51  lr: 0.000019  loss: 3.6139 (3.4378)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [13]  [1030/1251]  eta: 0:01:46  lr: 0.000019  loss: 3.6125 (3.4383)  time: 0.4557  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4132, ratio_loss=0.0070, pruning_loss=0.1430, mse_loss=0.5937
Epoch: [13]  [1040/1251]  eta: 0:01:41  lr: 0.000019  loss: 3.6022 (3.4394)  time: 0.4727  data: 0.0004  max mem: 19734
Epoch: [13]  [1050/1251]  eta: 0:01:37  lr: 0.000019  loss: 3.6022 (3.4405)  time: 0.4951  data: 0.0004  max mem: 19734
Epoch: [13]  [1060/1251]  eta: 0:01:32  lr: 0.000019  loss: 3.5987 (3.4398)  time: 0.4946  data: 0.0004  max mem: 19734
Epoch: [13]  [1070/1251]  eta: 0:01:27  lr: 0.000019  loss: 3.6253 (3.4416)  time: 0.4706  data: 0.0004  max mem: 19734
Epoch: [13]  [1080/1251]  eta: 0:01:22  lr: 0.000019  loss: 3.6039 (3.4387)  time: 0.4515  data: 0.0007  max mem: 19734
Epoch: [13]  [1090/1251]  eta: 0:01:17  lr: 0.000019  loss: 3.4497 (3.4404)  time: 0.4493  data: 0.0007  max mem: 19734
Epoch: [13]  [1100/1251]  eta: 0:01:12  lr: 0.000019  loss: 3.5815 (3.4388)  time: 0.4502  data: 0.0005  max mem: 19734
Epoch: [13]  [1110/1251]  eta: 0:01:07  lr: 0.000019  loss: 3.4499 (3.4377)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [13]  [1120/1251]  eta: 0:01:03  lr: 0.000019  loss: 3.0811 (3.4345)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [13]  [1130/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.3876 (3.4354)  time: 0.4676  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3791, ratio_loss=0.0064, pruning_loss=0.1430, mse_loss=0.6278
Epoch: [13]  [1140/1251]  eta: 0:00:53  lr: 0.000019  loss: 3.5714 (3.4357)  time: 0.4692  data: 0.0005  max mem: 19734
Epoch: [13]  [1150/1251]  eta: 0:00:48  lr: 0.000019  loss: 3.4335 (3.4358)  time: 0.4580  data: 0.0005  max mem: 19734
Epoch: [13]  [1160/1251]  eta: 0:00:43  lr: 0.000019  loss: 3.6585 (3.4381)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [13]  [1170/1251]  eta: 0:00:38  lr: 0.000019  loss: 3.6585 (3.4374)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [13]  [1180/1251]  eta: 0:00:34  lr: 0.000019  loss: 3.4119 (3.4369)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [13]  [1190/1251]  eta: 0:00:29  lr: 0.000019  loss: 3.3327 (3.4349)  time: 0.4833  data: 0.0008  max mem: 19734
Epoch: [13]  [1200/1251]  eta: 0:00:24  lr: 0.000019  loss: 3.2715 (3.4333)  time: 0.4906  data: 0.0007  max mem: 19734
Epoch: [13]  [1210/1251]  eta: 0:00:19  lr: 0.000019  loss: 3.4957 (3.4331)  time: 0.4692  data: 0.0001  max mem: 19734
Epoch: [13]  [1220/1251]  eta: 0:00:14  lr: 0.000019  loss: 3.4999 (3.4330)  time: 0.4573  data: 0.0001  max mem: 19734
Epoch: [13]  [1230/1251]  eta: 0:00:10  lr: 0.000019  loss: 3.5413 (3.4334)  time: 0.4453  data: 0.0001  max mem: 19734
loss info: cls_loss=3.3689, ratio_loss=0.0063, pruning_loss=0.1445, mse_loss=0.6341
Epoch: [13]  [1240/1251]  eta: 0:00:05  lr: 0.000019  loss: 3.2875 (3.4309)  time: 0.4449  data: 0.0001  max mem: 19734
Epoch: [13]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.4824 (3.4323)  time: 0.4451  data: 0.0001  max mem: 19734
Epoch: [13] Total time: 0:10:00 (0.4802 s / it)
Averaged stats: lr: 0.000019  loss: 3.4824 (3.4464)
Test:  [  0/261]  eta: 2:03:13  loss: 0.7621 (0.7621)  acc1: 81.7708 (81.7708)  acc5: 95.3125 (95.3125)  time: 28.3265  data: 28.1312  max mem: 19734
Test:  [ 10/261]  eta: 0:11:39  loss: 0.7579 (0.7552)  acc1: 82.2917 (83.1913)  acc5: 96.3542 (95.5966)  time: 2.7866  data: 2.5724  max mem: 19734
Test:  [ 20/261]  eta: 0:06:27  loss: 0.9555 (0.9235)  acc1: 78.1250 (78.9435)  acc5: 92.7083 (94.1220)  time: 0.2700  data: 0.0170  max mem: 19734
Test:  [ 30/261]  eta: 0:04:29  loss: 0.8357 (0.8456)  acc1: 81.7708 (81.6196)  acc5: 94.7917 (94.8085)  time: 0.2737  data: 0.0181  max mem: 19734
Test:  [ 40/261]  eta: 0:04:00  loss: 0.6217 (0.8100)  acc1: 87.5000 (82.6220)  acc5: 96.8750 (95.2236)  time: 0.5427  data: 0.3437  max mem: 19734
Test:  [ 50/261]  eta: 0:03:11  loss: 0.9267 (0.8754)  acc1: 76.5625 (80.4432)  acc5: 94.2708 (94.8019)  time: 0.5042  data: 0.3428  max mem: 19734
Test:  [ 60/261]  eta: 0:02:37  loss: 0.9771 (0.8853)  acc1: 76.0417 (79.9266)  acc5: 93.7500 (94.8344)  time: 0.1627  data: 0.0149  max mem: 19734
Test:  [ 70/261]  eta: 0:02:15  loss: 0.9482 (0.8839)  acc1: 77.0833 (79.5775)  acc5: 95.8333 (95.0778)  time: 0.1998  data: 0.0552  max mem: 19734
Test:  [ 80/261]  eta: 0:02:02  loss: 0.8466 (0.8859)  acc1: 79.6875 (79.6618)  acc5: 96.8750 (95.1453)  time: 0.3557  data: 0.2177  max mem: 19734
Test:  [ 90/261]  eta: 0:01:46  loss: 0.8202 (0.8715)  acc1: 82.2917 (80.0881)  acc5: 95.8333 (95.2610)  time: 0.3192  data: 0.1815  max mem: 19734
Test:  [100/261]  eta: 0:01:44  loss: 0.8202 (0.8728)  acc1: 82.8125 (80.1207)  acc5: 95.3125 (95.3331)  time: 0.5197  data: 0.3568  max mem: 19734
Test:  [110/261]  eta: 0:01:31  loss: 0.8793 (0.8943)  acc1: 78.6458 (79.6875)  acc5: 94.2708 (95.0450)  time: 0.5351  data: 0.3516  max mem: 19734
Test:  [120/261]  eta: 0:01:20  loss: 1.1981 (0.9363)  acc1: 69.7917 (78.6760)  acc5: 89.0625 (94.5076)  time: 0.1839  data: 0.0129  max mem: 19734
Test:  [130/261]  eta: 0:01:12  loss: 1.3930 (0.9819)  acc1: 66.6667 (77.6757)  acc5: 87.5000 (93.9090)  time: 0.2609  data: 0.1085  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: 1.2830 (1.0071)  acc1: 66.6667 (77.0612)  acc5: 89.5833 (93.6466)  time: 0.3184  data: 0.1917  max mem: 19734
Test:  [150/261]  eta: 0:00:57  loss: 1.2428 (1.0128)  acc1: 72.3958 (77.0695)  acc5: 90.6250 (93.4844)  time: 0.3117  data: 0.2127  max mem: 19734
Test:  [160/261]  eta: 0:00:49  loss: 1.0436 (1.0331)  acc1: 77.0833 (76.6919)  acc5: 91.6667 (93.2001)  time: 0.2124  data: 0.1293  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: 1.3353 (1.0633)  acc1: 65.6250 (75.8985)  acc5: 85.9375 (92.8545)  time: 0.1514  data: 0.0772  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4776 (1.0816)  acc1: 64.0625 (75.4633)  acc5: 88.0208 (92.6652)  time: 0.1404  data: 0.0734  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: 1.3917 (1.0958)  acc1: 66.6667 (75.2072)  acc5: 89.0625 (92.4956)  time: 0.0652  data: 0.0020  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.3838 (1.1124)  acc1: 71.3542 (74.8756)  acc5: 89.0625 (92.2678)  time: 0.0625  data: 0.0007  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.3838 (1.1270)  acc1: 69.7917 (74.5927)  acc5: 87.5000 (92.0493)  time: 0.0618  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.4484 (1.1461)  acc1: 66.1458 (74.1327)  acc5: 87.5000 (91.8505)  time: 0.0642  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4303 (1.1564)  acc1: 65.6250 (73.9110)  acc5: 89.0625 (91.7411)  time: 0.0641  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3752 (1.1658)  acc1: 66.1458 (73.6882)  acc5: 90.1042 (91.6753)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0896 (1.1592)  acc1: 74.4792 (73.8691)  acc5: 92.7083 (91.7912)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0040 (1.1584)  acc1: 76.0417 (73.9100)  acc5: 94.7917 (91.8480)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:27 (0.3360 s / it)
* Acc@1 73.910 Acc@5 91.848 loss 1.158
Accuracy of the network on the 50000 test images: 73.9%
Max accuracy: 73.99%
Epoch: [14]  [   0/1251]  eta: 5:45:41  lr: 0.000019  loss: 3.9776 (3.9776)  time: 16.5800  data: 8.6197  max mem: 19734
Epoch: [14]  [  10/1251]  eta: 0:44:24  lr: 0.000019  loss: 3.8510 (3.6604)  time: 2.1467  data: 0.9480  max mem: 19734
Epoch: [14]  [  20/1251]  eta: 0:27:32  lr: 0.000019  loss: 3.6448 (3.5190)  time: 0.5807  data: 0.0906  max mem: 19734
Epoch: [14]  [  30/1251]  eta: 0:21:35  lr: 0.000019  loss: 3.6055 (3.5464)  time: 0.4638  data: 0.0005  max mem: 19734
Epoch: [14]  [  40/1251]  eta: 0:18:27  lr: 0.000019  loss: 3.4118 (3.4651)  time: 0.4654  data: 0.0004  max mem: 19734
Epoch: [14]  [  50/1251]  eta: 0:16:31  lr: 0.000019  loss: 3.3408 (3.4559)  time: 0.4617  data: 0.0004  max mem: 19734
Epoch: [14]  [  60/1251]  eta: 0:15:12  lr: 0.000019  loss: 3.7766 (3.5020)  time: 0.4607  data: 0.0005  max mem: 19734
Epoch: [14]  [  70/1251]  eta: 0:14:16  lr: 0.000019  loss: 3.7686 (3.4879)  time: 0.4686  data: 0.0004  max mem: 19734
Epoch: [14]  [  80/1251]  eta: 0:13:36  lr: 0.000019  loss: 3.6179 (3.4940)  time: 0.4891  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4509, ratio_loss=0.0063, pruning_loss=0.1433, mse_loss=0.6190
Epoch: [14]  [  90/1251]  eta: 0:13:05  lr: 0.000019  loss: 3.6178 (3.4994)  time: 0.5052  data: 0.0004  max mem: 19734
Epoch: [14]  [ 100/1251]  eta: 0:12:34  lr: 0.000019  loss: 3.5843 (3.5036)  time: 0.4844  data: 0.0004  max mem: 19734
Epoch: [14]  [ 110/1251]  eta: 0:12:07  lr: 0.000019  loss: 3.6276 (3.4967)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [14]  [ 120/1251]  eta: 0:11:43  lr: 0.000019  loss: 3.5899 (3.4764)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [14]  [ 130/1251]  eta: 0:11:23  lr: 0.000019  loss: 3.6009 (3.4869)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [14]  [ 140/1251]  eta: 0:11:05  lr: 0.000019  loss: 3.6017 (3.4822)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [14]  [ 150/1251]  eta: 0:10:50  lr: 0.000019  loss: 3.5408 (3.4719)  time: 0.4677  data: 0.0005  max mem: 19734
Epoch: [14]  [ 160/1251]  eta: 0:10:35  lr: 0.000019  loss: 3.6725 (3.4918)  time: 0.4653  data: 0.0006  max mem: 19734
Epoch: [14]  [ 170/1251]  eta: 0:10:21  lr: 0.000019  loss: 3.7507 (3.4677)  time: 0.4550  data: 0.0006  max mem: 19734
Epoch: [14]  [ 180/1251]  eta: 0:10:08  lr: 0.000019  loss: 2.7224 (3.4342)  time: 0.4577  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3670, ratio_loss=0.0065, pruning_loss=0.1443, mse_loss=0.6062
Epoch: [14]  [ 190/1251]  eta: 0:09:56  lr: 0.000019  loss: 3.3407 (3.4527)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [14]  [ 200/1251]  eta: 0:09:45  lr: 0.000019  loss: 3.7131 (3.4483)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [14]  [ 210/1251]  eta: 0:09:35  lr: 0.000019  loss: 3.2874 (3.4410)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [14]  [ 220/1251]  eta: 0:09:28  lr: 0.000019  loss: 3.1978 (3.4338)  time: 0.4867  data: 0.0004  max mem: 19734
Epoch: [14]  [ 230/1251]  eta: 0:09:20  lr: 0.000019  loss: 3.6265 (3.4404)  time: 0.5134  data: 0.0004  max mem: 19734
Epoch: [14]  [ 240/1251]  eta: 0:09:12  lr: 0.000019  loss: 3.7077 (3.4510)  time: 0.4927  data: 0.0004  max mem: 19734
Epoch: [14]  [ 250/1251]  eta: 0:09:03  lr: 0.000019  loss: 3.6711 (3.4520)  time: 0.4651  data: 0.0004  max mem: 19734
Epoch: [14]  [ 260/1251]  eta: 0:08:54  lr: 0.000019  loss: 3.4257 (3.4454)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [14]  [ 270/1251]  eta: 0:08:46  lr: 0.000019  loss: 3.3404 (3.4388)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [14]  [ 280/1251]  eta: 0:08:38  lr: 0.000019  loss: 3.5021 (3.4414)  time: 0.4590  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4014, ratio_loss=0.0062, pruning_loss=0.1426, mse_loss=0.6039
Epoch: [14]  [ 290/1251]  eta: 0:08:30  lr: 0.000019  loss: 3.5118 (3.4413)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [14]  [ 300/1251]  eta: 0:08:23  lr: 0.000019  loss: 3.5118 (3.4413)  time: 0.4664  data: 0.0005  max mem: 19734
Epoch: [14]  [ 310/1251]  eta: 0:08:15  lr: 0.000019  loss: 3.3187 (3.4347)  time: 0.4680  data: 0.0007  max mem: 19734
Epoch: [14]  [ 320/1251]  eta: 0:08:08  lr: 0.000019  loss: 3.2291 (3.4301)  time: 0.4591  data: 0.0006  max mem: 19734
Epoch: [14]  [ 330/1251]  eta: 0:08:01  lr: 0.000019  loss: 3.5386 (3.4327)  time: 0.4575  data: 0.0005  max mem: 19734
Epoch: [14]  [ 340/1251]  eta: 0:07:54  lr: 0.000019  loss: 3.3590 (3.4263)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [14]  [ 350/1251]  eta: 0:07:47  lr: 0.000019  loss: 3.3113 (3.4251)  time: 0.4573  data: 0.0004  max mem: 19734
Epoch: [14]  [ 360/1251]  eta: 0:07:41  lr: 0.000019  loss: 3.6489 (3.4285)  time: 0.4584  data: 0.0004  max mem: 19734
Epoch: [14]  [ 370/1251]  eta: 0:07:35  lr: 0.000019  loss: 3.6570 (3.4242)  time: 0.4856  data: 0.0005  max mem: 19734
Epoch: [14]  [ 380/1251]  eta: 0:07:30  lr: 0.000019  loss: 3.6091 (3.4277)  time: 0.5187  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3595, ratio_loss=0.0063, pruning_loss=0.1447, mse_loss=0.6117
Epoch: [14]  [ 390/1251]  eta: 0:07:24  lr: 0.000019  loss: 3.5807 (3.4291)  time: 0.4910  data: 0.0004  max mem: 19734
Epoch: [14]  [ 400/1251]  eta: 0:07:17  lr: 0.000019  loss: 3.5619 (3.4325)  time: 0.4561  data: 0.0004  max mem: 19734
Epoch: [14]  [ 410/1251]  eta: 0:07:11  lr: 0.000019  loss: 3.5902 (3.4365)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [14]  [ 420/1251]  eta: 0:07:05  lr: 0.000019  loss: 3.5732 (3.4326)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [14]  [ 430/1251]  eta: 0:06:58  lr: 0.000019  loss: 3.5119 (3.4349)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [14]  [ 440/1251]  eta: 0:06:52  lr: 0.000019  loss: 3.4412 (3.4293)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [14]  [ 450/1251]  eta: 0:06:47  lr: 0.000019  loss: 3.1994 (3.4300)  time: 0.4679  data: 0.0005  max mem: 19734
Epoch: [14]  [ 460/1251]  eta: 0:06:41  lr: 0.000019  loss: 3.7455 (3.4352)  time: 0.4671  data: 0.0004  max mem: 19734
Epoch: [14]  [ 470/1251]  eta: 0:06:35  lr: 0.000019  loss: 3.4938 (3.4282)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [14]  [ 480/1251]  eta: 0:06:29  lr: 0.000019  loss: 3.0921 (3.4247)  time: 0.4559  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3933, ratio_loss=0.0062, pruning_loss=0.1443, mse_loss=0.6000
Epoch: [14]  [ 490/1251]  eta: 0:06:23  lr: 0.000019  loss: 3.6545 (3.4273)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [14]  [ 500/1251]  eta: 0:06:17  lr: 0.000019  loss: 3.6545 (3.4285)  time: 0.4569  data: 0.0005  max mem: 19734
Epoch: [14]  [ 510/1251]  eta: 0:06:12  lr: 0.000019  loss: 3.5777 (3.4312)  time: 0.4842  data: 0.0004  max mem: 19734
Epoch: [14]  [ 520/1251]  eta: 0:06:07  lr: 0.000019  loss: 3.5051 (3.4300)  time: 0.5021  data: 0.0005  max mem: 19734
Epoch: [14]  [ 530/1251]  eta: 0:06:02  lr: 0.000019  loss: 3.5304 (3.4305)  time: 0.4865  data: 0.0004  max mem: 19734
Epoch: [14]  [ 540/1251]  eta: 0:05:56  lr: 0.000019  loss: 3.5304 (3.4257)  time: 0.4678  data: 0.0004  max mem: 19734
Epoch: [14]  [ 550/1251]  eta: 0:05:51  lr: 0.000019  loss: 3.5599 (3.4261)  time: 0.4577  data: 0.0006  max mem: 19734
Epoch: [14]  [ 560/1251]  eta: 0:05:45  lr: 0.000019  loss: 3.6026 (3.4290)  time: 0.4584  data: 0.0007  max mem: 19734
Epoch: [14]  [ 570/1251]  eta: 0:05:40  lr: 0.000019  loss: 3.5964 (3.4303)  time: 0.4602  data: 0.0005  max mem: 19734
Epoch: [14]  [ 580/1251]  eta: 0:05:34  lr: 0.000019  loss: 3.6264 (3.4357)  time: 0.4598  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4733, ratio_loss=0.0065, pruning_loss=0.1425, mse_loss=0.6045
Epoch: [14]  [ 590/1251]  eta: 0:05:29  lr: 0.000019  loss: 3.8733 (3.4398)  time: 0.4561  data: 0.0004  max mem: 19734
Epoch: [14]  [ 600/1251]  eta: 0:05:24  lr: 0.000019  loss: 3.7741 (3.4419)  time: 0.4653  data: 0.0004  max mem: 19734
Epoch: [14]  [ 610/1251]  eta: 0:05:18  lr: 0.000019  loss: 3.5804 (3.4403)  time: 0.4662  data: 0.0004  max mem: 19734
Epoch: [14]  [ 620/1251]  eta: 0:05:13  lr: 0.000019  loss: 3.3177 (3.4368)  time: 0.4561  data: 0.0004  max mem: 19734
Epoch: [14]  [ 630/1251]  eta: 0:05:07  lr: 0.000019  loss: 3.4753 (3.4380)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [14]  [ 640/1251]  eta: 0:05:02  lr: 0.000019  loss: 3.5410 (3.4365)  time: 0.4514  data: 0.0005  max mem: 19734
Epoch: [14]  [ 650/1251]  eta: 0:04:57  lr: 0.000019  loss: 3.3615 (3.4337)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [14]  [ 660/1251]  eta: 0:04:52  lr: 0.000019  loss: 3.6428 (3.4376)  time: 0.4894  data: 0.0004  max mem: 19734
Epoch: [14]  [ 670/1251]  eta: 0:04:47  lr: 0.000019  loss: 3.6440 (3.4375)  time: 0.5169  data: 0.0004  max mem: 19734
Epoch: [14]  [ 680/1251]  eta: 0:04:42  lr: 0.000019  loss: 3.4646 (3.4382)  time: 0.4806  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4129, ratio_loss=0.0063, pruning_loss=0.1411, mse_loss=0.5975
Epoch: [14]  [ 690/1251]  eta: 0:04:37  lr: 0.000019  loss: 3.5942 (3.4395)  time: 0.4543  data: 0.0005  max mem: 19734
Epoch: [14]  [ 700/1251]  eta: 0:04:31  lr: 0.000019  loss: 3.5451 (3.4409)  time: 0.4544  data: 0.0006  max mem: 19734
Epoch: [14]  [ 710/1251]  eta: 0:04:26  lr: 0.000019  loss: 3.4736 (3.4410)  time: 0.4555  data: 0.0005  max mem: 19734
Epoch: [14]  [ 720/1251]  eta: 0:04:21  lr: 0.000019  loss: 3.6455 (3.4430)  time: 0.4586  data: 0.0004  max mem: 19734
Epoch: [14]  [ 730/1251]  eta: 0:04:16  lr: 0.000019  loss: 3.7221 (3.4440)  time: 0.4594  data: 0.0004  max mem: 19734
Epoch: [14]  [ 740/1251]  eta: 0:04:11  lr: 0.000019  loss: 3.6813 (3.4465)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [14]  [ 750/1251]  eta: 0:04:06  lr: 0.000019  loss: 3.6711 (3.4502)  time: 0.4689  data: 0.0004  max mem: 19734
Epoch: [14]  [ 760/1251]  eta: 0:04:00  lr: 0.000019  loss: 3.7617 (3.4534)  time: 0.4672  data: 0.0005  max mem: 19734
Epoch: [14]  [ 770/1251]  eta: 0:03:55  lr: 0.000019  loss: 3.7092 (3.4550)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [14]  [ 780/1251]  eta: 0:03:50  lr: 0.000019  loss: 3.4743 (3.4518)  time: 0.4536  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5203, ratio_loss=0.0064, pruning_loss=0.1407, mse_loss=0.5806
Epoch: [14]  [ 790/1251]  eta: 0:03:45  lr: 0.000019  loss: 3.3575 (3.4541)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [14]  [ 800/1251]  eta: 0:03:40  lr: 0.000019  loss: 3.4049 (3.4521)  time: 0.4786  data: 0.0004  max mem: 19734
Epoch: [14]  [ 810/1251]  eta: 0:03:36  lr: 0.000019  loss: 3.5136 (3.4552)  time: 0.5076  data: 0.0004  max mem: 19734
Epoch: [14]  [ 820/1251]  eta: 0:03:31  lr: 0.000019  loss: 3.6226 (3.4543)  time: 0.4935  data: 0.0004  max mem: 19734
Epoch: [14]  [ 830/1251]  eta: 0:03:25  lr: 0.000019  loss: 3.4522 (3.4537)  time: 0.4660  data: 0.0004  max mem: 19734
Epoch: [14]  [ 840/1251]  eta: 0:03:20  lr: 0.000019  loss: 3.2934 (3.4512)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [14]  [ 850/1251]  eta: 0:03:15  lr: 0.000019  loss: 3.4068 (3.4503)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [14]  [ 860/1251]  eta: 0:03:10  lr: 0.000019  loss: 3.4486 (3.4512)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [14]  [ 870/1251]  eta: 0:03:05  lr: 0.000019  loss: 3.6419 (3.4530)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [14]  [ 880/1251]  eta: 0:03:00  lr: 0.000019  loss: 3.7704 (3.4525)  time: 0.4567  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4309, ratio_loss=0.0066, pruning_loss=0.1419, mse_loss=0.5838
Epoch: [14]  [ 890/1251]  eta: 0:02:55  lr: 0.000019  loss: 3.7463 (3.4551)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [14]  [ 900/1251]  eta: 0:02:50  lr: 0.000019  loss: 3.7463 (3.4571)  time: 0.4634  data: 0.0004  max mem: 19734
Epoch: [14]  [ 910/1251]  eta: 0:02:45  lr: 0.000019  loss: 3.6225 (3.4575)  time: 0.4642  data: 0.0004  max mem: 19734
Epoch: [14]  [ 920/1251]  eta: 0:02:40  lr: 0.000019  loss: 3.4684 (3.4559)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [14]  [ 930/1251]  eta: 0:02:35  lr: 0.000019  loss: 3.4342 (3.4545)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [14]  [ 940/1251]  eta: 0:02:31  lr: 0.000019  loss: 3.2447 (3.4524)  time: 0.4704  data: 0.0004  max mem: 19734
Epoch: [14]  [ 950/1251]  eta: 0:02:26  lr: 0.000019  loss: 3.2447 (3.4520)  time: 0.4957  data: 0.0004  max mem: 19734
Epoch: [14]  [ 960/1251]  eta: 0:02:21  lr: 0.000019  loss: 3.5973 (3.4546)  time: 0.4969  data: 0.0005  max mem: 19734
Epoch: [14]  [ 970/1251]  eta: 0:02:16  lr: 0.000019  loss: 3.7309 (3.4549)  time: 0.4722  data: 0.0005  max mem: 19734
Epoch: [14]  [ 980/1251]  eta: 0:02:11  lr: 0.000019  loss: 3.5467 (3.4569)  time: 0.4540  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4544, ratio_loss=0.0064, pruning_loss=0.1426, mse_loss=0.5585
Epoch: [14]  [ 990/1251]  eta: 0:02:06  lr: 0.000019  loss: 3.6579 (3.4578)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [14]  [1000/1251]  eta: 0:02:01  lr: 0.000019  loss: 3.6693 (3.4582)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [14]  [1010/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.6268 (3.4589)  time: 0.4519  data: 0.0005  max mem: 19734
Epoch: [14]  [1020/1251]  eta: 0:01:51  lr: 0.000019  loss: 3.3927 (3.4585)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [14]  [1030/1251]  eta: 0:01:46  lr: 0.000019  loss: 3.0988 (3.4530)  time: 0.4516  data: 0.0005  max mem: 19734
Epoch: [14]  [1040/1251]  eta: 0:01:42  lr: 0.000019  loss: 3.2341 (3.4551)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [14]  [1050/1251]  eta: 0:01:37  lr: 0.000019  loss: 3.6642 (3.4537)  time: 0.4600  data: 0.0005  max mem: 19734
Epoch: [14]  [1060/1251]  eta: 0:01:32  lr: 0.000019  loss: 3.5337 (3.4533)  time: 0.4612  data: 0.0005  max mem: 19734
Epoch: [14]  [1070/1251]  eta: 0:01:27  lr: 0.000019  loss: 3.5556 (3.4534)  time: 0.4545  data: 0.0005  max mem: 19734
Epoch: [14]  [1080/1251]  eta: 0:01:22  lr: 0.000019  loss: 3.5463 (3.4527)  time: 0.4536  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3726, ratio_loss=0.0063, pruning_loss=0.1424, mse_loss=0.5844
Epoch: [14]  [1090/1251]  eta: 0:01:17  lr: 0.000019  loss: 3.4549 (3.4530)  time: 0.4681  data: 0.0005  max mem: 19734
Epoch: [14]  [1100/1251]  eta: 0:01:12  lr: 0.000019  loss: 3.5784 (3.4528)  time: 0.5002  data: 0.0004  max mem: 19734
Epoch: [14]  [1110/1251]  eta: 0:01:08  lr: 0.000019  loss: 3.4556 (3.4502)  time: 0.4979  data: 0.0004  max mem: 19734
Epoch: [14]  [1120/1251]  eta: 0:01:03  lr: 0.000019  loss: 3.2798 (3.4483)  time: 0.4664  data: 0.0004  max mem: 19734
Epoch: [14]  [1130/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.3939 (3.4505)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [14]  [1140/1251]  eta: 0:00:53  lr: 0.000019  loss: 3.6614 (3.4507)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [14]  [1150/1251]  eta: 0:00:48  lr: 0.000019  loss: 3.5820 (3.4507)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [14]  [1160/1251]  eta: 0:00:43  lr: 0.000019  loss: 3.4921 (3.4499)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [14]  [1170/1251]  eta: 0:00:38  lr: 0.000019  loss: 3.5282 (3.4503)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [14]  [1180/1251]  eta: 0:00:34  lr: 0.000019  loss: 3.6679 (3.4525)  time: 0.4555  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4155, ratio_loss=0.0062, pruning_loss=0.1427, mse_loss=0.5745
Epoch: [14]  [1190/1251]  eta: 0:00:29  lr: 0.000019  loss: 3.6679 (3.4514)  time: 0.4613  data: 0.0007  max mem: 19734
Epoch: [14]  [1200/1251]  eta: 0:00:24  lr: 0.000019  loss: 3.2935 (3.4489)  time: 0.4553  data: 0.0006  max mem: 19734
Epoch: [14]  [1210/1251]  eta: 0:00:19  lr: 0.000019  loss: 3.2935 (3.4482)  time: 0.4428  data: 0.0002  max mem: 19734
Epoch: [14]  [1220/1251]  eta: 0:00:14  lr: 0.000019  loss: 3.2492 (3.4465)  time: 0.4430  data: 0.0002  max mem: 19734
Epoch: [14]  [1230/1251]  eta: 0:00:10  lr: 0.000019  loss: 3.2492 (3.4465)  time: 0.4429  data: 0.0002  max mem: 19734
Epoch: [14]  [1240/1251]  eta: 0:00:05  lr: 0.000019  loss: 3.3992 (3.4454)  time: 0.4752  data: 0.0002  max mem: 19734
Epoch: [14]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.3914 (3.4450)  time: 0.4955  data: 0.0002  max mem: 19734
Epoch: [14] Total time: 0:10:01 (0.4808 s / it)
Averaged stats: lr: 0.000019  loss: 3.3914 (3.4514)
Test:  [  0/261]  eta: 0:41:07  loss: 0.7125 (0.7125)  acc1: 83.8542 (83.8542)  acc5: 95.8333 (95.8333)  time: 9.4534  data: 9.2988  max mem: 19734
Test:  [ 10/261]  eta: 0:09:20  loss: 0.7125 (0.7492)  acc1: 83.8542 (83.2386)  acc5: 96.8750 (96.4015)  time: 2.2319  data: 2.0508  max mem: 19734
Test:  [ 20/261]  eta: 0:05:00  loss: 0.9910 (0.9130)  acc1: 77.0833 (78.9931)  acc5: 93.7500 (94.7917)  time: 0.8348  data: 0.6722  max mem: 19734
Test:  [ 30/261]  eta: 0:03:34  loss: 0.8260 (0.8333)  acc1: 81.7708 (81.9220)  acc5: 93.7500 (95.2453)  time: 0.2097  data: 0.0164  max mem: 19734
Test:  [ 40/261]  eta: 0:02:52  loss: 0.5899 (0.8019)  acc1: 88.0208 (82.7617)  acc5: 96.3542 (95.5412)  time: 0.2894  data: 0.0386  max mem: 19734
Test:  [ 50/261]  eta: 0:02:19  loss: 0.9172 (0.8644)  acc1: 77.6042 (80.9641)  acc5: 94.2708 (95.0266)  time: 0.2481  data: 0.0418  max mem: 19734
Test:  [ 60/261]  eta: 0:01:54  loss: 0.9929 (0.8766)  acc1: 75.5208 (80.4559)  acc5: 94.2708 (95.0820)  time: 0.1466  data: 0.0155  max mem: 19734
Test:  [ 70/261]  eta: 0:01:52  loss: 0.9570 (0.8774)  acc1: 76.0417 (79.9956)  acc5: 95.8333 (95.2758)  time: 0.4080  data: 0.2979  max mem: 19734
Test:  [ 80/261]  eta: 0:01:38  loss: 0.8848 (0.8798)  acc1: 76.5625 (80.0540)  acc5: 96.8750 (95.3832)  time: 0.4676  data: 0.2999  max mem: 19734
Test:  [ 90/261]  eta: 0:01:27  loss: 0.8354 (0.8676)  acc1: 83.8542 (80.4659)  acc5: 96.3542 (95.4556)  time: 0.2348  data: 0.0130  max mem: 19734
Test:  [100/261]  eta: 0:01:26  loss: 0.8354 (0.8701)  acc1: 84.3750 (80.4507)  acc5: 95.8333 (95.5085)  time: 0.5023  data: 0.2570  max mem: 19734
Test:  [110/261]  eta: 0:01:16  loss: 0.9158 (0.8936)  acc1: 77.0833 (79.9456)  acc5: 94.2708 (95.2093)  time: 0.5002  data: 0.2810  max mem: 19734
Test:  [120/261]  eta: 0:01:07  loss: 1.2370 (0.9359)  acc1: 70.3125 (78.9687)  acc5: 89.5833 (94.6668)  time: 0.2006  data: 0.0351  max mem: 19734
Test:  [130/261]  eta: 0:01:03  loss: 1.4723 (0.9834)  acc1: 66.6667 (77.9540)  acc5: 86.4583 (94.0482)  time: 0.3522  data: 0.1876  max mem: 19734
Test:  [140/261]  eta: 0:00:57  loss: 1.3834 (1.0099)  acc1: 69.2708 (77.3160)  acc5: 90.1042 (93.7315)  time: 0.4571  data: 0.2705  max mem: 19734
Test:  [150/261]  eta: 0:00:51  loss: 1.2201 (1.0158)  acc1: 72.3958 (77.3179)  acc5: 90.6250 (93.5430)  time: 0.3366  data: 0.1569  max mem: 19734
Test:  [160/261]  eta: 0:00:45  loss: 1.0139 (1.0355)  acc1: 78.1250 (76.9798)  acc5: 91.1458 (93.2292)  time: 0.2361  data: 0.0802  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: 1.3602 (1.0671)  acc1: 66.1458 (76.1909)  acc5: 86.9792 (92.8454)  time: 0.4658  data: 0.3022  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4880 (1.0847)  acc1: 65.1042 (75.7597)  acc5: 86.9792 (92.6681)  time: 0.4622  data: 0.3005  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: 1.4019 (1.0980)  acc1: 66.6667 (75.5072)  acc5: 90.6250 (92.5011)  time: 0.2305  data: 0.0852  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3427 (1.1136)  acc1: 71.3542 (75.2125)  acc5: 88.5417 (92.2704)  time: 0.2757  data: 0.1394  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3683 (1.1277)  acc1: 71.3542 (74.9729)  acc5: 88.0208 (92.0320)  time: 0.2215  data: 0.1233  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4711 (1.1465)  acc1: 68.2292 (74.4862)  acc5: 86.9792 (91.8222)  time: 0.1209  data: 0.0594  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4199 (1.1559)  acc1: 66.6667 (74.2537)  acc5: 88.5417 (91.7095)  time: 0.0616  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3680 (1.1667)  acc1: 67.7083 (73.9886)  acc5: 90.6250 (91.6494)  time: 0.0617  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.1153 (1.1598)  acc1: 76.0417 (74.1658)  acc5: 92.7083 (91.7642)  time: 0.0627  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9805 (1.1600)  acc1: 76.5625 (74.1640)  acc5: 94.7917 (91.8220)  time: 0.0610  data: 0.0001  max mem: 19734
Test: Total time: 0:01:32 (0.3525 s / it)
* Acc@1 74.164 Acc@5 91.822 loss 1.160
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.16%
Epoch: [15]  [   0/1251]  eta: 4:03:24  lr: 0.000019  loss: 3.5110 (3.5110)  time: 11.6745  data: 11.1677  max mem: 19734
Epoch: [15]  [  10/1251]  eta: 0:32:28  lr: 0.000019  loss: 3.6366 (3.6187)  time: 1.5699  data: 1.0811  max mem: 19734
Epoch: [15]  [  20/1251]  eta: 0:21:24  lr: 0.000019  loss: 3.6366 (3.6155)  time: 0.5118  data: 0.0364  max mem: 19734
Epoch: [15]  [  30/1251]  eta: 0:17:25  lr: 0.000019  loss: 3.5704 (3.5683)  time: 0.4637  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3678, ratio_loss=0.0060, pruning_loss=0.1433, mse_loss=0.6090
Epoch: [15]  [  40/1251]  eta: 0:15:25  lr: 0.000019  loss: 3.4951 (3.5219)  time: 0.4707  data: 0.0004  max mem: 19734
Epoch: [15]  [  50/1251]  eta: 0:14:05  lr: 0.000019  loss: 3.4951 (3.5393)  time: 0.4682  data: 0.0005  max mem: 19734
Epoch: [15]  [  60/1251]  eta: 0:13:10  lr: 0.000019  loss: 3.3627 (3.5021)  time: 0.4581  data: 0.0005  max mem: 19734
Epoch: [15]  [  70/1251]  eta: 0:12:29  lr: 0.000019  loss: 3.2490 (3.4659)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [15]  [  80/1251]  eta: 0:11:57  lr: 0.000019  loss: 3.4872 (3.4557)  time: 0.4578  data: 0.0005  max mem: 19734
Epoch: [15]  [  90/1251]  eta: 0:11:32  lr: 0.000019  loss: 3.3730 (3.4362)  time: 0.4588  data: 0.0009  max mem: 19734
Epoch: [15]  [ 100/1251]  eta: 0:11:10  lr: 0.000019  loss: 3.5816 (3.4672)  time: 0.4597  data: 0.0008  max mem: 19734
Epoch: [15]  [ 110/1251]  eta: 0:10:52  lr: 0.000019  loss: 3.6539 (3.4564)  time: 0.4613  data: 0.0008  max mem: 19734
Epoch: [15]  [ 120/1251]  eta: 0:10:44  lr: 0.000019  loss: 3.5810 (3.4718)  time: 0.5038  data: 0.0009  max mem: 19734
Epoch: [15]  [ 130/1251]  eta: 0:10:34  lr: 0.000019  loss: 3.7184 (3.4984)  time: 0.5329  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4383, ratio_loss=0.0061, pruning_loss=0.1418, mse_loss=0.5986
Epoch: [15]  [ 140/1251]  eta: 0:10:20  lr: 0.000019  loss: 3.7295 (3.5016)  time: 0.4924  data: 0.0005  max mem: 19734
Epoch: [15]  [ 150/1251]  eta: 0:10:07  lr: 0.000019  loss: 3.4452 (3.4900)  time: 0.4589  data: 0.0005  max mem: 19734
Epoch: [15]  [ 160/1251]  eta: 0:09:55  lr: 0.000019  loss: 3.4452 (3.4887)  time: 0.4575  data: 0.0005  max mem: 19734
Epoch: [15]  [ 170/1251]  eta: 0:09:44  lr: 0.000019  loss: 3.5791 (3.4823)  time: 0.4584  data: 0.0004  max mem: 19734
Epoch: [15]  [ 180/1251]  eta: 0:09:35  lr: 0.000019  loss: 3.4149 (3.4686)  time: 0.4672  data: 0.0004  max mem: 19734
Epoch: [15]  [ 190/1251]  eta: 0:09:25  lr: 0.000019  loss: 3.2982 (3.4512)  time: 0.4677  data: 0.0004  max mem: 19734
Epoch: [15]  [ 200/1251]  eta: 0:09:16  lr: 0.000019  loss: 3.4776 (3.4533)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [15]  [ 210/1251]  eta: 0:09:07  lr: 0.000019  loss: 3.7427 (3.4679)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [15]  [ 220/1251]  eta: 0:08:59  lr: 0.000019  loss: 3.7467 (3.4591)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [15]  [ 230/1251]  eta: 0:08:51  lr: 0.000019  loss: 3.4882 (3.4552)  time: 0.4595  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3629, ratio_loss=0.0064, pruning_loss=0.1414, mse_loss=0.5838
Epoch: [15]  [ 240/1251]  eta: 0:08:43  lr: 0.000019  loss: 3.5036 (3.4535)  time: 0.4592  data: 0.0004  max mem: 19734
Epoch: [15]  [ 250/1251]  eta: 0:08:35  lr: 0.000019  loss: 3.5036 (3.4535)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [15]  [ 260/1251]  eta: 0:08:30  lr: 0.000019  loss: 3.5120 (3.4505)  time: 0.4795  data: 0.0006  max mem: 19734
Epoch: [15]  [ 270/1251]  eta: 0:08:25  lr: 0.000019  loss: 3.4990 (3.4460)  time: 0.5141  data: 0.0009  max mem: 19734
Epoch: [15]  [ 280/1251]  eta: 0:08:18  lr: 0.000019  loss: 3.2215 (3.4389)  time: 0.4985  data: 0.0008  max mem: 19734
Epoch: [15]  [ 290/1251]  eta: 0:08:11  lr: 0.000019  loss: 3.6010 (3.4420)  time: 0.4645  data: 0.0004  max mem: 19734
Epoch: [15]  [ 300/1251]  eta: 0:08:04  lr: 0.000019  loss: 3.6687 (3.4466)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [15]  [ 310/1251]  eta: 0:07:58  lr: 0.000019  loss: 3.5063 (3.4473)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [15]  [ 320/1251]  eta: 0:07:51  lr: 0.000019  loss: 3.6331 (3.4540)  time: 0.4569  data: 0.0005  max mem: 19734
Epoch: [15]  [ 330/1251]  eta: 0:07:45  lr: 0.000019  loss: 3.6331 (3.4527)  time: 0.4646  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4474, ratio_loss=0.0063, pruning_loss=0.1401, mse_loss=0.5855
Epoch: [15]  [ 340/1251]  eta: 0:07:39  lr: 0.000019  loss: 3.6247 (3.4635)  time: 0.4638  data: 0.0005  max mem: 19734
Epoch: [15]  [ 350/1251]  eta: 0:07:32  lr: 0.000019  loss: 3.7577 (3.4627)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [15]  [ 360/1251]  eta: 0:07:26  lr: 0.000019  loss: 3.4841 (3.4609)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [15]  [ 370/1251]  eta: 0:07:20  lr: 0.000019  loss: 3.3485 (3.4496)  time: 0.4574  data: 0.0007  max mem: 19734
Epoch: [15]  [ 380/1251]  eta: 0:07:14  lr: 0.000019  loss: 3.4321 (3.4517)  time: 0.4565  data: 0.0007  max mem: 19734
Epoch: [15]  [ 390/1251]  eta: 0:07:08  lr: 0.000019  loss: 3.5972 (3.4488)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [15]  [ 400/1251]  eta: 0:07:02  lr: 0.000019  loss: 3.5561 (3.4446)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [15]  [ 410/1251]  eta: 0:06:57  lr: 0.000019  loss: 3.7660 (3.4546)  time: 0.4808  data: 0.0004  max mem: 19734
Epoch: [15]  [ 420/1251]  eta: 0:06:53  lr: 0.000019  loss: 3.7985 (3.4558)  time: 0.5041  data: 0.0005  max mem: 19734
Epoch: [15]  [ 430/1251]  eta: 0:06:47  lr: 0.000019  loss: 3.5489 (3.4557)  time: 0.4799  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4201, ratio_loss=0.0062, pruning_loss=0.1431, mse_loss=0.5916
Epoch: [15]  [ 440/1251]  eta: 0:06:41  lr: 0.000019  loss: 3.5433 (3.4554)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [15]  [ 450/1251]  eta: 0:06:36  lr: 0.000019  loss: 3.5433 (3.4603)  time: 0.4555  data: 0.0005  max mem: 19734
Epoch: [15]  [ 460/1251]  eta: 0:06:30  lr: 0.000019  loss: 3.5885 (3.4646)  time: 0.4586  data: 0.0004  max mem: 19734
Epoch: [15]  [ 470/1251]  eta: 0:06:24  lr: 0.000019  loss: 3.5538 (3.4670)  time: 0.4593  data: 0.0004  max mem: 19734
Epoch: [15]  [ 480/1251]  eta: 0:06:19  lr: 0.000019  loss: 3.4957 (3.4627)  time: 0.4651  data: 0.0004  max mem: 19734
Epoch: [15]  [ 490/1251]  eta: 0:06:14  lr: 0.000019  loss: 3.3906 (3.4590)  time: 0.4635  data: 0.0004  max mem: 19734
Epoch: [15]  [ 500/1251]  eta: 0:06:08  lr: 0.000019  loss: 3.5220 (3.4567)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [15]  [ 510/1251]  eta: 0:06:03  lr: 0.000019  loss: 3.5220 (3.4524)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [15]  [ 520/1251]  eta: 0:05:57  lr: 0.000019  loss: 3.5835 (3.4496)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [15]  [ 530/1251]  eta: 0:05:52  lr: 0.000019  loss: 3.6026 (3.4506)  time: 0.4573  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3979, ratio_loss=0.0062, pruning_loss=0.1414, mse_loss=0.5751
Epoch: [15]  [ 540/1251]  eta: 0:05:47  lr: 0.000019  loss: 3.5171 (3.4517)  time: 0.4581  data: 0.0005  max mem: 19734
Epoch: [15]  [ 550/1251]  eta: 0:05:42  lr: 0.000019  loss: 3.4178 (3.4506)  time: 0.4692  data: 0.0005  max mem: 19734
Epoch: [15]  [ 560/1251]  eta: 0:05:38  lr: 0.000019  loss: 3.4178 (3.4480)  time: 0.5142  data: 0.0004  max mem: 19734
Epoch: [15]  [ 570/1251]  eta: 0:05:33  lr: 0.000019  loss: 3.4263 (3.4459)  time: 0.5096  data: 0.0004  max mem: 19734
Epoch: [15]  [ 580/1251]  eta: 0:05:27  lr: 0.000019  loss: 3.5715 (3.4506)  time: 0.4637  data: 0.0004  max mem: 19734
Epoch: [15]  [ 590/1251]  eta: 0:05:22  lr: 0.000019  loss: 3.6478 (3.4495)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [15]  [ 600/1251]  eta: 0:05:17  lr: 0.000019  loss: 3.6387 (3.4520)  time: 0.4520  data: 0.0005  max mem: 19734
Epoch: [15]  [ 610/1251]  eta: 0:05:11  lr: 0.000019  loss: 3.5537 (3.4490)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [15]  [ 620/1251]  eta: 0:05:06  lr: 0.000019  loss: 3.1097 (3.4442)  time: 0.4627  data: 0.0005  max mem: 19734
Epoch: [15]  [ 630/1251]  eta: 0:05:01  lr: 0.000019  loss: 3.0936 (3.4381)  time: 0.4621  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3435, ratio_loss=0.0061, pruning_loss=0.1447, mse_loss=0.5845
Epoch: [15]  [ 640/1251]  eta: 0:04:56  lr: 0.000019  loss: 3.3445 (3.4399)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [15]  [ 650/1251]  eta: 0:04:51  lr: 0.000019  loss: 3.6287 (3.4418)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [15]  [ 660/1251]  eta: 0:04:46  lr: 0.000019  loss: 3.6540 (3.4461)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [15]  [ 670/1251]  eta: 0:04:41  lr: 0.000019  loss: 3.5541 (3.4454)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [15]  [ 680/1251]  eta: 0:04:36  lr: 0.000019  loss: 3.4329 (3.4453)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [15]  [ 690/1251]  eta: 0:04:31  lr: 0.000019  loss: 3.5669 (3.4493)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [15]  [ 700/1251]  eta: 0:04:26  lr: 0.000019  loss: 3.6978 (3.4504)  time: 0.4890  data: 0.0004  max mem: 19734
Epoch: [15]  [ 710/1251]  eta: 0:04:21  lr: 0.000019  loss: 3.5973 (3.4506)  time: 0.5138  data: 0.0004  max mem: 19734
Epoch: [15]  [ 720/1251]  eta: 0:04:16  lr: 0.000019  loss: 3.6106 (3.4534)  time: 0.4820  data: 0.0004  max mem: 19734
Epoch: [15]  [ 730/1251]  eta: 0:04:11  lr: 0.000019  loss: 3.6547 (3.4524)  time: 0.4545  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5093, ratio_loss=0.0062, pruning_loss=0.1388, mse_loss=0.5739
Epoch: [15]  [ 740/1251]  eta: 0:04:06  lr: 0.000019  loss: 3.2419 (3.4493)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [15]  [ 750/1251]  eta: 0:04:01  lr: 0.000019  loss: 3.4602 (3.4485)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [15]  [ 760/1251]  eta: 0:03:56  lr: 0.000019  loss: 3.5712 (3.4496)  time: 0.4552  data: 0.0007  max mem: 19734
Epoch: [15]  [ 770/1251]  eta: 0:03:51  lr: 0.000019  loss: 3.5101 (3.4516)  time: 0.4642  data: 0.0007  max mem: 19734
Epoch: [15]  [ 780/1251]  eta: 0:03:46  lr: 0.000019  loss: 3.5991 (3.4532)  time: 0.4618  data: 0.0004  max mem: 19734
Epoch: [15]  [ 790/1251]  eta: 0:03:41  lr: 0.000019  loss: 3.5663 (3.4517)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [15]  [ 800/1251]  eta: 0:03:36  lr: 0.000019  loss: 3.4573 (3.4533)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [15]  [ 810/1251]  eta: 0:03:31  lr: 0.000019  loss: 3.6051 (3.4556)  time: 0.4520  data: 0.0005  max mem: 19734
Epoch: [15]  [ 820/1251]  eta: 0:03:27  lr: 0.000019  loss: 3.7601 (3.4541)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [15]  [ 830/1251]  eta: 0:03:22  lr: 0.000019  loss: 3.3445 (3.4557)  time: 0.4524  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4488, ratio_loss=0.0060, pruning_loss=0.1407, mse_loss=0.5719
Epoch: [15]  [ 840/1251]  eta: 0:03:17  lr: 0.000019  loss: 3.6426 (3.4571)  time: 0.4647  data: 0.0005  max mem: 19734
Epoch: [15]  [ 850/1251]  eta: 0:03:12  lr: 0.000019  loss: 3.5130 (3.4566)  time: 0.4844  data: 0.0005  max mem: 19734
Epoch: [15]  [ 860/1251]  eta: 0:03:07  lr: 0.000019  loss: 3.5106 (3.4576)  time: 0.4940  data: 0.0004  max mem: 19734
Epoch: [15]  [ 870/1251]  eta: 0:03:02  lr: 0.000019  loss: 3.7403 (3.4564)  time: 0.4752  data: 0.0005  max mem: 19734
Epoch: [15]  [ 880/1251]  eta: 0:02:57  lr: 0.000019  loss: 3.5837 (3.4558)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [15]  [ 890/1251]  eta: 0:02:53  lr: 0.000019  loss: 3.2908 (3.4524)  time: 0.4515  data: 0.0004  max mem: 19734
Epoch: [15]  [ 900/1251]  eta: 0:02:48  lr: 0.000019  loss: 3.2908 (3.4512)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [15]  [ 910/1251]  eta: 0:02:43  lr: 0.000019  loss: 3.4162 (3.4514)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [15]  [ 920/1251]  eta: 0:02:38  lr: 0.000019  loss: 3.5844 (3.4514)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [15]  [ 930/1251]  eta: 0:02:33  lr: 0.000019  loss: 3.5774 (3.4503)  time: 0.4614  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4006, ratio_loss=0.0062, pruning_loss=0.1430, mse_loss=0.5591
Epoch: [15]  [ 940/1251]  eta: 0:02:28  lr: 0.000019  loss: 3.6690 (3.4522)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [15]  [ 950/1251]  eta: 0:02:23  lr: 0.000019  loss: 3.5482 (3.4494)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [15]  [ 960/1251]  eta: 0:02:18  lr: 0.000019  loss: 3.4696 (3.4525)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [15]  [ 970/1251]  eta: 0:02:14  lr: 0.000019  loss: 3.6084 (3.4530)  time: 0.4528  data: 0.0007  max mem: 19734
Epoch: [15]  [ 980/1251]  eta: 0:02:09  lr: 0.000019  loss: 3.4839 (3.4531)  time: 0.4532  data: 0.0006  max mem: 19734
Epoch: [15]  [ 990/1251]  eta: 0:02:04  lr: 0.000019  loss: 3.6652 (3.4531)  time: 0.4951  data: 0.0004  max mem: 19734
Epoch: [15]  [1000/1251]  eta: 0:01:59  lr: 0.000019  loss: 3.6462 (3.4541)  time: 0.5134  data: 0.0005  max mem: 19734
Epoch: [15]  [1010/1251]  eta: 0:01:55  lr: 0.000019  loss: 3.5800 (3.4521)  time: 0.4715  data: 0.0005  max mem: 19734
Epoch: [15]  [1020/1251]  eta: 0:01:50  lr: 0.000019  loss: 3.2675 (3.4503)  time: 0.4508  data: 0.0005  max mem: 19734
Epoch: [15]  [1030/1251]  eta: 0:01:45  lr: 0.000019  loss: 3.5924 (3.4503)  time: 0.4497  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4055, ratio_loss=0.0059, pruning_loss=0.1407, mse_loss=0.5939
Epoch: [15]  [1040/1251]  eta: 0:01:40  lr: 0.000019  loss: 3.7196 (3.4497)  time: 0.4510  data: 0.0004  max mem: 19734
Epoch: [15]  [1050/1251]  eta: 0:01:35  lr: 0.000019  loss: 3.3807 (3.4472)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [15]  [1060/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.3656 (3.4456)  time: 0.4640  data: 0.0004  max mem: 19734
Epoch: [15]  [1070/1251]  eta: 0:01:26  lr: 0.000019  loss: 3.5924 (3.4458)  time: 0.4639  data: 0.0004  max mem: 19734
Epoch: [15]  [1080/1251]  eta: 0:01:21  lr: 0.000019  loss: 3.5989 (3.4466)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [15]  [1090/1251]  eta: 0:01:16  lr: 0.000019  loss: 3.5552 (3.4450)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [15]  [1100/1251]  eta: 0:01:11  lr: 0.000019  loss: 3.6289 (3.4471)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [15]  [1110/1251]  eta: 0:01:07  lr: 0.000019  loss: 3.6337 (3.4469)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [15]  [1120/1251]  eta: 0:01:02  lr: 0.000019  loss: 3.6098 (3.4481)  time: 0.4562  data: 0.0006  max mem: 19734
Epoch: [15]  [1130/1251]  eta: 0:00:57  lr: 0.000019  loss: 3.6347 (3.4491)  time: 0.4787  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4031, ratio_loss=0.0064, pruning_loss=0.1419, mse_loss=0.5755
Epoch: [15]  [1140/1251]  eta: 0:00:52  lr: 0.000019  loss: 3.4015 (3.4488)  time: 0.5096  data: 0.0005  max mem: 19734
Epoch: [15]  [1150/1251]  eta: 0:00:48  lr: 0.000019  loss: 3.7029 (3.4504)  time: 0.4959  data: 0.0005  max mem: 19734
Epoch: [15]  [1160/1251]  eta: 0:00:43  lr: 0.000019  loss: 3.8303 (3.4518)  time: 0.4633  data: 0.0004  max mem: 19734
Epoch: [15]  [1170/1251]  eta: 0:00:38  lr: 0.000019  loss: 3.4628 (3.4528)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [15]  [1180/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.4306 (3.4515)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [15]  [1190/1251]  eta: 0:00:28  lr: 0.000019  loss: 3.4747 (3.4525)  time: 0.4524  data: 0.0007  max mem: 19734
Epoch: [15]  [1200/1251]  eta: 0:00:24  lr: 0.000019  loss: 3.4747 (3.4520)  time: 0.4483  data: 0.0005  max mem: 19734
Epoch: [15]  [1210/1251]  eta: 0:00:19  lr: 0.000019  loss: 3.3874 (3.4503)  time: 0.4529  data: 0.0001  max mem: 19734
Epoch: [15]  [1220/1251]  eta: 0:00:14  lr: 0.000019  loss: 3.1910 (3.4495)  time: 0.4538  data: 0.0002  max mem: 19734
Epoch: [15]  [1230/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.3772 (3.4493)  time: 0.4460  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4154, ratio_loss=0.0063, pruning_loss=0.1410, mse_loss=0.5662
Epoch: [15]  [1240/1251]  eta: 0:00:05  lr: 0.000019  loss: 3.4166 (3.4498)  time: 0.4458  data: 0.0002  max mem: 19734
Epoch: [15]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.4127 (3.4489)  time: 0.4464  data: 0.0001  max mem: 19734
Epoch: [15] Total time: 0:09:53 (0.4744 s / it)
Averaged stats: lr: 0.000019  loss: 3.4127 (3.4463)
Test:  [  0/261]  eta: 2:24:52  loss: 0.6892 (0.6892)  acc1: 83.3333 (83.3333)  acc5: 96.3542 (96.3542)  time: 33.3065  data: 33.2138  max mem: 19734
Test:  [ 10/261]  eta: 0:13:23  loss: 0.6892 (0.7406)  acc1: 83.3333 (83.6648)  acc5: 96.8750 (96.3068)  time: 3.1998  data: 3.0317  max mem: 19734
Test:  [ 20/261]  eta: 0:06:57  loss: 0.9268 (0.9077)  acc1: 80.2083 (79.3899)  acc5: 94.2708 (94.5685)  time: 0.1557  data: 0.0141  max mem: 19734
Test:  [ 30/261]  eta: 0:04:56  loss: 0.8288 (0.8291)  acc1: 83.8542 (82.1237)  acc5: 93.2292 (95.0605)  time: 0.2311  data: 0.0166  max mem: 19734
Test:  [ 40/261]  eta: 0:03:57  loss: 0.5691 (0.8017)  acc1: 88.0208 (82.8760)  acc5: 96.8750 (95.3887)  time: 0.3835  data: 0.1631  max mem: 19734
Test:  [ 50/261]  eta: 0:03:06  loss: 0.9628 (0.8603)  acc1: 77.0833 (81.1785)  acc5: 93.7500 (94.9551)  time: 0.2621  data: 0.1597  max mem: 19734
Test:  [ 60/261]  eta: 0:02:31  loss: 0.9982 (0.8749)  acc1: 76.0417 (80.4986)  acc5: 94.2708 (95.0051)  time: 0.0925  data: 0.0108  max mem: 19734
Test:  [ 70/261]  eta: 0:02:10  loss: 0.9647 (0.8760)  acc1: 76.0417 (80.0029)  acc5: 95.8333 (95.2098)  time: 0.1681  data: 0.0360  max mem: 19734
Test:  [ 80/261]  eta: 0:01:56  loss: 0.8759 (0.8781)  acc1: 80.7292 (80.1633)  acc5: 96.3542 (95.3061)  time: 0.3120  data: 0.1514  max mem: 19734
Test:  [ 90/261]  eta: 0:01:46  loss: 0.8251 (0.8634)  acc1: 84.3750 (80.6834)  acc5: 96.3542 (95.4384)  time: 0.4007  data: 0.1739  max mem: 19734
Test:  [100/261]  eta: 0:01:36  loss: 0.8251 (0.8672)  acc1: 83.8542 (80.6106)  acc5: 96.3542 (95.4620)  time: 0.4221  data: 0.1743  max mem: 19734
Test:  [110/261]  eta: 0:01:32  loss: 0.9229 (0.8897)  acc1: 76.5625 (80.0676)  acc5: 94.2708 (95.1436)  time: 0.5855  data: 0.4348  max mem: 19734
Test:  [120/261]  eta: 0:01:21  loss: 1.2756 (0.9318)  acc1: 69.7917 (79.0160)  acc5: 90.1042 (94.5851)  time: 0.4702  data: 0.3213  max mem: 19734
Test:  [130/261]  eta: 0:01:12  loss: 1.4125 (0.9787)  acc1: 69.2708 (78.1091)  acc5: 87.5000 (93.9607)  time: 0.2034  data: 0.0136  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: 1.3710 (1.0085)  acc1: 69.2708 (77.3641)  acc5: 89.5833 (93.6724)  time: 0.4032  data: 0.1756  max mem: 19734
Test:  [150/261]  eta: 0:00:59  loss: 1.2708 (1.0148)  acc1: 71.8750 (77.3903)  acc5: 90.1042 (93.4810)  time: 0.4162  data: 0.1796  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: 1.0687 (1.0348)  acc1: 79.1667 (77.0510)  acc5: 91.1458 (93.2259)  time: 0.2261  data: 0.0219  max mem: 19734
Test:  [170/261]  eta: 0:00:44  loss: 1.2934 (1.0665)  acc1: 64.0625 (76.2396)  acc5: 87.5000 (92.8667)  time: 0.1781  data: 0.0178  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4597 (1.0838)  acc1: 65.1042 (75.8431)  acc5: 88.0208 (92.7055)  time: 0.1519  data: 0.0108  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4081 (1.0972)  acc1: 66.1458 (75.5972)  acc5: 90.6250 (92.5365)  time: 0.1555  data: 0.0340  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.4081 (1.1124)  acc1: 71.3542 (75.3265)  acc5: 89.0625 (92.2989)  time: 0.2271  data: 0.1424  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: 1.4138 (1.1275)  acc1: 69.7917 (75.0444)  acc5: 88.0208 (92.0666)  time: 0.2554  data: 0.1858  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: 1.4748 (1.1467)  acc1: 66.1458 (74.5546)  acc5: 86.9792 (91.8505)  time: 0.1378  data: 0.0737  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4654 (1.1570)  acc1: 65.6250 (74.3033)  acc5: 88.5417 (91.7456)  time: 0.0624  data: 0.0009  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3599 (1.1668)  acc1: 66.6667 (74.0815)  acc5: 90.6250 (91.6580)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.1056 (1.1599)  acc1: 75.0000 (74.2260)  acc5: 93.7500 (91.7725)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9867 (1.1602)  acc1: 76.0417 (74.2480)  acc5: 94.7917 (91.8080)  time: 0.0599  data: 0.0002  max mem: 19734
Test: Total time: 0:01:35 (0.3663 s / it)
* Acc@1 74.248 Acc@5 91.808 loss 1.160
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.25%
Epoch: [16]  [   0/1251]  eta: 4:23:28  lr: 0.000019  loss: 3.8249 (3.8249)  time: 12.6367  data: 12.1660  max mem: 19734
Epoch: [16]  [  10/1251]  eta: 0:36:16  lr: 0.000019  loss: 3.6279 (3.4237)  time: 1.7537  data: 1.2189  max mem: 19734
Epoch: [16]  [  20/1251]  eta: 0:24:08  lr: 0.000019  loss: 3.6118 (3.5309)  time: 0.6038  data: 0.0623  max mem: 19734
Epoch: [16]  [  30/1251]  eta: 0:19:39  lr: 0.000019  loss: 3.6131 (3.5676)  time: 0.5332  data: 0.0004  max mem: 19734
Epoch: [16]  [  40/1251]  eta: 0:17:08  lr: 0.000019  loss: 3.6131 (3.5567)  time: 0.5058  data: 0.0004  max mem: 19734
Epoch: [16]  [  50/1251]  eta: 0:15:28  lr: 0.000019  loss: 3.5948 (3.5351)  time: 0.4735  data: 0.0004  max mem: 19734
Epoch: [16]  [  60/1251]  eta: 0:14:19  lr: 0.000019  loss: 3.5987 (3.5527)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [16]  [  70/1251]  eta: 0:13:29  lr: 0.000019  loss: 3.5613 (3.5472)  time: 0.4611  data: 0.0004  max mem: 19734
Epoch: [16]  [  80/1251]  eta: 0:12:49  lr: 0.000019  loss: 3.6094 (3.5618)  time: 0.4589  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5187, ratio_loss=0.0062, pruning_loss=0.1372, mse_loss=0.5703
Epoch: [16]  [  90/1251]  eta: 0:12:17  lr: 0.000019  loss: 3.6094 (3.5353)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [16]  [ 100/1251]  eta: 0:11:50  lr: 0.000019  loss: 3.3008 (3.5080)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [16]  [ 110/1251]  eta: 0:11:28  lr: 0.000019  loss: 3.3207 (3.4917)  time: 0.4578  data: 0.0009  max mem: 19734
Epoch: [16]  [ 120/1251]  eta: 0:11:07  lr: 0.000019  loss: 3.3833 (3.4730)  time: 0.4564  data: 0.0010  max mem: 19734
Epoch: [16]  [ 130/1251]  eta: 0:10:50  lr: 0.000019  loss: 3.3773 (3.4642)  time: 0.4532  data: 0.0006  max mem: 19734
Epoch: [16]  [ 140/1251]  eta: 0:10:35  lr: 0.000019  loss: 3.5335 (3.4732)  time: 0.4564  data: 0.0005  max mem: 19734
Epoch: [16]  [ 150/1251]  eta: 0:10:20  lr: 0.000019  loss: 3.6029 (3.4696)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [16]  [ 160/1251]  eta: 0:10:10  lr: 0.000019  loss: 3.3663 (3.4676)  time: 0.4737  data: 0.0004  max mem: 19734
Epoch: [16]  [ 170/1251]  eta: 0:10:01  lr: 0.000019  loss: 3.2826 (3.4492)  time: 0.4993  data: 0.0005  max mem: 19734
Epoch: [16]  [ 180/1251]  eta: 0:09:52  lr: 0.000019  loss: 3.2269 (3.4495)  time: 0.5004  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3079, ratio_loss=0.0061, pruning_loss=0.1430, mse_loss=0.5924
Epoch: [16]  [ 190/1251]  eta: 0:09:42  lr: 0.000019  loss: 3.6590 (3.4447)  time: 0.4850  data: 0.0004  max mem: 19734
Epoch: [16]  [ 200/1251]  eta: 0:09:32  lr: 0.000019  loss: 3.4781 (3.4390)  time: 0.4677  data: 0.0005  max mem: 19734
Epoch: [16]  [ 210/1251]  eta: 0:09:22  lr: 0.000019  loss: 3.4794 (3.4458)  time: 0.4575  data: 0.0005  max mem: 19734
Epoch: [16]  [ 220/1251]  eta: 0:09:13  lr: 0.000019  loss: 3.4808 (3.4443)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [16]  [ 230/1251]  eta: 0:09:04  lr: 0.000019  loss: 3.6449 (3.4496)  time: 0.4600  data: 0.0004  max mem: 19734
Epoch: [16]  [ 240/1251]  eta: 0:08:55  lr: 0.000019  loss: 3.6273 (3.4352)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [16]  [ 250/1251]  eta: 0:08:47  lr: 0.000019  loss: 3.1168 (3.4363)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [16]  [ 260/1251]  eta: 0:08:39  lr: 0.000019  loss: 3.4877 (3.4358)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [16]  [ 270/1251]  eta: 0:08:31  lr: 0.000019  loss: 3.6570 (3.4436)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [16]  [ 280/1251]  eta: 0:08:24  lr: 0.000019  loss: 3.8092 (3.4559)  time: 0.4529  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4394, ratio_loss=0.0064, pruning_loss=0.1397, mse_loss=0.5709
Epoch: [16]  [ 290/1251]  eta: 0:08:17  lr: 0.000019  loss: 3.4720 (3.4393)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [16]  [ 300/1251]  eta: 0:08:09  lr: 0.000019  loss: 3.4454 (3.4466)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [16]  [ 310/1251]  eta: 0:08:05  lr: 0.000019  loss: 3.6021 (3.4471)  time: 0.4938  data: 0.0004  max mem: 19734
Epoch: [16]  [ 320/1251]  eta: 0:07:59  lr: 0.000019  loss: 3.6237 (3.4544)  time: 0.5085  data: 0.0004  max mem: 19734
Epoch: [16]  [ 330/1251]  eta: 0:07:52  lr: 0.000019  loss: 3.4161 (3.4440)  time: 0.4794  data: 0.0005  max mem: 19734
Epoch: [16]  [ 340/1251]  eta: 0:07:46  lr: 0.000019  loss: 2.9987 (3.4410)  time: 0.4649  data: 0.0004  max mem: 19734
Epoch: [16]  [ 350/1251]  eta: 0:07:39  lr: 0.000019  loss: 3.1481 (3.4374)  time: 0.4527  data: 0.0007  max mem: 19734
Epoch: [16]  [ 360/1251]  eta: 0:07:33  lr: 0.000019  loss: 3.5393 (3.4421)  time: 0.4527  data: 0.0007  max mem: 19734
Epoch: [16]  [ 370/1251]  eta: 0:07:26  lr: 0.000019  loss: 3.5889 (3.4418)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [16]  [ 380/1251]  eta: 0:07:20  lr: 0.000019  loss: 3.5429 (3.4462)  time: 0.4543  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4019, ratio_loss=0.0059, pruning_loss=0.1394, mse_loss=0.5691
Epoch: [16]  [ 390/1251]  eta: 0:07:14  lr: 0.000019  loss: 3.4762 (3.4426)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [16]  [ 400/1251]  eta: 0:07:08  lr: 0.000019  loss: 3.4939 (3.4422)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [16]  [ 410/1251]  eta: 0:07:02  lr: 0.000019  loss: 3.6745 (3.4474)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [16]  [ 420/1251]  eta: 0:06:56  lr: 0.000019  loss: 3.6721 (3.4467)  time: 0.4535  data: 0.0004  max mem: 19734
Epoch: [16]  [ 430/1251]  eta: 0:06:50  lr: 0.000019  loss: 3.3587 (3.4430)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [16]  [ 440/1251]  eta: 0:06:44  lr: 0.000019  loss: 3.4388 (3.4452)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [16]  [ 450/1251]  eta: 0:06:39  lr: 0.000019  loss: 3.4873 (3.4448)  time: 0.4826  data: 0.0005  max mem: 19734
Epoch: [16]  [ 460/1251]  eta: 0:06:34  lr: 0.000019  loss: 3.5569 (3.4478)  time: 0.5149  data: 0.0004  max mem: 19734
Epoch: [16]  [ 470/1251]  eta: 0:06:29  lr: 0.000019  loss: 3.6602 (3.4523)  time: 0.4943  data: 0.0004  max mem: 19734
Epoch: [16]  [ 480/1251]  eta: 0:06:24  lr: 0.000019  loss: 3.6588 (3.4489)  time: 0.4737  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4222, ratio_loss=0.0061, pruning_loss=0.1395, mse_loss=0.5658
Epoch: [16]  [ 490/1251]  eta: 0:06:18  lr: 0.000019  loss: 3.4228 (3.4482)  time: 0.4653  data: 0.0005  max mem: 19734
Epoch: [16]  [ 500/1251]  eta: 0:06:12  lr: 0.000019  loss: 3.6029 (3.4498)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [16]  [ 510/1251]  eta: 0:06:07  lr: 0.000019  loss: 3.5832 (3.4504)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [16]  [ 520/1251]  eta: 0:06:01  lr: 0.000019  loss: 3.5405 (3.4520)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [16]  [ 530/1251]  eta: 0:05:56  lr: 0.000019  loss: 3.5447 (3.4551)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [16]  [ 540/1251]  eta: 0:05:50  lr: 0.000019  loss: 3.4860 (3.4503)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [16]  [ 550/1251]  eta: 0:05:45  lr: 0.000019  loss: 3.4860 (3.4527)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [16]  [ 560/1251]  eta: 0:05:39  lr: 0.000019  loss: 3.5698 (3.4545)  time: 0.4545  data: 0.0006  max mem: 19734
Epoch: [16]  [ 570/1251]  eta: 0:05:34  lr: 0.000019  loss: 3.6243 (3.4580)  time: 0.4560  data: 0.0007  max mem: 19734
Epoch: [16]  [ 580/1251]  eta: 0:05:29  lr: 0.000019  loss: 3.7155 (3.4571)  time: 0.4548  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4887, ratio_loss=0.0060, pruning_loss=0.1378, mse_loss=0.5778
Epoch: [16]  [ 590/1251]  eta: 0:05:23  lr: 0.000019  loss: 3.4874 (3.4587)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [16]  [ 600/1251]  eta: 0:05:19  lr: 0.000019  loss: 3.6383 (3.4649)  time: 0.4956  data: 0.0004  max mem: 19734
Epoch: [16]  [ 610/1251]  eta: 0:05:14  lr: 0.000019  loss: 3.6072 (3.4641)  time: 0.5244  data: 0.0005  max mem: 19734
Epoch: [16]  [ 620/1251]  eta: 0:05:09  lr: 0.000019  loss: 3.4579 (3.4630)  time: 0.4809  data: 0.0004  max mem: 19734
Epoch: [16]  [ 630/1251]  eta: 0:05:04  lr: 0.000019  loss: 3.4579 (3.4615)  time: 0.4602  data: 0.0004  max mem: 19734
Epoch: [16]  [ 640/1251]  eta: 0:04:59  lr: 0.000019  loss: 3.5448 (3.4637)  time: 0.4607  data: 0.0006  max mem: 19734
Epoch: [16]  [ 650/1251]  eta: 0:04:53  lr: 0.000019  loss: 3.4858 (3.4604)  time: 0.4544  data: 0.0006  max mem: 19734
Epoch: [16]  [ 660/1251]  eta: 0:04:48  lr: 0.000019  loss: 3.2606 (3.4600)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [16]  [ 670/1251]  eta: 0:04:43  lr: 0.000019  loss: 3.4300 (3.4606)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [16]  [ 680/1251]  eta: 0:04:38  lr: 0.000019  loss: 3.5483 (3.4625)  time: 0.4518  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4591, ratio_loss=0.0060, pruning_loss=0.1388, mse_loss=0.5781
Epoch: [16]  [ 690/1251]  eta: 0:04:33  lr: 0.000019  loss: 3.5665 (3.4625)  time: 0.4509  data: 0.0004  max mem: 19734
Epoch: [16]  [ 700/1251]  eta: 0:04:27  lr: 0.000019  loss: 3.5400 (3.4623)  time: 0.4519  data: 0.0005  max mem: 19734
Epoch: [16]  [ 710/1251]  eta: 0:04:22  lr: 0.000019  loss: 3.5926 (3.4643)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [16]  [ 720/1251]  eta: 0:04:17  lr: 0.000019  loss: 3.4407 (3.4616)  time: 0.4515  data: 0.0003  max mem: 19734
Epoch: [16]  [ 730/1251]  eta: 0:04:12  lr: 0.000019  loss: 3.4407 (3.4627)  time: 0.4526  data: 0.0006  max mem: 19734
Epoch: [16]  [ 740/1251]  eta: 0:04:07  lr: 0.000019  loss: 3.6062 (3.4625)  time: 0.4794  data: 0.0006  max mem: 19734
Epoch: [16]  [ 750/1251]  eta: 0:04:03  lr: 0.000019  loss: 3.6062 (3.4628)  time: 0.5068  data: 0.0005  max mem: 19734
Epoch: [16]  [ 760/1251]  eta: 0:03:58  lr: 0.000019  loss: 3.6251 (3.4630)  time: 0.4809  data: 0.0004  max mem: 19734
Epoch: [16]  [ 770/1251]  eta: 0:03:53  lr: 0.000019  loss: 3.7696 (3.4664)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [16]  [ 780/1251]  eta: 0:03:48  lr: 0.000019  loss: 3.6100 (3.4609)  time: 0.4654  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4041, ratio_loss=0.0060, pruning_loss=0.1386, mse_loss=0.5638
Epoch: [16]  [ 790/1251]  eta: 0:03:43  lr: 0.000019  loss: 3.2486 (3.4590)  time: 0.4660  data: 0.0004  max mem: 19734
Epoch: [16]  [ 800/1251]  eta: 0:03:38  lr: 0.000019  loss: 3.6110 (3.4603)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [16]  [ 810/1251]  eta: 0:03:33  lr: 0.000019  loss: 3.6110 (3.4597)  time: 0.4510  data: 0.0005  max mem: 19734
Epoch: [16]  [ 820/1251]  eta: 0:03:28  lr: 0.000019  loss: 3.6649 (3.4618)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [16]  [ 830/1251]  eta: 0:03:23  lr: 0.000019  loss: 3.6649 (3.4632)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [16]  [ 840/1251]  eta: 0:03:18  lr: 0.000019  loss: 3.5275 (3.4619)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [16]  [ 850/1251]  eta: 0:03:13  lr: 0.000019  loss: 3.3578 (3.4581)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [16]  [ 860/1251]  eta: 0:03:08  lr: 0.000019  loss: 3.5101 (3.4587)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [16]  [ 870/1251]  eta: 0:03:03  lr: 0.000019  loss: 3.3397 (3.4551)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [16]  [ 880/1251]  eta: 0:02:58  lr: 0.000019  loss: 3.3397 (3.4556)  time: 0.4523  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3916, ratio_loss=0.0062, pruning_loss=0.1393, mse_loss=0.5666
Epoch: [16]  [ 890/1251]  eta: 0:02:53  lr: 0.000019  loss: 3.4090 (3.4559)  time: 0.4892  data: 0.0004  max mem: 19734
Epoch: [16]  [ 900/1251]  eta: 0:02:49  lr: 0.000019  loss: 3.4090 (3.4564)  time: 0.5094  data: 0.0004  max mem: 19734
Epoch: [16]  [ 910/1251]  eta: 0:02:44  lr: 0.000019  loss: 3.5629 (3.4556)  time: 0.4718  data: 0.0004  max mem: 19734
Epoch: [16]  [ 920/1251]  eta: 0:02:39  lr: 0.000019  loss: 3.5504 (3.4551)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [16]  [ 930/1251]  eta: 0:02:34  lr: 0.000019  loss: 3.1036 (3.4506)  time: 0.4637  data: 0.0004  max mem: 19734
Epoch: [16]  [ 940/1251]  eta: 0:02:29  lr: 0.000019  loss: 3.3581 (3.4539)  time: 0.4626  data: 0.0004  max mem: 19734
Epoch: [16]  [ 950/1251]  eta: 0:02:24  lr: 0.000019  loss: 3.8050 (3.4531)  time: 0.4508  data: 0.0004  max mem: 19734
Epoch: [16]  [ 960/1251]  eta: 0:02:19  lr: 0.000019  loss: 3.6211 (3.4519)  time: 0.4513  data: 0.0004  max mem: 19734
Epoch: [16]  [ 970/1251]  eta: 0:02:14  lr: 0.000019  loss: 3.4730 (3.4512)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [16]  [ 980/1251]  eta: 0:02:09  lr: 0.000019  loss: 3.6079 (3.4532)  time: 0.4544  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4018, ratio_loss=0.0059, pruning_loss=0.1411, mse_loss=0.5668
Epoch: [16]  [ 990/1251]  eta: 0:02:05  lr: 0.000019  loss: 3.6079 (3.4546)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [16]  [1000/1251]  eta: 0:02:00  lr: 0.000019  loss: 3.5759 (3.4520)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [16]  [1010/1251]  eta: 0:01:55  lr: 0.000019  loss: 3.0206 (3.4506)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [16]  [1020/1251]  eta: 0:01:50  lr: 0.000019  loss: 3.5422 (3.4503)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [16]  [1030/1251]  eta: 0:01:45  lr: 0.000019  loss: 3.4807 (3.4507)  time: 0.4905  data: 0.0004  max mem: 19734
Epoch: [16]  [1040/1251]  eta: 0:01:41  lr: 0.000019  loss: 3.3609 (3.4491)  time: 0.5095  data: 0.0004  max mem: 19734
Epoch: [16]  [1050/1251]  eta: 0:01:36  lr: 0.000019  loss: 3.3609 (3.4494)  time: 0.4792  data: 0.0004  max mem: 19734
Epoch: [16]  [1060/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.5457 (3.4478)  time: 0.4618  data: 0.0004  max mem: 19734
Epoch: [16]  [1070/1251]  eta: 0:01:26  lr: 0.000019  loss: 3.3300 (3.4473)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [16]  [1080/1251]  eta: 0:01:21  lr: 0.000019  loss: 3.5003 (3.4462)  time: 0.4628  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3442, ratio_loss=0.0064, pruning_loss=0.1395, mse_loss=0.5455
Epoch: [16]  [1090/1251]  eta: 0:01:16  lr: 0.000019  loss: 3.5002 (3.4442)  time: 0.4625  data: 0.0005  max mem: 19734
Epoch: [16]  [1100/1251]  eta: 0:01:12  lr: 0.000019  loss: 3.3079 (3.4441)  time: 0.4523  data: 0.0006  max mem: 19734
Epoch: [16]  [1110/1251]  eta: 0:01:07  lr: 0.000019  loss: 3.3079 (3.4428)  time: 0.4547  data: 0.0007  max mem: 19734
Epoch: [16]  [1120/1251]  eta: 0:01:02  lr: 0.000019  loss: 3.2544 (3.4402)  time: 0.4540  data: 0.0006  max mem: 19734
Epoch: [16]  [1130/1251]  eta: 0:00:57  lr: 0.000019  loss: 3.5478 (3.4418)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [16]  [1140/1251]  eta: 0:00:52  lr: 0.000019  loss: 3.6447 (3.4424)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [16]  [1150/1251]  eta: 0:00:48  lr: 0.000019  loss: 3.6398 (3.4426)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [16]  [1160/1251]  eta: 0:00:43  lr: 0.000019  loss: 3.3689 (3.4412)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [16]  [1170/1251]  eta: 0:00:38  lr: 0.000019  loss: 3.6623 (3.4425)  time: 0.4675  data: 0.0004  max mem: 19734
Epoch: [16]  [1180/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.7003 (3.4444)  time: 0.4936  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3888, ratio_loss=0.0062, pruning_loss=0.1393, mse_loss=0.5792
Epoch: [16]  [1190/1251]  eta: 0:00:29  lr: 0.000019  loss: 3.6777 (3.4445)  time: 0.4937  data: 0.0007  max mem: 19734
Epoch: [16]  [1200/1251]  eta: 0:00:24  lr: 0.000019  loss: 3.6997 (3.4467)  time: 0.4630  data: 0.0006  max mem: 19734
Epoch: [16]  [1210/1251]  eta: 0:00:19  lr: 0.000019  loss: 3.7860 (3.4465)  time: 0.4446  data: 0.0001  max mem: 19734
Epoch: [16]  [1220/1251]  eta: 0:00:14  lr: 0.000019  loss: 3.5633 (3.4462)  time: 0.4431  data: 0.0001  max mem: 19734
Epoch: [16]  [1230/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.5610 (3.4471)  time: 0.4508  data: 0.0001  max mem: 19734
Epoch: [16]  [1240/1251]  eta: 0:00:05  lr: 0.000019  loss: 3.5610 (3.4467)  time: 0.4510  data: 0.0002  max mem: 19734
Epoch: [16]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.6104 (3.4470)  time: 0.4441  data: 0.0002  max mem: 19734
Epoch: [16] Total time: 0:09:55 (0.4761 s / it)
Averaged stats: lr: 0.000019  loss: 3.6104 (3.4437)
Test:  [  0/261]  eta: 2:01:07  loss: 0.6714 (0.6714)  acc1: 82.2917 (82.2917)  acc5: 97.3958 (97.3958)  time: 27.8459  data: 27.4128  max mem: 19734
Test:  [ 10/261]  eta: 0:12:14  loss: 0.6714 (0.7243)  acc1: 83.8542 (83.6174)  acc5: 97.3958 (96.5436)  time: 2.9259  data: 2.5808  max mem: 19734
Test:  [ 20/261]  eta: 0:06:39  loss: 0.9377 (0.8972)  acc1: 78.1250 (79.1171)  acc5: 93.7500 (94.6925)  time: 0.3501  data: 0.0536  max mem: 19734
Test:  [ 30/261]  eta: 0:04:31  loss: 0.8011 (0.8179)  acc1: 81.7708 (81.7876)  acc5: 94.7917 (95.1613)  time: 0.2123  data: 0.0145  max mem: 19734
Test:  [ 40/261]  eta: 0:03:52  loss: 0.5696 (0.7831)  acc1: 88.5417 (82.7871)  acc5: 96.3542 (95.4395)  time: 0.4125  data: 0.2205  max mem: 19734
Test:  [ 50/261]  eta: 0:03:09  loss: 0.9132 (0.8471)  acc1: 78.6458 (80.9334)  acc5: 94.7917 (94.9449)  time: 0.4711  data: 0.2492  max mem: 19734
Test:  [ 60/261]  eta: 0:02:38  loss: 1.0031 (0.8612)  acc1: 76.0417 (80.3449)  acc5: 93.7500 (94.9283)  time: 0.2472  data: 0.0472  max mem: 19734
Test:  [ 70/261]  eta: 0:02:36  loss: 0.9337 (0.8606)  acc1: 76.5625 (79.9002)  acc5: 95.8333 (95.1511)  time: 0.6170  data: 0.4329  max mem: 19734
Test:  [ 80/261]  eta: 0:02:16  loss: 0.8197 (0.8639)  acc1: 81.2500 (80.0347)  acc5: 96.3542 (95.2482)  time: 0.6419  data: 0.4340  max mem: 19734
Test:  [ 90/261]  eta: 0:01:58  loss: 0.7987 (0.8502)  acc1: 83.8542 (80.4659)  acc5: 96.3542 (95.3755)  time: 0.2358  data: 0.0222  max mem: 19734
Test:  [100/261]  eta: 0:01:53  loss: 0.7884 (0.8536)  acc1: 83.8542 (80.3734)  acc5: 95.8333 (95.4260)  time: 0.5173  data: 0.3191  max mem: 19734
Test:  [110/261]  eta: 0:01:40  loss: 0.9021 (0.8760)  acc1: 75.5208 (79.9127)  acc5: 94.2708 (95.1295)  time: 0.5339  data: 0.3519  max mem: 19734
Test:  [120/261]  eta: 0:01:28  loss: 1.2199 (0.9175)  acc1: 70.3125 (78.9127)  acc5: 89.0625 (94.5592)  time: 0.2351  data: 0.0571  max mem: 19734
Test:  [130/261]  eta: 0:01:17  loss: 1.3954 (0.9657)  acc1: 66.6667 (77.8387)  acc5: 86.9792 (93.9766)  time: 0.2126  data: 0.0228  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: 1.3118 (0.9918)  acc1: 68.2292 (77.2052)  acc5: 89.5833 (93.7352)  time: 0.1605  data: 0.0183  max mem: 19734
Test:  [150/261]  eta: 0:01:00  loss: 1.2620 (0.9980)  acc1: 72.3958 (77.1696)  acc5: 91.1458 (93.5741)  time: 0.2339  data: 0.0972  max mem: 19734
Test:  [160/261]  eta: 0:00:52  loss: 1.0617 (1.0189)  acc1: 76.5625 (76.8278)  acc5: 91.1458 (93.2906)  time: 0.2474  data: 0.0952  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: 1.2687 (1.0495)  acc1: 66.1458 (76.0508)  acc5: 88.5417 (92.9672)  time: 0.2728  data: 0.1635  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4149 (1.0672)  acc1: 64.5833 (75.6705)  acc5: 88.5417 (92.7716)  time: 0.2253  data: 0.1581  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: 1.4032 (1.0811)  acc1: 68.7500 (75.4336)  acc5: 90.1042 (92.5993)  time: 0.0620  data: 0.0005  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3697 (1.0979)  acc1: 70.3125 (75.1011)  acc5: 89.0625 (92.3611)  time: 0.0619  data: 0.0004  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3940 (1.1121)  acc1: 69.2708 (74.8420)  acc5: 88.0208 (92.1628)  time: 0.0690  data: 0.0075  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4507 (1.1311)  acc1: 67.1875 (74.3519)  acc5: 88.0208 (91.9613)  time: 0.0689  data: 0.0074  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4889 (1.1414)  acc1: 64.0625 (74.1026)  acc5: 89.0625 (91.8380)  time: 0.0616  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3572 (1.1506)  acc1: 69.2708 (73.9194)  acc5: 90.6250 (91.7639)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0525 (1.1440)  acc1: 75.5208 (74.0911)  acc5: 92.1875 (91.8845)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9216 (1.1431)  acc1: 76.0417 (74.1120)  acc5: 94.7917 (91.9420)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:33 (0.3597 s / it)
* Acc@1 74.112 Acc@5 91.942 loss 1.143
Accuracy of the network on the 50000 test images: 74.1%
Max accuracy: 74.25%
Epoch: [17]  [   0/1251]  eta: 5:44:19  lr: 0.000019  loss: 3.1721 (3.1721)  time: 16.5147  data: 7.0052  max mem: 19734
Epoch: [17]  [  10/1251]  eta: 0:42:35  lr: 0.000019  loss: 3.6483 (3.6146)  time: 2.0594  data: 0.6375  max mem: 19734
Epoch: [17]  [  20/1251]  eta: 0:26:37  lr: 0.000019  loss: 3.6380 (3.5831)  time: 0.5369  data: 0.0005  max mem: 19734
Epoch: [17]  [  30/1251]  eta: 0:20:56  lr: 0.000019  loss: 3.3776 (3.4256)  time: 0.4623  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4538, ratio_loss=0.0061, pruning_loss=0.1371, mse_loss=0.5546
Epoch: [17]  [  40/1251]  eta: 0:17:58  lr: 0.000019  loss: 3.0598 (3.3550)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [17]  [  50/1251]  eta: 0:16:08  lr: 0.000019  loss: 2.8004 (3.2701)  time: 0.4604  data: 0.0005  max mem: 19734
Epoch: [17]  [  60/1251]  eta: 0:15:00  lr: 0.000019  loss: 3.0958 (3.3115)  time: 0.4803  data: 0.0005  max mem: 19734
Epoch: [17]  [  70/1251]  eta: 0:14:16  lr: 0.000019  loss: 3.6780 (3.3473)  time: 0.5194  data: 0.0005  max mem: 19734
Epoch: [17]  [  80/1251]  eta: 0:13:31  lr: 0.000019  loss: 3.6780 (3.3773)  time: 0.5000  data: 0.0005  max mem: 19734
Epoch: [17]  [  90/1251]  eta: 0:12:56  lr: 0.000019  loss: 3.5589 (3.3977)  time: 0.4675  data: 0.0005  max mem: 19734
Epoch: [17]  [ 100/1251]  eta: 0:12:25  lr: 0.000019  loss: 3.5589 (3.4260)  time: 0.4646  data: 0.0005  max mem: 19734
Epoch: [17]  [ 110/1251]  eta: 0:11:59  lr: 0.000019  loss: 3.5531 (3.4217)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [17]  [ 120/1251]  eta: 0:11:36  lr: 0.000019  loss: 3.4555 (3.4151)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [17]  [ 130/1251]  eta: 0:11:16  lr: 0.000019  loss: 3.3351 (3.4204)  time: 0.4544  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3945, ratio_loss=0.0064, pruning_loss=0.1398, mse_loss=0.5714
Epoch: [17]  [ 140/1251]  eta: 0:10:58  lr: 0.000019  loss: 3.3580 (3.4184)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [17]  [ 150/1251]  eta: 0:10:42  lr: 0.000019  loss: 3.3580 (3.4059)  time: 0.4545  data: 0.0005  max mem: 19734
Epoch: [17]  [ 160/1251]  eta: 0:10:28  lr: 0.000019  loss: 3.5745 (3.4122)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [17]  [ 170/1251]  eta: 0:10:14  lr: 0.000019  loss: 3.4621 (3.4088)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [17]  [ 180/1251]  eta: 0:10:02  lr: 0.000019  loss: 3.3890 (3.4105)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [17]  [ 190/1251]  eta: 0:09:50  lr: 0.000019  loss: 3.5214 (3.4082)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [17]  [ 200/1251]  eta: 0:09:40  lr: 0.000019  loss: 3.4289 (3.4011)  time: 0.4638  data: 0.0004  max mem: 19734
Epoch: [17]  [ 210/1251]  eta: 0:09:33  lr: 0.000019  loss: 3.4642 (3.4097)  time: 0.4931  data: 0.0004  max mem: 19734
Epoch: [17]  [ 220/1251]  eta: 0:09:25  lr: 0.000019  loss: 3.5774 (3.4099)  time: 0.5119  data: 0.0004  max mem: 19734
Epoch: [17]  [ 230/1251]  eta: 0:09:16  lr: 0.000019  loss: 3.3703 (3.4050)  time: 0.4841  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3456, ratio_loss=0.0063, pruning_loss=0.1423, mse_loss=0.5349
Epoch: [17]  [ 240/1251]  eta: 0:09:08  lr: 0.000019  loss: 3.2392 (3.4010)  time: 0.4694  data: 0.0004  max mem: 19734
Epoch: [17]  [ 250/1251]  eta: 0:08:59  lr: 0.000019  loss: 3.2031 (3.3956)  time: 0.4705  data: 0.0004  max mem: 19734
Epoch: [17]  [ 260/1251]  eta: 0:08:50  lr: 0.000019  loss: 3.5857 (3.4096)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [17]  [ 270/1251]  eta: 0:08:42  lr: 0.000019  loss: 3.5737 (3.4010)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [17]  [ 280/1251]  eta: 0:08:34  lr: 0.000019  loss: 2.9145 (3.3942)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [17]  [ 290/1251]  eta: 0:08:27  lr: 0.000019  loss: 3.1026 (3.3857)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [17]  [ 300/1251]  eta: 0:08:19  lr: 0.000019  loss: 3.3877 (3.3900)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [17]  [ 310/1251]  eta: 0:08:12  lr: 0.000019  loss: 3.5566 (3.3974)  time: 0.4567  data: 0.0005  max mem: 19734
Epoch: [17]  [ 320/1251]  eta: 0:08:05  lr: 0.000019  loss: 3.6421 (3.3995)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [17]  [ 330/1251]  eta: 0:07:57  lr: 0.000019  loss: 3.5126 (3.3852)  time: 0.4562  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3016, ratio_loss=0.0058, pruning_loss=0.1424, mse_loss=0.5555
Epoch: [17]  [ 340/1251]  eta: 0:07:51  lr: 0.000019  loss: 3.3350 (3.3829)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [17]  [ 350/1251]  eta: 0:07:45  lr: 0.000019  loss: 3.0361 (3.3719)  time: 0.4868  data: 0.0004  max mem: 19734
Epoch: [17]  [ 360/1251]  eta: 0:07:40  lr: 0.000019  loss: 3.4759 (3.3790)  time: 0.5162  data: 0.0004  max mem: 19734
Epoch: [17]  [ 370/1251]  eta: 0:07:34  lr: 0.000019  loss: 3.8927 (3.3863)  time: 0.4839  data: 0.0006  max mem: 19734
Epoch: [17]  [ 380/1251]  eta: 0:07:27  lr: 0.000019  loss: 3.8235 (3.3925)  time: 0.4537  data: 0.0006  max mem: 19734
Epoch: [17]  [ 390/1251]  eta: 0:07:21  lr: 0.000019  loss: 3.4787 (3.3927)  time: 0.4657  data: 0.0004  max mem: 19734
Epoch: [17]  [ 400/1251]  eta: 0:07:15  lr: 0.000019  loss: 3.2067 (3.3921)  time: 0.4653  data: 0.0004  max mem: 19734
Epoch: [17]  [ 410/1251]  eta: 0:07:08  lr: 0.000019  loss: 3.2769 (3.3927)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [17]  [ 420/1251]  eta: 0:07:02  lr: 0.000019  loss: 3.4212 (3.3919)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [17]  [ 430/1251]  eta: 0:06:56  lr: 0.000019  loss: 3.4506 (3.3935)  time: 0.4514  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4065, ratio_loss=0.0063, pruning_loss=0.1393, mse_loss=0.5428
Epoch: [17]  [ 440/1251]  eta: 0:06:50  lr: 0.000019  loss: 3.4628 (3.3936)  time: 0.4509  data: 0.0005  max mem: 19734
Epoch: [17]  [ 450/1251]  eta: 0:06:44  lr: 0.000019  loss: 3.4628 (3.3965)  time: 0.4544  data: 0.0006  max mem: 19734
Epoch: [17]  [ 460/1251]  eta: 0:06:38  lr: 0.000019  loss: 3.5221 (3.3997)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [17]  [ 470/1251]  eta: 0:06:32  lr: 0.000019  loss: 3.5221 (3.3986)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [17]  [ 480/1251]  eta: 0:06:26  lr: 0.000019  loss: 3.6566 (3.3997)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [17]  [ 490/1251]  eta: 0:06:21  lr: 0.000019  loss: 3.6566 (3.4034)  time: 0.4661  data: 0.0004  max mem: 19734
Epoch: [17]  [ 500/1251]  eta: 0:06:16  lr: 0.000019  loss: 3.7142 (3.4091)  time: 0.5002  data: 0.0005  max mem: 19734
Epoch: [17]  [ 510/1251]  eta: 0:06:11  lr: 0.000019  loss: 3.6861 (3.4072)  time: 0.5101  data: 0.0004  max mem: 19734
Epoch: [17]  [ 520/1251]  eta: 0:06:06  lr: 0.000019  loss: 3.8152 (3.4140)  time: 0.4740  data: 0.0004  max mem: 19734
Epoch: [17]  [ 530/1251]  eta: 0:06:00  lr: 0.000019  loss: 3.3477 (3.4047)  time: 0.4550  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4295, ratio_loss=0.0061, pruning_loss=0.1393, mse_loss=0.5740
Epoch: [17]  [ 540/1251]  eta: 0:05:55  lr: 0.000019  loss: 3.3477 (3.4095)  time: 0.4645  data: 0.0004  max mem: 19734
Epoch: [17]  [ 550/1251]  eta: 0:05:49  lr: 0.000019  loss: 3.7336 (3.4160)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [17]  [ 560/1251]  eta: 0:05:43  lr: 0.000019  loss: 3.7981 (3.4200)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [17]  [ 570/1251]  eta: 0:05:38  lr: 0.000019  loss: 3.6878 (3.4177)  time: 0.4551  data: 0.0006  max mem: 19734
Epoch: [17]  [ 580/1251]  eta: 0:05:33  lr: 0.000019  loss: 3.3477 (3.4183)  time: 0.4548  data: 0.0006  max mem: 19734
Epoch: [17]  [ 590/1251]  eta: 0:05:27  lr: 0.000019  loss: 3.3477 (3.4130)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [17]  [ 600/1251]  eta: 0:05:22  lr: 0.000019  loss: 3.1397 (3.4142)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [17]  [ 610/1251]  eta: 0:05:16  lr: 0.000019  loss: 3.5784 (3.4159)  time: 0.4566  data: 0.0006  max mem: 19734
Epoch: [17]  [ 620/1251]  eta: 0:05:11  lr: 0.000019  loss: 3.3646 (3.4115)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [17]  [ 630/1251]  eta: 0:05:06  lr: 0.000019  loss: 3.4529 (3.4168)  time: 0.4575  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4509, ratio_loss=0.0061, pruning_loss=0.1358, mse_loss=0.5329
Epoch: [17]  [ 640/1251]  eta: 0:05:01  lr: 0.000019  loss: 3.6204 (3.4163)  time: 0.4874  data: 0.0004  max mem: 19734
Epoch: [17]  [ 650/1251]  eta: 0:04:56  lr: 0.000019  loss: 3.5066 (3.4153)  time: 0.5176  data: 0.0004  max mem: 19734
Epoch: [17]  [ 660/1251]  eta: 0:04:51  lr: 0.000019  loss: 3.6272 (3.4181)  time: 0.4861  data: 0.0004  max mem: 19734
Epoch: [17]  [ 670/1251]  eta: 0:04:46  lr: 0.000019  loss: 3.5950 (3.4177)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [17]  [ 680/1251]  eta: 0:04:41  lr: 0.000019  loss: 3.5784 (3.4202)  time: 0.4519  data: 0.0005  max mem: 19734
Epoch: [17]  [ 690/1251]  eta: 0:04:35  lr: 0.000019  loss: 3.6127 (3.4189)  time: 0.4628  data: 0.0006  max mem: 19734
Epoch: [17]  [ 700/1251]  eta: 0:04:30  lr: 0.000019  loss: 3.2188 (3.4174)  time: 0.4641  data: 0.0004  max mem: 19734
Epoch: [17]  [ 710/1251]  eta: 0:04:25  lr: 0.000019  loss: 3.3190 (3.4211)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [17]  [ 720/1251]  eta: 0:04:20  lr: 0.000019  loss: 3.7783 (3.4235)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [17]  [ 730/1251]  eta: 0:04:15  lr: 0.000019  loss: 3.5730 (3.4235)  time: 0.4530  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4351, ratio_loss=0.0061, pruning_loss=0.1381, mse_loss=0.5595
Epoch: [17]  [ 740/1251]  eta: 0:04:10  lr: 0.000019  loss: 3.6784 (3.4280)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [17]  [ 750/1251]  eta: 0:04:04  lr: 0.000019  loss: 3.6748 (3.4269)  time: 0.4510  data: 0.0005  max mem: 19734
Epoch: [17]  [ 760/1251]  eta: 0:03:59  lr: 0.000019  loss: 3.2293 (3.4251)  time: 0.4506  data: 0.0005  max mem: 19734
Epoch: [17]  [ 770/1251]  eta: 0:03:54  lr: 0.000019  loss: 3.2137 (3.4215)  time: 0.4522  data: 0.0005  max mem: 19734
Epoch: [17]  [ 780/1251]  eta: 0:03:49  lr: 0.000019  loss: 3.5557 (3.4242)  time: 0.4665  data: 0.0004  max mem: 19734
Epoch: [17]  [ 790/1251]  eta: 0:03:44  lr: 0.000019  loss: 3.6634 (3.4250)  time: 0.4835  data: 0.0004  max mem: 19734
Epoch: [17]  [ 800/1251]  eta: 0:03:40  lr: 0.000019  loss: 3.6796 (3.4266)  time: 0.4932  data: 0.0004  max mem: 19734
Epoch: [17]  [ 810/1251]  eta: 0:03:34  lr: 0.000019  loss: 3.6464 (3.4250)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [17]  [ 820/1251]  eta: 0:03:29  lr: 0.000019  loss: 3.3934 (3.4237)  time: 0.4535  data: 0.0004  max mem: 19734
Epoch: [17]  [ 830/1251]  eta: 0:03:24  lr: 0.000019  loss: 3.3825 (3.4239)  time: 0.4603  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3960, ratio_loss=0.0060, pruning_loss=0.1387, mse_loss=0.5542
Epoch: [17]  [ 840/1251]  eta: 0:03:19  lr: 0.000019  loss: 3.5947 (3.4268)  time: 0.4611  data: 0.0004  max mem: 19734
Epoch: [17]  [ 850/1251]  eta: 0:03:14  lr: 0.000019  loss: 3.4168 (3.4231)  time: 0.4525  data: 0.0006  max mem: 19734
Epoch: [17]  [ 860/1251]  eta: 0:03:09  lr: 0.000019  loss: 3.1349 (3.4237)  time: 0.4520  data: 0.0006  max mem: 19734
Epoch: [17]  [ 870/1251]  eta: 0:03:04  lr: 0.000019  loss: 3.3537 (3.4237)  time: 0.4513  data: 0.0004  max mem: 19734
Epoch: [17]  [ 880/1251]  eta: 0:02:59  lr: 0.000019  loss: 3.3850 (3.4235)  time: 0.4515  data: 0.0004  max mem: 19734
Epoch: [17]  [ 890/1251]  eta: 0:02:54  lr: 0.000019  loss: 3.5670 (3.4231)  time: 0.4510  data: 0.0005  max mem: 19734
Epoch: [17]  [ 900/1251]  eta: 0:02:49  lr: 0.000019  loss: 3.4728 (3.4213)  time: 0.4516  data: 0.0005  max mem: 19734
Epoch: [17]  [ 910/1251]  eta: 0:02:44  lr: 0.000019  loss: 3.6662 (3.4230)  time: 0.4541  data: 0.0009  max mem: 19734
Epoch: [17]  [ 920/1251]  eta: 0:02:40  lr: 0.000019  loss: 3.7249 (3.4266)  time: 0.4624  data: 0.0009  max mem: 19734
Epoch: [17]  [ 930/1251]  eta: 0:02:35  lr: 0.000019  loss: 3.7027 (3.4244)  time: 0.4802  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3962, ratio_loss=0.0061, pruning_loss=0.1385, mse_loss=0.5295
Epoch: [17]  [ 940/1251]  eta: 0:02:30  lr: 0.000019  loss: 3.2958 (3.4244)  time: 0.5130  data: 0.0004  max mem: 19734
Epoch: [17]  [ 950/1251]  eta: 0:02:25  lr: 0.000019  loss: 3.5331 (3.4229)  time: 0.4944  data: 0.0004  max mem: 19734
Epoch: [17]  [ 960/1251]  eta: 0:02:20  lr: 0.000019  loss: 3.5331 (3.4247)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [17]  [ 970/1251]  eta: 0:02:15  lr: 0.000019  loss: 3.4335 (3.4240)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [17]  [ 980/1251]  eta: 0:02:10  lr: 0.000019  loss: 3.4895 (3.4262)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [17]  [ 990/1251]  eta: 0:02:06  lr: 0.000019  loss: 3.5926 (3.4282)  time: 0.4600  data: 0.0004  max mem: 19734
Epoch: [17]  [1000/1251]  eta: 0:02:01  lr: 0.000019  loss: 3.7927 (3.4318)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [17]  [1010/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.8856 (3.4348)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [17]  [1020/1251]  eta: 0:01:51  lr: 0.000019  loss: 3.7613 (3.4349)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [17]  [1030/1251]  eta: 0:01:46  lr: 0.000019  loss: 3.4651 (3.4352)  time: 0.4528  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5051, ratio_loss=0.0061, pruning_loss=0.1362, mse_loss=0.5416
Epoch: [17]  [1040/1251]  eta: 0:01:41  lr: 0.000019  loss: 3.4039 (3.4348)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [17]  [1050/1251]  eta: 0:01:36  lr: 0.000019  loss: 3.4492 (3.4360)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [17]  [1060/1251]  eta: 0:01:31  lr: 0.000019  loss: 3.6304 (3.4364)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [17]  [1070/1251]  eta: 0:01:27  lr: 0.000019  loss: 3.7238 (3.4394)  time: 0.4651  data: 0.0004  max mem: 19734
Epoch: [17]  [1080/1251]  eta: 0:01:22  lr: 0.000019  loss: 3.7182 (3.4408)  time: 0.4949  data: 0.0004  max mem: 19734
Epoch: [17]  [1090/1251]  eta: 0:01:17  lr: 0.000019  loss: 3.3870 (3.4395)  time: 0.4994  data: 0.0004  max mem: 19734
Epoch: [17]  [1100/1251]  eta: 0:01:12  lr: 0.000019  loss: 3.3446 (3.4411)  time: 0.4693  data: 0.0005  max mem: 19734
Epoch: [17]  [1110/1251]  eta: 0:01:07  lr: 0.000019  loss: 3.6958 (3.4413)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [17]  [1120/1251]  eta: 0:01:02  lr: 0.000019  loss: 3.7251 (3.4430)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [17]  [1130/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.7251 (3.4438)  time: 0.4633  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5075, ratio_loss=0.0059, pruning_loss=0.1343, mse_loss=0.5472
Epoch: [17]  [1140/1251]  eta: 0:00:53  lr: 0.000019  loss: 3.6602 (3.4436)  time: 0.4638  data: 0.0005  max mem: 19734
Epoch: [17]  [1150/1251]  eta: 0:00:48  lr: 0.000019  loss: 3.6844 (3.4454)  time: 0.4552  data: 0.0008  max mem: 19734
Epoch: [17]  [1160/1251]  eta: 0:00:43  lr: 0.000019  loss: 3.6017 (3.4442)  time: 0.4549  data: 0.0007  max mem: 19734
Epoch: [17]  [1170/1251]  eta: 0:00:38  lr: 0.000019  loss: 3.6200 (3.4459)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [17]  [1180/1251]  eta: 0:00:34  lr: 0.000019  loss: 3.6633 (3.4479)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [17]  [1190/1251]  eta: 0:00:29  lr: 0.000019  loss: 3.6279 (3.4478)  time: 0.4520  data: 0.0007  max mem: 19734
Epoch: [17]  [1200/1251]  eta: 0:00:24  lr: 0.000019  loss: 3.5889 (3.4496)  time: 0.4486  data: 0.0005  max mem: 19734
Epoch: [17]  [1210/1251]  eta: 0:00:19  lr: 0.000019  loss: 3.6524 (3.4485)  time: 0.4469  data: 0.0002  max mem: 19734
Epoch: [17]  [1220/1251]  eta: 0:00:14  lr: 0.000019  loss: 3.5681 (3.4488)  time: 0.4648  data: 0.0002  max mem: 19734
Epoch: [17]  [1230/1251]  eta: 0:00:10  lr: 0.000019  loss: 3.2704 (3.4477)  time: 0.4899  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4642, ratio_loss=0.0058, pruning_loss=0.1362, mse_loss=0.5497
Epoch: [17]  [1240/1251]  eta: 0:00:05  lr: 0.000019  loss: 3.5454 (3.4495)  time: 0.4717  data: 0.0002  max mem: 19734
Epoch: [17]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.7934 (3.4526)  time: 0.4474  data: 0.0002  max mem: 19734
Epoch: [17] Total time: 0:09:59 (0.4789 s / it)
Averaged stats: lr: 0.000019  loss: 3.7934 (3.4518)
Test:  [  0/261]  eta: 1:31:07  loss: 0.7372 (0.7372)  acc1: 82.2917 (82.2917)  acc5: 95.8333 (95.8333)  time: 20.9479  data: 20.6168  max mem: 19734
Test:  [ 10/261]  eta: 0:09:26  loss: 0.7372 (0.7366)  acc1: 83.8542 (83.6648)  acc5: 96.3542 (96.2595)  time: 2.2588  data: 2.1618  max mem: 19734
Test:  [ 20/261]  eta: 0:05:07  loss: 0.9101 (0.9089)  acc1: 79.1667 (79.0675)  acc5: 94.2708 (94.5437)  time: 0.2937  data: 0.2057  max mem: 19734
Test:  [ 30/261]  eta: 0:03:29  loss: 0.8264 (0.8336)  acc1: 82.2917 (81.8380)  acc5: 93.7500 (95.0773)  time: 0.1656  data: 0.0574  max mem: 19734
Test:  [ 40/261]  eta: 0:03:14  loss: 0.6274 (0.8033)  acc1: 86.9792 (82.7871)  acc5: 96.8750 (95.3760)  time: 0.4587  data: 0.3546  max mem: 19734
Test:  [ 50/261]  eta: 0:02:42  loss: 0.9327 (0.8647)  acc1: 77.6042 (81.0049)  acc5: 95.3125 (94.9755)  time: 0.5527  data: 0.4328  max mem: 19734
Test:  [ 60/261]  eta: 0:02:13  loss: 1.0033 (0.8754)  acc1: 76.0417 (80.4986)  acc5: 94.2708 (95.0307)  time: 0.2280  data: 0.0938  max mem: 19734
Test:  [ 70/261]  eta: 0:02:14  loss: 0.9598 (0.8747)  acc1: 76.0417 (80.0029)  acc5: 96.3542 (95.2538)  time: 0.5338  data: 0.4239  max mem: 19734
Test:  [ 80/261]  eta: 0:01:54  loss: 0.8428 (0.8760)  acc1: 78.6458 (80.1183)  acc5: 96.3542 (95.3189)  time: 0.5259  data: 0.4264  max mem: 19734
Test:  [ 90/261]  eta: 0:01:38  loss: 0.8237 (0.8621)  acc1: 82.8125 (80.5231)  acc5: 96.3542 (95.4499)  time: 0.1266  data: 0.0169  max mem: 19734
Test:  [100/261]  eta: 0:01:40  loss: 0.8299 (0.8658)  acc1: 83.3333 (80.4455)  acc5: 95.3125 (95.4827)  time: 0.6133  data: 0.5033  max mem: 19734
Test:  [110/261]  eta: 0:01:28  loss: 0.9005 (0.8874)  acc1: 76.5625 (79.9550)  acc5: 93.7500 (95.1858)  time: 0.6480  data: 0.5001  max mem: 19734
Test:  [120/261]  eta: 0:01:17  loss: 1.1911 (0.9265)  acc1: 71.3542 (79.0935)  acc5: 90.1042 (94.6927)  time: 0.1775  data: 0.0108  max mem: 19734
Test:  [130/261]  eta: 0:01:08  loss: 1.4147 (0.9737)  acc1: 68.2292 (78.0852)  acc5: 88.0208 (94.0999)  time: 0.1556  data: 0.0143  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: 1.3303 (1.0019)  acc1: 68.2292 (77.3936)  acc5: 89.0625 (93.8017)  time: 0.4078  data: 0.2801  max mem: 19734
Test:  [150/261]  eta: 0:00:56  loss: 1.2784 (1.0067)  acc1: 71.8750 (77.4076)  acc5: 91.1458 (93.6879)  time: 0.4148  data: 0.2770  max mem: 19734
Test:  [160/261]  eta: 0:00:48  loss: 1.0257 (1.0268)  acc1: 75.5208 (77.0704)  acc5: 92.1875 (93.3942)  time: 0.1512  data: 0.0118  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.3423 (1.0598)  acc1: 66.1458 (76.2396)  acc5: 89.5833 (92.9977)  time: 0.3948  data: 0.2858  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4859 (1.0779)  acc1: 64.0625 (75.8259)  acc5: 89.0625 (92.8234)  time: 0.3702  data: 0.2850  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4452 (1.0919)  acc1: 65.6250 (75.5290)  acc5: 90.1042 (92.6538)  time: 0.1060  data: 0.0382  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3885 (1.1084)  acc1: 70.3125 (75.2280)  acc5: 89.0625 (92.4052)  time: 0.1255  data: 0.0617  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3885 (1.1225)  acc1: 69.7917 (74.9630)  acc5: 88.0208 (92.1850)  time: 0.0899  data: 0.0285  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4432 (1.1409)  acc1: 67.7083 (74.5310)  acc5: 88.0208 (92.0013)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4821 (1.1511)  acc1: 67.7083 (74.2762)  acc5: 89.0625 (91.8876)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3365 (1.1601)  acc1: 68.2292 (74.0664)  acc5: 90.6250 (91.8115)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.1112 (1.1541)  acc1: 75.0000 (74.2032)  acc5: 92.7083 (91.9219)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0059 (1.1539)  acc1: 76.5625 (74.2220)  acc5: 94.7917 (91.9720)  time: 0.0596  data: 0.0002  max mem: 19734
Test: Total time: 0:01:31 (0.3523 s / it)
* Acc@1 74.222 Acc@5 91.972 loss 1.154
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.25%
Epoch: [18]  [   0/1251]  eta: 6:02:02  lr: 0.000019  loss: 3.2773 (3.2773)  time: 17.3642  data: 8.2505  max mem: 19734
Epoch: [18]  [  10/1251]  eta: 0:44:15  lr: 0.000019  loss: 3.5486 (3.4786)  time: 2.1395  data: 0.7884  max mem: 19734
Epoch: [18]  [  20/1251]  eta: 0:27:32  lr: 0.000019  loss: 3.5486 (3.4525)  time: 0.5411  data: 0.0215  max mem: 19734
Epoch: [18]  [  30/1251]  eta: 0:21:33  lr: 0.000019  loss: 3.5347 (3.3976)  time: 0.4651  data: 0.0006  max mem: 19734
Epoch: [18]  [  40/1251]  eta: 0:18:25  lr: 0.000019  loss: 3.5347 (3.4301)  time: 0.4624  data: 0.0006  max mem: 19734
Epoch: [18]  [  50/1251]  eta: 0:16:29  lr: 0.000019  loss: 3.2138 (3.3285)  time: 0.4596  data: 0.0006  max mem: 19734
Epoch: [18]  [  60/1251]  eta: 0:15:10  lr: 0.000019  loss: 3.5952 (3.3817)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [18]  [  70/1251]  eta: 0:14:11  lr: 0.000019  loss: 3.6768 (3.3713)  time: 0.4586  data: 0.0005  max mem: 19734
Epoch: [18]  [  80/1251]  eta: 0:13:26  lr: 0.000019  loss: 3.1493 (3.3631)  time: 0.4590  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4140, ratio_loss=0.0062, pruning_loss=0.1389, mse_loss=0.5458
Epoch: [18]  [  90/1251]  eta: 0:12:50  lr: 0.000019  loss: 3.5493 (3.3809)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [18]  [ 100/1251]  eta: 0:12:25  lr: 0.000019  loss: 3.5493 (3.3760)  time: 0.4817  data: 0.0005  max mem: 19734
Epoch: [18]  [ 110/1251]  eta: 0:12:04  lr: 0.000019  loss: 3.6962 (3.4044)  time: 0.5025  data: 0.0005  max mem: 19734
Epoch: [18]  [ 120/1251]  eta: 0:11:45  lr: 0.000019  loss: 3.6962 (3.4178)  time: 0.5004  data: 0.0005  max mem: 19734
Epoch: [18]  [ 130/1251]  eta: 0:11:24  lr: 0.000019  loss: 3.4421 (3.4183)  time: 0.4789  data: 0.0005  max mem: 19734
Epoch: [18]  [ 140/1251]  eta: 0:11:08  lr: 0.000019  loss: 3.6565 (3.4371)  time: 0.4714  data: 0.0005  max mem: 19734
Epoch: [18]  [ 150/1251]  eta: 0:10:51  lr: 0.000019  loss: 3.6688 (3.4362)  time: 0.4695  data: 0.0004  max mem: 19734
Epoch: [18]  [ 160/1251]  eta: 0:10:36  lr: 0.000019  loss: 3.3944 (3.4221)  time: 0.4561  data: 0.0004  max mem: 19734
Epoch: [18]  [ 170/1251]  eta: 0:10:23  lr: 0.000019  loss: 3.1256 (3.3935)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [18]  [ 180/1251]  eta: 0:10:10  lr: 0.000019  loss: 3.0120 (3.3857)  time: 0.4572  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3748, ratio_loss=0.0061, pruning_loss=0.1383, mse_loss=0.5455
Epoch: [18]  [ 190/1251]  eta: 0:09:58  lr: 0.000019  loss: 3.1712 (3.3824)  time: 0.4564  data: 0.0005  max mem: 19734
Epoch: [18]  [ 200/1251]  eta: 0:09:46  lr: 0.000019  loss: 3.5630 (3.3911)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [18]  [ 210/1251]  eta: 0:09:36  lr: 0.000019  loss: 3.5630 (3.3900)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [18]  [ 220/1251]  eta: 0:09:26  lr: 0.000019  loss: 3.4609 (3.3966)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [18]  [ 230/1251]  eta: 0:09:16  lr: 0.000019  loss: 3.5866 (3.3961)  time: 0.4565  data: 0.0005  max mem: 19734
Epoch: [18]  [ 240/1251]  eta: 0:09:07  lr: 0.000019  loss: 3.7111 (3.4018)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [18]  [ 250/1251]  eta: 0:09:00  lr: 0.000019  loss: 3.7375 (3.4102)  time: 0.4779  data: 0.0004  max mem: 19734
Epoch: [18]  [ 260/1251]  eta: 0:08:54  lr: 0.000019  loss: 3.5945 (3.4054)  time: 0.5137  data: 0.0004  max mem: 19734
Epoch: [18]  [ 270/1251]  eta: 0:08:46  lr: 0.000019  loss: 3.5388 (3.4083)  time: 0.4915  data: 0.0004  max mem: 19734
Epoch: [18]  [ 280/1251]  eta: 0:08:38  lr: 0.000019  loss: 3.5972 (3.4121)  time: 0.4586  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4342, ratio_loss=0.0057, pruning_loss=0.1376, mse_loss=0.5367
Epoch: [18]  [ 290/1251]  eta: 0:08:31  lr: 0.000019  loss: 3.6543 (3.4225)  time: 0.4731  data: 0.0007  max mem: 19734
Epoch: [18]  [ 300/1251]  eta: 0:08:23  lr: 0.000019  loss: 3.6163 (3.4184)  time: 0.4707  data: 0.0005  max mem: 19734
Epoch: [18]  [ 310/1251]  eta: 0:08:16  lr: 0.000019  loss: 3.3365 (3.4117)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [18]  [ 320/1251]  eta: 0:08:08  lr: 0.000019  loss: 3.1208 (3.4112)  time: 0.4578  data: 0.0005  max mem: 19734
Epoch: [18]  [ 330/1251]  eta: 0:08:01  lr: 0.000019  loss: 3.6092 (3.4169)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [18]  [ 340/1251]  eta: 0:07:54  lr: 0.000019  loss: 3.5595 (3.4155)  time: 0.4593  data: 0.0005  max mem: 19734
Epoch: [18]  [ 350/1251]  eta: 0:07:47  lr: 0.000019  loss: 3.5039 (3.4166)  time: 0.4593  data: 0.0004  max mem: 19734
Epoch: [18]  [ 360/1251]  eta: 0:07:41  lr: 0.000019  loss: 3.1964 (3.4038)  time: 0.4607  data: 0.0004  max mem: 19734
Epoch: [18]  [ 370/1251]  eta: 0:07:34  lr: 0.000019  loss: 3.2363 (3.4059)  time: 0.4591  data: 0.0005  max mem: 19734
Epoch: [18]  [ 380/1251]  eta: 0:07:28  lr: 0.000019  loss: 3.6014 (3.4051)  time: 0.4553  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3552, ratio_loss=0.0063, pruning_loss=0.1399, mse_loss=0.5371
Epoch: [18]  [ 390/1251]  eta: 0:07:22  lr: 0.000019  loss: 3.5942 (3.4065)  time: 0.4740  data: 0.0005  max mem: 19734
Epoch: [18]  [ 400/1251]  eta: 0:07:17  lr: 0.000019  loss: 3.6131 (3.4145)  time: 0.4995  data: 0.0005  max mem: 19734
Epoch: [18]  [ 410/1251]  eta: 0:07:11  lr: 0.000019  loss: 3.5917 (3.4126)  time: 0.5036  data: 0.0005  max mem: 19734
Epoch: [18]  [ 420/1251]  eta: 0:07:05  lr: 0.000019  loss: 3.4926 (3.4157)  time: 0.4768  data: 0.0006  max mem: 19734
Epoch: [18]  [ 430/1251]  eta: 0:06:59  lr: 0.000019  loss: 3.5125 (3.4149)  time: 0.4522  data: 0.0006  max mem: 19734
Epoch: [18]  [ 440/1251]  eta: 0:06:53  lr: 0.000019  loss: 3.5125 (3.4164)  time: 0.4626  data: 0.0005  max mem: 19734
Epoch: [18]  [ 450/1251]  eta: 0:06:47  lr: 0.000019  loss: 3.5760 (3.4155)  time: 0.4630  data: 0.0005  max mem: 19734
Epoch: [18]  [ 460/1251]  eta: 0:06:41  lr: 0.000019  loss: 3.4075 (3.4135)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [18]  [ 470/1251]  eta: 0:06:35  lr: 0.000019  loss: 3.4871 (3.4172)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [18]  [ 480/1251]  eta: 0:06:29  lr: 0.000019  loss: 3.5260 (3.4145)  time: 0.4523  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4221, ratio_loss=0.0057, pruning_loss=0.1383, mse_loss=0.5629
Epoch: [18]  [ 490/1251]  eta: 0:06:23  lr: 0.000019  loss: 3.6032 (3.4198)  time: 0.4524  data: 0.0006  max mem: 19734
Epoch: [18]  [ 500/1251]  eta: 0:06:17  lr: 0.000019  loss: 3.7466 (3.4244)  time: 0.4514  data: 0.0005  max mem: 19734
Epoch: [18]  [ 510/1251]  eta: 0:06:11  lr: 0.000019  loss: 3.6396 (3.4230)  time: 0.4520  data: 0.0005  max mem: 19734
Epoch: [18]  [ 520/1251]  eta: 0:06:06  lr: 0.000019  loss: 3.2988 (3.4207)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [18]  [ 530/1251]  eta: 0:06:00  lr: 0.000019  loss: 3.1339 (3.4150)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [18]  [ 540/1251]  eta: 0:05:55  lr: 0.000019  loss: 3.1472 (3.4128)  time: 0.4864  data: 0.0004  max mem: 19734
Epoch: [18]  [ 550/1251]  eta: 0:05:51  lr: 0.000019  loss: 3.5425 (3.4149)  time: 0.5263  data: 0.0004  max mem: 19734
Epoch: [18]  [ 560/1251]  eta: 0:05:45  lr: 0.000019  loss: 3.7482 (3.4147)  time: 0.4930  data: 0.0004  max mem: 19734
Epoch: [18]  [ 570/1251]  eta: 0:05:40  lr: 0.000019  loss: 3.5901 (3.4163)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [18]  [ 580/1251]  eta: 0:05:34  lr: 0.000019  loss: 3.5901 (3.4160)  time: 0.4564  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3990, ratio_loss=0.0057, pruning_loss=0.1387, mse_loss=0.5464
Epoch: [18]  [ 590/1251]  eta: 0:05:29  lr: 0.000019  loss: 3.6114 (3.4193)  time: 0.4647  data: 0.0004  max mem: 19734
Epoch: [18]  [ 600/1251]  eta: 0:05:23  lr: 0.000019  loss: 3.5638 (3.4154)  time: 0.4629  data: 0.0004  max mem: 19734
Epoch: [18]  [ 610/1251]  eta: 0:05:18  lr: 0.000019  loss: 3.3204 (3.4158)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [18]  [ 620/1251]  eta: 0:05:13  lr: 0.000019  loss: 3.3204 (3.4156)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [18]  [ 630/1251]  eta: 0:05:07  lr: 0.000019  loss: 3.6553 (3.4170)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [18]  [ 640/1251]  eta: 0:05:02  lr: 0.000019  loss: 3.6148 (3.4188)  time: 0.4606  data: 0.0005  max mem: 19734
Epoch: [18]  [ 650/1251]  eta: 0:04:57  lr: 0.000019  loss: 3.5205 (3.4149)  time: 0.4580  data: 0.0005  max mem: 19734
Epoch: [18]  [ 660/1251]  eta: 0:04:51  lr: 0.000019  loss: 3.3058 (3.4143)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [18]  [ 670/1251]  eta: 0:04:46  lr: 0.000019  loss: 3.4041 (3.4111)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [18]  [ 680/1251]  eta: 0:04:41  lr: 0.000019  loss: 3.6348 (3.4145)  time: 0.4765  data: 0.0008  max mem: 19734
loss info: cls_loss=3.3591, ratio_loss=0.0056, pruning_loss=0.1386, mse_loss=0.5434
Epoch: [18]  [ 690/1251]  eta: 0:04:36  lr: 0.000019  loss: 3.7212 (3.4134)  time: 0.4867  data: 0.0008  max mem: 19734
Epoch: [18]  [ 700/1251]  eta: 0:04:31  lr: 0.000019  loss: 3.5573 (3.4148)  time: 0.4945  data: 0.0004  max mem: 19734
Epoch: [18]  [ 710/1251]  eta: 0:04:26  lr: 0.000019  loss: 3.5573 (3.4170)  time: 0.4846  data: 0.0005  max mem: 19734
Epoch: [18]  [ 720/1251]  eta: 0:04:21  lr: 0.000019  loss: 3.5743 (3.4199)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [18]  [ 730/1251]  eta: 0:04:16  lr: 0.000019  loss: 3.4368 (3.4175)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [18]  [ 740/1251]  eta: 0:04:11  lr: 0.000019  loss: 3.3374 (3.4183)  time: 0.4646  data: 0.0004  max mem: 19734
Epoch: [18]  [ 750/1251]  eta: 0:04:05  lr: 0.000019  loss: 3.5953 (3.4215)  time: 0.4647  data: 0.0004  max mem: 19734
Epoch: [18]  [ 760/1251]  eta: 0:04:00  lr: 0.000019  loss: 3.6116 (3.4214)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [18]  [ 770/1251]  eta: 0:03:55  lr: 0.000019  loss: 3.0453 (3.4160)  time: 0.4542  data: 0.0006  max mem: 19734
Epoch: [18]  [ 780/1251]  eta: 0:03:50  lr: 0.000019  loss: 3.3943 (3.4152)  time: 0.4551  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4091, ratio_loss=0.0056, pruning_loss=0.1367, mse_loss=0.5435
Epoch: [18]  [ 790/1251]  eta: 0:03:45  lr: 0.000019  loss: 3.5665 (3.4175)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [18]  [ 800/1251]  eta: 0:03:40  lr: 0.000019  loss: 3.8296 (3.4213)  time: 0.4537  data: 0.0007  max mem: 19734
Epoch: [18]  [ 810/1251]  eta: 0:03:35  lr: 0.000019  loss: 3.6794 (3.4212)  time: 0.4560  data: 0.0007  max mem: 19734
Epoch: [18]  [ 820/1251]  eta: 0:03:30  lr: 0.000019  loss: 3.6070 (3.4202)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [18]  [ 830/1251]  eta: 0:03:25  lr: 0.000019  loss: 3.6684 (3.4220)  time: 0.4821  data: 0.0005  max mem: 19734
Epoch: [18]  [ 840/1251]  eta: 0:03:20  lr: 0.000019  loss: 3.4438 (3.4212)  time: 0.5143  data: 0.0004  max mem: 19734
Epoch: [18]  [ 850/1251]  eta: 0:03:15  lr: 0.000019  loss: 3.4736 (3.4210)  time: 0.4861  data: 0.0004  max mem: 19734
Epoch: [18]  [ 860/1251]  eta: 0:03:10  lr: 0.000019  loss: 3.6072 (3.4198)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [18]  [ 870/1251]  eta: 0:03:05  lr: 0.000019  loss: 3.6481 (3.4185)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [18]  [ 880/1251]  eta: 0:03:00  lr: 0.000019  loss: 3.4492 (3.4166)  time: 0.4539  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3842, ratio_loss=0.0061, pruning_loss=0.1376, mse_loss=0.5253
Epoch: [18]  [ 890/1251]  eta: 0:02:55  lr: 0.000019  loss: 3.4492 (3.4143)  time: 0.4674  data: 0.0005  max mem: 19734
Epoch: [18]  [ 900/1251]  eta: 0:02:50  lr: 0.000019  loss: 3.4526 (3.4132)  time: 0.4702  data: 0.0005  max mem: 19734
Epoch: [18]  [ 910/1251]  eta: 0:02:45  lr: 0.000019  loss: 3.5670 (3.4116)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [18]  [ 920/1251]  eta: 0:02:40  lr: 0.000019  loss: 3.5670 (3.4117)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [18]  [ 930/1251]  eta: 0:02:35  lr: 0.000019  loss: 3.5953 (3.4132)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [18]  [ 940/1251]  eta: 0:02:30  lr: 0.000019  loss: 3.6641 (3.4141)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [18]  [ 950/1251]  eta: 0:02:25  lr: 0.000019  loss: 3.6011 (3.4154)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [18]  [ 960/1251]  eta: 0:02:21  lr: 0.000019  loss: 3.5724 (3.4162)  time: 0.4547  data: 0.0006  max mem: 19734
Epoch: [18]  [ 970/1251]  eta: 0:02:16  lr: 0.000019  loss: 3.5148 (3.4154)  time: 0.4809  data: 0.0011  max mem: 19734
Epoch: [18]  [ 980/1251]  eta: 0:02:11  lr: 0.000019  loss: 3.1732 (3.4127)  time: 0.5123  data: 0.0009  max mem: 19734
loss info: cls_loss=3.3620, ratio_loss=0.0060, pruning_loss=0.1384, mse_loss=0.5560
Epoch: [18]  [ 990/1251]  eta: 0:02:06  lr: 0.000019  loss: 3.1732 (3.4133)  time: 0.4953  data: 0.0005  max mem: 19734
Epoch: [18]  [1000/1251]  eta: 0:02:01  lr: 0.000019  loss: 3.6482 (3.4145)  time: 0.4648  data: 0.0005  max mem: 19734
Epoch: [18]  [1010/1251]  eta: 0:01:56  lr: 0.000019  loss: 3.3929 (3.4133)  time: 0.4577  data: 0.0005  max mem: 19734
Epoch: [18]  [1020/1251]  eta: 0:01:51  lr: 0.000019  loss: 3.4333 (3.4135)  time: 0.4580  data: 0.0005  max mem: 19734
Epoch: [18]  [1030/1251]  eta: 0:01:46  lr: 0.000019  loss: 3.6209 (3.4137)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [18]  [1040/1251]  eta: 0:01:42  lr: 0.000019  loss: 3.6386 (3.4153)  time: 0.4658  data: 0.0005  max mem: 19734
Epoch: [18]  [1050/1251]  eta: 0:01:37  lr: 0.000019  loss: 3.6650 (3.4171)  time: 0.4653  data: 0.0005  max mem: 19734
Epoch: [18]  [1060/1251]  eta: 0:01:32  lr: 0.000019  loss: 3.6414 (3.4184)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [18]  [1070/1251]  eta: 0:01:27  lr: 0.000019  loss: 3.4724 (3.4185)  time: 0.4610  data: 0.0005  max mem: 19734
Epoch: [18]  [1080/1251]  eta: 0:01:22  lr: 0.000019  loss: 3.4724 (3.4201)  time: 0.4592  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4693, ratio_loss=0.0059, pruning_loss=0.1343, mse_loss=0.5140
Epoch: [18]  [1090/1251]  eta: 0:01:17  lr: 0.000019  loss: 3.8083 (3.4234)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [18]  [1100/1251]  eta: 0:01:12  lr: 0.000019  loss: 3.8147 (3.4254)  time: 0.4603  data: 0.0005  max mem: 19734
Epoch: [18]  [1110/1251]  eta: 0:01:08  lr: 0.000019  loss: 3.5404 (3.4260)  time: 0.4681  data: 0.0004  max mem: 19734
Epoch: [18]  [1120/1251]  eta: 0:01:03  lr: 0.000019  loss: 3.5047 (3.4261)  time: 0.4846  data: 0.0004  max mem: 19734
Epoch: [18]  [1130/1251]  eta: 0:00:58  lr: 0.000019  loss: 3.4256 (3.4246)  time: 0.5104  data: 0.0004  max mem: 19734
Epoch: [18]  [1140/1251]  eta: 0:00:53  lr: 0.000019  loss: 3.4603 (3.4240)  time: 0.4921  data: 0.0004  max mem: 19734
Epoch: [18]  [1150/1251]  eta: 0:00:48  lr: 0.000019  loss: 3.5937 (3.4243)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [18]  [1160/1251]  eta: 0:00:43  lr: 0.000019  loss: 3.6499 (3.4266)  time: 0.4529  data: 0.0007  max mem: 19734
Epoch: [18]  [1170/1251]  eta: 0:00:39  lr: 0.000019  loss: 3.6499 (3.4261)  time: 0.4513  data: 0.0007  max mem: 19734
Epoch: [18]  [1180/1251]  eta: 0:00:34  lr: 0.000019  loss: 3.3496 (3.4268)  time: 0.4530  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4651, ratio_loss=0.0057, pruning_loss=0.1363, mse_loss=0.5484
Epoch: [18]  [1190/1251]  eta: 0:00:29  lr: 0.000019  loss: 3.4566 (3.4270)  time: 0.4614  data: 0.0008  max mem: 19734
Epoch: [18]  [1200/1251]  eta: 0:00:24  lr: 0.000019  loss: 3.6452 (3.4278)  time: 0.4568  data: 0.0007  max mem: 19734
Epoch: [18]  [1210/1251]  eta: 0:00:19  lr: 0.000019  loss: 3.6073 (3.4261)  time: 0.4458  data: 0.0002  max mem: 19734
Epoch: [18]  [1220/1251]  eta: 0:00:14  lr: 0.000019  loss: 3.4320 (3.4271)  time: 0.4449  data: 0.0002  max mem: 19734
Epoch: [18]  [1230/1251]  eta: 0:00:10  lr: 0.000019  loss: 3.4629 (3.4272)  time: 0.4442  data: 0.0001  max mem: 19734
Epoch: [18]  [1240/1251]  eta: 0:00:05  lr: 0.000019  loss: 3.4494 (3.4263)  time: 0.4448  data: 0.0002  max mem: 19734
Epoch: [18]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.5657 (3.4282)  time: 0.4455  data: 0.0002  max mem: 19734
Epoch: [18] Total time: 0:10:01 (0.4805 s / it)
Averaged stats: lr: 0.000019  loss: 3.5657 (3.4454)
Test:  [  0/261]  eta: 1:28:43  loss: 0.6922 (0.6922)  acc1: 83.8542 (83.8542)  acc5: 95.3125 (95.3125)  time: 20.3965  data: 20.2941  max mem: 19734
Test:  [ 10/261]  eta: 0:09:32  loss: 0.6922 (0.7206)  acc1: 84.8958 (84.0909)  acc5: 96.3542 (96.2595)  time: 2.2825  data: 2.0718  max mem: 19734
Test:  [ 20/261]  eta: 0:05:12  loss: 0.9253 (0.9023)  acc1: 79.1667 (79.0427)  acc5: 93.2292 (94.6429)  time: 0.3401  data: 0.1322  max mem: 19734
Test:  [ 30/261]  eta: 0:03:36  loss: 0.8520 (0.8263)  acc1: 81.7708 (81.7036)  acc5: 93.7500 (95.0101)  time: 0.1943  data: 0.0174  max mem: 19734
Test:  [ 40/261]  eta: 0:03:00  loss: 0.5887 (0.7934)  acc1: 85.9375 (82.8252)  acc5: 96.8750 (95.3760)  time: 0.3145  data: 0.1720  max mem: 19734
Test:  [ 50/261]  eta: 0:02:22  loss: 0.8960 (0.8530)  acc1: 78.6458 (81.0151)  acc5: 94.2708 (94.9449)  time: 0.2727  data: 0.1671  max mem: 19734
Test:  [ 60/261]  eta: 0:01:59  loss: 0.9610 (0.8638)  acc1: 77.0833 (80.5157)  acc5: 94.2708 (94.9710)  time: 0.1423  data: 0.0139  max mem: 19734
Test:  [ 70/261]  eta: 0:01:54  loss: 0.9383 (0.8674)  acc1: 77.0833 (80.0103)  acc5: 95.3125 (95.1731)  time: 0.4048  data: 0.2271  max mem: 19734
Test:  [ 80/261]  eta: 0:01:45  loss: 0.8661 (0.8684)  acc1: 81.2500 (80.1569)  acc5: 96.8750 (95.2611)  time: 0.5392  data: 0.3570  max mem: 19734
Test:  [ 90/261]  eta: 0:01:32  loss: 0.8232 (0.8546)  acc1: 83.3333 (80.5517)  acc5: 95.8333 (95.3526)  time: 0.3272  data: 0.1468  max mem: 19734
Test:  [100/261]  eta: 0:01:23  loss: 0.8232 (0.8570)  acc1: 83.8542 (80.5023)  acc5: 95.3125 (95.3950)  time: 0.2782  data: 0.0958  max mem: 19734
Test:  [110/261]  eta: 0:01:21  loss: 0.9005 (0.8808)  acc1: 77.0833 (80.0160)  acc5: 94.2708 (95.1248)  time: 0.5497  data: 0.3485  max mem: 19734
Test:  [120/261]  eta: 0:01:12  loss: 1.2618 (0.9219)  acc1: 69.2708 (79.0203)  acc5: 89.5833 (94.5851)  time: 0.4717  data: 0.2731  max mem: 19734
Test:  [130/261]  eta: 0:01:04  loss: 1.3962 (0.9674)  acc1: 67.7083 (78.0654)  acc5: 87.5000 (94.0084)  time: 0.2343  data: 0.0287  max mem: 19734
Test:  [140/261]  eta: 0:00:59  loss: 1.3346 (0.9943)  acc1: 67.7083 (77.4232)  acc5: 89.0625 (93.7352)  time: 0.3697  data: 0.1474  max mem: 19734
Test:  [150/261]  eta: 0:00:55  loss: 1.2014 (1.0003)  acc1: 72.9167 (77.3834)  acc5: 90.6250 (93.5810)  time: 0.5199  data: 0.3454  max mem: 19734
Test:  [160/261]  eta: 0:00:48  loss: 1.0171 (1.0196)  acc1: 76.5625 (77.0672)  acc5: 92.1875 (93.3165)  time: 0.3997  data: 0.2222  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: 1.3249 (1.0507)  acc1: 63.0208 (76.2884)  acc5: 88.0208 (92.9946)  time: 0.2003  data: 0.0196  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4973 (1.0681)  acc1: 63.5417 (75.8863)  acc5: 87.5000 (92.8292)  time: 0.2476  data: 0.0723  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3636 (1.0827)  acc1: 67.7083 (75.6190)  acc5: 90.1042 (92.6483)  time: 0.3592  data: 0.1574  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3711 (1.0994)  acc1: 71.3542 (75.2876)  acc5: 88.5417 (92.3896)  time: 0.2937  data: 0.1460  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3711 (1.1140)  acc1: 70.3125 (74.9926)  acc5: 88.0208 (92.1678)  time: 0.2076  data: 0.1140  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4352 (1.1336)  acc1: 65.6250 (74.5192)  acc5: 88.0208 (91.9730)  time: 0.1394  data: 0.0648  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4333 (1.1419)  acc1: 66.1458 (74.2965)  acc5: 89.0625 (91.8899)  time: 0.0703  data: 0.0088  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3969 (1.1512)  acc1: 69.2708 (74.1139)  acc5: 91.1458 (91.8136)  time: 0.0703  data: 0.0088  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0508 (1.1456)  acc1: 75.5208 (74.2447)  acc5: 93.2292 (91.9260)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9929 (1.1443)  acc1: 76.5625 (74.2740)  acc5: 94.7917 (91.9900)  time: 0.0598  data: 0.0001  max mem: 19734
Test: Total time: 0:01:34 (0.3604 s / it)
* Acc@1 74.274 Acc@5 91.990 loss 1.144
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.27%
Epoch: [19]  [   0/1251]  eta: 2:53:54  lr: 0.000019  loss: 3.3479 (3.3479)  time: 8.3407  data: 7.8018  max mem: 19734
Epoch: [19]  [  10/1251]  eta: 0:26:40  lr: 0.000019  loss: 3.3479 (3.4254)  time: 1.2898  data: 0.7236  max mem: 19734
Epoch: [19]  [  20/1251]  eta: 0:19:02  lr: 0.000019  loss: 3.2534 (3.2804)  time: 0.5577  data: 0.0081  max mem: 19734
loss info: cls_loss=3.3729, ratio_loss=0.0060, pruning_loss=0.1394, mse_loss=0.5202
Epoch: [19]  [  30/1251]  eta: 0:15:54  lr: 0.000019  loss: 3.0246 (3.2960)  time: 0.5017  data: 0.0004  max mem: 19734
Epoch: [19]  [  40/1251]  eta: 0:14:10  lr: 0.000019  loss: 3.4063 (3.3132)  time: 0.4658  data: 0.0004  max mem: 19734
Epoch: [19]  [  50/1251]  eta: 0:13:06  lr: 0.000019  loss: 3.4063 (3.2871)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [19]  [  60/1251]  eta: 0:12:22  lr: 0.000019  loss: 3.4201 (3.3196)  time: 0.4617  data: 0.0005  max mem: 19734
Epoch: [19]  [  70/1251]  eta: 0:11:49  lr: 0.000019  loss: 3.6972 (3.3577)  time: 0.4614  data: 0.0005  max mem: 19734
Epoch: [19]  [  80/1251]  eta: 0:11:23  lr: 0.000019  loss: 3.5367 (3.3356)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [19]  [  90/1251]  eta: 0:11:01  lr: 0.000019  loss: 3.4108 (3.3671)  time: 0.4610  data: 0.0004  max mem: 19734
Epoch: [19]  [ 100/1251]  eta: 0:10:43  lr: 0.000019  loss: 3.7276 (3.4113)  time: 0.4601  data: 0.0005  max mem: 19734
Epoch: [19]  [ 110/1251]  eta: 0:10:27  lr: 0.000019  loss: 3.6704 (3.4014)  time: 0.4589  data: 0.0005  max mem: 19734
Epoch: [19]  [ 120/1251]  eta: 0:10:13  lr: 0.000019  loss: 3.5213 (3.4150)  time: 0.4597  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4239, ratio_loss=0.0056, pruning_loss=0.1376, mse_loss=0.5411
Epoch: [19]  [ 130/1251]  eta: 0:10:01  lr: 0.000019  loss: 3.6570 (3.4153)  time: 0.4612  data: 0.0005  max mem: 19734
Epoch: [19]  [ 140/1251]  eta: 0:09:49  lr: 0.000019  loss: 3.6570 (3.4229)  time: 0.4607  data: 0.0006  max mem: 19734
Epoch: [19]  [ 150/1251]  eta: 0:09:44  lr: 0.000019  loss: 3.6530 (3.4188)  time: 0.4919  data: 0.0005  max mem: 19734
Epoch: [19]  [ 160/1251]  eta: 0:09:38  lr: 0.000019  loss: 3.4921 (3.4209)  time: 0.5280  data: 0.0005  max mem: 19734
Epoch: [19]  [ 170/1251]  eta: 0:09:31  lr: 0.000019  loss: 3.5534 (3.4299)  time: 0.5111  data: 0.0005  max mem: 19734
Epoch: [19]  [ 180/1251]  eta: 0:09:21  lr: 0.000019  loss: 3.5513 (3.4297)  time: 0.4733  data: 0.0005  max mem: 19734
Epoch: [19]  [ 190/1251]  eta: 0:09:12  lr: 0.000019  loss: 3.5513 (3.4370)  time: 0.4608  data: 0.0005  max mem: 19734
Epoch: [19]  [ 200/1251]  eta: 0:09:04  lr: 0.000019  loss: 3.6466 (3.4407)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [19]  [ 210/1251]  eta: 0:08:56  lr: 0.000019  loss: 3.6415 (3.4367)  time: 0.4608  data: 0.0005  max mem: 19734
Epoch: [19]  [ 220/1251]  eta: 0:08:48  lr: 0.000019  loss: 3.3610 (3.4422)  time: 0.4601  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4477, ratio_loss=0.0059, pruning_loss=0.1367, mse_loss=0.5174
Epoch: [19]  [ 230/1251]  eta: 0:08:41  lr: 0.000019  loss: 3.6267 (3.4414)  time: 0.4600  data: 0.0004  max mem: 19734
Epoch: [19]  [ 240/1251]  eta: 0:08:34  lr: 0.000019  loss: 3.7160 (3.4558)  time: 0.4591  data: 0.0008  max mem: 19734
Epoch: [19]  [ 250/1251]  eta: 0:08:26  lr: 0.000019  loss: 3.6486 (3.4490)  time: 0.4566  data: 0.0007  max mem: 19734
Epoch: [19]  [ 260/1251]  eta: 0:08:19  lr: 0.000019  loss: 3.4174 (3.4458)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [19]  [ 270/1251]  eta: 0:08:12  lr: 0.000019  loss: 3.1365 (3.4333)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [19]  [ 280/1251]  eta: 0:08:06  lr: 0.000019  loss: 3.4540 (3.4354)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [19]  [ 290/1251]  eta: 0:08:00  lr: 0.000019  loss: 3.5900 (3.4356)  time: 0.4655  data: 0.0004  max mem: 19734
Epoch: [19]  [ 300/1251]  eta: 0:07:56  lr: 0.000019  loss: 3.4649 (3.4323)  time: 0.4995  data: 0.0004  max mem: 19734
Epoch: [19]  [ 310/1251]  eta: 0:07:51  lr: 0.000019  loss: 3.4468 (3.4312)  time: 0.5212  data: 0.0005  max mem: 19734
Epoch: [19]  [ 320/1251]  eta: 0:07:45  lr: 0.000019  loss: 3.5555 (3.4356)  time: 0.4952  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4156, ratio_loss=0.0059, pruning_loss=0.1348, mse_loss=0.5271
Epoch: [19]  [ 330/1251]  eta: 0:07:39  lr: 0.000019  loss: 3.5555 (3.4421)  time: 0.4641  data: 0.0006  max mem: 19734
Epoch: [19]  [ 340/1251]  eta: 0:07:33  lr: 0.000019  loss: 3.7673 (3.4442)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [19]  [ 350/1251]  eta: 0:07:27  lr: 0.000019  loss: 3.6273 (3.4410)  time: 0.4602  data: 0.0005  max mem: 19734
Epoch: [19]  [ 360/1251]  eta: 0:07:21  lr: 0.000019  loss: 3.4277 (3.4331)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [19]  [ 370/1251]  eta: 0:07:15  lr: 0.000019  loss: 3.4645 (3.4392)  time: 0.4553  data: 0.0005  max mem: 19734
Epoch: [19]  [ 380/1251]  eta: 0:07:10  lr: 0.000019  loss: 3.6729 (3.4430)  time: 0.4569  data: 0.0005  max mem: 19734
Epoch: [19]  [ 390/1251]  eta: 0:07:04  lr: 0.000019  loss: 3.6672 (3.4497)  time: 0.4574  data: 0.0005  max mem: 19734
Epoch: [19]  [ 400/1251]  eta: 0:06:58  lr: 0.000019  loss: 3.6672 (3.4517)  time: 0.4555  data: 0.0005  max mem: 19734
Epoch: [19]  [ 410/1251]  eta: 0:06:52  lr: 0.000019  loss: 3.6443 (3.4483)  time: 0.4561  data: 0.0004  max mem: 19734
Epoch: [19]  [ 420/1251]  eta: 0:06:47  lr: 0.000019  loss: 3.5091 (3.4499)  time: 0.4573  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4552, ratio_loss=0.0058, pruning_loss=0.1351, mse_loss=0.5293
Epoch: [19]  [ 430/1251]  eta: 0:06:41  lr: 0.000019  loss: 3.5687 (3.4516)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [19]  [ 440/1251]  eta: 0:06:37  lr: 0.000019  loss: 3.6969 (3.4539)  time: 0.4827  data: 0.0004  max mem: 19734
Epoch: [19]  [ 450/1251]  eta: 0:06:33  lr: 0.000019  loss: 3.6430 (3.4522)  time: 0.5185  data: 0.0005  max mem: 19734
Epoch: [19]  [ 460/1251]  eta: 0:06:27  lr: 0.000019  loss: 3.5564 (3.4489)  time: 0.4933  data: 0.0005  max mem: 19734
Epoch: [19]  [ 470/1251]  eta: 0:06:22  lr: 0.000019  loss: 3.1567 (3.4455)  time: 0.4661  data: 0.0004  max mem: 19734
Epoch: [19]  [ 480/1251]  eta: 0:06:17  lr: 0.000019  loss: 3.5078 (3.4478)  time: 0.4662  data: 0.0005  max mem: 19734
Epoch: [19]  [ 490/1251]  eta: 0:06:11  lr: 0.000019  loss: 3.5078 (3.4474)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [19]  [ 500/1251]  eta: 0:06:06  lr: 0.000019  loss: 3.4975 (3.4489)  time: 0.4592  data: 0.0004  max mem: 19734
Epoch: [19]  [ 510/1251]  eta: 0:06:00  lr: 0.000019  loss: 3.6381 (3.4518)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [19]  [ 520/1251]  eta: 0:05:55  lr: 0.000019  loss: 3.2822 (3.4453)  time: 0.4542  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3963, ratio_loss=0.0058, pruning_loss=0.1355, mse_loss=0.5496
Epoch: [19]  [ 530/1251]  eta: 0:05:50  lr: 0.000019  loss: 3.3245 (3.4465)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [19]  [ 540/1251]  eta: 0:05:45  lr: 0.000019  loss: 3.6734 (3.4514)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [19]  [ 550/1251]  eta: 0:05:39  lr: 0.000019  loss: 3.6939 (3.4521)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [19]  [ 560/1251]  eta: 0:05:34  lr: 0.000019  loss: 3.3898 (3.4498)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [19]  [ 570/1251]  eta: 0:05:29  lr: 0.000019  loss: 3.4350 (3.4504)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [19]  [ 580/1251]  eta: 0:05:24  lr: 0.000019  loss: 3.5470 (3.4505)  time: 0.4703  data: 0.0005  max mem: 19734
Epoch: [19]  [ 590/1251]  eta: 0:05:19  lr: 0.000019  loss: 3.4545 (3.4474)  time: 0.4914  data: 0.0005  max mem: 19734
Epoch: [19]  [ 600/1251]  eta: 0:05:15  lr: 0.000019  loss: 3.2618 (3.4459)  time: 0.4929  data: 0.0005  max mem: 19734
Epoch: [19]  [ 610/1251]  eta: 0:05:09  lr: 0.000019  loss: 3.2064 (3.4437)  time: 0.4740  data: 0.0004  max mem: 19734
Epoch: [19]  [ 620/1251]  eta: 0:05:04  lr: 0.000019  loss: 3.1713 (3.4448)  time: 0.4647  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4118, ratio_loss=0.0058, pruning_loss=0.1345, mse_loss=0.5214
Epoch: [19]  [ 630/1251]  eta: 0:04:59  lr: 0.000019  loss: 3.4075 (3.4454)  time: 0.4618  data: 0.0004  max mem: 19734
Epoch: [19]  [ 640/1251]  eta: 0:04:54  lr: 0.000019  loss: 3.6255 (3.4484)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [19]  [ 650/1251]  eta: 0:04:49  lr: 0.000019  loss: 3.6807 (3.4476)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [19]  [ 660/1251]  eta: 0:04:44  lr: 0.000019  loss: 3.2744 (3.4416)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [19]  [ 670/1251]  eta: 0:04:39  lr: 0.000019  loss: 3.3798 (3.4419)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [19]  [ 680/1251]  eta: 0:04:34  lr: 0.000019  loss: 2.9451 (3.4348)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [19]  [ 690/1251]  eta: 0:04:29  lr: 0.000019  loss: 2.9451 (3.4353)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [19]  [ 700/1251]  eta: 0:04:24  lr: 0.000019  loss: 3.6251 (3.4363)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [19]  [ 710/1251]  eta: 0:04:19  lr: 0.000019  loss: 3.5805 (3.4390)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [19]  [ 720/1251]  eta: 0:04:14  lr: 0.000019  loss: 3.5716 (3.4392)  time: 0.4539  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3696, ratio_loss=0.0055, pruning_loss=0.1354, mse_loss=0.5235
Epoch: [19]  [ 730/1251]  eta: 0:04:09  lr: 0.000019  loss: 3.5037 (3.4389)  time: 0.4831  data: 0.0004  max mem: 19734
Epoch: [19]  [ 740/1251]  eta: 0:04:05  lr: 0.000019  loss: 3.5132 (3.4386)  time: 0.4999  data: 0.0004  max mem: 19734
Epoch: [19]  [ 750/1251]  eta: 0:04:00  lr: 0.000019  loss: 3.5132 (3.4377)  time: 0.4807  data: 0.0006  max mem: 19734
Epoch: [19]  [ 760/1251]  eta: 0:03:55  lr: 0.000019  loss: 3.4695 (3.4356)  time: 0.4623  data: 0.0006  max mem: 19734
Epoch: [19]  [ 770/1251]  eta: 0:03:50  lr: 0.000019  loss: 3.6537 (3.4408)  time: 0.4608  data: 0.0004  max mem: 19734
Epoch: [19]  [ 780/1251]  eta: 0:03:45  lr: 0.000019  loss: 3.7428 (3.4432)  time: 0.4610  data: 0.0004  max mem: 19734
Epoch: [19]  [ 790/1251]  eta: 0:03:40  lr: 0.000019  loss: 3.4325 (3.4408)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [19]  [ 800/1251]  eta: 0:03:35  lr: 0.000019  loss: 3.4732 (3.4407)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [19]  [ 810/1251]  eta: 0:03:30  lr: 0.000019  loss: 3.6692 (3.4437)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [19]  [ 820/1251]  eta: 0:03:25  lr: 0.000019  loss: 3.6692 (3.4430)  time: 0.4536  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4488, ratio_loss=0.0053, pruning_loss=0.1341, mse_loss=0.5400
Epoch: [19]  [ 830/1251]  eta: 0:03:20  lr: 0.000019  loss: 3.4707 (3.4434)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [19]  [ 840/1251]  eta: 0:03:16  lr: 0.000019  loss: 3.4412 (3.4421)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [19]  [ 850/1251]  eta: 0:03:11  lr: 0.000019  loss: 3.4322 (3.4413)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [19]  [ 860/1251]  eta: 0:03:06  lr: 0.000019  loss: 3.5695 (3.4431)  time: 0.4511  data: 0.0004  max mem: 19734
Epoch: [19]  [ 870/1251]  eta: 0:03:01  lr: 0.000019  loss: 3.5695 (3.4420)  time: 0.4728  data: 0.0004  max mem: 19734
Epoch: [19]  [ 880/1251]  eta: 0:02:57  lr: 0.000019  loss: 3.3948 (3.4439)  time: 0.5032  data: 0.0004  max mem: 19734
Epoch: [19]  [ 890/1251]  eta: 0:02:52  lr: 0.000019  loss: 3.7201 (3.4460)  time: 0.4898  data: 0.0004  max mem: 19734
Epoch: [19]  [ 900/1251]  eta: 0:02:47  lr: 0.000019  loss: 3.5997 (3.4467)  time: 0.4611  data: 0.0004  max mem: 19734
Epoch: [19]  [ 910/1251]  eta: 0:02:42  lr: 0.000019  loss: 3.5006 (3.4461)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [19]  [ 920/1251]  eta: 0:02:37  lr: 0.000019  loss: 3.4146 (3.4455)  time: 0.4612  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4411, ratio_loss=0.0058, pruning_loss=0.1338, mse_loss=0.5103
Epoch: [19]  [ 930/1251]  eta: 0:02:32  lr: 0.000019  loss: 3.4146 (3.4461)  time: 0.4595  data: 0.0004  max mem: 19734
Epoch: [19]  [ 940/1251]  eta: 0:02:27  lr: 0.000019  loss: 3.4296 (3.4458)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [19]  [ 950/1251]  eta: 0:02:23  lr: 0.000019  loss: 3.6840 (3.4476)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [19]  [ 960/1251]  eta: 0:02:18  lr: 0.000019  loss: 3.7259 (3.4479)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [19]  [ 970/1251]  eta: 0:02:13  lr: 0.000019  loss: 3.7594 (3.4518)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [19]  [ 980/1251]  eta: 0:02:08  lr: 0.000019  loss: 3.7594 (3.4544)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [19]  [ 990/1251]  eta: 0:02:03  lr: 0.000019  loss: 3.7084 (3.4559)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [19]  [1000/1251]  eta: 0:01:59  lr: 0.000019  loss: 3.6066 (3.4568)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [19]  [1010/1251]  eta: 0:01:54  lr: 0.000019  loss: 3.7065 (3.4577)  time: 0.4622  data: 0.0005  max mem: 19734
Epoch: [19]  [1020/1251]  eta: 0:01:49  lr: 0.000019  loss: 3.1753 (3.4524)  time: 0.4792  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4746, ratio_loss=0.0057, pruning_loss=0.1337, mse_loss=0.5380
Epoch: [19]  [1030/1251]  eta: 0:01:44  lr: 0.000019  loss: 3.0905 (3.4516)  time: 0.5017  data: 0.0005  max mem: 19734
Epoch: [19]  [1040/1251]  eta: 0:01:40  lr: 0.000019  loss: 3.5699 (3.4514)  time: 0.4847  data: 0.0005  max mem: 19734
Epoch: [19]  [1050/1251]  eta: 0:01:35  lr: 0.000019  loss: 3.6765 (3.4528)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [19]  [1060/1251]  eta: 0:01:30  lr: 0.000019  loss: 3.4231 (3.4505)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [19]  [1070/1251]  eta: 0:01:25  lr: 0.000019  loss: 3.1274 (3.4498)  time: 0.4643  data: 0.0005  max mem: 19734
Epoch: [19]  [1080/1251]  eta: 0:01:21  lr: 0.000019  loss: 3.5470 (3.4514)  time: 0.4635  data: 0.0005  max mem: 19734
Epoch: [19]  [1090/1251]  eta: 0:01:16  lr: 0.000019  loss: 3.5940 (3.4525)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [19]  [1100/1251]  eta: 0:01:11  lr: 0.000019  loss: 3.6191 (3.4535)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [19]  [1110/1251]  eta: 0:01:06  lr: 0.000019  loss: 3.3654 (3.4517)  time: 0.4524  data: 0.0005  max mem: 19734
Epoch: [19]  [1120/1251]  eta: 0:01:02  lr: 0.000019  loss: 3.4890 (3.4527)  time: 0.4516  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4076, ratio_loss=0.0055, pruning_loss=0.1359, mse_loss=0.5300
Epoch: [19]  [1130/1251]  eta: 0:00:57  lr: 0.000019  loss: 3.5909 (3.4501)  time: 0.4513  data: 0.0004  max mem: 19734
Epoch: [19]  [1140/1251]  eta: 0:00:52  lr: 0.000019  loss: 3.3680 (3.4490)  time: 0.4509  data: 0.0004  max mem: 19734
Epoch: [19]  [1150/1251]  eta: 0:00:47  lr: 0.000019  loss: 3.3431 (3.4472)  time: 0.4511  data: 0.0005  max mem: 19734
Epoch: [19]  [1160/1251]  eta: 0:00:43  lr: 0.000019  loss: 3.3431 (3.4470)  time: 0.4698  data: 0.0005  max mem: 19734
Epoch: [19]  [1170/1251]  eta: 0:00:38  lr: 0.000019  loss: 3.6875 (3.4471)  time: 0.4989  data: 0.0004  max mem: 19734
Epoch: [19]  [1180/1251]  eta: 0:00:33  lr: 0.000019  loss: 3.6328 (3.4478)  time: 0.4979  data: 0.0005  max mem: 19734
Epoch: [19]  [1190/1251]  eta: 0:00:28  lr: 0.000019  loss: 3.6098 (3.4504)  time: 0.4675  data: 0.0008  max mem: 19734
Epoch: [19]  [1200/1251]  eta: 0:00:24  lr: 0.000019  loss: 3.6169 (3.4500)  time: 0.4469  data: 0.0006  max mem: 19734
Epoch: [19]  [1210/1251]  eta: 0:00:19  lr: 0.000019  loss: 3.7125 (3.4536)  time: 0.4452  data: 0.0002  max mem: 19734
Epoch: [19]  [1220/1251]  eta: 0:00:14  lr: 0.000019  loss: 3.6665 (3.4516)  time: 0.4497  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4449, ratio_loss=0.0057, pruning_loss=0.1356, mse_loss=0.5445
Epoch: [19]  [1230/1251]  eta: 0:00:09  lr: 0.000019  loss: 3.5232 (3.4520)  time: 0.4499  data: 0.0002  max mem: 19734
Epoch: [19]  [1240/1251]  eta: 0:00:05  lr: 0.000019  loss: 3.5780 (3.4523)  time: 0.4461  data: 0.0002  max mem: 19734
Epoch: [19]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 3.5900 (3.4521)  time: 0.4461  data: 0.0002  max mem: 19734
Epoch: [19] Total time: 0:09:51 (0.4725 s / it)
Averaged stats: lr: 0.000019  loss: 3.5900 (3.4439)
Test:  [  0/261]  eta: 2:31:45  loss: 0.6876 (0.6876)  acc1: 81.7708 (81.7708)  acc5: 95.8333 (95.8333)  time: 34.8867  data: 34.5789  max mem: 19734
Test:  [ 10/261]  eta: 0:14:00  loss: 0.6876 (0.7251)  acc1: 83.8542 (83.5227)  acc5: 96.8750 (96.3068)  time: 3.3467  data: 3.1525  max mem: 19734
Test:  [ 20/261]  eta: 0:07:24  loss: 0.9507 (0.9118)  acc1: 78.6458 (79.0675)  acc5: 93.7500 (94.3204)  time: 0.1940  data: 0.0096  max mem: 19734
Test:  [ 30/261]  eta: 0:05:05  loss: 0.8528 (0.8310)  acc1: 82.2917 (81.8380)  acc5: 93.2292 (94.9093)  time: 0.2075  data: 0.0123  max mem: 19734
Test:  [ 40/261]  eta: 0:04:04  loss: 0.5721 (0.7961)  acc1: 89.0625 (82.8125)  acc5: 96.3542 (95.2236)  time: 0.3259  data: 0.1464  max mem: 19734
Test:  [ 50/261]  eta: 0:03:15  loss: 0.9064 (0.8566)  acc1: 78.1250 (80.9436)  acc5: 94.7917 (94.8019)  time: 0.3153  data: 0.1438  max mem: 19734
Test:  [ 60/261]  eta: 0:02:40  loss: 0.9993 (0.8670)  acc1: 76.0417 (80.5499)  acc5: 93.7500 (94.8258)  time: 0.1658  data: 0.0084  max mem: 19734
Test:  [ 70/261]  eta: 0:02:26  loss: 0.9238 (0.8658)  acc1: 78.1250 (80.1056)  acc5: 95.3125 (95.0337)  time: 0.3630  data: 0.2355  max mem: 19734
Test:  [ 80/261]  eta: 0:02:07  loss: 0.8531 (0.8674)  acc1: 79.6875 (80.1826)  acc5: 96.8750 (95.1518)  time: 0.4141  data: 0.2403  max mem: 19734
Test:  [ 90/261]  eta: 0:01:50  loss: 0.8129 (0.8524)  acc1: 84.3750 (80.6319)  acc5: 95.8333 (95.2724)  time: 0.2187  data: 0.0130  max mem: 19734
Test:  [100/261]  eta: 0:01:45  loss: 0.8129 (0.8562)  acc1: 84.8958 (80.5642)  acc5: 95.3125 (95.3228)  time: 0.4759  data: 0.2840  max mem: 19734
Test:  [110/261]  eta: 0:01:33  loss: 0.8695 (0.8792)  acc1: 76.5625 (80.0441)  acc5: 94.2708 (95.0216)  time: 0.5068  data: 0.2908  max mem: 19734
Test:  [120/261]  eta: 0:01:22  loss: 1.2016 (0.9210)  acc1: 70.3125 (79.0203)  acc5: 90.1042 (94.4688)  time: 0.2145  data: 0.0197  max mem: 19734
Test:  [130/261]  eta: 0:01:16  loss: 1.3856 (0.9673)  acc1: 67.1875 (78.0654)  acc5: 86.4583 (93.8414)  time: 0.3657  data: 0.2084  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: 1.3592 (0.9947)  acc1: 68.2292 (77.4047)  acc5: 88.0208 (93.5949)  time: 0.3832  data: 0.2040  max mem: 19734
Test:  [150/261]  eta: 0:00:59  loss: 1.2209 (0.9999)  acc1: 72.3958 (77.3455)  acc5: 91.1458 (93.4499)  time: 0.2215  data: 0.0074  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: 1.0714 (1.0201)  acc1: 77.6042 (76.9701)  acc5: 90.6250 (93.1871)  time: 0.1751  data: 0.0117  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: 1.3180 (1.0506)  acc1: 66.1458 (76.2001)  acc5: 88.5417 (92.8545)  time: 0.3153  data: 0.1950  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4569 (1.0686)  acc1: 64.5833 (75.7712)  acc5: 88.0208 (92.6709)  time: 0.3344  data: 0.2249  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: 1.4128 (1.0818)  acc1: 67.1875 (75.5372)  acc5: 90.6250 (92.5093)  time: 0.1043  data: 0.0385  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3854 (1.0976)  acc1: 71.3542 (75.2306)  acc5: 89.0625 (92.2704)  time: 0.0626  data: 0.0008  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: 1.4106 (1.1117)  acc1: 70.3125 (74.9704)  acc5: 88.0208 (92.0690)  time: 0.0962  data: 0.0320  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: 1.4346 (1.1314)  acc1: 66.1458 (74.4815)  acc5: 88.5417 (91.8623)  time: 0.0958  data: 0.0319  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4683 (1.1413)  acc1: 66.1458 (74.2334)  acc5: 88.5417 (91.7614)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3502 (1.1513)  acc1: 67.7083 (73.9821)  acc5: 90.1042 (91.6688)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0613 (1.1436)  acc1: 76.0417 (74.1430)  acc5: 93.2292 (91.7974)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9508 (1.1427)  acc1: 76.5625 (74.1640)  acc5: 95.8333 (91.8720)  time: 0.0602  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3617 s / it)
* Acc@1 74.164 Acc@5 91.872 loss 1.143
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.27%
Epoch: [20]  [   0/1251]  eta: 4:54:25  lr: 0.000018  loss: 3.2248 (3.2248)  time: 14.1208  data: 12.1525  max mem: 19734
Epoch: [20]  [  10/1251]  eta: 0:43:06  lr: 0.000018  loss: 3.5791 (3.5120)  time: 2.0842  data: 1.1062  max mem: 19734
Epoch: [20]  [  20/1251]  eta: 0:26:55  lr: 0.000018  loss: 3.6776 (3.5773)  time: 0.6720  data: 0.0011  max mem: 19734
Epoch: [20]  [  30/1251]  eta: 0:21:07  lr: 0.000018  loss: 3.5871 (3.5281)  time: 0.4633  data: 0.0005  max mem: 19734
Epoch: [20]  [  40/1251]  eta: 0:18:06  lr: 0.000018  loss: 3.3133 (3.4587)  time: 0.4618  data: 0.0005  max mem: 19734
Epoch: [20]  [  50/1251]  eta: 0:16:26  lr: 0.000018  loss: 3.1488 (3.4295)  time: 0.4850  data: 0.0005  max mem: 19734
Epoch: [20]  [  60/1251]  eta: 0:15:24  lr: 0.000018  loss: 3.3475 (3.4281)  time: 0.5286  data: 0.0005  max mem: 19734
Epoch: [20]  [  70/1251]  eta: 0:14:28  lr: 0.000018  loss: 3.4559 (3.4368)  time: 0.5153  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4131, ratio_loss=0.0056, pruning_loss=0.1349, mse_loss=0.5188
Epoch: [20]  [  80/1251]  eta: 0:13:44  lr: 0.000018  loss: 3.4776 (3.4382)  time: 0.4817  data: 0.0004  max mem: 19734
Epoch: [20]  [  90/1251]  eta: 0:13:05  lr: 0.000018  loss: 3.4522 (3.4151)  time: 0.4698  data: 0.0005  max mem: 19734
Epoch: [20]  [ 100/1251]  eta: 0:12:33  lr: 0.000018  loss: 2.9423 (3.3775)  time: 0.4580  data: 0.0007  max mem: 19734
Epoch: [20]  [ 110/1251]  eta: 0:12:06  lr: 0.000018  loss: 3.2812 (3.3813)  time: 0.4565  data: 0.0006  max mem: 19734
Epoch: [20]  [ 120/1251]  eta: 0:11:43  lr: 0.000018  loss: 3.3746 (3.3846)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [20]  [ 130/1251]  eta: 0:11:23  lr: 0.000018  loss: 3.4082 (3.3826)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [20]  [ 140/1251]  eta: 0:11:05  lr: 0.000018  loss: 3.4172 (3.3816)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [20]  [ 150/1251]  eta: 0:10:48  lr: 0.000018  loss: 3.5325 (3.3927)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [20]  [ 160/1251]  eta: 0:10:34  lr: 0.000018  loss: 3.6070 (3.3969)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [20]  [ 170/1251]  eta: 0:10:20  lr: 0.000018  loss: 3.4738 (3.3783)  time: 0.4562  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3278, ratio_loss=0.0057, pruning_loss=0.1366, mse_loss=0.5273
Epoch: [20]  [ 180/1251]  eta: 0:10:07  lr: 0.000018  loss: 3.5452 (3.3931)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [20]  [ 190/1251]  eta: 0:09:57  lr: 0.000018  loss: 3.6651 (3.4049)  time: 0.4676  data: 0.0005  max mem: 19734
Epoch: [20]  [ 200/1251]  eta: 0:09:48  lr: 0.000018  loss: 3.6869 (3.4246)  time: 0.4896  data: 0.0005  max mem: 19734
Epoch: [20]  [ 210/1251]  eta: 0:09:39  lr: 0.000018  loss: 3.7495 (3.4333)  time: 0.5002  data: 0.0004  max mem: 19734
Epoch: [20]  [ 220/1251]  eta: 0:09:29  lr: 0.000018  loss: 3.6024 (3.4353)  time: 0.4790  data: 0.0004  max mem: 19734
Epoch: [20]  [ 230/1251]  eta: 0:09:20  lr: 0.000018  loss: 3.5681 (3.4388)  time: 0.4651  data: 0.0005  max mem: 19734
Epoch: [20]  [ 240/1251]  eta: 0:09:11  lr: 0.000018  loss: 3.4967 (3.4347)  time: 0.4649  data: 0.0005  max mem: 19734
Epoch: [20]  [ 250/1251]  eta: 0:09:02  lr: 0.000018  loss: 3.4700 (3.4341)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [20]  [ 260/1251]  eta: 0:08:53  lr: 0.000018  loss: 3.4274 (3.4350)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [20]  [ 270/1251]  eta: 0:08:45  lr: 0.000018  loss: 3.4290 (3.4357)  time: 0.4579  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5074, ratio_loss=0.0055, pruning_loss=0.1326, mse_loss=0.5138
Epoch: [20]  [ 280/1251]  eta: 0:08:36  lr: 0.000018  loss: 3.6361 (3.4406)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [20]  [ 290/1251]  eta: 0:08:29  lr: 0.000018  loss: 3.6023 (3.4393)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [20]  [ 300/1251]  eta: 0:08:21  lr: 0.000018  loss: 3.4396 (3.4383)  time: 0.4567  data: 0.0005  max mem: 19734
Epoch: [20]  [ 310/1251]  eta: 0:08:14  lr: 0.000018  loss: 3.4815 (3.4377)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [20]  [ 320/1251]  eta: 0:08:06  lr: 0.000018  loss: 3.5022 (3.4426)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [20]  [ 330/1251]  eta: 0:07:59  lr: 0.000018  loss: 3.5130 (3.4406)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [20]  [ 340/1251]  eta: 0:07:53  lr: 0.000018  loss: 3.5711 (3.4428)  time: 0.4781  data: 0.0005  max mem: 19734
Epoch: [20]  [ 350/1251]  eta: 0:07:49  lr: 0.000018  loss: 3.6302 (3.4436)  time: 0.5169  data: 0.0005  max mem: 19734
Epoch: [20]  [ 360/1251]  eta: 0:07:42  lr: 0.000018  loss: 3.4828 (3.4360)  time: 0.4942  data: 0.0005  max mem: 19734
Epoch: [20]  [ 370/1251]  eta: 0:07:36  lr: 0.000018  loss: 3.3121 (3.4353)  time: 0.4668  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3749, ratio_loss=0.0056, pruning_loss=0.1363, mse_loss=0.5505
Epoch: [20]  [ 380/1251]  eta: 0:07:29  lr: 0.000018  loss: 3.3156 (3.4329)  time: 0.4674  data: 0.0005  max mem: 19734
Epoch: [20]  [ 390/1251]  eta: 0:07:23  lr: 0.000018  loss: 3.3245 (3.4286)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [20]  [ 400/1251]  eta: 0:07:16  lr: 0.000018  loss: 3.3245 (3.4265)  time: 0.4605  data: 0.0005  max mem: 19734
Epoch: [20]  [ 410/1251]  eta: 0:07:10  lr: 0.000018  loss: 3.3798 (3.4242)  time: 0.4567  data: 0.0005  max mem: 19734
Epoch: [20]  [ 420/1251]  eta: 0:07:04  lr: 0.000018  loss: 3.4465 (3.4298)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [20]  [ 430/1251]  eta: 0:06:58  lr: 0.000018  loss: 3.7717 (3.4381)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [20]  [ 440/1251]  eta: 0:06:52  lr: 0.000018  loss: 3.5599 (3.4308)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [20]  [ 450/1251]  eta: 0:06:46  lr: 0.000018  loss: 2.8980 (3.4246)  time: 0.4592  data: 0.0004  max mem: 19734
Epoch: [20]  [ 460/1251]  eta: 0:06:40  lr: 0.000018  loss: 3.4362 (3.4264)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [20]  [ 470/1251]  eta: 0:06:34  lr: 0.000018  loss: 3.6546 (3.4258)  time: 0.4561  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3655, ratio_loss=0.0056, pruning_loss=0.1370, mse_loss=0.5146
Epoch: [20]  [ 480/1251]  eta: 0:06:28  lr: 0.000018  loss: 3.2713 (3.4210)  time: 0.4691  data: 0.0004  max mem: 19734
Epoch: [20]  [ 490/1251]  eta: 0:06:23  lr: 0.000018  loss: 3.2713 (3.4180)  time: 0.4950  data: 0.0004  max mem: 19734
Epoch: [20]  [ 500/1251]  eta: 0:06:18  lr: 0.000018  loss: 3.3646 (3.4143)  time: 0.5022  data: 0.0004  max mem: 19734
Epoch: [20]  [ 510/1251]  eta: 0:06:12  lr: 0.000018  loss: 3.1550 (3.4070)  time: 0.4754  data: 0.0006  max mem: 19734
Epoch: [20]  [ 520/1251]  eta: 0:06:07  lr: 0.000018  loss: 3.1550 (3.4036)  time: 0.4614  data: 0.0006  max mem: 19734
Epoch: [20]  [ 530/1251]  eta: 0:06:01  lr: 0.000018  loss: 3.4568 (3.4054)  time: 0.4616  data: 0.0004  max mem: 19734
Epoch: [20]  [ 540/1251]  eta: 0:05:56  lr: 0.000018  loss: 3.4568 (3.4009)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [20]  [ 550/1251]  eta: 0:05:50  lr: 0.000018  loss: 3.4134 (3.4039)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [20]  [ 560/1251]  eta: 0:05:45  lr: 0.000018  loss: 3.4872 (3.4062)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [20]  [ 570/1251]  eta: 0:05:39  lr: 0.000018  loss: 3.6266 (3.4105)  time: 0.4552  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3107, ratio_loss=0.0054, pruning_loss=0.1389, mse_loss=0.5470
Epoch: [20]  [ 580/1251]  eta: 0:05:33  lr: 0.000018  loss: 3.5221 (3.4105)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [20]  [ 590/1251]  eta: 0:05:28  lr: 0.000018  loss: 3.5221 (3.4117)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [20]  [ 600/1251]  eta: 0:05:23  lr: 0.000018  loss: 3.6760 (3.4168)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [20]  [ 610/1251]  eta: 0:05:17  lr: 0.000018  loss: 3.7356 (3.4184)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [20]  [ 620/1251]  eta: 0:05:12  lr: 0.000018  loss: 3.5645 (3.4176)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [20]  [ 630/1251]  eta: 0:05:07  lr: 0.000018  loss: 3.5560 (3.4156)  time: 0.4787  data: 0.0007  max mem: 19734
Epoch: [20]  [ 640/1251]  eta: 0:05:02  lr: 0.000018  loss: 3.5758 (3.4168)  time: 0.5122  data: 0.0007  max mem: 19734
Epoch: [20]  [ 650/1251]  eta: 0:04:57  lr: 0.000018  loss: 3.5758 (3.4146)  time: 0.4949  data: 0.0005  max mem: 19734
Epoch: [20]  [ 660/1251]  eta: 0:04:52  lr: 0.000018  loss: 3.3948 (3.4133)  time: 0.4727  data: 0.0004  max mem: 19734
Epoch: [20]  [ 670/1251]  eta: 0:04:47  lr: 0.000018  loss: 3.2744 (3.4109)  time: 0.4658  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3739, ratio_loss=0.0056, pruning_loss=0.1373, mse_loss=0.5051
Epoch: [20]  [ 680/1251]  eta: 0:04:41  lr: 0.000018  loss: 3.1093 (3.4084)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [20]  [ 690/1251]  eta: 0:04:36  lr: 0.000018  loss: 3.4377 (3.4124)  time: 0.4513  data: 0.0006  max mem: 19734
Epoch: [20]  [ 700/1251]  eta: 0:04:31  lr: 0.000018  loss: 3.8663 (3.4156)  time: 0.4506  data: 0.0010  max mem: 19734
Epoch: [20]  [ 710/1251]  eta: 0:04:26  lr: 0.000018  loss: 3.7248 (3.4143)  time: 0.4517  data: 0.0008  max mem: 19734
Epoch: [20]  [ 720/1251]  eta: 0:04:20  lr: 0.000018  loss: 3.6537 (3.4175)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [20]  [ 730/1251]  eta: 0:04:15  lr: 0.000018  loss: 3.5611 (3.4164)  time: 0.4513  data: 0.0005  max mem: 19734
Epoch: [20]  [ 740/1251]  eta: 0:04:10  lr: 0.000018  loss: 3.6556 (3.4196)  time: 0.4519  data: 0.0005  max mem: 19734
Epoch: [20]  [ 750/1251]  eta: 0:04:05  lr: 0.000018  loss: 3.3450 (3.4141)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [20]  [ 760/1251]  eta: 0:04:00  lr: 0.000018  loss: 3.1747 (3.4128)  time: 0.4547  data: 0.0006  max mem: 19734
Epoch: [20]  [ 770/1251]  eta: 0:03:55  lr: 0.000018  loss: 3.2860 (3.4138)  time: 0.4763  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4318, ratio_loss=0.0056, pruning_loss=0.1362, mse_loss=0.4962
Epoch: [20]  [ 780/1251]  eta: 0:03:50  lr: 0.000018  loss: 3.4796 (3.4152)  time: 0.5037  data: 0.0004  max mem: 19734
Epoch: [20]  [ 790/1251]  eta: 0:03:45  lr: 0.000018  loss: 3.5259 (3.4174)  time: 0.5015  data: 0.0004  max mem: 19734
Epoch: [20]  [ 800/1251]  eta: 0:03:40  lr: 0.000018  loss: 3.5259 (3.4177)  time: 0.4769  data: 0.0004  max mem: 19734
Epoch: [20]  [ 810/1251]  eta: 0:03:35  lr: 0.000018  loss: 3.5447 (3.4184)  time: 0.4673  data: 0.0006  max mem: 19734
Epoch: [20]  [ 820/1251]  eta: 0:03:30  lr: 0.000018  loss: 3.5447 (3.4195)  time: 0.4679  data: 0.0006  max mem: 19734
Epoch: [20]  [ 830/1251]  eta: 0:03:25  lr: 0.000018  loss: 3.7312 (3.4231)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [20]  [ 840/1251]  eta: 0:03:20  lr: 0.000018  loss: 3.7749 (3.4248)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [20]  [ 850/1251]  eta: 0:03:15  lr: 0.000018  loss: 3.7723 (3.4272)  time: 0.4535  data: 0.0004  max mem: 19734
Epoch: [20]  [ 860/1251]  eta: 0:03:10  lr: 0.000018  loss: 3.4302 (3.4233)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [20]  [ 870/1251]  eta: 0:03:05  lr: 0.000018  loss: 3.1894 (3.4189)  time: 0.4514  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4292, ratio_loss=0.0057, pruning_loss=0.1366, mse_loss=0.5163
Epoch: [20]  [ 880/1251]  eta: 0:03:00  lr: 0.000018  loss: 3.2136 (3.4191)  time: 0.4509  data: 0.0004  max mem: 19734
Epoch: [20]  [ 890/1251]  eta: 0:02:55  lr: 0.000018  loss: 3.3137 (3.4153)  time: 0.4516  data: 0.0005  max mem: 19734
Epoch: [20]  [ 900/1251]  eta: 0:02:50  lr: 0.000018  loss: 3.3426 (3.4165)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [20]  [ 910/1251]  eta: 0:02:45  lr: 0.000018  loss: 3.5921 (3.4180)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [20]  [ 920/1251]  eta: 0:02:40  lr: 0.000018  loss: 3.6290 (3.4176)  time: 0.4737  data: 0.0004  max mem: 19734
Epoch: [20]  [ 930/1251]  eta: 0:02:36  lr: 0.000018  loss: 3.2316 (3.4150)  time: 0.5231  data: 0.0004  max mem: 19734
Epoch: [20]  [ 940/1251]  eta: 0:02:31  lr: 0.000018  loss: 3.2777 (3.4143)  time: 0.5027  data: 0.0004  max mem: 19734
Epoch: [20]  [ 950/1251]  eta: 0:02:26  lr: 0.000018  loss: 3.5020 (3.4141)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [20]  [ 960/1251]  eta: 0:02:21  lr: 0.000018  loss: 3.4839 (3.4129)  time: 0.4621  data: 0.0004  max mem: 19734
Epoch: [20]  [ 970/1251]  eta: 0:02:16  lr: 0.000018  loss: 3.3675 (3.4110)  time: 0.4612  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2943, ratio_loss=0.0056, pruning_loss=0.1393, mse_loss=0.5295
Epoch: [20]  [ 980/1251]  eta: 0:02:11  lr: 0.000018  loss: 3.3518 (3.4093)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [20]  [ 990/1251]  eta: 0:02:06  lr: 0.000018  loss: 3.4109 (3.4096)  time: 0.4524  data: 0.0005  max mem: 19734
Epoch: [20]  [1000/1251]  eta: 0:02:01  lr: 0.000018  loss: 3.4712 (3.4092)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [20]  [1010/1251]  eta: 0:01:56  lr: 0.000018  loss: 3.4748 (3.4098)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [20]  [1020/1251]  eta: 0:01:51  lr: 0.000018  loss: 3.4962 (3.4097)  time: 0.4512  data: 0.0004  max mem: 19734
Epoch: [20]  [1030/1251]  eta: 0:01:46  lr: 0.000018  loss: 3.2899 (3.4078)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [20]  [1040/1251]  eta: 0:01:41  lr: 0.000018  loss: 3.2738 (3.4075)  time: 0.4554  data: 0.0005  max mem: 19734
Epoch: [20]  [1050/1251]  eta: 0:01:36  lr: 0.000018  loss: 3.4108 (3.4058)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [20]  [1060/1251]  eta: 0:01:32  lr: 0.000018  loss: 3.4108 (3.4051)  time: 0.4656  data: 0.0004  max mem: 19734
Epoch: [20]  [1070/1251]  eta: 0:01:27  lr: 0.000018  loss: 3.5541 (3.4081)  time: 0.5090  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3609, ratio_loss=0.0058, pruning_loss=0.1370, mse_loss=0.5074
Epoch: [20]  [1080/1251]  eta: 0:01:22  lr: 0.000018  loss: 3.6957 (3.4083)  time: 0.5145  data: 0.0004  max mem: 19734
Epoch: [20]  [1090/1251]  eta: 0:01:17  lr: 0.000018  loss: 3.6097 (3.4100)  time: 0.4708  data: 0.0004  max mem: 19734
Epoch: [20]  [1100/1251]  eta: 0:01:12  lr: 0.000018  loss: 3.4025 (3.4081)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [20]  [1110/1251]  eta: 0:01:08  lr: 0.000018  loss: 3.3100 (3.4075)  time: 0.4623  data: 0.0004  max mem: 19734
Epoch: [20]  [1120/1251]  eta: 0:01:03  lr: 0.000018  loss: 3.5614 (3.4090)  time: 0.4640  data: 0.0005  max mem: 19734
Epoch: [20]  [1130/1251]  eta: 0:00:58  lr: 0.000018  loss: 3.5614 (3.4085)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [20]  [1140/1251]  eta: 0:00:53  lr: 0.000018  loss: 3.3129 (3.4075)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [20]  [1150/1251]  eta: 0:00:48  lr: 0.000018  loss: 3.6529 (3.4091)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [20]  [1160/1251]  eta: 0:00:43  lr: 0.000018  loss: 3.6561 (3.4102)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [20]  [1170/1251]  eta: 0:00:38  lr: 0.000018  loss: 3.5014 (3.4104)  time: 0.4565  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4241, ratio_loss=0.0057, pruning_loss=0.1364, mse_loss=0.5221
Epoch: [20]  [1180/1251]  eta: 0:00:34  lr: 0.000018  loss: 3.5619 (3.4115)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [20]  [1190/1251]  eta: 0:00:29  lr: 0.000018  loss: 3.4033 (3.4100)  time: 0.4548  data: 0.0008  max mem: 19734
Epoch: [20]  [1200/1251]  eta: 0:00:24  lr: 0.000018  loss: 3.2974 (3.4110)  time: 0.4501  data: 0.0007  max mem: 19734
Epoch: [20]  [1210/1251]  eta: 0:00:19  lr: 0.000018  loss: 3.5684 (3.4100)  time: 0.4625  data: 0.0002  max mem: 19734
Epoch: [20]  [1220/1251]  eta: 0:00:14  lr: 0.000018  loss: 3.5745 (3.4099)  time: 0.4898  data: 0.0002  max mem: 19734
Epoch: [20]  [1230/1251]  eta: 0:00:10  lr: 0.000018  loss: 3.6467 (3.4111)  time: 0.4804  data: 0.0001  max mem: 19734
Epoch: [20]  [1240/1251]  eta: 0:00:05  lr: 0.000018  loss: 3.6999 (3.4129)  time: 0.4536  data: 0.0001  max mem: 19734
Epoch: [20]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.6965 (3.4136)  time: 0.4459  data: 0.0001  max mem: 19734
Epoch: [20] Total time: 0:10:00 (0.4804 s / it)
Averaged stats: lr: 0.000018  loss: 3.6965 (3.4421)
Test:  [  0/261]  eta: 1:33:39  loss: 0.7266 (0.7266)  acc1: 82.8125 (82.8125)  acc5: 94.7917 (94.7917)  time: 21.5298  data: 21.3246  max mem: 19734
Test:  [ 10/261]  eta: 0:10:10  loss: 0.7175 (0.7335)  acc1: 85.4167 (83.8068)  acc5: 96.3542 (96.0701)  time: 2.4312  data: 2.2989  max mem: 19734
Test:  [ 20/261]  eta: 0:05:25  loss: 0.9180 (0.9055)  acc1: 80.2083 (78.8939)  acc5: 93.2292 (94.5189)  time: 0.3398  data: 0.2038  max mem: 19734
Test:  [ 30/261]  eta: 0:03:42  loss: 0.8539 (0.8250)  acc1: 82.8125 (81.9052)  acc5: 94.2708 (95.0437)  time: 0.1558  data: 0.0113  max mem: 19734
Test:  [ 40/261]  eta: 0:03:05  loss: 0.6011 (0.7904)  acc1: 87.5000 (82.9395)  acc5: 96.8750 (95.4014)  time: 0.3027  data: 0.1221  max mem: 19734
Test:  [ 50/261]  eta: 0:02:32  loss: 0.9007 (0.8497)  acc1: 78.6458 (81.1989)  acc5: 95.3125 (94.9857)  time: 0.3479  data: 0.1230  max mem: 19734
Test:  [ 60/261]  eta: 0:02:08  loss: 0.9622 (0.8620)  acc1: 76.5625 (80.7036)  acc5: 94.2708 (95.0051)  time: 0.2263  data: 0.0126  max mem: 19734
Test:  [ 70/261]  eta: 0:01:56  loss: 0.9195 (0.8656)  acc1: 78.1250 (80.2817)  acc5: 95.3125 (95.1878)  time: 0.3299  data: 0.1481  max mem: 19734
Test:  [ 80/261]  eta: 0:01:48  loss: 0.8210 (0.8657)  acc1: 81.2500 (80.3691)  acc5: 96.8750 (95.2804)  time: 0.4871  data: 0.3102  max mem: 19734
Test:  [ 90/261]  eta: 0:01:34  loss: 0.8134 (0.8513)  acc1: 83.8542 (80.7120)  acc5: 95.3125 (95.3812)  time: 0.3305  data: 0.1753  max mem: 19734
Test:  [100/261]  eta: 0:01:31  loss: 0.8187 (0.8551)  acc1: 83.8542 (80.6621)  acc5: 95.3125 (95.4260)  time: 0.4298  data: 0.2803  max mem: 19734
Test:  [110/261]  eta: 0:01:22  loss: 0.8864 (0.8779)  acc1: 78.6458 (80.2224)  acc5: 94.2708 (95.1107)  time: 0.5132  data: 0.3351  max mem: 19734
Test:  [120/261]  eta: 0:01:13  loss: 1.2647 (0.9191)  acc1: 70.3125 (79.3001)  acc5: 89.5833 (94.5894)  time: 0.2710  data: 0.0668  max mem: 19734
Test:  [130/261]  eta: 0:01:04  loss: 1.3846 (0.9649)  acc1: 67.7083 (78.3278)  acc5: 88.0208 (93.9766)  time: 0.2339  data: 0.0082  max mem: 19734
Test:  [140/261]  eta: 0:01:00  loss: 1.2915 (0.9908)  acc1: 68.2292 (77.7113)  acc5: 90.1042 (93.7278)  time: 0.3905  data: 0.1524  max mem: 19734
Test:  [150/261]  eta: 0:00:55  loss: 1.2204 (0.9955)  acc1: 73.9583 (77.7180)  acc5: 90.6250 (93.5672)  time: 0.5595  data: 0.3590  max mem: 19734
Test:  [160/261]  eta: 0:00:48  loss: 0.9952 (1.0156)  acc1: 76.5625 (77.3357)  acc5: 91.6667 (93.2680)  time: 0.3661  data: 0.2189  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: 1.3256 (1.0475)  acc1: 63.0208 (76.4620)  acc5: 87.5000 (92.9185)  time: 0.2946  data: 0.1556  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: 1.4983 (1.0652)  acc1: 64.5833 (76.0244)  acc5: 88.0208 (92.7601)  time: 0.3277  data: 0.2019  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4067 (1.0785)  acc1: 68.2292 (75.7990)  acc5: 90.1042 (92.5884)  time: 0.2026  data: 0.0999  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3890 (1.0943)  acc1: 71.8750 (75.4846)  acc5: 88.5417 (92.3533)  time: 0.1517  data: 0.0779  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3908 (1.1078)  acc1: 69.2708 (75.2098)  acc5: 88.0208 (92.1604)  time: 0.0987  data: 0.0367  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4389 (1.1278)  acc1: 66.6667 (74.7054)  acc5: 88.0208 (91.9471)  time: 0.0790  data: 0.0174  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4389 (1.1375)  acc1: 66.6667 (74.4499)  acc5: 89.5833 (91.8403)  time: 0.0754  data: 0.0137  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3275 (1.1469)  acc1: 69.2708 (74.2371)  acc5: 90.1042 (91.7596)  time: 0.0618  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0203 (1.1393)  acc1: 76.0417 (74.4211)  acc5: 94.2708 (91.8949)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9671 (1.1393)  acc1: 76.5625 (74.4140)  acc5: 95.3125 (91.9620)  time: 0.0596  data: 0.0001  max mem: 19734
Test: Total time: 0:01:31 (0.3513 s / it)
* Acc@1 74.414 Acc@5 91.962 loss 1.139
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.41%
Epoch: [21]  [   0/1251]  eta: 5:20:57  lr: 0.000018  loss: 3.0638 (3.0638)  time: 15.3941  data: 14.8746  max mem: 19734
Epoch: [21]  [  10/1251]  eta: 0:39:09  lr: 0.000018  loss: 3.6094 (3.6066)  time: 1.8935  data: 1.3535  max mem: 19734
Epoch: [21]  [  20/1251]  eta: 0:24:50  lr: 0.000018  loss: 3.6094 (3.5595)  time: 0.5017  data: 0.0009  max mem: 19734
loss info: cls_loss=3.4346, ratio_loss=0.0055, pruning_loss=0.1329, mse_loss=0.4912
Epoch: [21]  [  30/1251]  eta: 0:19:41  lr: 0.000018  loss: 3.5261 (3.5229)  time: 0.4587  data: 0.0004  max mem: 19734
Epoch: [21]  [  40/1251]  eta: 0:17:01  lr: 0.000018  loss: 3.2819 (3.4712)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [21]  [  50/1251]  eta: 0:15:22  lr: 0.000018  loss: 3.2819 (3.4754)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [21]  [  60/1251]  eta: 0:14:14  lr: 0.000018  loss: 3.6143 (3.4604)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [21]  [  70/1251]  eta: 0:13:25  lr: 0.000018  loss: 3.7277 (3.4909)  time: 0.4620  data: 0.0004  max mem: 19734
Epoch: [21]  [  80/1251]  eta: 0:12:46  lr: 0.000018  loss: 3.5407 (3.4776)  time: 0.4623  data: 0.0005  max mem: 19734
Epoch: [21]  [  90/1251]  eta: 0:12:20  lr: 0.000018  loss: 3.4597 (3.4869)  time: 0.4823  data: 0.0005  max mem: 19734
Epoch: [21]  [ 100/1251]  eta: 0:12:00  lr: 0.000018  loss: 3.8277 (3.4954)  time: 0.5116  data: 0.0005  max mem: 19734
Epoch: [21]  [ 110/1251]  eta: 0:11:42  lr: 0.000018  loss: 3.6908 (3.5032)  time: 0.5158  data: 0.0005  max mem: 19734
Epoch: [21]  [ 120/1251]  eta: 0:11:21  lr: 0.000018  loss: 3.5084 (3.5039)  time: 0.4833  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4811, ratio_loss=0.0058, pruning_loss=0.1332, mse_loss=0.5021
Epoch: [21]  [ 130/1251]  eta: 0:11:03  lr: 0.000018  loss: 3.4896 (3.5001)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [21]  [ 140/1251]  eta: 0:10:46  lr: 0.000018  loss: 3.4896 (3.4847)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [21]  [ 150/1251]  eta: 0:10:31  lr: 0.000018  loss: 3.6392 (3.4973)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [21]  [ 160/1251]  eta: 0:10:18  lr: 0.000018  loss: 3.6753 (3.4942)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [21]  [ 170/1251]  eta: 0:10:05  lr: 0.000018  loss: 3.6105 (3.4921)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [21]  [ 180/1251]  eta: 0:09:53  lr: 0.000018  loss: 3.5583 (3.4948)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [21]  [ 190/1251]  eta: 0:09:42  lr: 0.000018  loss: 3.5510 (3.4932)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [21]  [ 200/1251]  eta: 0:09:32  lr: 0.000018  loss: 3.5231 (3.4866)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [21]  [ 210/1251]  eta: 0:09:22  lr: 0.000018  loss: 3.2569 (3.4710)  time: 0.4574  data: 0.0005  max mem: 19734
Epoch: [21]  [ 220/1251]  eta: 0:09:13  lr: 0.000018  loss: 3.2800 (3.4709)  time: 0.4566  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3918, ratio_loss=0.0057, pruning_loss=0.1351, mse_loss=0.5090
Epoch: [21]  [ 230/1251]  eta: 0:09:04  lr: 0.000018  loss: 3.5520 (3.4695)  time: 0.4583  data: 0.0005  max mem: 19734
Epoch: [21]  [ 240/1251]  eta: 0:08:58  lr: 0.000018  loss: 3.5851 (3.4638)  time: 0.4867  data: 0.0005  max mem: 19734
Epoch: [21]  [ 250/1251]  eta: 0:08:52  lr: 0.000018  loss: 3.5499 (3.4617)  time: 0.5104  data: 0.0007  max mem: 19734
Epoch: [21]  [ 260/1251]  eta: 0:08:43  lr: 0.000018  loss: 3.5019 (3.4609)  time: 0.4802  data: 0.0007  max mem: 19734
Epoch: [21]  [ 270/1251]  eta: 0:08:35  lr: 0.000018  loss: 3.6320 (3.4736)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [21]  [ 280/1251]  eta: 0:08:28  lr: 0.000018  loss: 3.7069 (3.4744)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [21]  [ 290/1251]  eta: 0:08:20  lr: 0.000018  loss: 3.4496 (3.4720)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [21]  [ 300/1251]  eta: 0:08:13  lr: 0.000018  loss: 3.6695 (3.4768)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [21]  [ 310/1251]  eta: 0:08:06  lr: 0.000018  loss: 3.6339 (3.4761)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [21]  [ 320/1251]  eta: 0:07:59  lr: 0.000018  loss: 3.5331 (3.4729)  time: 0.4574  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4499, ratio_loss=0.0054, pruning_loss=0.1327, mse_loss=0.5209
Epoch: [21]  [ 330/1251]  eta: 0:07:52  lr: 0.000018  loss: 3.5331 (3.4690)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [21]  [ 340/1251]  eta: 0:07:45  lr: 0.000018  loss: 3.4094 (3.4673)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [21]  [ 350/1251]  eta: 0:07:39  lr: 0.000018  loss: 3.2060 (3.4612)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [21]  [ 360/1251]  eta: 0:07:32  lr: 0.000018  loss: 3.2060 (3.4597)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [21]  [ 370/1251]  eta: 0:07:26  lr: 0.000018  loss: 3.2437 (3.4499)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [21]  [ 380/1251]  eta: 0:07:20  lr: 0.000018  loss: 3.5347 (3.4566)  time: 0.4624  data: 0.0004  max mem: 19734
Epoch: [21]  [ 390/1251]  eta: 0:07:15  lr: 0.000018  loss: 3.7780 (3.4658)  time: 0.4919  data: 0.0004  max mem: 19734
Epoch: [21]  [ 400/1251]  eta: 0:07:10  lr: 0.000018  loss: 3.9169 (3.4701)  time: 0.5044  data: 0.0004  max mem: 19734
Epoch: [21]  [ 410/1251]  eta: 0:07:04  lr: 0.000018  loss: 3.5360 (3.4738)  time: 0.4760  data: 0.0005  max mem: 19734
Epoch: [21]  [ 420/1251]  eta: 0:06:58  lr: 0.000018  loss: 3.5144 (3.4732)  time: 0.4569  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4474, ratio_loss=0.0054, pruning_loss=0.1344, mse_loss=0.5031
Epoch: [21]  [ 430/1251]  eta: 0:06:52  lr: 0.000018  loss: 3.4398 (3.4708)  time: 0.4593  data: 0.0005  max mem: 19734
Epoch: [21]  [ 440/1251]  eta: 0:06:46  lr: 0.000018  loss: 3.5768 (3.4733)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [21]  [ 450/1251]  eta: 0:06:40  lr: 0.000018  loss: 3.6082 (3.4756)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [21]  [ 460/1251]  eta: 0:06:35  lr: 0.000018  loss: 3.5846 (3.4737)  time: 0.4564  data: 0.0005  max mem: 19734
Epoch: [21]  [ 470/1251]  eta: 0:06:29  lr: 0.000018  loss: 3.3504 (3.4713)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [21]  [ 480/1251]  eta: 0:06:23  lr: 0.000018  loss: 3.3511 (3.4720)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [21]  [ 490/1251]  eta: 0:06:18  lr: 0.000018  loss: 3.1623 (3.4634)  time: 0.4583  data: 0.0004  max mem: 19734
Epoch: [21]  [ 500/1251]  eta: 0:06:12  lr: 0.000018  loss: 3.2313 (3.4654)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [21]  [ 510/1251]  eta: 0:06:06  lr: 0.000018  loss: 3.6022 (3.4658)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [21]  [ 520/1251]  eta: 0:06:01  lr: 0.000018  loss: 3.5687 (3.4646)  time: 0.4546  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4205, ratio_loss=0.0060, pruning_loss=0.1321, mse_loss=0.5066
Epoch: [21]  [ 530/1251]  eta: 0:05:57  lr: 0.000018  loss: 3.5811 (3.4680)  time: 0.4925  data: 0.0005  max mem: 19734
Epoch: [21]  [ 540/1251]  eta: 0:05:52  lr: 0.000018  loss: 3.6021 (3.4720)  time: 0.5171  data: 0.0005  max mem: 19734
Epoch: [21]  [ 550/1251]  eta: 0:05:46  lr: 0.000018  loss: 3.6418 (3.4708)  time: 0.4788  data: 0.0005  max mem: 19734
Epoch: [21]  [ 560/1251]  eta: 0:05:41  lr: 0.000018  loss: 3.6424 (3.4723)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [21]  [ 570/1251]  eta: 0:05:35  lr: 0.000018  loss: 3.4869 (3.4674)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [21]  [ 580/1251]  eta: 0:05:30  lr: 0.000018  loss: 3.4482 (3.4699)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [21]  [ 590/1251]  eta: 0:05:25  lr: 0.000018  loss: 3.6684 (3.4743)  time: 0.4532  data: 0.0006  max mem: 19734
Epoch: [21]  [ 600/1251]  eta: 0:05:19  lr: 0.000018  loss: 3.7516 (3.4755)  time: 0.4531  data: 0.0006  max mem: 19734
Epoch: [21]  [ 610/1251]  eta: 0:05:14  lr: 0.000018  loss: 3.6299 (3.4773)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [21]  [ 620/1251]  eta: 0:05:09  lr: 0.000018  loss: 3.5616 (3.4766)  time: 0.4569  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5052, ratio_loss=0.0052, pruning_loss=0.1337, mse_loss=0.5135
Epoch: [21]  [ 630/1251]  eta: 0:05:03  lr: 0.000018  loss: 3.5616 (3.4774)  time: 0.4588  data: 0.0005  max mem: 19734
Epoch: [21]  [ 640/1251]  eta: 0:04:58  lr: 0.000018  loss: 3.5548 (3.4756)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [21]  [ 650/1251]  eta: 0:04:53  lr: 0.000018  loss: 3.5577 (3.4759)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [21]  [ 660/1251]  eta: 0:04:48  lr: 0.000018  loss: 3.5577 (3.4729)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [21]  [ 670/1251]  eta: 0:04:43  lr: 0.000018  loss: 3.3085 (3.4719)  time: 0.4620  data: 0.0004  max mem: 19734
Epoch: [21]  [ 680/1251]  eta: 0:04:38  lr: 0.000018  loss: 3.5680 (3.4732)  time: 0.4911  data: 0.0004  max mem: 19734
Epoch: [21]  [ 690/1251]  eta: 0:04:33  lr: 0.000018  loss: 3.7485 (3.4751)  time: 0.4998  data: 0.0004  max mem: 19734
Epoch: [21]  [ 700/1251]  eta: 0:04:28  lr: 0.000018  loss: 3.6221 (3.4728)  time: 0.4710  data: 0.0004  max mem: 19734
Epoch: [21]  [ 710/1251]  eta: 0:04:23  lr: 0.000018  loss: 3.4497 (3.4707)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [21]  [ 720/1251]  eta: 0:04:18  lr: 0.000018  loss: 3.3229 (3.4691)  time: 0.4535  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4088, ratio_loss=0.0053, pruning_loss=0.1325, mse_loss=0.4996
Epoch: [21]  [ 730/1251]  eta: 0:04:13  lr: 0.000018  loss: 3.4267 (3.4694)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [21]  [ 740/1251]  eta: 0:04:08  lr: 0.000018  loss: 3.3228 (3.4668)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [21]  [ 750/1251]  eta: 0:04:03  lr: 0.000018  loss: 3.3228 (3.4659)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [21]  [ 760/1251]  eta: 0:03:58  lr: 0.000018  loss: 3.5937 (3.4665)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [21]  [ 770/1251]  eta: 0:03:53  lr: 0.000018  loss: 3.5937 (3.4652)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [21]  [ 780/1251]  eta: 0:03:47  lr: 0.000018  loss: 3.5850 (3.4689)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [21]  [ 790/1251]  eta: 0:03:42  lr: 0.000018  loss: 3.5599 (3.4669)  time: 0.4512  data: 0.0004  max mem: 19734
Epoch: [21]  [ 800/1251]  eta: 0:03:37  lr: 0.000018  loss: 3.2846 (3.4655)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [21]  [ 810/1251]  eta: 0:03:32  lr: 0.000018  loss: 3.5933 (3.4650)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [21]  [ 820/1251]  eta: 0:03:28  lr: 0.000018  loss: 3.6458 (3.4701)  time: 0.4834  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4462, ratio_loss=0.0058, pruning_loss=0.1334, mse_loss=0.5282
Epoch: [21]  [ 830/1251]  eta: 0:03:23  lr: 0.000018  loss: 3.7528 (3.4710)  time: 0.5221  data: 0.0004  max mem: 19734
Epoch: [21]  [ 840/1251]  eta: 0:03:18  lr: 0.000018  loss: 3.5870 (3.4713)  time: 0.4918  data: 0.0004  max mem: 19734
Epoch: [21]  [ 850/1251]  eta: 0:03:13  lr: 0.000018  loss: 3.4956 (3.4718)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [21]  [ 860/1251]  eta: 0:03:08  lr: 0.000018  loss: 3.4956 (3.4723)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [21]  [ 870/1251]  eta: 0:03:03  lr: 0.000018  loss: 3.5083 (3.4722)  time: 0.4509  data: 0.0004  max mem: 19734
Epoch: [21]  [ 880/1251]  eta: 0:02:58  lr: 0.000018  loss: 3.6263 (3.4732)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [21]  [ 890/1251]  eta: 0:02:53  lr: 0.000018  loss: 3.6263 (3.4757)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [21]  [ 900/1251]  eta: 0:02:48  lr: 0.000018  loss: 3.5164 (3.4747)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [21]  [ 910/1251]  eta: 0:02:44  lr: 0.000018  loss: 3.5115 (3.4743)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [21]  [ 920/1251]  eta: 0:02:39  lr: 0.000018  loss: 3.4765 (3.4720)  time: 0.4529  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4559, ratio_loss=0.0056, pruning_loss=0.1324, mse_loss=0.4995
Epoch: [21]  [ 930/1251]  eta: 0:02:34  lr: 0.000018  loss: 3.4838 (3.4740)  time: 0.4530  data: 0.0006  max mem: 19734
Epoch: [21]  [ 940/1251]  eta: 0:02:29  lr: 0.000018  loss: 3.4377 (3.4703)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [21]  [ 950/1251]  eta: 0:02:24  lr: 0.000018  loss: 3.2575 (3.4684)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [21]  [ 960/1251]  eta: 0:02:19  lr: 0.000018  loss: 3.1673 (3.4649)  time: 0.4722  data: 0.0004  max mem: 19734
Epoch: [21]  [ 970/1251]  eta: 0:02:14  lr: 0.000018  loss: 3.2039 (3.4645)  time: 0.4974  data: 0.0004  max mem: 19734
Epoch: [21]  [ 980/1251]  eta: 0:02:10  lr: 0.000018  loss: 3.4919 (3.4631)  time: 0.4935  data: 0.0004  max mem: 19734
Epoch: [21]  [ 990/1251]  eta: 0:02:05  lr: 0.000018  loss: 3.6928 (3.4659)  time: 0.4700  data: 0.0004  max mem: 19734
Epoch: [21]  [1000/1251]  eta: 0:02:00  lr: 0.000018  loss: 3.6928 (3.4676)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [21]  [1010/1251]  eta: 0:01:55  lr: 0.000018  loss: 3.6597 (3.4670)  time: 0.4634  data: 0.0005  max mem: 19734
Epoch: [21]  [1020/1251]  eta: 0:01:50  lr: 0.000018  loss: 3.6910 (3.4684)  time: 0.4598  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3719, ratio_loss=0.0054, pruning_loss=0.1354, mse_loss=0.5110
Epoch: [21]  [1030/1251]  eta: 0:01:45  lr: 0.000018  loss: 3.4469 (3.4657)  time: 0.4564  data: 0.0006  max mem: 19734
Epoch: [21]  [1040/1251]  eta: 0:01:41  lr: 0.000018  loss: 3.2282 (3.4635)  time: 0.4588  data: 0.0004  max mem: 19734
Epoch: [21]  [1050/1251]  eta: 0:01:36  lr: 0.000018  loss: 3.4289 (3.4644)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [21]  [1060/1251]  eta: 0:01:31  lr: 0.000018  loss: 3.7219 (3.4652)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [21]  [1070/1251]  eta: 0:01:26  lr: 0.000018  loss: 3.2588 (3.4611)  time: 0.4553  data: 0.0005  max mem: 19734
Epoch: [21]  [1080/1251]  eta: 0:01:21  lr: 0.000018  loss: 3.2381 (3.4621)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [21]  [1090/1251]  eta: 0:01:16  lr: 0.000018  loss: 3.7422 (3.4631)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [21]  [1100/1251]  eta: 0:01:12  lr: 0.000018  loss: 3.3835 (3.4612)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [21]  [1110/1251]  eta: 0:01:07  lr: 0.000018  loss: 3.1805 (3.4614)  time: 0.4810  data: 0.0004  max mem: 19734
Epoch: [21]  [1120/1251]  eta: 0:01:02  lr: 0.000018  loss: 3.6214 (3.4629)  time: 0.5203  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4036, ratio_loss=0.0053, pruning_loss=0.1358, mse_loss=0.5117
Epoch: [21]  [1130/1251]  eta: 0:00:57  lr: 0.000018  loss: 3.6752 (3.4633)  time: 0.5055  data: 0.0005  max mem: 19734
Epoch: [21]  [1140/1251]  eta: 0:00:53  lr: 0.000018  loss: 3.6752 (3.4630)  time: 0.4661  data: 0.0005  max mem: 19734
Epoch: [21]  [1150/1251]  eta: 0:00:48  lr: 0.000018  loss: 3.6158 (3.4614)  time: 0.4554  data: 0.0005  max mem: 19734
Epoch: [21]  [1160/1251]  eta: 0:00:43  lr: 0.000018  loss: 3.4782 (3.4598)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [21]  [1170/1251]  eta: 0:00:38  lr: 0.000018  loss: 3.4938 (3.4610)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [21]  [1180/1251]  eta: 0:00:33  lr: 0.000018  loss: 3.4887 (3.4599)  time: 0.4508  data: 0.0005  max mem: 19734
Epoch: [21]  [1190/1251]  eta: 0:00:29  lr: 0.000018  loss: 3.2806 (3.4594)  time: 0.4495  data: 0.0009  max mem: 19734
Epoch: [21]  [1200/1251]  eta: 0:00:24  lr: 0.000018  loss: 3.2806 (3.4577)  time: 0.4471  data: 0.0008  max mem: 19734
Epoch: [21]  [1210/1251]  eta: 0:00:19  lr: 0.000018  loss: 3.6224 (3.4589)  time: 0.4451  data: 0.0002  max mem: 19734
Epoch: [21]  [1220/1251]  eta: 0:00:14  lr: 0.000018  loss: 3.7600 (3.4592)  time: 0.4449  data: 0.0002  max mem: 19734
loss info: cls_loss=3.3932, ratio_loss=0.0058, pruning_loss=0.1358, mse_loss=0.5130
Epoch: [21]  [1230/1251]  eta: 0:00:09  lr: 0.000018  loss: 3.4560 (3.4588)  time: 0.4447  data: 0.0002  max mem: 19734
Epoch: [21]  [1240/1251]  eta: 0:00:05  lr: 0.000018  loss: 3.4509 (3.4583)  time: 0.4448  data: 0.0001  max mem: 19734
Epoch: [21]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.3767 (3.4565)  time: 0.4451  data: 0.0001  max mem: 19734
Epoch: [21] Total time: 0:09:55 (0.4762 s / it)
Averaged stats: lr: 0.000018  loss: 3.3767 (3.4350)
Test:  [  0/261]  eta: 1:45:01  loss: 0.6966 (0.6966)  acc1: 83.3333 (83.3333)  acc5: 96.3542 (96.3542)  time: 24.1453  data: 23.8821  max mem: 19734
Test:  [ 10/261]  eta: 0:13:08  loss: 0.6966 (0.7428)  acc1: 83.3333 (83.5701)  acc5: 96.3542 (96.0227)  time: 3.1423  data: 2.9688  max mem: 19734
Test:  [ 20/261]  eta: 0:07:03  loss: 0.9417 (0.9025)  acc1: 79.1667 (79.1915)  acc5: 94.2708 (94.7917)  time: 0.6365  data: 0.4427  max mem: 19734
Test:  [ 30/261]  eta: 0:04:55  loss: 0.8482 (0.8265)  acc1: 81.7708 (81.7708)  acc5: 94.2708 (95.1781)  time: 0.2566  data: 0.0114  max mem: 19734
Test:  [ 40/261]  eta: 0:04:13  loss: 0.6080 (0.7929)  acc1: 87.5000 (82.7363)  acc5: 96.3542 (95.4268)  time: 0.5059  data: 0.2863  max mem: 19734
Test:  [ 50/261]  eta: 0:03:23  loss: 0.9087 (0.8475)  acc1: 78.6458 (81.1172)  acc5: 95.8333 (95.0776)  time: 0.4766  data: 0.2895  max mem: 19734
Test:  [ 60/261]  eta: 0:02:48  loss: 0.9495 (0.8592)  acc1: 76.0417 (80.5243)  acc5: 94.2708 (95.0990)  time: 0.2131  data: 0.0166  max mem: 19734
Test:  [ 70/261]  eta: 0:02:30  loss: 0.9330 (0.8592)  acc1: 76.0417 (80.1937)  acc5: 96.3542 (95.2685)  time: 0.3373  data: 0.1735  max mem: 19734
Test:  [ 80/261]  eta: 0:02:09  loss: 0.8469 (0.8616)  acc1: 79.1667 (80.3048)  acc5: 96.8750 (95.3511)  time: 0.3333  data: 0.1724  max mem: 19734
Test:  [ 90/261]  eta: 0:01:53  loss: 0.8036 (0.8473)  acc1: 84.3750 (80.7292)  acc5: 95.8333 (95.4728)  time: 0.2148  data: 0.0107  max mem: 19734
Test:  [100/261]  eta: 0:01:47  loss: 0.8296 (0.8507)  acc1: 83.3333 (80.6982)  acc5: 95.3125 (95.4930)  time: 0.4688  data: 0.2751  max mem: 19734
Test:  [110/261]  eta: 0:01:33  loss: 0.8873 (0.8759)  acc1: 78.6458 (80.1286)  acc5: 94.2708 (95.1811)  time: 0.4279  data: 0.2776  max mem: 19734
Test:  [120/261]  eta: 0:01:21  loss: 1.2624 (0.9182)  acc1: 70.8333 (79.1064)  acc5: 90.1042 (94.6066)  time: 0.1486  data: 0.0174  max mem: 19734
Test:  [130/261]  eta: 0:01:11  loss: 1.4034 (0.9623)  acc1: 69.2708 (78.1687)  acc5: 86.4583 (94.0402)  time: 0.1534  data: 0.0172  max mem: 19734
Test:  [140/261]  eta: 0:01:06  loss: 1.3438 (0.9893)  acc1: 69.7917 (77.5783)  acc5: 87.5000 (93.7205)  time: 0.3748  data: 0.1824  max mem: 19734
Test:  [150/261]  eta: 0:00:59  loss: 1.2475 (0.9955)  acc1: 72.3958 (77.5490)  acc5: 91.1458 (93.5499)  time: 0.4631  data: 0.1913  max mem: 19734
Test:  [160/261]  eta: 0:00:53  loss: 0.9820 (1.0154)  acc1: 77.0833 (77.2127)  acc5: 91.6667 (93.2842)  time: 0.3362  data: 0.1214  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: 1.2806 (1.0465)  acc1: 65.6250 (76.4376)  acc5: 87.5000 (92.9398)  time: 0.3063  data: 0.1983  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4494 (1.0641)  acc1: 64.5833 (76.0071)  acc5: 88.0208 (92.7716)  time: 0.1715  data: 0.0946  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3811 (1.0771)  acc1: 66.1458 (75.7363)  acc5: 90.1042 (92.6402)  time: 0.0655  data: 0.0024  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3811 (1.0933)  acc1: 70.8333 (75.3939)  acc5: 89.5833 (92.4078)  time: 0.0655  data: 0.0026  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.4052 (1.1081)  acc1: 69.7917 (75.1308)  acc5: 87.5000 (92.1727)  time: 0.0659  data: 0.0009  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4724 (1.1280)  acc1: 67.7083 (74.6300)  acc5: 88.5417 (91.9778)  time: 0.0649  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4778 (1.1387)  acc1: 66.1458 (74.3687)  acc5: 89.0625 (91.8831)  time: 0.0621  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3131 (1.1484)  acc1: 67.7083 (74.1723)  acc5: 90.6250 (91.7985)  time: 0.0620  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0611 (1.1410)  acc1: 76.0417 (74.3422)  acc5: 93.7500 (91.9219)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9497 (1.1409)  acc1: 76.5625 (74.3700)  acc5: 95.3125 (91.9840)  time: 0.0596  data: 0.0002  max mem: 19734
Test: Total time: 0:01:33 (0.3574 s / it)
* Acc@1 74.370 Acc@5 91.984 loss 1.141
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.41%
Epoch: [22]  [   0/1251]  eta: 6:07:23  lr: 0.000018  loss: 3.9416 (3.9416)  time: 17.6208  data: 8.2314  max mem: 19734
Epoch: [22]  [  10/1251]  eta: 0:45:19  lr: 0.000018  loss: 3.6437 (3.6174)  time: 2.1913  data: 0.7502  max mem: 19734
Epoch: [22]  [  20/1251]  eta: 0:28:03  lr: 0.000018  loss: 3.8490 (3.6604)  time: 0.5551  data: 0.0012  max mem: 19734
Epoch: [22]  [  30/1251]  eta: 0:21:52  lr: 0.000018  loss: 3.7954 (3.6282)  time: 0.4610  data: 0.0004  max mem: 19734
Epoch: [22]  [  40/1251]  eta: 0:18:41  lr: 0.000018  loss: 3.7499 (3.5978)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [22]  [  50/1251]  eta: 0:16:42  lr: 0.000018  loss: 3.4786 (3.5284)  time: 0.4618  data: 0.0005  max mem: 19734
Epoch: [22]  [  60/1251]  eta: 0:15:20  lr: 0.000018  loss: 3.4207 (3.4930)  time: 0.4606  data: 0.0005  max mem: 19734
Epoch: [22]  [  70/1251]  eta: 0:14:20  lr: 0.000018  loss: 3.4056 (3.4821)  time: 0.4598  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3864, ratio_loss=0.0057, pruning_loss=0.1350, mse_loss=0.5012
Epoch: [22]  [  80/1251]  eta: 0:13:34  lr: 0.000018  loss: 3.4056 (3.4725)  time: 0.4599  data: 0.0005  max mem: 19734
Epoch: [22]  [  90/1251]  eta: 0:12:57  lr: 0.000018  loss: 3.6361 (3.4695)  time: 0.4610  data: 0.0005  max mem: 19734
Epoch: [22]  [ 100/1251]  eta: 0:12:27  lr: 0.000018  loss: 3.2671 (3.4339)  time: 0.4594  data: 0.0006  max mem: 19734
Epoch: [22]  [ 110/1251]  eta: 0:12:00  lr: 0.000018  loss: 3.2671 (3.4489)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [22]  [ 120/1251]  eta: 0:11:38  lr: 0.000018  loss: 3.5808 (3.4404)  time: 0.4589  data: 0.0005  max mem: 19734
Epoch: [22]  [ 130/1251]  eta: 0:11:18  lr: 0.000018  loss: 3.3838 (3.4327)  time: 0.4592  data: 0.0005  max mem: 19734
Epoch: [22]  [ 140/1251]  eta: 0:11:04  lr: 0.000018  loss: 3.7405 (3.4511)  time: 0.4785  data: 0.0005  max mem: 19734
Epoch: [22]  [ 150/1251]  eta: 0:10:53  lr: 0.000018  loss: 3.7235 (3.4478)  time: 0.5162  data: 0.0005  max mem: 19734
Epoch: [22]  [ 160/1251]  eta: 0:10:38  lr: 0.000018  loss: 3.3966 (3.4293)  time: 0.4959  data: 0.0005  max mem: 19734
Epoch: [22]  [ 170/1251]  eta: 0:10:24  lr: 0.000018  loss: 3.4138 (3.4349)  time: 0.4574  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3995, ratio_loss=0.0055, pruning_loss=0.1346, mse_loss=0.5042
Epoch: [22]  [ 180/1251]  eta: 0:10:11  lr: 0.000018  loss: 3.5695 (3.4380)  time: 0.4555  data: 0.0006  max mem: 19734
Epoch: [22]  [ 190/1251]  eta: 0:09:59  lr: 0.000018  loss: 3.4875 (3.4276)  time: 0.4557  data: 0.0006  max mem: 19734
Epoch: [22]  [ 200/1251]  eta: 0:09:48  lr: 0.000018  loss: 3.3845 (3.4326)  time: 0.4577  data: 0.0004  max mem: 19734
Epoch: [22]  [ 210/1251]  eta: 0:09:37  lr: 0.000018  loss: 3.4810 (3.4280)  time: 0.4586  data: 0.0004  max mem: 19734
Epoch: [22]  [ 220/1251]  eta: 0:09:27  lr: 0.000018  loss: 3.2498 (3.4167)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [22]  [ 230/1251]  eta: 0:09:17  lr: 0.000018  loss: 3.2498 (3.4145)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [22]  [ 240/1251]  eta: 0:09:08  lr: 0.000018  loss: 3.4967 (3.4152)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [22]  [ 250/1251]  eta: 0:08:59  lr: 0.000018  loss: 3.5175 (3.4173)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [22]  [ 260/1251]  eta: 0:08:51  lr: 0.000018  loss: 3.5449 (3.4124)  time: 0.4571  data: 0.0005  max mem: 19734
Epoch: [22]  [ 270/1251]  eta: 0:08:42  lr: 0.000018  loss: 3.2133 (3.4089)  time: 0.4555  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3538, ratio_loss=0.0056, pruning_loss=0.1342, mse_loss=0.5035
Epoch: [22]  [ 280/1251]  eta: 0:08:35  lr: 0.000018  loss: 3.5574 (3.4186)  time: 0.4636  data: 0.0005  max mem: 19734
Epoch: [22]  [ 290/1251]  eta: 0:08:28  lr: 0.000018  loss: 3.6945 (3.4195)  time: 0.4828  data: 0.0006  max mem: 19734
Epoch: [22]  [ 300/1251]  eta: 0:08:22  lr: 0.000018  loss: 3.6558 (3.4230)  time: 0.4937  data: 0.0008  max mem: 19734
Epoch: [22]  [ 310/1251]  eta: 0:08:14  lr: 0.000018  loss: 3.5380 (3.4239)  time: 0.4732  data: 0.0007  max mem: 19734
Epoch: [22]  [ 320/1251]  eta: 0:08:07  lr: 0.000018  loss: 3.6889 (3.4316)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [22]  [ 330/1251]  eta: 0:08:00  lr: 0.000018  loss: 3.7210 (3.4391)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [22]  [ 340/1251]  eta: 0:07:53  lr: 0.000018  loss: 3.6575 (3.4416)  time: 0.4556  data: 0.0006  max mem: 19734
Epoch: [22]  [ 350/1251]  eta: 0:07:46  lr: 0.000018  loss: 3.7052 (3.4463)  time: 0.4569  data: 0.0006  max mem: 19734
Epoch: [22]  [ 360/1251]  eta: 0:07:39  lr: 0.000018  loss: 3.7816 (3.4491)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [22]  [ 370/1251]  eta: 0:07:33  lr: 0.000018  loss: 3.5549 (3.4466)  time: 0.4550  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4803, ratio_loss=0.0051, pruning_loss=0.1327, mse_loss=0.5151
Epoch: [22]  [ 380/1251]  eta: 0:07:26  lr: 0.000018  loss: 3.3467 (3.4408)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [22]  [ 390/1251]  eta: 0:07:20  lr: 0.000018  loss: 3.1589 (3.4344)  time: 0.4577  data: 0.0007  max mem: 19734
Epoch: [22]  [ 400/1251]  eta: 0:07:14  lr: 0.000018  loss: 3.3477 (3.4358)  time: 0.4556  data: 0.0006  max mem: 19734
Epoch: [22]  [ 410/1251]  eta: 0:07:07  lr: 0.000018  loss: 3.4329 (3.4363)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [22]  [ 420/1251]  eta: 0:07:01  lr: 0.000018  loss: 3.3766 (3.4294)  time: 0.4594  data: 0.0004  max mem: 19734
Epoch: [22]  [ 430/1251]  eta: 0:06:56  lr: 0.000018  loss: 3.3766 (3.4320)  time: 0.4837  data: 0.0004  max mem: 19734
Epoch: [22]  [ 440/1251]  eta: 0:06:52  lr: 0.000018  loss: 3.5467 (3.4317)  time: 0.5195  data: 0.0005  max mem: 19734
Epoch: [22]  [ 450/1251]  eta: 0:06:46  lr: 0.000018  loss: 3.5769 (3.4362)  time: 0.4972  data: 0.0005  max mem: 19734
Epoch: [22]  [ 460/1251]  eta: 0:06:40  lr: 0.000018  loss: 3.5769 (3.4325)  time: 0.4604  data: 0.0005  max mem: 19734
Epoch: [22]  [ 470/1251]  eta: 0:06:34  lr: 0.000018  loss: 3.3265 (3.4327)  time: 0.4604  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3791, ratio_loss=0.0060, pruning_loss=0.1338, mse_loss=0.5069
Epoch: [22]  [ 480/1251]  eta: 0:06:28  lr: 0.000018  loss: 3.5383 (3.4345)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [22]  [ 490/1251]  eta: 0:06:23  lr: 0.000018  loss: 3.5570 (3.4311)  time: 0.4600  data: 0.0005  max mem: 19734
Epoch: [22]  [ 500/1251]  eta: 0:06:17  lr: 0.000018  loss: 3.3420 (3.4314)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [22]  [ 510/1251]  eta: 0:06:11  lr: 0.000018  loss: 3.3420 (3.4295)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [22]  [ 520/1251]  eta: 0:06:05  lr: 0.000018  loss: 3.5255 (3.4323)  time: 0.4538  data: 0.0006  max mem: 19734
Epoch: [22]  [ 530/1251]  eta: 0:06:00  lr: 0.000018  loss: 3.5255 (3.4306)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [22]  [ 540/1251]  eta: 0:05:54  lr: 0.000018  loss: 3.2690 (3.4286)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [22]  [ 550/1251]  eta: 0:05:49  lr: 0.000018  loss: 3.3322 (3.4261)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [22]  [ 560/1251]  eta: 0:05:43  lr: 0.000018  loss: 3.6520 (3.4309)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [22]  [ 570/1251]  eta: 0:05:38  lr: 0.000018  loss: 3.7041 (3.4356)  time: 0.4724  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3943, ratio_loss=0.0055, pruning_loss=0.1352, mse_loss=0.4944
Epoch: [22]  [ 580/1251]  eta: 0:05:33  lr: 0.000018  loss: 3.5755 (3.4283)  time: 0.4917  data: 0.0005  max mem: 19734
Epoch: [22]  [ 590/1251]  eta: 0:05:28  lr: 0.000018  loss: 3.5068 (3.4325)  time: 0.4914  data: 0.0004  max mem: 19734
Epoch: [22]  [ 600/1251]  eta: 0:05:23  lr: 0.000018  loss: 3.5413 (3.4310)  time: 0.4712  data: 0.0004  max mem: 19734
Epoch: [22]  [ 610/1251]  eta: 0:05:17  lr: 0.000018  loss: 3.2639 (3.4268)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [22]  [ 620/1251]  eta: 0:05:12  lr: 0.000018  loss: 3.2865 (3.4258)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [22]  [ 630/1251]  eta: 0:05:06  lr: 0.000018  loss: 3.5990 (3.4318)  time: 0.4537  data: 0.0006  max mem: 19734
Epoch: [22]  [ 640/1251]  eta: 0:05:01  lr: 0.000018  loss: 3.6422 (3.4283)  time: 0.4557  data: 0.0006  max mem: 19734
Epoch: [22]  [ 650/1251]  eta: 0:04:56  lr: 0.000018  loss: 3.6114 (3.4310)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [22]  [ 660/1251]  eta: 0:04:51  lr: 0.000018  loss: 3.5091 (3.4319)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [22]  [ 670/1251]  eta: 0:04:45  lr: 0.000018  loss: 3.3631 (3.4284)  time: 0.4572  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3901, ratio_loss=0.0058, pruning_loss=0.1354, mse_loss=0.4932
Epoch: [22]  [ 680/1251]  eta: 0:04:40  lr: 0.000018  loss: 3.3685 (3.4309)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [22]  [ 690/1251]  eta: 0:04:35  lr: 0.000018  loss: 3.5902 (3.4291)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [22]  [ 700/1251]  eta: 0:04:30  lr: 0.000018  loss: 3.2507 (3.4286)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [22]  [ 710/1251]  eta: 0:04:25  lr: 0.000018  loss: 3.5789 (3.4312)  time: 0.4722  data: 0.0005  max mem: 19734
Epoch: [22]  [ 720/1251]  eta: 0:04:20  lr: 0.000018  loss: 3.3714 (3.4261)  time: 0.4777  data: 0.0005  max mem: 19734
Epoch: [22]  [ 730/1251]  eta: 0:04:15  lr: 0.000018  loss: 3.3714 (3.4289)  time: 0.5082  data: 0.0004  max mem: 19734
Epoch: [22]  [ 740/1251]  eta: 0:04:10  lr: 0.000018  loss: 3.7258 (3.4330)  time: 0.5010  data: 0.0004  max mem: 19734
Epoch: [22]  [ 750/1251]  eta: 0:04:05  lr: 0.000018  loss: 3.5733 (3.4328)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [22]  [ 760/1251]  eta: 0:04:00  lr: 0.000018  loss: 3.4646 (3.4327)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [22]  [ 770/1251]  eta: 0:03:55  lr: 0.000018  loss: 3.4729 (3.4331)  time: 0.4510  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4172, ratio_loss=0.0056, pruning_loss=0.1345, mse_loss=0.5027
Epoch: [22]  [ 780/1251]  eta: 0:03:50  lr: 0.000018  loss: 3.4729 (3.4317)  time: 0.4509  data: 0.0004  max mem: 19734
Epoch: [22]  [ 790/1251]  eta: 0:03:44  lr: 0.000018  loss: 3.5146 (3.4332)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [22]  [ 800/1251]  eta: 0:03:39  lr: 0.000018  loss: 3.5146 (3.4317)  time: 0.4522  data: 0.0005  max mem: 19734
Epoch: [22]  [ 810/1251]  eta: 0:03:34  lr: 0.000018  loss: 3.2628 (3.4299)  time: 0.4522  data: 0.0005  max mem: 19734
Epoch: [22]  [ 820/1251]  eta: 0:03:29  lr: 0.000018  loss: 3.3395 (3.4284)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [22]  [ 830/1251]  eta: 0:03:24  lr: 0.000018  loss: 3.3372 (3.4257)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [22]  [ 840/1251]  eta: 0:03:19  lr: 0.000018  loss: 3.4258 (3.4284)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [22]  [ 850/1251]  eta: 0:03:14  lr: 0.000018  loss: 3.6431 (3.4286)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [22]  [ 860/1251]  eta: 0:03:09  lr: 0.000018  loss: 3.2929 (3.4274)  time: 0.4693  data: 0.0005  max mem: 19734
Epoch: [22]  [ 870/1251]  eta: 0:03:04  lr: 0.000018  loss: 3.2929 (3.4255)  time: 0.4888  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3533, ratio_loss=0.0053, pruning_loss=0.1344, mse_loss=0.5091
Epoch: [22]  [ 880/1251]  eta: 0:03:00  lr: 0.000018  loss: 3.4927 (3.4250)  time: 0.4975  data: 0.0005  max mem: 19734
Epoch: [22]  [ 890/1251]  eta: 0:02:55  lr: 0.000018  loss: 3.5257 (3.4243)  time: 0.4793  data: 0.0006  max mem: 19734
Epoch: [22]  [ 900/1251]  eta: 0:02:50  lr: 0.000018  loss: 3.5257 (3.4240)  time: 0.4543  data: 0.0005  max mem: 19734
Epoch: [22]  [ 910/1251]  eta: 0:02:45  lr: 0.000018  loss: 3.6374 (3.4235)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [22]  [ 920/1251]  eta: 0:02:40  lr: 0.000018  loss: 3.4431 (3.4222)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [22]  [ 930/1251]  eta: 0:02:35  lr: 0.000018  loss: 3.2608 (3.4217)  time: 0.4584  data: 0.0004  max mem: 19734
Epoch: [22]  [ 940/1251]  eta: 0:02:30  lr: 0.000018  loss: 3.5338 (3.4227)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [22]  [ 950/1251]  eta: 0:02:25  lr: 0.000018  loss: 3.6585 (3.4230)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [22]  [ 960/1251]  eta: 0:02:20  lr: 0.000018  loss: 3.4201 (3.4216)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [22]  [ 970/1251]  eta: 0:02:15  lr: 0.000018  loss: 3.4201 (3.4216)  time: 0.4551  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3565, ratio_loss=0.0055, pruning_loss=0.1357, mse_loss=0.5299
Epoch: [22]  [ 980/1251]  eta: 0:02:10  lr: 0.000018  loss: 3.6435 (3.4222)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [22]  [ 990/1251]  eta: 0:02:05  lr: 0.000018  loss: 3.6435 (3.4245)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [22]  [1000/1251]  eta: 0:02:01  lr: 0.000018  loss: 3.4969 (3.4242)  time: 0.4662  data: 0.0004  max mem: 19734
Epoch: [22]  [1010/1251]  eta: 0:01:56  lr: 0.000018  loss: 3.4236 (3.4237)  time: 0.4757  data: 0.0004  max mem: 19734
Epoch: [22]  [1020/1251]  eta: 0:01:51  lr: 0.000018  loss: 3.5130 (3.4248)  time: 0.5050  data: 0.0004  max mem: 19734
Epoch: [22]  [1030/1251]  eta: 0:01:46  lr: 0.000018  loss: 3.5130 (3.4240)  time: 0.5014  data: 0.0004  max mem: 19734
Epoch: [22]  [1040/1251]  eta: 0:01:41  lr: 0.000018  loss: 3.5662 (3.4257)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [22]  [1050/1251]  eta: 0:01:36  lr: 0.000018  loss: 3.5971 (3.4237)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [22]  [1060/1251]  eta: 0:01:32  lr: 0.000018  loss: 3.4835 (3.4247)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [22]  [1070/1251]  eta: 0:01:27  lr: 0.000018  loss: 3.5215 (3.4251)  time: 0.4515  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4514, ratio_loss=0.0054, pruning_loss=0.1323, mse_loss=0.4862
Epoch: [22]  [1080/1251]  eta: 0:01:22  lr: 0.000018  loss: 3.6546 (3.4256)  time: 0.4525  data: 0.0006  max mem: 19734
Epoch: [22]  [1090/1251]  eta: 0:01:17  lr: 0.000018  loss: 3.7010 (3.4272)  time: 0.4538  data: 0.0007  max mem: 19734
Epoch: [22]  [1100/1251]  eta: 0:01:12  lr: 0.000018  loss: 3.7498 (3.4304)  time: 0.4529  data: 0.0007  max mem: 19734
Epoch: [22]  [1110/1251]  eta: 0:01:07  lr: 0.000018  loss: 3.7498 (3.4304)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [22]  [1120/1251]  eta: 0:01:02  lr: 0.000018  loss: 3.5471 (3.4306)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [22]  [1130/1251]  eta: 0:00:58  lr: 0.000018  loss: 3.5471 (3.4308)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [22]  [1140/1251]  eta: 0:00:53  lr: 0.000018  loss: 3.5064 (3.4325)  time: 0.4545  data: 0.0007  max mem: 19734
Epoch: [22]  [1150/1251]  eta: 0:00:48  lr: 0.000018  loss: 3.4183 (3.4309)  time: 0.4752  data: 0.0007  max mem: 19734
Epoch: [22]  [1160/1251]  eta: 0:00:43  lr: 0.000018  loss: 3.3330 (3.4317)  time: 0.4913  data: 0.0004  max mem: 19734
Epoch: [22]  [1170/1251]  eta: 0:00:38  lr: 0.000018  loss: 3.5485 (3.4326)  time: 0.4918  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4960, ratio_loss=0.0054, pruning_loss=0.1328, mse_loss=0.4754
Epoch: [22]  [1180/1251]  eta: 0:00:34  lr: 0.000018  loss: 3.6063 (3.4342)  time: 0.4890  data: 0.0005  max mem: 19734
Epoch: [22]  [1190/1251]  eta: 0:00:29  lr: 0.000018  loss: 3.6063 (3.4341)  time: 0.4670  data: 0.0009  max mem: 19734
Epoch: [22]  [1200/1251]  eta: 0:00:24  lr: 0.000018  loss: 3.6002 (3.4351)  time: 0.4518  data: 0.0007  max mem: 19734
Epoch: [22]  [1210/1251]  eta: 0:00:19  lr: 0.000018  loss: 3.7097 (3.4371)  time: 0.4483  data: 0.0002  max mem: 19734
Epoch: [22]  [1220/1251]  eta: 0:00:14  lr: 0.000018  loss: 3.7097 (3.4361)  time: 0.4460  data: 0.0002  max mem: 19734
Epoch: [22]  [1230/1251]  eta: 0:00:10  lr: 0.000018  loss: 3.4345 (3.4373)  time: 0.4451  data: 0.0002  max mem: 19734
Epoch: [22]  [1240/1251]  eta: 0:00:05  lr: 0.000018  loss: 3.5409 (3.4364)  time: 0.4447  data: 0.0001  max mem: 19734
Epoch: [22]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.4276 (3.4340)  time: 0.4442  data: 0.0001  max mem: 19734
Epoch: [22] Total time: 0:09:59 (0.4789 s / it)
Averaged stats: lr: 0.000018  loss: 3.4276 (3.4351)
Test:  [  0/261]  eta: 2:19:46  loss: 0.7102 (0.7102)  acc1: 83.8542 (83.8542)  acc5: 95.8333 (95.8333)  time: 32.1319  data: 31.8230  max mem: 19734
Test:  [ 10/261]  eta: 0:13:20  loss: 0.7025 (0.7244)  acc1: 84.3750 (84.1856)  acc5: 96.3542 (96.2595)  time: 3.1878  data: 2.9097  max mem: 19734
Test:  [ 20/261]  eta: 0:07:18  loss: 0.9253 (0.8967)  acc1: 81.2500 (79.3155)  acc5: 93.2292 (94.5685)  time: 0.3023  data: 0.0166  max mem: 19734
Test:  [ 30/261]  eta: 0:05:05  loss: 0.8318 (0.8161)  acc1: 82.8125 (81.9892)  acc5: 94.2708 (95.1109)  time: 0.2962  data: 0.0089  max mem: 19734
Test:  [ 40/261]  eta: 0:04:04  loss: 0.5723 (0.7803)  acc1: 86.9792 (82.9649)  acc5: 96.8750 (95.4141)  time: 0.3554  data: 0.1166  max mem: 19734
Test:  [ 50/261]  eta: 0:03:15  loss: 0.8820 (0.8425)  acc1: 78.1250 (81.0355)  acc5: 94.7917 (95.0266)  time: 0.3159  data: 0.1216  max mem: 19734
Test:  [ 60/261]  eta: 0:02:42  loss: 0.9847 (0.8546)  acc1: 76.0417 (80.4986)  acc5: 94.2708 (94.9710)  time: 0.2051  data: 0.0158  max mem: 19734
Test:  [ 70/261]  eta: 0:02:19  loss: 0.9244 (0.8565)  acc1: 77.0833 (80.0396)  acc5: 95.8333 (95.1951)  time: 0.2259  data: 0.0175  max mem: 19734
Test:  [ 80/261]  eta: 0:01:59  loss: 0.8374 (0.8561)  acc1: 78.6458 (80.2341)  acc5: 96.8750 (95.3061)  time: 0.2034  data: 0.0144  max mem: 19734
Test:  [ 90/261]  eta: 0:01:46  loss: 0.8280 (0.8420)  acc1: 83.3333 (80.6376)  acc5: 95.8333 (95.3926)  time: 0.2388  data: 0.0710  max mem: 19734
Test:  [100/261]  eta: 0:01:38  loss: 0.7992 (0.8458)  acc1: 83.3333 (80.5848)  acc5: 95.3125 (95.4260)  time: 0.4317  data: 0.2467  max mem: 19734
Test:  [110/261]  eta: 0:01:28  loss: 0.8922 (0.8698)  acc1: 77.6042 (80.0206)  acc5: 94.7917 (95.1436)  time: 0.4339  data: 0.2622  max mem: 19734
Test:  [120/261]  eta: 0:01:21  loss: 1.2128 (0.9103)  acc1: 71.3542 (79.0806)  acc5: 89.5833 (94.5764)  time: 0.3905  data: 0.2343  max mem: 19734
Test:  [130/261]  eta: 0:01:14  loss: 1.3655 (0.9564)  acc1: 68.2292 (78.1131)  acc5: 87.5000 (94.0084)  time: 0.4570  data: 0.3104  max mem: 19734
Test:  [140/261]  eta: 0:01:05  loss: 1.2748 (0.9817)  acc1: 68.2292 (77.4786)  acc5: 88.5417 (93.7168)  time: 0.3122  data: 0.1942  max mem: 19734
Test:  [150/261]  eta: 0:00:58  loss: 1.2468 (0.9865)  acc1: 72.9167 (77.4869)  acc5: 90.6250 (93.5534)  time: 0.2516  data: 0.1110  max mem: 19734
Test:  [160/261]  eta: 0:00:50  loss: 1.0350 (1.0072)  acc1: 77.6042 (77.1351)  acc5: 91.1458 (93.2648)  time: 0.2508  data: 0.0888  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.3046 (1.0381)  acc1: 65.1042 (76.4163)  acc5: 88.0208 (92.9490)  time: 0.3126  data: 0.1830  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4580 (1.0557)  acc1: 65.1042 (76.0071)  acc5: 88.0208 (92.7860)  time: 0.2905  data: 0.1741  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4475 (1.0695)  acc1: 66.6667 (75.7363)  acc5: 90.1042 (92.6429)  time: 0.1429  data: 0.0448  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3443 (1.0865)  acc1: 70.8333 (75.4016)  acc5: 89.0625 (92.3948)  time: 0.1249  data: 0.0482  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3605 (1.1001)  acc1: 69.7917 (75.1407)  acc5: 89.0625 (92.1924)  time: 0.1049  data: 0.0388  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4767 (1.1200)  acc1: 67.1875 (74.6606)  acc5: 88.5417 (92.0013)  time: 0.0907  data: 0.0290  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4670 (1.1305)  acc1: 67.1875 (74.3957)  acc5: 88.5417 (91.9012)  time: 0.0638  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3586 (1.1401)  acc1: 67.7083 (74.1701)  acc5: 90.1042 (91.8028)  time: 0.0687  data: 0.0051  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.1040 (1.1335)  acc1: 75.0000 (74.3194)  acc5: 92.7083 (91.9136)  time: 0.0665  data: 0.0051  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9800 (1.1336)  acc1: 75.5208 (74.3380)  acc5: 95.3125 (91.9900)  time: 0.0596  data: 0.0001  max mem: 19734
Test: Total time: 0:01:34 (0.3604 s / it)
* Acc@1 74.338 Acc@5 91.990 loss 1.134
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.41%
Epoch: [23]  [   0/1251]  eta: 6:18:40  lr: 0.000018  loss: 2.4686 (2.4686)  time: 18.1618  data: 7.7821  max mem: 19734
Epoch: [23]  [  10/1251]  eta: 0:43:39  lr: 0.000018  loss: 3.4582 (3.3265)  time: 2.1104  data: 0.7078  max mem: 19734
Epoch: [23]  [  20/1251]  eta: 0:27:07  lr: 0.000018  loss: 3.5561 (3.4375)  time: 0.4803  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3896, ratio_loss=0.0053, pruning_loss=0.1344, mse_loss=0.4762
Epoch: [23]  [  30/1251]  eta: 0:21:13  lr: 0.000018  loss: 3.4491 (3.4089)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [23]  [  40/1251]  eta: 0:18:28  lr: 0.000018  loss: 3.4491 (3.3853)  time: 0.4889  data: 0.0004  max mem: 19734
Epoch: [23]  [  50/1251]  eta: 0:16:43  lr: 0.000018  loss: 3.5263 (3.3916)  time: 0.5144  data: 0.0004  max mem: 19734
Epoch: [23]  [  60/1251]  eta: 0:15:28  lr: 0.000018  loss: 3.5490 (3.3940)  time: 0.5010  data: 0.0004  max mem: 19734
Epoch: [23]  [  70/1251]  eta: 0:14:27  lr: 0.000018  loss: 3.4731 (3.3980)  time: 0.4758  data: 0.0004  max mem: 19734
Epoch: [23]  [  80/1251]  eta: 0:13:40  lr: 0.000018  loss: 3.4402 (3.4029)  time: 0.4586  data: 0.0004  max mem: 19734
Epoch: [23]  [  90/1251]  eta: 0:13:02  lr: 0.000018  loss: 3.7254 (3.4368)  time: 0.4608  data: 0.0005  max mem: 19734
Epoch: [23]  [ 100/1251]  eta: 0:12:32  lr: 0.000018  loss: 3.8070 (3.4705)  time: 0.4634  data: 0.0005  max mem: 19734
Epoch: [23]  [ 110/1251]  eta: 0:12:05  lr: 0.000018  loss: 3.7380 (3.4729)  time: 0.4627  data: 0.0006  max mem: 19734
Epoch: [23]  [ 120/1251]  eta: 0:11:43  lr: 0.000018  loss: 3.3405 (3.4445)  time: 0.4621  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4225, ratio_loss=0.0058, pruning_loss=0.1330, mse_loss=0.4881
Epoch: [23]  [ 130/1251]  eta: 0:11:23  lr: 0.000018  loss: 3.3405 (3.4345)  time: 0.4610  data: 0.0005  max mem: 19734
Epoch: [23]  [ 140/1251]  eta: 0:11:05  lr: 0.000018  loss: 3.3648 (3.4211)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [23]  [ 150/1251]  eta: 0:10:49  lr: 0.000018  loss: 3.2625 (3.4066)  time: 0.4599  data: 0.0004  max mem: 19734
Epoch: [23]  [ 160/1251]  eta: 0:10:34  lr: 0.000018  loss: 3.3272 (3.4038)  time: 0.4632  data: 0.0005  max mem: 19734
Epoch: [23]  [ 170/1251]  eta: 0:10:21  lr: 0.000018  loss: 3.2407 (3.3990)  time: 0.4608  data: 0.0005  max mem: 19734
Epoch: [23]  [ 180/1251]  eta: 0:10:11  lr: 0.000018  loss: 3.0948 (3.3857)  time: 0.4802  data: 0.0005  max mem: 19734
Epoch: [23]  [ 190/1251]  eta: 0:10:01  lr: 0.000018  loss: 3.4376 (3.3998)  time: 0.4975  data: 0.0005  max mem: 19734
Epoch: [23]  [ 200/1251]  eta: 0:09:53  lr: 0.000018  loss: 3.5297 (3.4057)  time: 0.5071  data: 0.0004  max mem: 19734
Epoch: [23]  [ 210/1251]  eta: 0:09:42  lr: 0.000018  loss: 3.4796 (3.4089)  time: 0.4891  data: 0.0005  max mem: 19734
Epoch: [23]  [ 220/1251]  eta: 0:09:31  lr: 0.000018  loss: 3.4796 (3.4065)  time: 0.4563  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3367, ratio_loss=0.0055, pruning_loss=0.1362, mse_loss=0.5222
Epoch: [23]  [ 230/1251]  eta: 0:09:21  lr: 0.000018  loss: 3.5549 (3.4079)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [23]  [ 240/1251]  eta: 0:09:12  lr: 0.000018  loss: 3.5425 (3.4035)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [23]  [ 250/1251]  eta: 0:09:03  lr: 0.000018  loss: 3.4828 (3.4031)  time: 0.4577  data: 0.0005  max mem: 19734
Epoch: [23]  [ 260/1251]  eta: 0:08:54  lr: 0.000018  loss: 3.5142 (3.4034)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [23]  [ 270/1251]  eta: 0:08:46  lr: 0.000018  loss: 3.5419 (3.4025)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [23]  [ 280/1251]  eta: 0:08:38  lr: 0.000018  loss: 3.5592 (3.4040)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [23]  [ 290/1251]  eta: 0:08:30  lr: 0.000018  loss: 3.5347 (3.4074)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [23]  [ 300/1251]  eta: 0:08:22  lr: 0.000018  loss: 3.4182 (3.4009)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [23]  [ 310/1251]  eta: 0:08:15  lr: 0.000018  loss: 3.4701 (3.4111)  time: 0.4574  data: 0.0005  max mem: 19734
Epoch: [23]  [ 320/1251]  eta: 0:08:08  lr: 0.000018  loss: 3.6030 (3.4110)  time: 0.4588  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4024, ratio_loss=0.0052, pruning_loss=0.1322, mse_loss=0.5051
Epoch: [23]  [ 330/1251]  eta: 0:08:02  lr: 0.000018  loss: 3.5384 (3.4110)  time: 0.4830  data: 0.0005  max mem: 19734
Epoch: [23]  [ 340/1251]  eta: 0:07:56  lr: 0.000018  loss: 3.5384 (3.4126)  time: 0.5030  data: 0.0005  max mem: 19734
Epoch: [23]  [ 350/1251]  eta: 0:07:50  lr: 0.000018  loss: 3.5279 (3.4080)  time: 0.4985  data: 0.0004  max mem: 19734
Epoch: [23]  [ 360/1251]  eta: 0:07:43  lr: 0.000018  loss: 3.3562 (3.4040)  time: 0.4762  data: 0.0004  max mem: 19734
Epoch: [23]  [ 370/1251]  eta: 0:07:37  lr: 0.000018  loss: 3.5337 (3.4084)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [23]  [ 380/1251]  eta: 0:07:30  lr: 0.000018  loss: 3.6397 (3.4112)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [23]  [ 390/1251]  eta: 0:07:24  lr: 0.000018  loss: 3.4780 (3.4136)  time: 0.4593  data: 0.0005  max mem: 19734
Epoch: [23]  [ 400/1251]  eta: 0:07:17  lr: 0.000018  loss: 3.4599 (3.4134)  time: 0.4571  data: 0.0005  max mem: 19734
Epoch: [23]  [ 410/1251]  eta: 0:07:11  lr: 0.000018  loss: 3.3544 (3.4116)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [23]  [ 420/1251]  eta: 0:07:05  lr: 0.000018  loss: 3.2349 (3.4066)  time: 0.4592  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3734, ratio_loss=0.0058, pruning_loss=0.1344, mse_loss=0.4805
Epoch: [23]  [ 430/1251]  eta: 0:06:59  lr: 0.000018  loss: 3.4384 (3.4096)  time: 0.4595  data: 0.0004  max mem: 19734
Epoch: [23]  [ 440/1251]  eta: 0:06:53  lr: 0.000018  loss: 3.5441 (3.4070)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [23]  [ 450/1251]  eta: 0:06:47  lr: 0.000018  loss: 3.4230 (3.4058)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [23]  [ 460/1251]  eta: 0:06:41  lr: 0.000018  loss: 3.4353 (3.4072)  time: 0.4560  data: 0.0006  max mem: 19734
Epoch: [23]  [ 470/1251]  eta: 0:06:35  lr: 0.000018  loss: 3.4353 (3.4041)  time: 0.4762  data: 0.0006  max mem: 19734
Epoch: [23]  [ 480/1251]  eta: 0:06:30  lr: 0.000018  loss: 3.6566 (3.4098)  time: 0.4877  data: 0.0004  max mem: 19734
Epoch: [23]  [ 490/1251]  eta: 0:06:25  lr: 0.000018  loss: 3.5653 (3.4092)  time: 0.4919  data: 0.0004  max mem: 19734
Epoch: [23]  [ 500/1251]  eta: 0:06:19  lr: 0.000018  loss: 3.5235 (3.4113)  time: 0.4811  data: 0.0004  max mem: 19734
Epoch: [23]  [ 510/1251]  eta: 0:06:13  lr: 0.000018  loss: 3.6250 (3.4171)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [23]  [ 520/1251]  eta: 0:06:07  lr: 0.000018  loss: 3.7128 (3.4252)  time: 0.4564  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4621, ratio_loss=0.0052, pruning_loss=0.1338, mse_loss=0.5106
Epoch: [23]  [ 530/1251]  eta: 0:06:02  lr: 0.000018  loss: 3.7682 (3.4268)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [23]  [ 540/1251]  eta: 0:05:56  lr: 0.000018  loss: 3.4307 (3.4274)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [23]  [ 550/1251]  eta: 0:05:50  lr: 0.000018  loss: 3.4307 (3.4265)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [23]  [ 560/1251]  eta: 0:05:45  lr: 0.000018  loss: 3.5340 (3.4287)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [23]  [ 570/1251]  eta: 0:05:39  lr: 0.000018  loss: 3.8250 (3.4364)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [23]  [ 580/1251]  eta: 0:05:34  lr: 0.000018  loss: 3.9308 (3.4398)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [23]  [ 590/1251]  eta: 0:05:28  lr: 0.000018  loss: 3.5332 (3.4377)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [23]  [ 600/1251]  eta: 0:05:23  lr: 0.000018  loss: 3.4095 (3.4372)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [23]  [ 610/1251]  eta: 0:05:18  lr: 0.000018  loss: 3.2946 (3.4312)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [23]  [ 620/1251]  eta: 0:05:13  lr: 0.000018  loss: 3.4284 (3.4344)  time: 0.4717  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4564, ratio_loss=0.0052, pruning_loss=0.1318, mse_loss=0.4795
Epoch: [23]  [ 630/1251]  eta: 0:05:08  lr: 0.000018  loss: 3.6690 (3.4349)  time: 0.4905  data: 0.0004  max mem: 19734
Epoch: [23]  [ 640/1251]  eta: 0:05:03  lr: 0.000018  loss: 3.6189 (3.4361)  time: 0.4987  data: 0.0004  max mem: 19734
Epoch: [23]  [ 650/1251]  eta: 0:04:57  lr: 0.000018  loss: 3.5555 (3.4350)  time: 0.4781  data: 0.0005  max mem: 19734
Epoch: [23]  [ 660/1251]  eta: 0:04:52  lr: 0.000018  loss: 3.4511 (3.4337)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [23]  [ 670/1251]  eta: 0:04:47  lr: 0.000018  loss: 3.5374 (3.4328)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [23]  [ 680/1251]  eta: 0:04:41  lr: 0.000018  loss: 3.3891 (3.4312)  time: 0.4554  data: 0.0005  max mem: 19734
Epoch: [23]  [ 690/1251]  eta: 0:04:36  lr: 0.000018  loss: 3.3979 (3.4334)  time: 0.4543  data: 0.0005  max mem: 19734
Epoch: [23]  [ 700/1251]  eta: 0:04:31  lr: 0.000018  loss: 3.4895 (3.4320)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [23]  [ 710/1251]  eta: 0:04:26  lr: 0.000018  loss: 3.6650 (3.4375)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [23]  [ 720/1251]  eta: 0:04:20  lr: 0.000018  loss: 3.8228 (3.4388)  time: 0.4528  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4553, ratio_loss=0.0054, pruning_loss=0.1337, mse_loss=0.5093
Epoch: [23]  [ 730/1251]  eta: 0:04:15  lr: 0.000018  loss: 3.5764 (3.4398)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [23]  [ 740/1251]  eta: 0:04:10  lr: 0.000018  loss: 3.6208 (3.4435)  time: 0.4537  data: 0.0008  max mem: 19734
Epoch: [23]  [ 750/1251]  eta: 0:04:05  lr: 0.000018  loss: 3.5588 (3.4394)  time: 0.4521  data: 0.0007  max mem: 19734
Epoch: [23]  [ 760/1251]  eta: 0:04:00  lr: 0.000018  loss: 3.1281 (3.4365)  time: 0.4709  data: 0.0004  max mem: 19734
Epoch: [23]  [ 770/1251]  eta: 0:03:55  lr: 0.000018  loss: 3.4567 (3.4385)  time: 0.4938  data: 0.0005  max mem: 19734
Epoch: [23]  [ 780/1251]  eta: 0:03:50  lr: 0.000018  loss: 3.5203 (3.4361)  time: 0.4923  data: 0.0004  max mem: 19734
Epoch: [23]  [ 790/1251]  eta: 0:03:45  lr: 0.000018  loss: 3.5369 (3.4392)  time: 0.4789  data: 0.0004  max mem: 19734
Epoch: [23]  [ 800/1251]  eta: 0:03:40  lr: 0.000018  loss: 3.6050 (3.4400)  time: 0.4631  data: 0.0004  max mem: 19734
Epoch: [23]  [ 810/1251]  eta: 0:03:35  lr: 0.000018  loss: 3.5357 (3.4401)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [23]  [ 820/1251]  eta: 0:03:30  lr: 0.000018  loss: 3.3426 (3.4366)  time: 0.4508  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3928, ratio_loss=0.0052, pruning_loss=0.1344, mse_loss=0.5110
Epoch: [23]  [ 830/1251]  eta: 0:03:25  lr: 0.000018  loss: 3.2950 (3.4361)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [23]  [ 840/1251]  eta: 0:03:20  lr: 0.000018  loss: 3.3395 (3.4331)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [23]  [ 850/1251]  eta: 0:03:15  lr: 0.000018  loss: 3.1625 (3.4318)  time: 0.4508  data: 0.0004  max mem: 19734
Epoch: [23]  [ 860/1251]  eta: 0:03:10  lr: 0.000018  loss: 3.3351 (3.4317)  time: 0.4511  data: 0.0004  max mem: 19734
Epoch: [23]  [ 870/1251]  eta: 0:03:05  lr: 0.000018  loss: 3.6986 (3.4341)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [23]  [ 880/1251]  eta: 0:03:00  lr: 0.000018  loss: 3.6986 (3.4353)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [23]  [ 890/1251]  eta: 0:02:55  lr: 0.000018  loss: 3.4894 (3.4345)  time: 0.4536  data: 0.0008  max mem: 19734
Epoch: [23]  [ 900/1251]  eta: 0:02:50  lr: 0.000018  loss: 3.6221 (3.4359)  time: 0.4617  data: 0.0007  max mem: 19734
Epoch: [23]  [ 910/1251]  eta: 0:02:45  lr: 0.000018  loss: 3.6395 (3.4381)  time: 0.4749  data: 0.0004  max mem: 19734
Epoch: [23]  [ 920/1251]  eta: 0:02:40  lr: 0.000018  loss: 3.6558 (3.4395)  time: 0.4911  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4167, ratio_loss=0.0053, pruning_loss=0.1332, mse_loss=0.4998
Epoch: [23]  [ 930/1251]  eta: 0:02:35  lr: 0.000018  loss: 3.6558 (3.4401)  time: 0.4882  data: 0.0004  max mem: 19734
Epoch: [23]  [ 940/1251]  eta: 0:02:30  lr: 0.000018  loss: 3.3973 (3.4386)  time: 0.4684  data: 0.0004  max mem: 19734
Epoch: [23]  [ 950/1251]  eta: 0:02:25  lr: 0.000018  loss: 3.5322 (3.4405)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [23]  [ 960/1251]  eta: 0:02:20  lr: 0.000018  loss: 3.4980 (3.4384)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [23]  [ 970/1251]  eta: 0:02:16  lr: 0.000018  loss: 3.2703 (3.4373)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [23]  [ 980/1251]  eta: 0:02:11  lr: 0.000018  loss: 3.3920 (3.4379)  time: 0.4507  data: 0.0004  max mem: 19734
Epoch: [23]  [ 990/1251]  eta: 0:02:06  lr: 0.000018  loss: 3.4193 (3.4352)  time: 0.4523  data: 0.0006  max mem: 19734
Epoch: [23]  [1000/1251]  eta: 0:02:01  lr: 0.000018  loss: 3.4193 (3.4349)  time: 0.4541  data: 0.0006  max mem: 19734
Epoch: [23]  [1010/1251]  eta: 0:01:56  lr: 0.000018  loss: 3.3544 (3.4330)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [23]  [1020/1251]  eta: 0:01:51  lr: 0.000018  loss: 3.2701 (3.4329)  time: 0.4529  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3639, ratio_loss=0.0052, pruning_loss=0.1336, mse_loss=0.5010
Epoch: [23]  [1030/1251]  eta: 0:01:46  lr: 0.000018  loss: 3.4341 (3.4347)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [23]  [1040/1251]  eta: 0:01:41  lr: 0.000018  loss: 3.7331 (3.4367)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [23]  [1050/1251]  eta: 0:01:36  lr: 0.000018  loss: 3.6667 (3.4372)  time: 0.4739  data: 0.0004  max mem: 19734
Epoch: [23]  [1060/1251]  eta: 0:01:32  lr: 0.000018  loss: 3.6667 (3.4376)  time: 0.4932  data: 0.0004  max mem: 19734
Epoch: [23]  [1070/1251]  eta: 0:01:27  lr: 0.000018  loss: 3.5393 (3.4379)  time: 0.4908  data: 0.0004  max mem: 19734
Epoch: [23]  [1080/1251]  eta: 0:01:22  lr: 0.000018  loss: 3.5555 (3.4381)  time: 0.4707  data: 0.0004  max mem: 19734
Epoch: [23]  [1090/1251]  eta: 0:01:17  lr: 0.000018  loss: 3.4665 (3.4373)  time: 0.4618  data: 0.0004  max mem: 19734
Epoch: [23]  [1100/1251]  eta: 0:01:12  lr: 0.000018  loss: 3.5059 (3.4392)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [23]  [1110/1251]  eta: 0:01:07  lr: 0.000018  loss: 3.5654 (3.4371)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [23]  [1120/1251]  eta: 0:01:03  lr: 0.000018  loss: 3.0182 (3.4353)  time: 0.4563  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4236, ratio_loss=0.0054, pruning_loss=0.1338, mse_loss=0.4896
Epoch: [23]  [1130/1251]  eta: 0:00:58  lr: 0.000018  loss: 3.4889 (3.4358)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [23]  [1140/1251]  eta: 0:00:53  lr: 0.000018  loss: 3.4889 (3.4354)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [23]  [1150/1251]  eta: 0:00:48  lr: 0.000018  loss: 3.4190 (3.4342)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [23]  [1160/1251]  eta: 0:00:43  lr: 0.000018  loss: 3.5561 (3.4343)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [23]  [1170/1251]  eta: 0:00:38  lr: 0.000018  loss: 3.6294 (3.4350)  time: 0.4511  data: 0.0004  max mem: 19734
Epoch: [23]  [1180/1251]  eta: 0:00:34  lr: 0.000018  loss: 3.6294 (3.4370)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [23]  [1190/1251]  eta: 0:00:29  lr: 0.000018  loss: 3.6575 (3.4377)  time: 0.4540  data: 0.0008  max mem: 19734
Epoch: [23]  [1200/1251]  eta: 0:00:24  lr: 0.000018  loss: 3.7294 (3.4382)  time: 0.4711  data: 0.0007  max mem: 19734
Epoch: [23]  [1210/1251]  eta: 0:00:19  lr: 0.000018  loss: 3.5528 (3.4360)  time: 0.4776  data: 0.0001  max mem: 19734
Epoch: [23]  [1220/1251]  eta: 0:00:14  lr: 0.000018  loss: 3.6347 (3.4382)  time: 0.4705  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4404, ratio_loss=0.0055, pruning_loss=0.1318, mse_loss=0.5080
Epoch: [23]  [1230/1251]  eta: 0:00:10  lr: 0.000018  loss: 3.6333 (3.4382)  time: 0.4719  data: 0.0002  max mem: 19734
Epoch: [23]  [1240/1251]  eta: 0:00:05  lr: 0.000018  loss: 3.4680 (3.4382)  time: 0.4582  data: 0.0002  max mem: 19734
Epoch: [23]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.5054 (3.4378)  time: 0.4490  data: 0.0001  max mem: 19734
Epoch: [23] Total time: 0:10:00 (0.4796 s / it)
Averaged stats: lr: 0.000018  loss: 3.5054 (3.4369)
Test:  [  0/261]  eta: 1:49:53  loss: 0.7026 (0.7026)  acc1: 83.8542 (83.8542)  acc5: 96.8750 (96.8750)  time: 25.2613  data: 25.1516  max mem: 19734
Test:  [ 10/261]  eta: 0:10:21  loss: 0.7026 (0.7045)  acc1: 83.8542 (84.3750)  acc5: 96.8750 (96.6383)  time: 2.4771  data: 2.3012  max mem: 19734
Test:  [ 20/261]  eta: 0:05:49  loss: 0.9281 (0.8846)  acc1: 79.1667 (79.7123)  acc5: 94.2708 (94.8661)  time: 0.2578  data: 0.0157  max mem: 19734
Test:  [ 30/261]  eta: 0:04:04  loss: 0.8050 (0.8056)  acc1: 83.3333 (82.4933)  acc5: 93.7500 (95.2453)  time: 0.2760  data: 0.0147  max mem: 19734
Test:  [ 40/261]  eta: 0:03:25  loss: 0.5637 (0.7731)  acc1: 88.0208 (83.2952)  acc5: 96.8750 (95.5285)  time: 0.3832  data: 0.1698  max mem: 19734
Test:  [ 50/261]  eta: 0:02:48  loss: 0.9135 (0.8395)  acc1: 78.6458 (81.3828)  acc5: 94.7917 (95.0674)  time: 0.4010  data: 0.1712  max mem: 19734
Test:  [ 60/261]  eta: 0:02:24  loss: 0.9750 (0.8496)  acc1: 76.5625 (80.8999)  acc5: 94.2708 (95.1588)  time: 0.2903  data: 0.0137  max mem: 19734
Test:  [ 70/261]  eta: 0:02:19  loss: 0.9225 (0.8522)  acc1: 77.6042 (80.3550)  acc5: 96.3542 (95.3418)  time: 0.5559  data: 0.3142  max mem: 19734
Test:  [ 80/261]  eta: 0:02:03  loss: 0.8941 (0.8550)  acc1: 79.1667 (80.4527)  acc5: 96.8750 (95.4540)  time: 0.5760  data: 0.4012  max mem: 19734
Test:  [ 90/261]  eta: 0:01:47  loss: 0.8145 (0.8435)  acc1: 84.3750 (80.7692)  acc5: 95.8333 (95.5357)  time: 0.2720  data: 0.0991  max mem: 19734
Test:  [100/261]  eta: 0:01:37  loss: 0.8226 (0.8487)  acc1: 83.8542 (80.6828)  acc5: 95.3125 (95.5961)  time: 0.3030  data: 0.1189  max mem: 19734
Test:  [110/261]  eta: 0:01:26  loss: 0.9158 (0.8738)  acc1: 75.0000 (80.1380)  acc5: 94.7917 (95.3219)  time: 0.3216  data: 0.1177  max mem: 19734
Test:  [120/261]  eta: 0:01:16  loss: 1.2363 (0.9141)  acc1: 68.7500 (79.1753)  acc5: 90.1042 (94.8003)  time: 0.2150  data: 0.0108  max mem: 19734
Test:  [130/261]  eta: 0:01:07  loss: 1.3810 (0.9594)  acc1: 67.7083 (78.2602)  acc5: 87.5000 (94.1834)  time: 0.1878  data: 0.0191  max mem: 19734
Test:  [140/261]  eta: 0:00:59  loss: 1.3004 (0.9870)  acc1: 67.7083 (77.5635)  acc5: 89.5833 (93.8830)  time: 0.2042  data: 0.0387  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: 1.2481 (0.9938)  acc1: 72.3958 (77.5145)  acc5: 90.6250 (93.6948)  time: 0.3191  data: 0.1794  max mem: 19734
Test:  [160/261]  eta: 0:00:47  loss: 1.0013 (1.0121)  acc1: 77.6042 (77.2257)  acc5: 91.6667 (93.4006)  time: 0.3119  data: 0.1842  max mem: 19734
Test:  [170/261]  eta: 0:00:41  loss: 1.2987 (1.0420)  acc1: 65.1042 (76.4772)  acc5: 89.0625 (93.0769)  time: 0.1839  data: 0.0505  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: 1.4750 (1.0609)  acc1: 65.1042 (75.9956)  acc5: 89.0625 (92.9184)  time: 0.4163  data: 0.2545  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3882 (1.0730)  acc1: 65.6250 (75.7935)  acc5: 90.6250 (92.7683)  time: 0.4526  data: 0.2914  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3636 (1.0891)  acc1: 71.8750 (75.5338)  acc5: 90.1042 (92.5347)  time: 0.1449  data: 0.0603  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3765 (1.1038)  acc1: 70.8333 (75.2666)  acc5: 88.0208 (92.3060)  time: 0.1202  data: 0.0584  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4126 (1.1230)  acc1: 67.7083 (74.7903)  acc5: 88.5417 (92.1003)  time: 0.1177  data: 0.0561  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4126 (1.1328)  acc1: 67.1875 (74.5468)  acc5: 89.0625 (91.9981)  time: 0.0617  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3368 (1.1415)  acc1: 68.2292 (74.3517)  acc5: 90.1042 (91.9346)  time: 0.0619  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.1012 (1.1354)  acc1: 74.4792 (74.4522)  acc5: 93.2292 (92.0485)  time: 0.0617  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9486 (1.1353)  acc1: 75.0000 (74.4640)  acc5: 95.3125 (92.1160)  time: 0.0596  data: 0.0001  max mem: 19734
Test: Total time: 0:01:32 (0.3539 s / it)
* Acc@1 74.464 Acc@5 92.116 loss 1.135
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.46%
Epoch: [24]  [   0/1251]  eta: 5:37:30  lr: 0.000018  loss: 3.7662 (3.7662)  time: 16.1876  data: 15.6559  max mem: 19734
Epoch: [24]  [  10/1251]  eta: 0:40:29  lr: 0.000018  loss: 3.5016 (3.3714)  time: 1.9579  data: 1.4237  max mem: 19734
Epoch: [24]  [  20/1251]  eta: 0:25:33  lr: 0.000018  loss: 3.4063 (3.3740)  time: 0.4985  data: 0.0005  max mem: 19734
Epoch: [24]  [  30/1251]  eta: 0:20:13  lr: 0.000018  loss: 3.5598 (3.4148)  time: 0.4642  data: 0.0005  max mem: 19734
Epoch: [24]  [  40/1251]  eta: 0:17:24  lr: 0.000018  loss: 3.4720 (3.4176)  time: 0.4599  data: 0.0005  max mem: 19734
Epoch: [24]  [  50/1251]  eta: 0:15:40  lr: 0.000018  loss: 3.5120 (3.4480)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [24]  [  60/1251]  eta: 0:14:32  lr: 0.000018  loss: 3.5120 (3.3960)  time: 0.4673  data: 0.0004  max mem: 19734
Epoch: [24]  [  70/1251]  eta: 0:13:40  lr: 0.000018  loss: 3.4911 (3.3903)  time: 0.4704  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3992, ratio_loss=0.0054, pruning_loss=0.1329, mse_loss=0.4733
Epoch: [24]  [  80/1251]  eta: 0:13:05  lr: 0.000018  loss: 3.7019 (3.4379)  time: 0.4824  data: 0.0004  max mem: 19734
Epoch: [24]  [  90/1251]  eta: 0:12:36  lr: 0.000018  loss: 3.9041 (3.4529)  time: 0.5001  data: 0.0005  max mem: 19734
Epoch: [24]  [ 100/1251]  eta: 0:12:13  lr: 0.000018  loss: 3.7439 (3.4646)  time: 0.4993  data: 0.0005  max mem: 19734
Epoch: [24]  [ 110/1251]  eta: 0:11:50  lr: 0.000018  loss: 3.7439 (3.4918)  time: 0.4888  data: 0.0005  max mem: 19734
Epoch: [24]  [ 120/1251]  eta: 0:11:29  lr: 0.000018  loss: 3.6944 (3.4799)  time: 0.4717  data: 0.0006  max mem: 19734
Epoch: [24]  [ 130/1251]  eta: 0:11:10  lr: 0.000018  loss: 3.6909 (3.4954)  time: 0.4638  data: 0.0004  max mem: 19734
Epoch: [24]  [ 140/1251]  eta: 0:10:53  lr: 0.000018  loss: 3.5869 (3.4832)  time: 0.4593  data: 0.0004  max mem: 19734
Epoch: [24]  [ 150/1251]  eta: 0:10:38  lr: 0.000018  loss: 3.5996 (3.4951)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [24]  [ 160/1251]  eta: 0:10:24  lr: 0.000018  loss: 3.4624 (3.4818)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [24]  [ 170/1251]  eta: 0:10:11  lr: 0.000018  loss: 3.4154 (3.4863)  time: 0.4588  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5094, ratio_loss=0.0056, pruning_loss=0.1303, mse_loss=0.4881
Epoch: [24]  [ 180/1251]  eta: 0:09:59  lr: 0.000018  loss: 3.4718 (3.4910)  time: 0.4590  data: 0.0005  max mem: 19734
Epoch: [24]  [ 190/1251]  eta: 0:09:48  lr: 0.000018  loss: 3.4445 (3.4866)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [24]  [ 200/1251]  eta: 0:09:37  lr: 0.000018  loss: 3.1383 (3.4680)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [24]  [ 210/1251]  eta: 0:09:28  lr: 0.000018  loss: 3.2301 (3.4582)  time: 0.4654  data: 0.0004  max mem: 19734
Epoch: [24]  [ 220/1251]  eta: 0:09:18  lr: 0.000018  loss: 3.4836 (3.4584)  time: 0.4652  data: 0.0004  max mem: 19734
Epoch: [24]  [ 230/1251]  eta: 0:09:10  lr: 0.000018  loss: 3.6995 (3.4692)  time: 0.4653  data: 0.0005  max mem: 19734
Epoch: [24]  [ 240/1251]  eta: 0:09:03  lr: 0.000018  loss: 3.6995 (3.4701)  time: 0.4879  data: 0.0004  max mem: 19734
Epoch: [24]  [ 250/1251]  eta: 0:08:56  lr: 0.000018  loss: 3.4704 (3.4653)  time: 0.4993  data: 0.0004  max mem: 19734
Epoch: [24]  [ 260/1251]  eta: 0:08:48  lr: 0.000018  loss: 3.4334 (3.4592)  time: 0.4777  data: 0.0004  max mem: 19734
Epoch: [24]  [ 270/1251]  eta: 0:08:39  lr: 0.000018  loss: 3.3331 (3.4528)  time: 0.4569  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3796, ratio_loss=0.0054, pruning_loss=0.1323, mse_loss=0.4875
Epoch: [24]  [ 280/1251]  eta: 0:08:32  lr: 0.000018  loss: 3.4914 (3.4543)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [24]  [ 290/1251]  eta: 0:08:24  lr: 0.000018  loss: 3.5916 (3.4551)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [24]  [ 300/1251]  eta: 0:08:17  lr: 0.000018  loss: 3.5824 (3.4559)  time: 0.4594  data: 0.0004  max mem: 19734
Epoch: [24]  [ 310/1251]  eta: 0:08:10  lr: 0.000018  loss: 3.4217 (3.4536)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [24]  [ 320/1251]  eta: 0:08:03  lr: 0.000018  loss: 3.4217 (3.4538)  time: 0.4607  data: 0.0005  max mem: 19734
Epoch: [24]  [ 330/1251]  eta: 0:07:56  lr: 0.000018  loss: 3.3871 (3.4487)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [24]  [ 340/1251]  eta: 0:07:49  lr: 0.000018  loss: 3.3468 (3.4531)  time: 0.4593  data: 0.0004  max mem: 19734
Epoch: [24]  [ 350/1251]  eta: 0:07:42  lr: 0.000018  loss: 3.3468 (3.4497)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [24]  [ 360/1251]  eta: 0:07:37  lr: 0.000018  loss: 3.3533 (3.4497)  time: 0.4703  data: 0.0004  max mem: 19734
Epoch: [24]  [ 370/1251]  eta: 0:07:31  lr: 0.000018  loss: 3.4868 (3.4483)  time: 0.4838  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3818, ratio_loss=0.0055, pruning_loss=0.1315, mse_loss=0.4758
Epoch: [24]  [ 380/1251]  eta: 0:07:25  lr: 0.000018  loss: 3.3628 (3.4411)  time: 0.4917  data: 0.0004  max mem: 19734
Epoch: [24]  [ 390/1251]  eta: 0:07:20  lr: 0.000018  loss: 3.1934 (3.4356)  time: 0.4962  data: 0.0004  max mem: 19734
Epoch: [24]  [ 400/1251]  eta: 0:07:13  lr: 0.000018  loss: 3.3791 (3.4403)  time: 0.4744  data: 0.0005  max mem: 19734
Epoch: [24]  [ 410/1251]  eta: 0:07:07  lr: 0.000018  loss: 3.6330 (3.4426)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [24]  [ 420/1251]  eta: 0:07:01  lr: 0.000018  loss: 3.6916 (3.4458)  time: 0.4504  data: 0.0004  max mem: 19734
Epoch: [24]  [ 430/1251]  eta: 0:06:55  lr: 0.000018  loss: 3.5592 (3.4394)  time: 0.4510  data: 0.0005  max mem: 19734
Epoch: [24]  [ 440/1251]  eta: 0:06:49  lr: 0.000018  loss: 3.4864 (3.4414)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [24]  [ 450/1251]  eta: 0:06:43  lr: 0.000018  loss: 3.6559 (3.4431)  time: 0.4522  data: 0.0005  max mem: 19734
Epoch: [24]  [ 460/1251]  eta: 0:06:37  lr: 0.000018  loss: 3.6698 (3.4477)  time: 0.4512  data: 0.0006  max mem: 19734
Epoch: [24]  [ 470/1251]  eta: 0:06:31  lr: 0.000018  loss: 3.5972 (3.4477)  time: 0.4533  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4322, ratio_loss=0.0053, pruning_loss=0.1317, mse_loss=0.4855
Epoch: [24]  [ 480/1251]  eta: 0:06:25  lr: 0.000018  loss: 3.4394 (3.4466)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [24]  [ 490/1251]  eta: 0:06:20  lr: 0.000018  loss: 3.1266 (3.4403)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [24]  [ 500/1251]  eta: 0:06:14  lr: 0.000018  loss: 3.0636 (3.4345)  time: 0.4647  data: 0.0004  max mem: 19734
Epoch: [24]  [ 510/1251]  eta: 0:06:09  lr: 0.000018  loss: 3.0493 (3.4285)  time: 0.4644  data: 0.0004  max mem: 19734
Epoch: [24]  [ 520/1251]  eta: 0:06:04  lr: 0.000018  loss: 3.0493 (3.4292)  time: 0.4741  data: 0.0004  max mem: 19734
Epoch: [24]  [ 530/1251]  eta: 0:05:59  lr: 0.000018  loss: 3.6165 (3.4300)  time: 0.4947  data: 0.0005  max mem: 19734
Epoch: [24]  [ 540/1251]  eta: 0:05:53  lr: 0.000018  loss: 3.2838 (3.4262)  time: 0.4920  data: 0.0004  max mem: 19734
Epoch: [24]  [ 550/1251]  eta: 0:05:48  lr: 0.000018  loss: 3.1964 (3.4211)  time: 0.4717  data: 0.0004  max mem: 19734
Epoch: [24]  [ 560/1251]  eta: 0:05:43  lr: 0.000018  loss: 3.3628 (3.4236)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [24]  [ 570/1251]  eta: 0:05:37  lr: 0.000018  loss: 3.5880 (3.4250)  time: 0.4585  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2986, ratio_loss=0.0052, pruning_loss=0.1348, mse_loss=0.4960
Epoch: [24]  [ 580/1251]  eta: 0:05:32  lr: 0.000018  loss: 3.5592 (3.4246)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [24]  [ 590/1251]  eta: 0:05:26  lr: 0.000018  loss: 3.5592 (3.4255)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [24]  [ 600/1251]  eta: 0:05:21  lr: 0.000018  loss: 3.5134 (3.4268)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [24]  [ 610/1251]  eta: 0:05:15  lr: 0.000018  loss: 3.5075 (3.4254)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [24]  [ 620/1251]  eta: 0:05:10  lr: 0.000018  loss: 3.5321 (3.4259)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [24]  [ 630/1251]  eta: 0:05:05  lr: 0.000018  loss: 3.6150 (3.4281)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [24]  [ 640/1251]  eta: 0:05:00  lr: 0.000018  loss: 3.6375 (3.4325)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [24]  [ 650/1251]  eta: 0:04:54  lr: 0.000018  loss: 3.6317 (3.4298)  time: 0.4644  data: 0.0004  max mem: 19734
Epoch: [24]  [ 660/1251]  eta: 0:04:50  lr: 0.000018  loss: 3.1816 (3.4257)  time: 0.4790  data: 0.0004  max mem: 19734
Epoch: [24]  [ 670/1251]  eta: 0:04:45  lr: 0.000018  loss: 3.1816 (3.4245)  time: 0.4843  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3813, ratio_loss=0.0055, pruning_loss=0.1329, mse_loss=0.4939
Epoch: [24]  [ 680/1251]  eta: 0:04:40  lr: 0.000018  loss: 3.4235 (3.4245)  time: 0.4973  data: 0.0006  max mem: 19734
Epoch: [24]  [ 690/1251]  eta: 0:04:35  lr: 0.000018  loss: 3.5513 (3.4250)  time: 0.4836  data: 0.0005  max mem: 19734
Epoch: [24]  [ 700/1251]  eta: 0:04:29  lr: 0.000018  loss: 3.5461 (3.4252)  time: 0.4542  data: 0.0006  max mem: 19734
Epoch: [24]  [ 710/1251]  eta: 0:04:24  lr: 0.000018  loss: 3.5042 (3.4263)  time: 0.4548  data: 0.0006  max mem: 19734
Epoch: [24]  [ 720/1251]  eta: 0:04:19  lr: 0.000018  loss: 3.5285 (3.4245)  time: 0.4551  data: 0.0007  max mem: 19734
Epoch: [24]  [ 730/1251]  eta: 0:04:14  lr: 0.000018  loss: 3.5547 (3.4281)  time: 0.4538  data: 0.0008  max mem: 19734
Epoch: [24]  [ 740/1251]  eta: 0:04:09  lr: 0.000018  loss: 3.8276 (3.4288)  time: 0.4548  data: 0.0009  max mem: 19734
Epoch: [24]  [ 750/1251]  eta: 0:04:04  lr: 0.000018  loss: 3.7541 (3.4329)  time: 0.4546  data: 0.0009  max mem: 19734
Epoch: [24]  [ 760/1251]  eta: 0:03:59  lr: 0.000018  loss: 3.6807 (3.4336)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [24]  [ 770/1251]  eta: 0:03:54  lr: 0.000018  loss: 3.5323 (3.4362)  time: 0.4543  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4954, ratio_loss=0.0052, pruning_loss=0.1295, mse_loss=0.5115
Epoch: [24]  [ 780/1251]  eta: 0:03:49  lr: 0.000018  loss: 3.5046 (3.4375)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [24]  [ 790/1251]  eta: 0:03:44  lr: 0.000018  loss: 3.7601 (3.4424)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [24]  [ 800/1251]  eta: 0:03:39  lr: 0.000018  loss: 3.7635 (3.4459)  time: 0.4788  data: 0.0005  max mem: 19734
Epoch: [24]  [ 810/1251]  eta: 0:03:34  lr: 0.000018  loss: 3.7218 (3.4469)  time: 0.4995  data: 0.0005  max mem: 19734
Epoch: [24]  [ 820/1251]  eta: 0:03:29  lr: 0.000018  loss: 3.6771 (3.4475)  time: 0.4961  data: 0.0004  max mem: 19734
Epoch: [24]  [ 830/1251]  eta: 0:03:24  lr: 0.000018  loss: 3.5002 (3.4486)  time: 0.4836  data: 0.0004  max mem: 19734
Epoch: [24]  [ 840/1251]  eta: 0:03:19  lr: 0.000018  loss: 3.6912 (3.4517)  time: 0.4632  data: 0.0004  max mem: 19734
Epoch: [24]  [ 850/1251]  eta: 0:03:14  lr: 0.000018  loss: 3.6606 (3.4529)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [24]  [ 860/1251]  eta: 0:03:09  lr: 0.000018  loss: 3.5589 (3.4532)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [24]  [ 870/1251]  eta: 0:03:04  lr: 0.000018  loss: 3.4134 (3.4506)  time: 0.4528  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5255, ratio_loss=0.0056, pruning_loss=0.1290, mse_loss=0.4965
Epoch: [24]  [ 880/1251]  eta: 0:02:59  lr: 0.000018  loss: 3.3377 (3.4492)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [24]  [ 890/1251]  eta: 0:02:54  lr: 0.000018  loss: 3.5680 (3.4509)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [24]  [ 900/1251]  eta: 0:02:49  lr: 0.000018  loss: 3.6528 (3.4494)  time: 0.4527  data: 0.0003  max mem: 19734
Epoch: [24]  [ 910/1251]  eta: 0:02:44  lr: 0.000018  loss: 3.7147 (3.4515)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [24]  [ 920/1251]  eta: 0:02:39  lr: 0.000018  loss: 3.5105 (3.4479)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [24]  [ 930/1251]  eta: 0:02:34  lr: 0.000018  loss: 3.3141 (3.4457)  time: 0.4508  data: 0.0005  max mem: 19734
Epoch: [24]  [ 940/1251]  eta: 0:02:29  lr: 0.000018  loss: 3.5781 (3.4470)  time: 0.4506  data: 0.0004  max mem: 19734
Epoch: [24]  [ 950/1251]  eta: 0:02:25  lr: 0.000018  loss: 3.5272 (3.4456)  time: 0.4764  data: 0.0004  max mem: 19734
Epoch: [24]  [ 960/1251]  eta: 0:02:20  lr: 0.000018  loss: 3.3643 (3.4447)  time: 0.4946  data: 0.0004  max mem: 19734
Epoch: [24]  [ 970/1251]  eta: 0:02:15  lr: 0.000018  loss: 3.3944 (3.4435)  time: 0.4842  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3649, ratio_loss=0.0058, pruning_loss=0.1333, mse_loss=0.4911
Epoch: [24]  [ 980/1251]  eta: 0:02:10  lr: 0.000018  loss: 3.5139 (3.4420)  time: 0.4663  data: 0.0004  max mem: 19734
Epoch: [24]  [ 990/1251]  eta: 0:02:05  lr: 0.000018  loss: 3.5739 (3.4438)  time: 0.4525  data: 0.0006  max mem: 19734
Epoch: [24]  [1000/1251]  eta: 0:02:00  lr: 0.000018  loss: 3.5314 (3.4421)  time: 0.4529  data: 0.0007  max mem: 19734
Epoch: [24]  [1010/1251]  eta: 0:01:55  lr: 0.000018  loss: 3.4279 (3.4435)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [24]  [1020/1251]  eta: 0:01:51  lr: 0.000018  loss: 3.5908 (3.4457)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [24]  [1030/1251]  eta: 0:01:46  lr: 0.000018  loss: 3.7144 (3.4477)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [24]  [1040/1251]  eta: 0:01:41  lr: 0.000018  loss: 3.6794 (3.4489)  time: 0.4540  data: 0.0006  max mem: 19734
Epoch: [24]  [1050/1251]  eta: 0:01:36  lr: 0.000018  loss: 3.6359 (3.4498)  time: 0.4528  data: 0.0010  max mem: 19734
Epoch: [24]  [1060/1251]  eta: 0:01:31  lr: 0.000018  loss: 3.7181 (3.4527)  time: 0.4513  data: 0.0010  max mem: 19734
Epoch: [24]  [1070/1251]  eta: 0:01:26  lr: 0.000018  loss: 3.5671 (3.4531)  time: 0.4515  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5255, ratio_loss=0.0055, pruning_loss=0.1293, mse_loss=0.4823
Epoch: [24]  [1080/1251]  eta: 0:01:21  lr: 0.000018  loss: 3.5421 (3.4552)  time: 0.4511  data: 0.0007  max mem: 19734
Epoch: [24]  [1090/1251]  eta: 0:01:17  lr: 0.000018  loss: 3.5421 (3.4558)  time: 0.4652  data: 0.0007  max mem: 19734
Epoch: [24]  [1100/1251]  eta: 0:01:12  lr: 0.000018  loss: 3.5921 (3.4558)  time: 0.4832  data: 0.0004  max mem: 19734
Epoch: [24]  [1110/1251]  eta: 0:01:07  lr: 0.000018  loss: 3.6604 (3.4586)  time: 0.4806  data: 0.0005  max mem: 19734
Epoch: [24]  [1120/1251]  eta: 0:01:02  lr: 0.000018  loss: 3.7228 (3.4593)  time: 0.4788  data: 0.0005  max mem: 19734
Epoch: [24]  [1130/1251]  eta: 0:00:57  lr: 0.000018  loss: 3.6400 (3.4594)  time: 0.4675  data: 0.0005  max mem: 19734
Epoch: [24]  [1140/1251]  eta: 0:00:53  lr: 0.000018  loss: 3.5700 (3.4582)  time: 0.4512  data: 0.0005  max mem: 19734
Epoch: [24]  [1150/1251]  eta: 0:00:48  lr: 0.000018  loss: 3.5124 (3.4584)  time: 0.4508  data: 0.0004  max mem: 19734
Epoch: [24]  [1160/1251]  eta: 0:00:43  lr: 0.000018  loss: 3.5004 (3.4573)  time: 0.4510  data: 0.0004  max mem: 19734
Epoch: [24]  [1170/1251]  eta: 0:00:38  lr: 0.000018  loss: 3.1439 (3.4553)  time: 0.4533  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4432, ratio_loss=0.0053, pruning_loss=0.1309, mse_loss=0.4936
Epoch: [24]  [1180/1251]  eta: 0:00:33  lr: 0.000018  loss: 3.1465 (3.4544)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [24]  [1190/1251]  eta: 0:00:29  lr: 0.000018  loss: 3.4313 (3.4536)  time: 0.4518  data: 0.0007  max mem: 19734
Epoch: [24]  [1200/1251]  eta: 0:00:24  lr: 0.000018  loss: 3.2971 (3.4521)  time: 0.4475  data: 0.0005  max mem: 19734
Epoch: [24]  [1210/1251]  eta: 0:00:19  lr: 0.000018  loss: 3.4669 (3.4529)  time: 0.4443  data: 0.0001  max mem: 19734
Epoch: [24]  [1220/1251]  eta: 0:00:14  lr: 0.000018  loss: 3.6759 (3.4539)  time: 0.4443  data: 0.0001  max mem: 19734
Epoch: [24]  [1230/1251]  eta: 0:00:10  lr: 0.000018  loss: 3.6759 (3.4544)  time: 0.4453  data: 0.0001  max mem: 19734
Epoch: [24]  [1240/1251]  eta: 0:00:05  lr: 0.000018  loss: 3.6778 (3.4554)  time: 0.4689  data: 0.0002  max mem: 19734
Epoch: [24]  [1250/1251]  eta: 0:00:00  lr: 0.000018  loss: 3.5700 (3.4545)  time: 0.4719  data: 0.0001  max mem: 19734
Epoch: [24] Total time: 0:09:57 (0.4773 s / it)
Averaged stats: lr: 0.000018  loss: 3.5700 (3.4394)
Test:  [  0/261]  eta: 2:09:32  loss: 0.7063 (0.7063)  acc1: 84.3750 (84.3750)  acc5: 96.3542 (96.3542)  time: 29.7809  data: 29.6891  max mem: 19734
Test:  [ 10/261]  eta: 0:11:55  loss: 0.7063 (0.7134)  acc1: 84.3750 (83.6174)  acc5: 96.8750 (96.4489)  time: 2.8523  data: 2.7119  max mem: 19734
Test:  [ 20/261]  eta: 0:06:25  loss: 0.9481 (0.8905)  acc1: 76.5625 (78.8194)  acc5: 94.2708 (94.8909)  time: 0.1915  data: 0.0138  max mem: 19734
Test:  [ 30/261]  eta: 0:04:28  loss: 0.7853 (0.8123)  acc1: 82.8125 (81.7876)  acc5: 94.7917 (95.2789)  time: 0.2306  data: 0.0079  max mem: 19734
Test:  [ 40/261]  eta: 0:03:53  loss: 0.5802 (0.7818)  acc1: 87.5000 (82.6982)  acc5: 96.8750 (95.5285)  time: 0.4837  data: 0.2610  max mem: 19734
Test:  [ 50/261]  eta: 0:03:10  loss: 0.9459 (0.8468)  acc1: 77.0833 (80.8517)  acc5: 94.2708 (95.0776)  time: 0.5086  data: 0.2657  max mem: 19734
Test:  [ 60/261]  eta: 0:02:42  loss: 0.9947 (0.8581)  acc1: 76.0417 (80.3364)  acc5: 93.7500 (95.0564)  time: 0.3082  data: 0.0205  max mem: 19734
Test:  [ 70/261]  eta: 0:02:29  loss: 0.9487 (0.8575)  acc1: 76.5625 (79.9809)  acc5: 95.8333 (95.2832)  time: 0.4783  data: 0.2437  max mem: 19734
Test:  [ 80/261]  eta: 0:02:07  loss: 0.8258 (0.8570)  acc1: 78.6458 (80.1119)  acc5: 96.3542 (95.3897)  time: 0.3915  data: 0.2359  max mem: 19734
Test:  [ 90/261]  eta: 0:01:51  loss: 0.7899 (0.8419)  acc1: 82.8125 (80.5174)  acc5: 96.3542 (95.5014)  time: 0.1771  data: 0.0121  max mem: 19734
Test:  [100/261]  eta: 0:01:42  loss: 0.8195 (0.8459)  acc1: 83.8542 (80.4765)  acc5: 95.3125 (95.5600)  time: 0.3662  data: 0.1643  max mem: 19734
Test:  [110/261]  eta: 0:01:31  loss: 0.8888 (0.8698)  acc1: 76.5625 (79.9925)  acc5: 94.7917 (95.2703)  time: 0.3871  data: 0.1634  max mem: 19734
Test:  [120/261]  eta: 0:01:20  loss: 1.2229 (0.9103)  acc1: 70.3125 (79.0461)  acc5: 90.1042 (94.7572)  time: 0.2286  data: 0.0223  max mem: 19734
Test:  [130/261]  eta: 0:01:10  loss: 1.4039 (0.9545)  acc1: 67.1875 (78.1369)  acc5: 87.5000 (94.1754)  time: 0.1864  data: 0.0316  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: 1.2927 (0.9815)  acc1: 69.2708 (77.4638)  acc5: 89.0625 (93.8904)  time: 0.3273  data: 0.2131  max mem: 19734
Test:  [150/261]  eta: 0:00:57  loss: 1.2243 (0.9880)  acc1: 71.8750 (77.4421)  acc5: 91.1458 (93.7086)  time: 0.3952  data: 0.2739  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: 1.0003 (1.0093)  acc1: 77.0833 (77.0413)  acc5: 92.1875 (93.4297)  time: 0.3363  data: 0.1780  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: 1.2493 (1.0408)  acc1: 64.5833 (76.3006)  acc5: 86.9792 (93.0799)  time: 0.4103  data: 0.2772  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4810 (1.0576)  acc1: 65.1042 (75.9064)  acc5: 87.5000 (92.9126)  time: 0.2512  data: 0.1741  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3486 (1.0707)  acc1: 67.7083 (75.6735)  acc5: 90.1042 (92.7492)  time: 0.0628  data: 0.0012  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3486 (1.0870)  acc1: 71.3542 (75.3524)  acc5: 90.1042 (92.4907)  time: 0.0644  data: 0.0013  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3531 (1.1017)  acc1: 69.2708 (75.0864)  acc5: 86.9792 (92.2517)  time: 0.0636  data: 0.0006  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4829 (1.1225)  acc1: 68.2292 (74.5734)  acc5: 86.9792 (92.0367)  time: 0.0618  data: 0.0003  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4801 (1.1327)  acc1: 65.6250 (74.3123)  acc5: 89.5833 (91.9305)  time: 0.0624  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3193 (1.1418)  acc1: 68.2292 (74.0923)  acc5: 90.1042 (91.8612)  time: 0.0645  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0699 (1.1338)  acc1: 74.4792 (74.2903)  acc5: 93.2292 (91.9862)  time: 0.0636  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.8875 (1.1338)  acc1: 77.0833 (74.3040)  acc5: 94.7917 (92.0300)  time: 0.0596  data: 0.0002  max mem: 19734
Test: Total time: 0:01:32 (0.3552 s / it)
* Acc@1 74.304 Acc@5 92.030 loss 1.134
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.46%
Epoch: [25]  [   0/1251]  eta: 6:10:37  lr: 0.000017  loss: 4.1205 (4.1205)  time: 17.7759  data: 8.0014  max mem: 19734
Epoch: [25]  [  10/1251]  eta: 0:44:27  lr: 0.000017  loss: 3.6408 (3.6304)  time: 2.1493  data: 0.8026  max mem: 19734
Epoch: [25]  [  20/1251]  eta: 0:27:37  lr: 0.000017  loss: 3.5127 (3.4504)  time: 0.5247  data: 0.0415  max mem: 19734
loss info: cls_loss=3.3910, ratio_loss=0.0055, pruning_loss=0.1333, mse_loss=0.5086
Epoch: [25]  [  30/1251]  eta: 0:21:36  lr: 0.000017  loss: 3.0596 (3.3540)  time: 0.4642  data: 0.0004  max mem: 19734
Epoch: [25]  [  40/1251]  eta: 0:18:29  lr: 0.000017  loss: 3.3248 (3.3828)  time: 0.4648  data: 0.0004  max mem: 19734
Epoch: [25]  [  50/1251]  eta: 0:16:32  lr: 0.000017  loss: 3.3248 (3.3361)  time: 0.4614  data: 0.0005  max mem: 19734
Epoch: [25]  [  60/1251]  eta: 0:15:13  lr: 0.000017  loss: 3.6875 (3.4140)  time: 0.4602  data: 0.0005  max mem: 19734
Epoch: [25]  [  70/1251]  eta: 0:14:15  lr: 0.000017  loss: 3.6908 (3.4164)  time: 0.4627  data: 0.0005  max mem: 19734
Epoch: [25]  [  80/1251]  eta: 0:13:29  lr: 0.000017  loss: 3.5122 (3.4045)  time: 0.4615  data: 0.0005  max mem: 19734
Epoch: [25]  [  90/1251]  eta: 0:12:53  lr: 0.000017  loss: 3.6397 (3.4373)  time: 0.4592  data: 0.0005  max mem: 19734
Epoch: [25]  [ 100/1251]  eta: 0:12:25  lr: 0.000017  loss: 3.6482 (3.4486)  time: 0.4694  data: 0.0005  max mem: 19734
Epoch: [25]  [ 110/1251]  eta: 0:11:59  lr: 0.000017  loss: 3.6129 (3.4415)  time: 0.4680  data: 0.0005  max mem: 19734
Epoch: [25]  [ 120/1251]  eta: 0:11:40  lr: 0.000017  loss: 3.6240 (3.4427)  time: 0.4773  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4327, ratio_loss=0.0054, pruning_loss=0.1324, mse_loss=0.4878
Epoch: [25]  [ 130/1251]  eta: 0:11:22  lr: 0.000017  loss: 3.6240 (3.4391)  time: 0.4888  data: 0.0005  max mem: 19734
Epoch: [25]  [ 140/1251]  eta: 0:11:08  lr: 0.000017  loss: 3.4072 (3.4341)  time: 0.4964  data: 0.0004  max mem: 19734
Epoch: [25]  [ 150/1251]  eta: 0:10:53  lr: 0.000017  loss: 3.4709 (3.4458)  time: 0.4963  data: 0.0004  max mem: 19734
Epoch: [25]  [ 160/1251]  eta: 0:10:38  lr: 0.000017  loss: 3.7286 (3.4536)  time: 0.4678  data: 0.0004  max mem: 19734
Epoch: [25]  [ 170/1251]  eta: 0:10:24  lr: 0.000017  loss: 3.5986 (3.4443)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [25]  [ 180/1251]  eta: 0:10:11  lr: 0.000017  loss: 3.5049 (3.4415)  time: 0.4588  data: 0.0004  max mem: 19734
Epoch: [25]  [ 190/1251]  eta: 0:09:59  lr: 0.000017  loss: 3.5844 (3.4455)  time: 0.4584  data: 0.0004  max mem: 19734
Epoch: [25]  [ 200/1251]  eta: 0:09:48  lr: 0.000017  loss: 3.6325 (3.4516)  time: 0.4595  data: 0.0004  max mem: 19734
Epoch: [25]  [ 210/1251]  eta: 0:09:38  lr: 0.000017  loss: 3.5330 (3.4469)  time: 0.4577  data: 0.0004  max mem: 19734
Epoch: [25]  [ 220/1251]  eta: 0:09:28  lr: 0.000017  loss: 3.6367 (3.4448)  time: 0.4586  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4424, ratio_loss=0.0055, pruning_loss=0.1307, mse_loss=0.4789
Epoch: [25]  [ 230/1251]  eta: 0:09:18  lr: 0.000017  loss: 3.7164 (3.4521)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [25]  [ 240/1251]  eta: 0:09:09  lr: 0.000017  loss: 3.5796 (3.4545)  time: 0.4592  data: 0.0005  max mem: 19734
Epoch: [25]  [ 250/1251]  eta: 0:09:01  lr: 0.000017  loss: 3.4758 (3.4601)  time: 0.4657  data: 0.0005  max mem: 19734
Epoch: [25]  [ 260/1251]  eta: 0:08:52  lr: 0.000017  loss: 3.5328 (3.4613)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [25]  [ 270/1251]  eta: 0:08:45  lr: 0.000017  loss: 3.4541 (3.4448)  time: 0.4777  data: 0.0005  max mem: 19734
Epoch: [25]  [ 280/1251]  eta: 0:08:38  lr: 0.000017  loss: 2.8969 (3.4403)  time: 0.4932  data: 0.0004  max mem: 19734
Epoch: [25]  [ 290/1251]  eta: 0:08:31  lr: 0.000017  loss: 3.4637 (3.4367)  time: 0.4886  data: 0.0006  max mem: 19734
Epoch: [25]  [ 300/1251]  eta: 0:08:24  lr: 0.000017  loss: 3.6079 (3.4438)  time: 0.4800  data: 0.0006  max mem: 19734
Epoch: [25]  [ 310/1251]  eta: 0:08:16  lr: 0.000017  loss: 3.6272 (3.4464)  time: 0.4628  data: 0.0005  max mem: 19734
Epoch: [25]  [ 320/1251]  eta: 0:08:09  lr: 0.000017  loss: 3.5089 (3.4506)  time: 0.4524  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4168, ratio_loss=0.0055, pruning_loss=0.1312, mse_loss=0.4789
Epoch: [25]  [ 330/1251]  eta: 0:08:02  lr: 0.000017  loss: 3.3230 (3.4438)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [25]  [ 340/1251]  eta: 0:07:55  lr: 0.000017  loss: 3.3230 (3.4471)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [25]  [ 350/1251]  eta: 0:07:48  lr: 0.000017  loss: 3.3673 (3.4443)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [25]  [ 360/1251]  eta: 0:07:41  lr: 0.000017  loss: 3.1843 (3.4409)  time: 0.4537  data: 0.0003  max mem: 19734
Epoch: [25]  [ 370/1251]  eta: 0:07:34  lr: 0.000017  loss: 3.5363 (3.4411)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [25]  [ 380/1251]  eta: 0:07:27  lr: 0.000017  loss: 3.5363 (3.4422)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [25]  [ 390/1251]  eta: 0:07:21  lr: 0.000017  loss: 3.5603 (3.4431)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [25]  [ 400/1251]  eta: 0:07:15  lr: 0.000017  loss: 3.6750 (3.4460)  time: 0.4666  data: 0.0004  max mem: 19734
Epoch: [25]  [ 410/1251]  eta: 0:07:09  lr: 0.000017  loss: 3.6739 (3.4475)  time: 0.4734  data: 0.0006  max mem: 19734
Epoch: [25]  [ 420/1251]  eta: 0:07:04  lr: 0.000017  loss: 3.5320 (3.4430)  time: 0.4889  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4160, ratio_loss=0.0054, pruning_loss=0.1304, mse_loss=0.4807
Epoch: [25]  [ 430/1251]  eta: 0:06:59  lr: 0.000017  loss: 3.5320 (3.4477)  time: 0.5080  data: 0.0005  max mem: 19734
Epoch: [25]  [ 440/1251]  eta: 0:06:53  lr: 0.000017  loss: 3.6241 (3.4474)  time: 0.4957  data: 0.0004  max mem: 19734
Epoch: [25]  [ 450/1251]  eta: 0:06:47  lr: 0.000017  loss: 3.4394 (3.4469)  time: 0.4673  data: 0.0004  max mem: 19734
Epoch: [25]  [ 460/1251]  eta: 0:06:41  lr: 0.000017  loss: 3.1841 (3.4392)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [25]  [ 470/1251]  eta: 0:06:35  lr: 0.000017  loss: 3.3547 (3.4406)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [25]  [ 480/1251]  eta: 0:06:29  lr: 0.000017  loss: 3.5612 (3.4404)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [25]  [ 490/1251]  eta: 0:06:23  lr: 0.000017  loss: 3.5890 (3.4433)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [25]  [ 500/1251]  eta: 0:06:18  lr: 0.000017  loss: 3.7602 (3.4482)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [25]  [ 510/1251]  eta: 0:06:12  lr: 0.000017  loss: 3.7999 (3.4510)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [25]  [ 520/1251]  eta: 0:06:06  lr: 0.000017  loss: 3.6296 (3.4550)  time: 0.4571  data: 0.0003  max mem: 19734
loss info: cls_loss=3.4560, ratio_loss=0.0056, pruning_loss=0.1292, mse_loss=0.4777
Epoch: [25]  [ 530/1251]  eta: 0:06:01  lr: 0.000017  loss: 3.5818 (3.4563)  time: 0.4535  data: 0.0003  max mem: 19734
Epoch: [25]  [ 540/1251]  eta: 0:05:55  lr: 0.000017  loss: 3.4897 (3.4566)  time: 0.4635  data: 0.0003  max mem: 19734
Epoch: [25]  [ 550/1251]  eta: 0:05:50  lr: 0.000017  loss: 3.4846 (3.4528)  time: 0.4712  data: 0.0004  max mem: 19734
Epoch: [25]  [ 560/1251]  eta: 0:05:45  lr: 0.000017  loss: 3.4832 (3.4548)  time: 0.4751  data: 0.0004  max mem: 19734
Epoch: [25]  [ 570/1251]  eta: 0:05:39  lr: 0.000017  loss: 3.4150 (3.4531)  time: 0.4767  data: 0.0004  max mem: 19734
Epoch: [25]  [ 580/1251]  eta: 0:05:34  lr: 0.000017  loss: 3.2593 (3.4487)  time: 0.4818  data: 0.0004  max mem: 19734
Epoch: [25]  [ 590/1251]  eta: 0:05:29  lr: 0.000017  loss: 3.7630 (3.4517)  time: 0.4841  data: 0.0006  max mem: 19734
Epoch: [25]  [ 600/1251]  eta: 0:05:24  lr: 0.000017  loss: 3.7630 (3.4524)  time: 0.4686  data: 0.0006  max mem: 19734
Epoch: [25]  [ 610/1251]  eta: 0:05:18  lr: 0.000017  loss: 3.4091 (3.4506)  time: 0.4586  data: 0.0005  max mem: 19734
Epoch: [25]  [ 620/1251]  eta: 0:05:13  lr: 0.000017  loss: 3.1968 (3.4470)  time: 0.4558  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3886, ratio_loss=0.0055, pruning_loss=0.1333, mse_loss=0.4863
Epoch: [25]  [ 630/1251]  eta: 0:05:07  lr: 0.000017  loss: 3.2730 (3.4488)  time: 0.4558  data: 0.0006  max mem: 19734
Epoch: [25]  [ 640/1251]  eta: 0:05:02  lr: 0.000017  loss: 3.6018 (3.4508)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [25]  [ 650/1251]  eta: 0:04:57  lr: 0.000017  loss: 3.6018 (3.4529)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [25]  [ 660/1251]  eta: 0:04:52  lr: 0.000017  loss: 3.5296 (3.4534)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [25]  [ 670/1251]  eta: 0:04:46  lr: 0.000017  loss: 3.5515 (3.4550)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [25]  [ 680/1251]  eta: 0:04:41  lr: 0.000017  loss: 3.7053 (3.4555)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [25]  [ 690/1251]  eta: 0:04:36  lr: 0.000017  loss: 3.6163 (3.4520)  time: 0.4673  data: 0.0004  max mem: 19734
Epoch: [25]  [ 700/1251]  eta: 0:04:31  lr: 0.000017  loss: 3.4084 (3.4509)  time: 0.4807  data: 0.0004  max mem: 19734
Epoch: [25]  [ 710/1251]  eta: 0:04:26  lr: 0.000017  loss: 3.2818 (3.4470)  time: 0.4859  data: 0.0004  max mem: 19734
Epoch: [25]  [ 720/1251]  eta: 0:04:21  lr: 0.000017  loss: 3.3752 (3.4476)  time: 0.4813  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4144, ratio_loss=0.0057, pruning_loss=0.1317, mse_loss=0.4711
Epoch: [25]  [ 730/1251]  eta: 0:04:16  lr: 0.000017  loss: 3.4723 (3.4467)  time: 0.4749  data: 0.0003  max mem: 19734
Epoch: [25]  [ 740/1251]  eta: 0:04:11  lr: 0.000017  loss: 3.4822 (3.4451)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [25]  [ 750/1251]  eta: 0:04:06  lr: 0.000017  loss: 3.5058 (3.4454)  time: 0.4529  data: 0.0006  max mem: 19734
Epoch: [25]  [ 760/1251]  eta: 0:04:00  lr: 0.000017  loss: 3.5532 (3.4438)  time: 0.4557  data: 0.0006  max mem: 19734
Epoch: [25]  [ 770/1251]  eta: 0:03:55  lr: 0.000017  loss: 3.1608 (3.4413)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [25]  [ 780/1251]  eta: 0:03:50  lr: 0.000017  loss: 3.1688 (3.4388)  time: 0.4612  data: 0.0004  max mem: 19734
Epoch: [25]  [ 790/1251]  eta: 0:03:45  lr: 0.000017  loss: 3.3256 (3.4404)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [25]  [ 800/1251]  eta: 0:03:40  lr: 0.000017  loss: 3.5596 (3.4406)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [25]  [ 810/1251]  eta: 0:03:35  lr: 0.000017  loss: 3.5388 (3.4412)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [25]  [ 820/1251]  eta: 0:03:30  lr: 0.000017  loss: 3.4346 (3.4396)  time: 0.4549  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3704, ratio_loss=0.0054, pruning_loss=0.1307, mse_loss=0.4752
Epoch: [25]  [ 830/1251]  eta: 0:03:25  lr: 0.000017  loss: 3.5291 (3.4428)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [25]  [ 840/1251]  eta: 0:03:20  lr: 0.000017  loss: 3.5727 (3.4430)  time: 0.4735  data: 0.0004  max mem: 19734
Epoch: [25]  [ 850/1251]  eta: 0:03:15  lr: 0.000017  loss: 3.5767 (3.4447)  time: 0.4956  data: 0.0004  max mem: 19734
Epoch: [25]  [ 860/1251]  eta: 0:03:10  lr: 0.000017  loss: 3.5849 (3.4442)  time: 0.4797  data: 0.0004  max mem: 19734
Epoch: [25]  [ 870/1251]  eta: 0:03:05  lr: 0.000017  loss: 3.5849 (3.4458)  time: 0.4753  data: 0.0004  max mem: 19734
Epoch: [25]  [ 880/1251]  eta: 0:03:00  lr: 0.000017  loss: 3.7041 (3.4480)  time: 0.4812  data: 0.0004  max mem: 19734
Epoch: [25]  [ 890/1251]  eta: 0:02:55  lr: 0.000017  loss: 3.7041 (3.4486)  time: 0.4645  data: 0.0004  max mem: 19734
Epoch: [25]  [ 900/1251]  eta: 0:02:50  lr: 0.000017  loss: 3.6681 (3.4496)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [25]  [ 910/1251]  eta: 0:02:45  lr: 0.000017  loss: 3.7333 (3.4519)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [25]  [ 920/1251]  eta: 0:02:40  lr: 0.000017  loss: 3.6599 (3.4529)  time: 0.4554  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5190, ratio_loss=0.0051, pruning_loss=0.1280, mse_loss=0.4759
Epoch: [25]  [ 930/1251]  eta: 0:02:35  lr: 0.000017  loss: 3.6599 (3.4526)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [25]  [ 940/1251]  eta: 0:02:30  lr: 0.000017  loss: 3.6701 (3.4548)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [25]  [ 950/1251]  eta: 0:02:26  lr: 0.000017  loss: 3.6057 (3.4545)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [25]  [ 960/1251]  eta: 0:02:21  lr: 0.000017  loss: 3.4781 (3.4536)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [25]  [ 970/1251]  eta: 0:02:16  lr: 0.000017  loss: 3.5506 (3.4559)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [25]  [ 980/1251]  eta: 0:02:11  lr: 0.000017  loss: 3.5082 (3.4552)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [25]  [ 990/1251]  eta: 0:02:06  lr: 0.000017  loss: 3.5056 (3.4552)  time: 0.4824  data: 0.0005  max mem: 19734
Epoch: [25]  [1000/1251]  eta: 0:02:01  lr: 0.000017  loss: 3.5056 (3.4539)  time: 0.4962  data: 0.0004  max mem: 19734
Epoch: [25]  [1010/1251]  eta: 0:01:56  lr: 0.000017  loss: 3.5172 (3.4534)  time: 0.4932  data: 0.0005  max mem: 19734
Epoch: [25]  [1020/1251]  eta: 0:01:51  lr: 0.000017  loss: 3.5103 (3.4519)  time: 0.4858  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4364, ratio_loss=0.0052, pruning_loss=0.1301, mse_loss=0.4681
Epoch: [25]  [1030/1251]  eta: 0:01:47  lr: 0.000017  loss: 3.5543 (3.4536)  time: 0.4634  data: 0.0004  max mem: 19734
Epoch: [25]  [1040/1251]  eta: 0:01:42  lr: 0.000017  loss: 3.6452 (3.4551)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [25]  [1050/1251]  eta: 0:01:37  lr: 0.000017  loss: 3.5726 (3.4553)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [25]  [1060/1251]  eta: 0:01:32  lr: 0.000017  loss: 3.5780 (3.4570)  time: 0.4574  data: 0.0003  max mem: 19734
Epoch: [25]  [1070/1251]  eta: 0:01:27  lr: 0.000017  loss: 3.6509 (3.4580)  time: 0.4641  data: 0.0003  max mem: 19734
Epoch: [25]  [1080/1251]  eta: 0:01:22  lr: 0.000017  loss: 3.5607 (3.4571)  time: 0.4622  data: 0.0004  max mem: 19734
Epoch: [25]  [1090/1251]  eta: 0:01:17  lr: 0.000017  loss: 3.3073 (3.4559)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [25]  [1100/1251]  eta: 0:01:12  lr: 0.000017  loss: 3.2822 (3.4549)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [25]  [1110/1251]  eta: 0:01:08  lr: 0.000017  loss: 3.4324 (3.4558)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [25]  [1120/1251]  eta: 0:01:03  lr: 0.000017  loss: 3.5607 (3.4560)  time: 0.4558  data: 0.0003  max mem: 19734
loss info: cls_loss=3.4600, ratio_loss=0.0052, pruning_loss=0.1279, mse_loss=0.4628
Epoch: [25]  [1130/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.6605 (3.4562)  time: 0.4668  data: 0.0004  max mem: 19734
Epoch: [25]  [1140/1251]  eta: 0:00:53  lr: 0.000017  loss: 3.6097 (3.4548)  time: 0.4860  data: 0.0004  max mem: 19734
Epoch: [25]  [1150/1251]  eta: 0:00:48  lr: 0.000017  loss: 3.6340 (3.4549)  time: 0.4992  data: 0.0005  max mem: 19734
Epoch: [25]  [1160/1251]  eta: 0:00:43  lr: 0.000017  loss: 3.6340 (3.4540)  time: 0.4867  data: 0.0005  max mem: 19734
Epoch: [25]  [1170/1251]  eta: 0:00:39  lr: 0.000017  loss: 3.6290 (3.4568)  time: 0.4704  data: 0.0005  max mem: 19734
Epoch: [25]  [1180/1251]  eta: 0:00:34  lr: 0.000017  loss: 3.6102 (3.4541)  time: 0.4623  data: 0.0005  max mem: 19734
Epoch: [25]  [1190/1251]  eta: 0:00:29  lr: 0.000017  loss: 3.2284 (3.4544)  time: 0.4513  data: 0.0009  max mem: 19734
Epoch: [25]  [1200/1251]  eta: 0:00:24  lr: 0.000017  loss: 3.6849 (3.4551)  time: 0.4472  data: 0.0008  max mem: 19734
Epoch: [25]  [1210/1251]  eta: 0:00:19  lr: 0.000017  loss: 3.4991 (3.4536)  time: 0.4448  data: 0.0002  max mem: 19734
Epoch: [25]  [1220/1251]  eta: 0:00:14  lr: 0.000017  loss: 3.2991 (3.4519)  time: 0.4470  data: 0.0002  max mem: 19734
loss info: cls_loss=3.3778, ratio_loss=0.0054, pruning_loss=0.1314, mse_loss=0.4560
Epoch: [25]  [1230/1251]  eta: 0:00:10  lr: 0.000017  loss: 3.2991 (3.4516)  time: 0.4472  data: 0.0002  max mem: 19734
Epoch: [25]  [1240/1251]  eta: 0:00:05  lr: 0.000017  loss: 3.1356 (3.4494)  time: 0.4476  data: 0.0002  max mem: 19734
Epoch: [25]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.3429 (3.4499)  time: 0.4494  data: 0.0002  max mem: 19734
Epoch: [25] Total time: 0:10:01 (0.4806 s / it)
Averaged stats: lr: 0.000017  loss: 3.3429 (3.4359)
Test:  [  0/261]  eta: 1:34:30  loss: 0.7118 (0.7118)  acc1: 81.7708 (81.7708)  acc5: 95.3125 (95.3125)  time: 21.7264  data: 21.5800  max mem: 19734
Test:  [ 10/261]  eta: 0:08:50  loss: 0.7118 (0.7253)  acc1: 84.3750 (84.1856)  acc5: 95.8333 (96.1174)  time: 2.1151  data: 1.9732  max mem: 19734
Test:  [ 20/261]  eta: 0:04:45  loss: 0.8954 (0.8962)  acc1: 80.2083 (79.6379)  acc5: 93.7500 (94.5933)  time: 0.1575  data: 0.0099  max mem: 19734
Test:  [ 30/261]  eta: 0:03:21  loss: 0.7848 (0.8165)  acc1: 83.3333 (82.4429)  acc5: 94.2708 (95.0941)  time: 0.1888  data: 0.0095  max mem: 19734
Test:  [ 40/261]  eta: 0:03:48  loss: 0.5895 (0.7849)  acc1: 88.0208 (83.1936)  acc5: 96.8750 (95.4014)  time: 0.8722  data: 0.7100  max mem: 19734
Test:  [ 50/261]  eta: 0:02:59  loss: 0.9575 (0.8519)  acc1: 77.6042 (81.1581)  acc5: 95.3125 (95.0266)  time: 0.8149  data: 0.7109  max mem: 19734
Test:  [ 60/261]  eta: 0:02:28  loss: 0.9742 (0.8587)  acc1: 75.5208 (80.7036)  acc5: 94.7917 (95.1076)  time: 0.1369  data: 0.0143  max mem: 19734
Test:  [ 70/261]  eta: 0:02:19  loss: 0.9265 (0.8603)  acc1: 76.0417 (80.2744)  acc5: 96.3542 (95.2538)  time: 0.4304  data: 0.2561  max mem: 19734
Test:  [ 80/261]  eta: 0:01:59  loss: 0.8341 (0.8611)  acc1: 78.6458 (80.3884)  acc5: 96.8750 (95.4090)  time: 0.4298  data: 0.2566  max mem: 19734
Test:  [ 90/261]  eta: 0:01:45  loss: 0.8194 (0.8490)  acc1: 83.8542 (80.7807)  acc5: 95.8333 (95.5071)  time: 0.2147  data: 0.0177  max mem: 19734
Test:  [100/261]  eta: 0:01:45  loss: 0.8194 (0.8528)  acc1: 84.3750 (80.7498)  acc5: 95.8333 (95.5497)  time: 0.6108  data: 0.4312  max mem: 19734
Test:  [110/261]  eta: 0:01:31  loss: 0.8676 (0.8777)  acc1: 77.0833 (80.2318)  acc5: 94.2708 (95.2468)  time: 0.5327  data: 0.4267  max mem: 19734
Test:  [120/261]  eta: 0:01:19  loss: 1.2780 (0.9196)  acc1: 69.7917 (79.2312)  acc5: 90.1042 (94.6711)  time: 0.1092  data: 0.0116  max mem: 19734
Test:  [130/261]  eta: 0:01:11  loss: 1.4569 (0.9666)  acc1: 67.1875 (78.2562)  acc5: 86.4583 (94.0442)  time: 0.2427  data: 0.1275  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: 1.3146 (0.9936)  acc1: 67.1875 (77.5783)  acc5: 89.5833 (93.7869)  time: 0.3694  data: 0.2400  max mem: 19734
Test:  [150/261]  eta: 0:00:56  loss: 1.2222 (0.9996)  acc1: 72.9167 (77.5490)  acc5: 91.1458 (93.6155)  time: 0.2593  data: 0.1227  max mem: 19734
Test:  [160/261]  eta: 0:00:52  loss: 1.0288 (1.0187)  acc1: 78.6458 (77.2192)  acc5: 92.1875 (93.3489)  time: 0.3938  data: 0.2735  max mem: 19734
Test:  [170/261]  eta: 0:00:44  loss: 1.3047 (1.0492)  acc1: 63.5417 (76.4742)  acc5: 88.0208 (93.0160)  time: 0.3568  data: 0.2729  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4753 (1.0675)  acc1: 64.5833 (76.0503)  acc5: 87.5000 (92.8465)  time: 0.2157  data: 0.1482  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4278 (1.0810)  acc1: 66.1458 (75.7853)  acc5: 90.6250 (92.6865)  time: 0.2121  data: 0.1466  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3936 (1.0968)  acc1: 70.8333 (75.4327)  acc5: 88.5417 (92.4518)  time: 0.0683  data: 0.0021  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3936 (1.1108)  acc1: 70.3125 (75.1506)  acc5: 88.0208 (92.2467)  time: 0.0660  data: 0.0006  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4176 (1.1313)  acc1: 66.6667 (74.6536)  acc5: 87.5000 (92.0084)  time: 0.0626  data: 0.0003  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4727 (1.1419)  acc1: 66.6667 (74.3755)  acc5: 89.0625 (91.8944)  time: 0.0622  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3085 (1.1506)  acc1: 67.1875 (74.1701)  acc5: 90.6250 (91.8244)  time: 0.0621  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0505 (1.1435)  acc1: 75.5208 (74.3318)  acc5: 92.7083 (91.9385)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9565 (1.1427)  acc1: 76.0417 (74.3520)  acc5: 95.3125 (92.0100)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:33 (0.3563 s / it)
* Acc@1 74.352 Acc@5 92.010 loss 1.143
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.46%
Epoch: [26]  [   0/1251]  eta: 5:23:30  lr: 0.000017  loss: 3.9222 (3.9222)  time: 15.5163  data: 11.3933  max mem: 19734
Epoch: [26]  [  10/1251]  eta: 0:43:24  lr: 0.000017  loss: 3.5180 (3.3866)  time: 2.0990  data: 1.2793  max mem: 19734
Epoch: [26]  [  20/1251]  eta: 0:27:14  lr: 0.000017  loss: 3.4822 (3.3784)  time: 0.6184  data: 0.1341  max mem: 19734
Epoch: [26]  [  30/1251]  eta: 0:21:34  lr: 0.000017  loss: 3.3798 (3.3256)  time: 0.4891  data: 0.0003  max mem: 19734
Epoch: [26]  [  40/1251]  eta: 0:18:35  lr: 0.000017  loss: 3.5971 (3.4078)  time: 0.4943  data: 0.0004  max mem: 19734
Epoch: [26]  [  50/1251]  eta: 0:16:53  lr: 0.000017  loss: 3.6810 (3.4239)  time: 0.5093  data: 0.0004  max mem: 19734
Epoch: [26]  [  60/1251]  eta: 0:15:30  lr: 0.000017  loss: 3.4813 (3.4390)  time: 0.4955  data: 0.0007  max mem: 19734
Epoch: [26]  [  70/1251]  eta: 0:14:29  lr: 0.000017  loss: 3.6491 (3.4409)  time: 0.4616  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3905, ratio_loss=0.0055, pruning_loss=0.1310, mse_loss=0.5023
Epoch: [26]  [  80/1251]  eta: 0:13:42  lr: 0.000017  loss: 3.5348 (3.4406)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [26]  [  90/1251]  eta: 0:13:05  lr: 0.000017  loss: 3.4203 (3.4397)  time: 0.4633  data: 0.0005  max mem: 19734
Epoch: [26]  [ 100/1251]  eta: 0:12:33  lr: 0.000017  loss: 3.3940 (3.4242)  time: 0.4628  data: 0.0005  max mem: 19734
Epoch: [26]  [ 110/1251]  eta: 0:12:07  lr: 0.000017  loss: 3.6150 (3.4456)  time: 0.4588  data: 0.0005  max mem: 19734
Epoch: [26]  [ 120/1251]  eta: 0:11:44  lr: 0.000017  loss: 3.8314 (3.4565)  time: 0.4604  data: 0.0005  max mem: 19734
Epoch: [26]  [ 130/1251]  eta: 0:11:23  lr: 0.000017  loss: 3.6486 (3.4536)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [26]  [ 140/1251]  eta: 0:11:05  lr: 0.000017  loss: 3.5990 (3.4447)  time: 0.4554  data: 0.0005  max mem: 19734
Epoch: [26]  [ 150/1251]  eta: 0:10:50  lr: 0.000017  loss: 3.4365 (3.4407)  time: 0.4670  data: 0.0005  max mem: 19734
Epoch: [26]  [ 160/1251]  eta: 0:10:37  lr: 0.000017  loss: 3.4747 (3.4425)  time: 0.4794  data: 0.0005  max mem: 19734
Epoch: [26]  [ 170/1251]  eta: 0:10:24  lr: 0.000017  loss: 3.4747 (3.4356)  time: 0.4745  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3950, ratio_loss=0.0051, pruning_loss=0.1296, mse_loss=0.4700
Epoch: [26]  [ 180/1251]  eta: 0:10:13  lr: 0.000017  loss: 3.1306 (3.4170)  time: 0.4799  data: 0.0005  max mem: 19734
Epoch: [26]  [ 190/1251]  eta: 0:10:03  lr: 0.000017  loss: 3.5026 (3.4325)  time: 0.4981  data: 0.0004  max mem: 19734
Epoch: [26]  [ 200/1251]  eta: 0:09:52  lr: 0.000017  loss: 3.6105 (3.4410)  time: 0.4872  data: 0.0006  max mem: 19734
Epoch: [26]  [ 210/1251]  eta: 0:09:41  lr: 0.000017  loss: 3.6100 (3.4519)  time: 0.4623  data: 0.0008  max mem: 19734
Epoch: [26]  [ 220/1251]  eta: 0:09:31  lr: 0.000017  loss: 3.5383 (3.4453)  time: 0.4573  data: 0.0006  max mem: 19734
Epoch: [26]  [ 230/1251]  eta: 0:09:21  lr: 0.000017  loss: 3.2698 (3.4416)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [26]  [ 240/1251]  eta: 0:09:12  lr: 0.000017  loss: 3.4148 (3.4417)  time: 0.4628  data: 0.0005  max mem: 19734
Epoch: [26]  [ 250/1251]  eta: 0:09:03  lr: 0.000017  loss: 3.5875 (3.4481)  time: 0.4622  data: 0.0004  max mem: 19734
Epoch: [26]  [ 260/1251]  eta: 0:08:55  lr: 0.000017  loss: 3.6399 (3.4539)  time: 0.4595  data: 0.0004  max mem: 19734
Epoch: [26]  [ 270/1251]  eta: 0:08:46  lr: 0.000017  loss: 3.5600 (3.4560)  time: 0.4597  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4732, ratio_loss=0.0051, pruning_loss=0.1286, mse_loss=0.4891
Epoch: [26]  [ 280/1251]  eta: 0:08:38  lr: 0.000017  loss: 3.4376 (3.4469)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [26]  [ 290/1251]  eta: 0:08:30  lr: 0.000017  loss: 3.5064 (3.4516)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [26]  [ 300/1251]  eta: 0:08:23  lr: 0.000017  loss: 3.5645 (3.4609)  time: 0.4672  data: 0.0004  max mem: 19734
Epoch: [26]  [ 310/1251]  eta: 0:08:17  lr: 0.000017  loss: 3.7156 (3.4639)  time: 0.4785  data: 0.0004  max mem: 19734
Epoch: [26]  [ 320/1251]  eta: 0:08:10  lr: 0.000017  loss: 3.5426 (3.4595)  time: 0.4868  data: 0.0004  max mem: 19734
Epoch: [26]  [ 330/1251]  eta: 0:08:04  lr: 0.000017  loss: 3.6903 (3.4704)  time: 0.4970  data: 0.0005  max mem: 19734
Epoch: [26]  [ 340/1251]  eta: 0:07:58  lr: 0.000017  loss: 3.7839 (3.4720)  time: 0.4958  data: 0.0005  max mem: 19734
Epoch: [26]  [ 350/1251]  eta: 0:07:51  lr: 0.000017  loss: 3.5862 (3.4647)  time: 0.4753  data: 0.0005  max mem: 19734
Epoch: [26]  [ 360/1251]  eta: 0:07:44  lr: 0.000017  loss: 3.3412 (3.4623)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [26]  [ 370/1251]  eta: 0:07:37  lr: 0.000017  loss: 3.4246 (3.4564)  time: 0.4550  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4530, ratio_loss=0.0053, pruning_loss=0.1292, mse_loss=0.4674
Epoch: [26]  [ 380/1251]  eta: 0:07:31  lr: 0.000017  loss: 3.5075 (3.4543)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [26]  [ 390/1251]  eta: 0:07:24  lr: 0.000017  loss: 3.5562 (3.4571)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [26]  [ 400/1251]  eta: 0:07:18  lr: 0.000017  loss: 3.5179 (3.4547)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [26]  [ 410/1251]  eta: 0:07:11  lr: 0.000017  loss: 3.5179 (3.4562)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [26]  [ 420/1251]  eta: 0:07:05  lr: 0.000017  loss: 3.5995 (3.4553)  time: 0.4583  data: 0.0005  max mem: 19734
Epoch: [26]  [ 430/1251]  eta: 0:06:59  lr: 0.000017  loss: 3.3113 (3.4513)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [26]  [ 440/1251]  eta: 0:06:53  lr: 0.000017  loss: 3.3113 (3.4489)  time: 0.4552  data: 0.0005  max mem: 19734
Epoch: [26]  [ 450/1251]  eta: 0:06:48  lr: 0.000017  loss: 3.3962 (3.4492)  time: 0.4768  data: 0.0005  max mem: 19734
Epoch: [26]  [ 460/1251]  eta: 0:06:42  lr: 0.000017  loss: 3.2939 (3.4384)  time: 0.4848  data: 0.0004  max mem: 19734
Epoch: [26]  [ 470/1251]  eta: 0:06:36  lr: 0.000017  loss: 3.3277 (3.4381)  time: 0.4825  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3204, ratio_loss=0.0050, pruning_loss=0.1312, mse_loss=0.4617
Epoch: [26]  [ 480/1251]  eta: 0:06:31  lr: 0.000017  loss: 3.4486 (3.4392)  time: 0.5009  data: 0.0004  max mem: 19734
Epoch: [26]  [ 490/1251]  eta: 0:06:25  lr: 0.000017  loss: 3.6251 (3.4397)  time: 0.4827  data: 0.0004  max mem: 19734
Epoch: [26]  [ 500/1251]  eta: 0:06:20  lr: 0.000017  loss: 3.7263 (3.4432)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [26]  [ 510/1251]  eta: 0:06:14  lr: 0.000017  loss: 3.6924 (3.4433)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [26]  [ 520/1251]  eta: 0:06:08  lr: 0.000017  loss: 3.6300 (3.4464)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [26]  [ 530/1251]  eta: 0:06:03  lr: 0.000017  loss: 3.6080 (3.4496)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [26]  [ 540/1251]  eta: 0:05:57  lr: 0.000017  loss: 3.6080 (3.4517)  time: 0.4581  data: 0.0005  max mem: 19734
Epoch: [26]  [ 550/1251]  eta: 0:05:51  lr: 0.000017  loss: 3.5852 (3.4489)  time: 0.4583  data: 0.0005  max mem: 19734
Epoch: [26]  [ 560/1251]  eta: 0:05:46  lr: 0.000017  loss: 3.5420 (3.4507)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [26]  [ 570/1251]  eta: 0:05:40  lr: 0.000017  loss: 3.5026 (3.4501)  time: 0.4573  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4982, ratio_loss=0.0056, pruning_loss=0.1287, mse_loss=0.4622
Epoch: [26]  [ 580/1251]  eta: 0:05:35  lr: 0.000017  loss: 3.4056 (3.4486)  time: 0.4590  data: 0.0005  max mem: 19734
Epoch: [26]  [ 590/1251]  eta: 0:05:29  lr: 0.000017  loss: 3.6343 (3.4532)  time: 0.4587  data: 0.0007  max mem: 19734
Epoch: [26]  [ 600/1251]  eta: 0:05:24  lr: 0.000017  loss: 3.6618 (3.4547)  time: 0.4748  data: 0.0007  max mem: 19734
Epoch: [26]  [ 610/1251]  eta: 0:05:19  lr: 0.000017  loss: 3.6065 (3.4544)  time: 0.4755  data: 0.0005  max mem: 19734
Epoch: [26]  [ 620/1251]  eta: 0:05:14  lr: 0.000017  loss: 3.2809 (3.4481)  time: 0.4873  data: 0.0005  max mem: 19734
Epoch: [26]  [ 630/1251]  eta: 0:05:09  lr: 0.000017  loss: 3.3001 (3.4471)  time: 0.5048  data: 0.0004  max mem: 19734
Epoch: [26]  [ 640/1251]  eta: 0:05:04  lr: 0.000017  loss: 3.5441 (3.4486)  time: 0.4751  data: 0.0004  max mem: 19734
Epoch: [26]  [ 650/1251]  eta: 0:04:58  lr: 0.000017  loss: 3.5165 (3.4466)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [26]  [ 660/1251]  eta: 0:04:53  lr: 0.000017  loss: 3.4122 (3.4453)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [26]  [ 670/1251]  eta: 0:04:48  lr: 0.000017  loss: 3.5103 (3.4453)  time: 0.4533  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3888, ratio_loss=0.0053, pruning_loss=0.1308, mse_loss=0.4798
Epoch: [26]  [ 680/1251]  eta: 0:04:42  lr: 0.000017  loss: 3.4753 (3.4466)  time: 0.4530  data: 0.0006  max mem: 19734
Epoch: [26]  [ 690/1251]  eta: 0:04:37  lr: 0.000017  loss: 3.4752 (3.4439)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [26]  [ 700/1251]  eta: 0:04:32  lr: 0.000017  loss: 3.3929 (3.4428)  time: 0.4575  data: 0.0005  max mem: 19734
Epoch: [26]  [ 710/1251]  eta: 0:04:26  lr: 0.000017  loss: 3.4387 (3.4440)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [26]  [ 720/1251]  eta: 0:04:21  lr: 0.000017  loss: 3.7307 (3.4472)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [26]  [ 730/1251]  eta: 0:04:16  lr: 0.000017  loss: 3.5291 (3.4435)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [26]  [ 740/1251]  eta: 0:04:11  lr: 0.000017  loss: 3.2759 (3.4454)  time: 0.4644  data: 0.0004  max mem: 19734
Epoch: [26]  [ 750/1251]  eta: 0:04:06  lr: 0.000017  loss: 3.5400 (3.4445)  time: 0.4779  data: 0.0005  max mem: 19734
Epoch: [26]  [ 760/1251]  eta: 0:04:01  lr: 0.000017  loss: 3.6254 (3.4474)  time: 0.4885  data: 0.0004  max mem: 19734
Epoch: [26]  [ 770/1251]  eta: 0:03:56  lr: 0.000017  loss: 3.5049 (3.4462)  time: 0.4955  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4319, ratio_loss=0.0053, pruning_loss=0.1303, mse_loss=0.4652
Epoch: [26]  [ 780/1251]  eta: 0:03:51  lr: 0.000017  loss: 3.5049 (3.4481)  time: 0.4852  data: 0.0005  max mem: 19734
Epoch: [26]  [ 790/1251]  eta: 0:03:46  lr: 0.000017  loss: 3.6063 (3.4487)  time: 0.4626  data: 0.0004  max mem: 19734
Epoch: [26]  [ 800/1251]  eta: 0:03:41  lr: 0.000017  loss: 3.6063 (3.4505)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [26]  [ 810/1251]  eta: 0:03:36  lr: 0.000017  loss: 3.7289 (3.4527)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [26]  [ 820/1251]  eta: 0:03:31  lr: 0.000017  loss: 3.5884 (3.4519)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [26]  [ 830/1251]  eta: 0:03:26  lr: 0.000017  loss: 3.2661 (3.4532)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [26]  [ 840/1251]  eta: 0:03:21  lr: 0.000017  loss: 3.3582 (3.4493)  time: 0.4522  data: 0.0005  max mem: 19734
Epoch: [26]  [ 850/1251]  eta: 0:03:15  lr: 0.000017  loss: 3.3582 (3.4504)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [26]  [ 860/1251]  eta: 0:03:10  lr: 0.000017  loss: 3.6215 (3.4521)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [26]  [ 870/1251]  eta: 0:03:05  lr: 0.000017  loss: 3.4987 (3.4503)  time: 0.4519  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4576, ratio_loss=0.0054, pruning_loss=0.1305, mse_loss=0.4725
Epoch: [26]  [ 880/1251]  eta: 0:03:00  lr: 0.000017  loss: 3.1986 (3.4479)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [26]  [ 890/1251]  eta: 0:02:55  lr: 0.000017  loss: 3.1986 (3.4444)  time: 0.4713  data: 0.0005  max mem: 19734
Epoch: [26]  [ 900/1251]  eta: 0:02:51  lr: 0.000017  loss: 3.3794 (3.4442)  time: 0.4887  data: 0.0005  max mem: 19734
Epoch: [26]  [ 910/1251]  eta: 0:02:46  lr: 0.000017  loss: 3.3213 (3.4403)  time: 0.4986  data: 0.0005  max mem: 19734
Epoch: [26]  [ 920/1251]  eta: 0:02:41  lr: 0.000017  loss: 3.4003 (3.4412)  time: 0.4876  data: 0.0004  max mem: 19734
Epoch: [26]  [ 930/1251]  eta: 0:02:36  lr: 0.000017  loss: 3.4567 (3.4409)  time: 0.4615  data: 0.0004  max mem: 19734
Epoch: [26]  [ 940/1251]  eta: 0:02:31  lr: 0.000017  loss: 3.4651 (3.4425)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [26]  [ 950/1251]  eta: 0:02:26  lr: 0.000017  loss: 3.5963 (3.4464)  time: 0.4516  data: 0.0005  max mem: 19734
Epoch: [26]  [ 960/1251]  eta: 0:02:21  lr: 0.000017  loss: 3.6618 (3.4468)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [26]  [ 970/1251]  eta: 0:02:16  lr: 0.000017  loss: 3.5343 (3.4464)  time: 0.4577  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3907, ratio_loss=0.0058, pruning_loss=0.1312, mse_loss=0.4662
Epoch: [26]  [ 980/1251]  eta: 0:02:11  lr: 0.000017  loss: 3.6521 (3.4482)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [26]  [ 990/1251]  eta: 0:02:06  lr: 0.000017  loss: 3.6960 (3.4501)  time: 0.4569  data: 0.0005  max mem: 19734
Epoch: [26]  [1000/1251]  eta: 0:02:01  lr: 0.000017  loss: 3.6756 (3.4505)  time: 0.4561  data: 0.0004  max mem: 19734
Epoch: [26]  [1010/1251]  eta: 0:01:56  lr: 0.000017  loss: 3.5097 (3.4513)  time: 0.4552  data: 0.0007  max mem: 19734
Epoch: [26]  [1020/1251]  eta: 0:01:51  lr: 0.000017  loss: 3.6161 (3.4524)  time: 0.4557  data: 0.0007  max mem: 19734
Epoch: [26]  [1030/1251]  eta: 0:01:47  lr: 0.000017  loss: 3.5260 (3.4516)  time: 0.4673  data: 0.0005  max mem: 19734
Epoch: [26]  [1040/1251]  eta: 0:01:42  lr: 0.000017  loss: 3.5260 (3.4520)  time: 0.4797  data: 0.0005  max mem: 19734
Epoch: [26]  [1050/1251]  eta: 0:01:37  lr: 0.000017  loss: 3.5599 (3.4526)  time: 0.4833  data: 0.0004  max mem: 19734
Epoch: [26]  [1060/1251]  eta: 0:01:32  lr: 0.000017  loss: 3.4695 (3.4521)  time: 0.4996  data: 0.0007  max mem: 19734
Epoch: [26]  [1070/1251]  eta: 0:01:27  lr: 0.000017  loss: 3.5028 (3.4532)  time: 0.4827  data: 0.0008  max mem: 19734
loss info: cls_loss=3.4837, ratio_loss=0.0054, pruning_loss=0.1276, mse_loss=0.4760
Epoch: [26]  [1080/1251]  eta: 0:01:22  lr: 0.000017  loss: 3.6365 (3.4537)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [26]  [1090/1251]  eta: 0:01:17  lr: 0.000017  loss: 3.6998 (3.4563)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [26]  [1100/1251]  eta: 0:01:12  lr: 0.000017  loss: 3.6867 (3.4557)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [26]  [1110/1251]  eta: 0:01:08  lr: 0.000017  loss: 3.5867 (3.4550)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [26]  [1120/1251]  eta: 0:01:03  lr: 0.000017  loss: 3.5730 (3.4520)  time: 0.4572  data: 0.0009  max mem: 19734
Epoch: [26]  [1130/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.0966 (3.4502)  time: 0.4542  data: 0.0010  max mem: 19734
Epoch: [26]  [1140/1251]  eta: 0:00:53  lr: 0.000017  loss: 3.3901 (3.4490)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [26]  [1150/1251]  eta: 0:00:48  lr: 0.000017  loss: 3.2545 (3.4480)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [26]  [1160/1251]  eta: 0:00:43  lr: 0.000017  loss: 3.3198 (3.4474)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [26]  [1170/1251]  eta: 0:00:39  lr: 0.000017  loss: 3.4157 (3.4472)  time: 0.4553  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3485, ratio_loss=0.0052, pruning_loss=0.1313, mse_loss=0.4711
Epoch: [26]  [1180/1251]  eta: 0:00:34  lr: 0.000017  loss: 3.4157 (3.4460)  time: 0.4646  data: 0.0005  max mem: 19734
Epoch: [26]  [1190/1251]  eta: 0:00:29  lr: 0.000017  loss: 3.4360 (3.4456)  time: 0.4881  data: 0.0008  max mem: 19734
Epoch: [26]  [1200/1251]  eta: 0:00:24  lr: 0.000017  loss: 3.3823 (3.4420)  time: 0.4813  data: 0.0006  max mem: 19734
Epoch: [26]  [1210/1251]  eta: 0:00:19  lr: 0.000017  loss: 3.4139 (3.4422)  time: 0.4696  data: 0.0002  max mem: 19734
Epoch: [26]  [1220/1251]  eta: 0:00:14  lr: 0.000017  loss: 3.6883 (3.4449)  time: 0.4644  data: 0.0002  max mem: 19734
Epoch: [26]  [1230/1251]  eta: 0:00:10  lr: 0.000017  loss: 3.5517 (3.4427)  time: 0.4495  data: 0.0002  max mem: 19734
Epoch: [26]  [1240/1251]  eta: 0:00:05  lr: 0.000017  loss: 3.1191 (3.4407)  time: 0.4469  data: 0.0002  max mem: 19734
Epoch: [26]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.3917 (3.4408)  time: 0.4463  data: 0.0002  max mem: 19734
Epoch: [26] Total time: 0:10:02 (0.4813 s / it)
Averaged stats: lr: 0.000017  loss: 3.3917 (3.4322)
Test:  [  0/261]  eta: 1:20:13  loss: 0.7357 (0.7357)  acc1: 83.3333 (83.3333)  acc5: 96.3542 (96.3542)  time: 18.4436  data: 18.3750  max mem: 19734
Test:  [ 10/261]  eta: 0:11:02  loss: 0.6982 (0.7287)  acc1: 84.8958 (84.1383)  acc5: 96.3542 (96.2595)  time: 2.6413  data: 2.3933  max mem: 19734
Test:  [ 20/261]  eta: 0:05:52  loss: 0.9008 (0.9072)  acc1: 78.6458 (78.8442)  acc5: 93.7500 (94.6925)  time: 0.6116  data: 0.4002  max mem: 19734
Test:  [ 30/261]  eta: 0:03:56  loss: 0.8459 (0.8283)  acc1: 82.2917 (81.7540)  acc5: 94.2708 (95.0773)  time: 0.1345  data: 0.0079  max mem: 19734
Test:  [ 40/261]  eta: 0:03:33  loss: 0.5786 (0.7923)  acc1: 88.0208 (82.6728)  acc5: 96.3542 (95.3887)  time: 0.4472  data: 0.2994  max mem: 19734
Test:  [ 50/261]  eta: 0:02:49  loss: 0.9082 (0.8517)  acc1: 77.6042 (80.8824)  acc5: 94.7917 (94.9653)  time: 0.4672  data: 0.3004  max mem: 19734
Test:  [ 60/261]  eta: 0:02:20  loss: 0.9702 (0.8625)  acc1: 76.0417 (80.4559)  acc5: 93.7500 (94.9795)  time: 0.1539  data: 0.0090  max mem: 19734
Test:  [ 70/261]  eta: 0:02:20  loss: 0.9310 (0.8620)  acc1: 78.1250 (80.0836)  acc5: 95.8333 (95.1951)  time: 0.5479  data: 0.4064  max mem: 19734
Test:  [ 80/261]  eta: 0:02:04  loss: 0.8126 (0.8626)  acc1: 79.6875 (80.2083)  acc5: 96.8750 (95.3511)  time: 0.6619  data: 0.5085  max mem: 19734
Test:  [ 90/261]  eta: 0:01:48  loss: 0.7946 (0.8491)  acc1: 83.3333 (80.6490)  acc5: 96.3542 (95.4785)  time: 0.2756  data: 0.1131  max mem: 19734
Test:  [100/261]  eta: 0:01:35  loss: 0.8158 (0.8531)  acc1: 84.8958 (80.6415)  acc5: 96.3542 (95.5497)  time: 0.1867  data: 0.0367  max mem: 19734
Test:  [110/261]  eta: 0:01:28  loss: 0.8730 (0.8762)  acc1: 77.6042 (80.1896)  acc5: 94.7917 (95.2562)  time: 0.3820  data: 0.2525  max mem: 19734
Test:  [120/261]  eta: 0:01:17  loss: 1.2308 (0.9181)  acc1: 70.3125 (79.1925)  acc5: 90.1042 (94.6841)  time: 0.3506  data: 0.2293  max mem: 19734
Test:  [130/261]  eta: 0:01:08  loss: 1.4064 (0.9640)  acc1: 68.2292 (78.2761)  acc5: 86.4583 (94.0363)  time: 0.1522  data: 0.0106  max mem: 19734
Test:  [140/261]  eta: 0:01:08  loss: 1.3640 (0.9913)  acc1: 68.7500 (77.6153)  acc5: 89.0625 (93.7759)  time: 0.6408  data: 0.5256  max mem: 19734
Test:  [150/261]  eta: 0:00:58  loss: 1.2480 (0.9973)  acc1: 73.4375 (77.5904)  acc5: 91.6667 (93.6224)  time: 0.5938  data: 0.5205  max mem: 19734
Test:  [160/261]  eta: 0:00:50  loss: 1.0452 (1.0166)  acc1: 77.0833 (77.2192)  acc5: 91.6667 (93.3521)  time: 0.0639  data: 0.0013  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: 1.3043 (1.0459)  acc1: 66.1458 (76.4529)  acc5: 89.0625 (93.0464)  time: 0.0694  data: 0.0016  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4320 (1.0631)  acc1: 65.6250 (75.9985)  acc5: 89.0625 (92.8695)  time: 0.0703  data: 0.0017  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: 1.3558 (1.0756)  acc1: 66.6667 (75.7744)  acc5: 90.1042 (92.7002)  time: 0.0690  data: 0.0039  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.3322 (1.0902)  acc1: 71.8750 (75.4872)  acc5: 89.0625 (92.4699)  time: 0.0660  data: 0.0029  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.3491 (1.1032)  acc1: 71.8750 (75.2518)  acc5: 88.5417 (92.2690)  time: 0.0615  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.4371 (1.1222)  acc1: 68.2292 (74.7926)  acc5: 88.0208 (92.0673)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4649 (1.1320)  acc1: 66.6667 (74.5378)  acc5: 89.0625 (91.9575)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3432 (1.1414)  acc1: 66.6667 (74.2912)  acc5: 90.6250 (91.8914)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0608 (1.1341)  acc1: 75.5208 (74.4729)  acc5: 93.7500 (92.0173)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9400 (1.1347)  acc1: 77.0833 (74.4800)  acc5: 94.7917 (92.0700)  time: 0.0602  data: 0.0002  max mem: 19734
Test: Total time: 0:01:27 (0.3348 s / it)
* Acc@1 74.480 Acc@5 92.070 loss 1.135
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.48%
Epoch: [27]  [   0/1251]  eta: 5:17:12  lr: 0.000017  loss: 3.7555 (3.7555)  time: 15.2141  data: 14.7498  max mem: 19734
Epoch: [27]  [  10/1251]  eta: 0:40:03  lr: 0.000017  loss: 3.3255 (3.1412)  time: 1.9368  data: 1.3824  max mem: 19734
Epoch: [27]  [  20/1251]  eta: 0:25:21  lr: 0.000017  loss: 3.3582 (3.2777)  time: 0.5372  data: 0.0230  max mem: 19734
loss info: cls_loss=3.3210, ratio_loss=0.0054, pruning_loss=0.1328, mse_loss=0.4769
Epoch: [27]  [  30/1251]  eta: 0:20:02  lr: 0.000017  loss: 3.6972 (3.4449)  time: 0.4617  data: 0.0004  max mem: 19734
Epoch: [27]  [  40/1251]  eta: 0:17:23  lr: 0.000017  loss: 3.6187 (3.4353)  time: 0.4693  data: 0.0004  max mem: 19734
Epoch: [27]  [  50/1251]  eta: 0:15:41  lr: 0.000017  loss: 3.4561 (3.4268)  time: 0.4714  data: 0.0005  max mem: 19734
Epoch: [27]  [  60/1251]  eta: 0:14:30  lr: 0.000017  loss: 3.6626 (3.4302)  time: 0.4613  data: 0.0005  max mem: 19734
Epoch: [27]  [  70/1251]  eta: 0:13:45  lr: 0.000017  loss: 3.5255 (3.4330)  time: 0.4838  data: 0.0006  max mem: 19734
Epoch: [27]  [  80/1251]  eta: 0:13:12  lr: 0.000017  loss: 3.5727 (3.4611)  time: 0.5107  data: 0.0006  max mem: 19734
Epoch: [27]  [  90/1251]  eta: 0:12:43  lr: 0.000017  loss: 3.7237 (3.4877)  time: 0.5114  data: 0.0004  max mem: 19734
Epoch: [27]  [ 100/1251]  eta: 0:12:17  lr: 0.000017  loss: 3.7237 (3.5024)  time: 0.4953  data: 0.0004  max mem: 19734
Epoch: [27]  [ 110/1251]  eta: 0:11:52  lr: 0.000017  loss: 3.4974 (3.4964)  time: 0.4709  data: 0.0004  max mem: 19734
Epoch: [27]  [ 120/1251]  eta: 0:11:30  lr: 0.000017  loss: 3.4974 (3.4970)  time: 0.4604  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4866, ratio_loss=0.0053, pruning_loss=0.1296, mse_loss=0.4812
Epoch: [27]  [ 130/1251]  eta: 0:11:11  lr: 0.000017  loss: 3.3768 (3.4659)  time: 0.4588  data: 0.0004  max mem: 19734
Epoch: [27]  [ 140/1251]  eta: 0:10:54  lr: 0.000017  loss: 3.2572 (3.4564)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [27]  [ 150/1251]  eta: 0:10:38  lr: 0.000017  loss: 3.3595 (3.4497)  time: 0.4584  data: 0.0004  max mem: 19734
Epoch: [27]  [ 160/1251]  eta: 0:10:24  lr: 0.000017  loss: 3.3595 (3.4513)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [27]  [ 170/1251]  eta: 0:10:11  lr: 0.000017  loss: 3.2458 (3.4417)  time: 0.4587  data: 0.0005  max mem: 19734
Epoch: [27]  [ 180/1251]  eta: 0:10:01  lr: 0.000017  loss: 2.9848 (3.4154)  time: 0.4690  data: 0.0005  max mem: 19734
Epoch: [27]  [ 190/1251]  eta: 0:09:49  lr: 0.000017  loss: 2.9394 (3.4159)  time: 0.4676  data: 0.0005  max mem: 19734
Epoch: [27]  [ 200/1251]  eta: 0:09:39  lr: 0.000017  loss: 3.5658 (3.4248)  time: 0.4575  data: 0.0005  max mem: 19734
Epoch: [27]  [ 210/1251]  eta: 0:09:30  lr: 0.000017  loss: 3.5586 (3.4163)  time: 0.4760  data: 0.0005  max mem: 19734
Epoch: [27]  [ 220/1251]  eta: 0:09:21  lr: 0.000017  loss: 3.2417 (3.4055)  time: 0.4853  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2916, ratio_loss=0.0051, pruning_loss=0.1330, mse_loss=0.4774
Epoch: [27]  [ 230/1251]  eta: 0:09:13  lr: 0.000017  loss: 3.4289 (3.4110)  time: 0.4829  data: 0.0004  max mem: 19734
Epoch: [27]  [ 240/1251]  eta: 0:09:07  lr: 0.000017  loss: 3.5805 (3.4066)  time: 0.4985  data: 0.0005  max mem: 19734
Epoch: [27]  [ 250/1251]  eta: 0:08:58  lr: 0.000017  loss: 3.3942 (3.4064)  time: 0.4829  data: 0.0005  max mem: 19734
Epoch: [27]  [ 260/1251]  eta: 0:08:49  lr: 0.000017  loss: 3.3666 (3.4074)  time: 0.4578  data: 0.0005  max mem: 19734
Epoch: [27]  [ 270/1251]  eta: 0:08:41  lr: 0.000017  loss: 3.4727 (3.4056)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [27]  [ 280/1251]  eta: 0:08:33  lr: 0.000017  loss: 3.4219 (3.4006)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [27]  [ 290/1251]  eta: 0:08:25  lr: 0.000017  loss: 3.4219 (3.4089)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [27]  [ 300/1251]  eta: 0:08:18  lr: 0.000017  loss: 3.4434 (3.4114)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [27]  [ 310/1251]  eta: 0:08:10  lr: 0.000017  loss: 3.2869 (3.4040)  time: 0.4543  data: 0.0005  max mem: 19734
Epoch: [27]  [ 320/1251]  eta: 0:08:03  lr: 0.000017  loss: 3.4741 (3.4092)  time: 0.4548  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3814, ratio_loss=0.0056, pruning_loss=0.1310, mse_loss=0.4836
Epoch: [27]  [ 330/1251]  eta: 0:07:57  lr: 0.000017  loss: 3.5415 (3.4024)  time: 0.4643  data: 0.0005  max mem: 19734
Epoch: [27]  [ 340/1251]  eta: 0:07:50  lr: 0.000017  loss: 3.2950 (3.4001)  time: 0.4641  data: 0.0005  max mem: 19734
Epoch: [27]  [ 350/1251]  eta: 0:07:43  lr: 0.000017  loss: 3.3512 (3.3977)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [27]  [ 360/1251]  eta: 0:07:38  lr: 0.000017  loss: 3.5700 (3.4044)  time: 0.4764  data: 0.0005  max mem: 19734
Epoch: [27]  [ 370/1251]  eta: 0:07:32  lr: 0.000017  loss: 3.5162 (3.3991)  time: 0.4929  data: 0.0006  max mem: 19734
Epoch: [27]  [ 380/1251]  eta: 0:07:26  lr: 0.000017  loss: 3.4875 (3.4022)  time: 0.4790  data: 0.0005  max mem: 19734
Epoch: [27]  [ 390/1251]  eta: 0:07:20  lr: 0.000017  loss: 3.7041 (3.4068)  time: 0.4765  data: 0.0005  max mem: 19734
Epoch: [27]  [ 400/1251]  eta: 0:07:14  lr: 0.000017  loss: 3.2544 (3.3958)  time: 0.4676  data: 0.0005  max mem: 19734
Epoch: [27]  [ 410/1251]  eta: 0:07:07  lr: 0.000017  loss: 3.6096 (3.4045)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [27]  [ 420/1251]  eta: 0:07:01  lr: 0.000017  loss: 3.6028 (3.3977)  time: 0.4557  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3227, ratio_loss=0.0051, pruning_loss=0.1336, mse_loss=0.4801
Epoch: [27]  [ 430/1251]  eta: 0:06:55  lr: 0.000017  loss: 3.1745 (3.3957)  time: 0.4552  data: 0.0006  max mem: 19734
Epoch: [27]  [ 440/1251]  eta: 0:06:49  lr: 0.000017  loss: 3.2409 (3.3954)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [27]  [ 450/1251]  eta: 0:06:43  lr: 0.000017  loss: 3.1524 (3.3851)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [27]  [ 460/1251]  eta: 0:06:37  lr: 0.000017  loss: 3.4001 (3.3899)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [27]  [ 470/1251]  eta: 0:06:32  lr: 0.000017  loss: 3.6362 (3.3891)  time: 0.4604  data: 0.0004  max mem: 19734
Epoch: [27]  [ 480/1251]  eta: 0:06:26  lr: 0.000017  loss: 3.5681 (3.3889)  time: 0.4668  data: 0.0004  max mem: 19734
Epoch: [27]  [ 490/1251]  eta: 0:06:20  lr: 0.000017  loss: 3.3456 (3.3870)  time: 0.4644  data: 0.0004  max mem: 19734
Epoch: [27]  [ 500/1251]  eta: 0:06:15  lr: 0.000017  loss: 3.6012 (3.3908)  time: 0.4773  data: 0.0004  max mem: 19734
Epoch: [27]  [ 510/1251]  eta: 0:06:10  lr: 0.000017  loss: 3.6041 (3.3925)  time: 0.4976  data: 0.0004  max mem: 19734
Epoch: [27]  [ 520/1251]  eta: 0:06:05  lr: 0.000017  loss: 3.5707 (3.3932)  time: 0.4966  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3702, ratio_loss=0.0052, pruning_loss=0.1296, mse_loss=0.4745
Epoch: [27]  [ 530/1251]  eta: 0:06:00  lr: 0.000017  loss: 3.6117 (3.3980)  time: 0.4855  data: 0.0005  max mem: 19734
Epoch: [27]  [ 540/1251]  eta: 0:05:54  lr: 0.000017  loss: 3.5646 (3.3968)  time: 0.4656  data: 0.0004  max mem: 19734
Epoch: [27]  [ 550/1251]  eta: 0:05:49  lr: 0.000017  loss: 3.6015 (3.4011)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [27]  [ 560/1251]  eta: 0:05:43  lr: 0.000017  loss: 3.6685 (3.3997)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [27]  [ 570/1251]  eta: 0:05:38  lr: 0.000017  loss: 3.2475 (3.3935)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [27]  [ 580/1251]  eta: 0:05:32  lr: 0.000017  loss: 3.3301 (3.3975)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [27]  [ 590/1251]  eta: 0:05:27  lr: 0.000017  loss: 3.7723 (3.4019)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [27]  [ 600/1251]  eta: 0:05:22  lr: 0.000017  loss: 3.7723 (3.4081)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [27]  [ 610/1251]  eta: 0:05:16  lr: 0.000017  loss: 3.6803 (3.4093)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [27]  [ 620/1251]  eta: 0:05:11  lr: 0.000017  loss: 3.4123 (3.4095)  time: 0.4660  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4627, ratio_loss=0.0055, pruning_loss=0.1281, mse_loss=0.4735
Epoch: [27]  [ 630/1251]  eta: 0:05:06  lr: 0.000017  loss: 3.5408 (3.4128)  time: 0.4652  data: 0.0005  max mem: 19734
Epoch: [27]  [ 640/1251]  eta: 0:05:01  lr: 0.000017  loss: 3.5408 (3.4113)  time: 0.4650  data: 0.0004  max mem: 19734
Epoch: [27]  [ 650/1251]  eta: 0:04:56  lr: 0.000017  loss: 3.2160 (3.4083)  time: 0.4722  data: 0.0004  max mem: 19734
Epoch: [27]  [ 660/1251]  eta: 0:04:51  lr: 0.000017  loss: 3.3430 (3.4066)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [27]  [ 670/1251]  eta: 0:04:46  lr: 0.000017  loss: 3.4929 (3.4077)  time: 0.4928  data: 0.0004  max mem: 19734
Epoch: [27]  [ 680/1251]  eta: 0:04:41  lr: 0.000017  loss: 3.4929 (3.4079)  time: 0.4834  data: 0.0005  max mem: 19734
Epoch: [27]  [ 690/1251]  eta: 0:04:35  lr: 0.000017  loss: 3.1586 (3.4058)  time: 0.4622  data: 0.0005  max mem: 19734
Epoch: [27]  [ 700/1251]  eta: 0:04:30  lr: 0.000017  loss: 3.4234 (3.4055)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [27]  [ 710/1251]  eta: 0:04:25  lr: 0.000017  loss: 3.5402 (3.4098)  time: 0.4507  data: 0.0004  max mem: 19734
Epoch: [27]  [ 720/1251]  eta: 0:04:20  lr: 0.000017  loss: 3.6417 (3.4099)  time: 0.4513  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3918, ratio_loss=0.0051, pruning_loss=0.1302, mse_loss=0.4795
Epoch: [27]  [ 730/1251]  eta: 0:04:15  lr: 0.000017  loss: 3.5635 (3.4114)  time: 0.4500  data: 0.0004  max mem: 19734
Epoch: [27]  [ 740/1251]  eta: 0:04:09  lr: 0.000017  loss: 3.5635 (3.4101)  time: 0.4501  data: 0.0004  max mem: 19734
Epoch: [27]  [ 750/1251]  eta: 0:04:04  lr: 0.000017  loss: 3.3416 (3.4101)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [27]  [ 760/1251]  eta: 0:03:59  lr: 0.000017  loss: 3.3416 (3.4084)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [27]  [ 770/1251]  eta: 0:03:54  lr: 0.000017  loss: 3.3818 (3.4076)  time: 0.4630  data: 0.0006  max mem: 19734
Epoch: [27]  [ 780/1251]  eta: 0:03:49  lr: 0.000017  loss: 3.4699 (3.4079)  time: 0.4635  data: 0.0005  max mem: 19734
Epoch: [27]  [ 790/1251]  eta: 0:03:44  lr: 0.000017  loss: 3.5412 (3.4085)  time: 0.4761  data: 0.0004  max mem: 19734
Epoch: [27]  [ 800/1251]  eta: 0:03:39  lr: 0.000017  loss: 3.6036 (3.4104)  time: 0.4943  data: 0.0004  max mem: 19734
Epoch: [27]  [ 810/1251]  eta: 0:03:34  lr: 0.000017  loss: 3.6036 (3.4107)  time: 0.4885  data: 0.0004  max mem: 19734
Epoch: [27]  [ 820/1251]  eta: 0:03:30  lr: 0.000017  loss: 3.3710 (3.4066)  time: 0.4786  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3622, ratio_loss=0.0051, pruning_loss=0.1309, mse_loss=0.4748
Epoch: [27]  [ 830/1251]  eta: 0:03:24  lr: 0.000017  loss: 3.2820 (3.4068)  time: 0.4640  data: 0.0005  max mem: 19734
Epoch: [27]  [ 840/1251]  eta: 0:03:19  lr: 0.000017  loss: 3.4125 (3.4061)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [27]  [ 850/1251]  eta: 0:03:14  lr: 0.000017  loss: 3.4125 (3.4052)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [27]  [ 860/1251]  eta: 0:03:09  lr: 0.000017  loss: 3.4816 (3.4049)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [27]  [ 870/1251]  eta: 0:03:04  lr: 0.000017  loss: 3.3448 (3.4025)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [27]  [ 880/1251]  eta: 0:03:00  lr: 0.000017  loss: 3.0948 (3.4008)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [27]  [ 890/1251]  eta: 0:02:55  lr: 0.000017  loss: 3.4156 (3.4022)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [27]  [ 900/1251]  eta: 0:02:50  lr: 0.000017  loss: 3.4795 (3.4013)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [27]  [ 910/1251]  eta: 0:02:45  lr: 0.000017  loss: 3.5920 (3.4033)  time: 0.4595  data: 0.0004  max mem: 19734
Epoch: [27]  [ 920/1251]  eta: 0:02:40  lr: 0.000017  loss: 3.6945 (3.4071)  time: 0.4703  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3817, ratio_loss=0.0052, pruning_loss=0.1311, mse_loss=0.4883
Epoch: [27]  [ 930/1251]  eta: 0:02:35  lr: 0.000017  loss: 3.8139 (3.4117)  time: 0.4751  data: 0.0005  max mem: 19734
Epoch: [27]  [ 940/1251]  eta: 0:02:30  lr: 0.000017  loss: 3.7184 (3.4138)  time: 0.4761  data: 0.0004  max mem: 19734
Epoch: [27]  [ 950/1251]  eta: 0:02:25  lr: 0.000017  loss: 3.6047 (3.4140)  time: 0.4849  data: 0.0004  max mem: 19734
Epoch: [27]  [ 960/1251]  eta: 0:02:20  lr: 0.000017  loss: 3.4372 (3.4141)  time: 0.4919  data: 0.0004  max mem: 19734
Epoch: [27]  [ 970/1251]  eta: 0:02:16  lr: 0.000017  loss: 3.5369 (3.4145)  time: 0.4849  data: 0.0004  max mem: 19734
Epoch: [27]  [ 980/1251]  eta: 0:02:11  lr: 0.000017  loss: 3.5886 (3.4158)  time: 0.4694  data: 0.0004  max mem: 19734
Epoch: [27]  [ 990/1251]  eta: 0:02:06  lr: 0.000017  loss: 3.8316 (3.4188)  time: 0.4608  data: 0.0005  max mem: 19734
Epoch: [27]  [1000/1251]  eta: 0:02:01  lr: 0.000017  loss: 3.7694 (3.4206)  time: 0.4585  data: 0.0005  max mem: 19734
Epoch: [27]  [1010/1251]  eta: 0:01:56  lr: 0.000017  loss: 3.5452 (3.4194)  time: 0.4602  data: 0.0005  max mem: 19734
Epoch: [27]  [1020/1251]  eta: 0:01:51  lr: 0.000017  loss: 3.3091 (3.4182)  time: 0.4592  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4766, ratio_loss=0.0057, pruning_loss=0.1301, mse_loss=0.4681
Epoch: [27]  [1030/1251]  eta: 0:01:46  lr: 0.000017  loss: 3.3756 (3.4163)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [27]  [1040/1251]  eta: 0:01:41  lr: 0.000017  loss: 3.5696 (3.4179)  time: 0.4577  data: 0.0004  max mem: 19734
Epoch: [27]  [1050/1251]  eta: 0:01:36  lr: 0.000017  loss: 3.3984 (3.4178)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [27]  [1060/1251]  eta: 0:01:32  lr: 0.000017  loss: 3.5011 (3.4208)  time: 0.4684  data: 0.0004  max mem: 19734
Epoch: [27]  [1070/1251]  eta: 0:01:27  lr: 0.000017  loss: 3.6592 (3.4195)  time: 0.4721  data: 0.0006  max mem: 19734
Epoch: [27]  [1080/1251]  eta: 0:01:22  lr: 0.000017  loss: 3.5388 (3.4215)  time: 0.4846  data: 0.0005  max mem: 19734
Epoch: [27]  [1090/1251]  eta: 0:01:17  lr: 0.000017  loss: 3.6342 (3.4223)  time: 0.5004  data: 0.0004  max mem: 19734
Epoch: [27]  [1100/1251]  eta: 0:01:12  lr: 0.000017  loss: 3.5678 (3.4238)  time: 0.4870  data: 0.0004  max mem: 19734
Epoch: [27]  [1110/1251]  eta: 0:01:08  lr: 0.000017  loss: 3.5378 (3.4220)  time: 0.4882  data: 0.0004  max mem: 19734
Epoch: [27]  [1120/1251]  eta: 0:01:03  lr: 0.000017  loss: 3.2626 (3.4204)  time: 0.4756  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4250, ratio_loss=0.0053, pruning_loss=0.1302, mse_loss=0.4775
Epoch: [27]  [1130/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.4667 (3.4208)  time: 0.4548  data: 0.0008  max mem: 19734
Epoch: [27]  [1140/1251]  eta: 0:00:53  lr: 0.000017  loss: 3.5031 (3.4212)  time: 0.4552  data: 0.0007  max mem: 19734
Epoch: [27]  [1150/1251]  eta: 0:00:48  lr: 0.000017  loss: 3.5031 (3.4218)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [27]  [1160/1251]  eta: 0:00:43  lr: 0.000017  loss: 3.4344 (3.4217)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [27]  [1170/1251]  eta: 0:00:38  lr: 0.000017  loss: 3.5446 (3.4233)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [27]  [1180/1251]  eta: 0:00:34  lr: 0.000017  loss: 3.5886 (3.4248)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [27]  [1190/1251]  eta: 0:00:29  lr: 0.000017  loss: 3.6662 (3.4268)  time: 0.4540  data: 0.0008  max mem: 19734
Epoch: [27]  [1200/1251]  eta: 0:00:24  lr: 0.000017  loss: 3.5734 (3.4266)  time: 0.4514  data: 0.0007  max mem: 19734
Epoch: [27]  [1210/1251]  eta: 0:00:19  lr: 0.000017  loss: 3.2224 (3.4230)  time: 0.4574  data: 0.0002  max mem: 19734
Epoch: [27]  [1220/1251]  eta: 0:00:14  lr: 0.000017  loss: 3.2561 (3.4231)  time: 0.4617  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4438, ratio_loss=0.0051, pruning_loss=0.1280, mse_loss=0.4645
Epoch: [27]  [1230/1251]  eta: 0:00:10  lr: 0.000017  loss: 3.6782 (3.4242)  time: 0.4595  data: 0.0002  max mem: 19734
Epoch: [27]  [1240/1251]  eta: 0:00:05  lr: 0.000017  loss: 3.6415 (3.4229)  time: 0.4677  data: 0.0002  max mem: 19734
Epoch: [27]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.1348 (3.4217)  time: 0.4756  data: 0.0002  max mem: 19734
Epoch: [27] Total time: 0:10:01 (0.4805 s / it)
Averaged stats: lr: 0.000017  loss: 3.1348 (3.4282)
Test:  [  0/261]  eta: 2:04:07  loss: 0.7467 (0.7467)  acc1: 82.2917 (82.2917)  acc5: 95.3125 (95.3125)  time: 28.5334  data: 28.3074  max mem: 19734
Test:  [ 10/261]  eta: 0:11:48  loss: 0.7376 (0.7362)  acc1: 84.3750 (83.9489)  acc5: 96.3542 (95.9280)  time: 2.8226  data: 2.6918  max mem: 19734
Test:  [ 20/261]  eta: 0:06:48  loss: 0.9542 (0.8995)  acc1: 78.6458 (79.3651)  acc5: 93.7500 (94.7421)  time: 0.3525  data: 0.2062  max mem: 19734
Test:  [ 30/261]  eta: 0:04:39  loss: 0.8288 (0.8184)  acc1: 82.8125 (82.3589)  acc5: 93.7500 (95.1277)  time: 0.3220  data: 0.1500  max mem: 19734
Test:  [ 40/261]  eta: 0:03:53  loss: 0.5639 (0.7850)  acc1: 89.5833 (83.1809)  acc5: 96.8750 (95.4268)  time: 0.3893  data: 0.1908  max mem: 19734
Test:  [ 50/261]  eta: 0:03:10  loss: 0.9515 (0.8502)  acc1: 77.6042 (81.1479)  acc5: 94.2708 (94.9959)  time: 0.4220  data: 0.2252  max mem: 19734
Test:  [ 60/261]  eta: 0:02:37  loss: 0.9925 (0.8595)  acc1: 76.0417 (80.5413)  acc5: 94.2708 (95.0137)  time: 0.2272  data: 0.0542  max mem: 19734
Test:  [ 70/261]  eta: 0:02:23  loss: 0.9459 (0.8604)  acc1: 77.6042 (80.1717)  acc5: 95.8333 (95.1951)  time: 0.3636  data: 0.2037  max mem: 19734
Test:  [ 80/261]  eta: 0:02:04  loss: 0.8240 (0.8593)  acc1: 79.1667 (80.3112)  acc5: 96.3542 (95.3189)  time: 0.3905  data: 0.2124  max mem: 19734
Test:  [ 90/261]  eta: 0:01:49  loss: 0.8240 (0.8477)  acc1: 82.8125 (80.6319)  acc5: 95.8333 (95.3926)  time: 0.2428  data: 0.0276  max mem: 19734
Test:  [100/261]  eta: 0:01:48  loss: 0.8310 (0.8513)  acc1: 83.3333 (80.5435)  acc5: 95.3125 (95.4414)  time: 0.6267  data: 0.3978  max mem: 19734
Test:  [110/261]  eta: 0:01:35  loss: 0.8790 (0.8748)  acc1: 74.4792 (79.9690)  acc5: 94.2708 (95.1436)  time: 0.6100  data: 0.3981  max mem: 19734
Test:  [120/261]  eta: 0:01:24  loss: 1.2246 (0.9157)  acc1: 70.3125 (79.0591)  acc5: 89.0625 (94.5721)  time: 0.1938  data: 0.0143  max mem: 19734
Test:  [130/261]  eta: 0:01:14  loss: 1.3715 (0.9598)  acc1: 68.7500 (78.1528)  acc5: 85.9375 (94.0204)  time: 0.1899  data: 0.0214  max mem: 19734
Test:  [140/261]  eta: 0:01:08  loss: 1.3106 (0.9869)  acc1: 69.2708 (77.4675)  acc5: 89.5833 (93.7352)  time: 0.4037  data: 0.2554  max mem: 19734
Test:  [150/261]  eta: 0:01:00  loss: 1.2426 (0.9929)  acc1: 71.8750 (77.4179)  acc5: 91.1458 (93.5879)  time: 0.4223  data: 0.2440  max mem: 19734
Test:  [160/261]  eta: 0:00:52  loss: 1.0018 (1.0113)  acc1: 77.6042 (77.1027)  acc5: 91.1458 (93.3359)  time: 0.2041  data: 0.0094  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.3229 (1.0421)  acc1: 65.1042 (76.3463)  acc5: 88.5417 (92.9977)  time: 0.1925  data: 0.0229  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4608 (1.0597)  acc1: 65.1042 (75.9467)  acc5: 88.5417 (92.8263)  time: 0.2111  data: 0.0750  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: 1.3999 (1.0723)  acc1: 67.1875 (75.7226)  acc5: 90.6250 (92.6838)  time: 0.1532  data: 0.0670  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3978 (1.0884)  acc1: 70.3125 (75.3602)  acc5: 89.0625 (92.4363)  time: 0.0882  data: 0.0213  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.4132 (1.1027)  acc1: 69.7917 (75.0987)  acc5: 87.5000 (92.2393)  time: 0.0772  data: 0.0155  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: 1.4495 (1.1229)  acc1: 66.6667 (74.5640)  acc5: 88.0208 (92.0343)  time: 0.0701  data: 0.0086  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4369 (1.1321)  acc1: 65.6250 (74.3304)  acc5: 89.0625 (91.9372)  time: 0.0667  data: 0.0052  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3018 (1.1408)  acc1: 69.2708 (74.1464)  acc5: 91.1458 (91.8741)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0425 (1.1331)  acc1: 75.5208 (74.3526)  acc5: 93.7500 (92.0007)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9387 (1.1336)  acc1: 78.1250 (74.3460)  acc5: 95.3125 (92.0520)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3616 s / it)
* Acc@1 74.346 Acc@5 92.052 loss 1.134
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.48%
Epoch: [28]  [   0/1251]  eta: 5:15:25  lr: 0.000017  loss: 4.1450 (4.1450)  time: 15.1281  data: 14.3476  max mem: 19734
Epoch: [28]  [  10/1251]  eta: 0:42:12  lr: 0.000017  loss: 3.8449 (3.6320)  time: 2.0408  data: 1.3073  max mem: 19734
Epoch: [28]  [  20/1251]  eta: 0:26:23  lr: 0.000017  loss: 3.3827 (3.3603)  time: 0.5944  data: 0.0019  max mem: 19734
Epoch: [28]  [  30/1251]  eta: 0:20:44  lr: 0.000017  loss: 3.2009 (3.3780)  time: 0.4578  data: 0.0005  max mem: 19734
Epoch: [28]  [  40/1251]  eta: 0:17:49  lr: 0.000017  loss: 3.2210 (3.3855)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [28]  [  50/1251]  eta: 0:16:00  lr: 0.000017  loss: 3.5961 (3.3428)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [28]  [  60/1251]  eta: 0:14:46  lr: 0.000017  loss: 3.5110 (3.3893)  time: 0.4595  data: 0.0004  max mem: 19734
Epoch: [28]  [  70/1251]  eta: 0:13:54  lr: 0.000017  loss: 3.5110 (3.3956)  time: 0.4676  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3379, ratio_loss=0.0051, pruning_loss=0.1306, mse_loss=0.4703
Epoch: [28]  [  80/1251]  eta: 0:13:11  lr: 0.000017  loss: 3.4063 (3.3668)  time: 0.4688  data: 0.0004  max mem: 19734
Epoch: [28]  [  90/1251]  eta: 0:12:37  lr: 0.000017  loss: 3.4896 (3.4078)  time: 0.4614  data: 0.0005  max mem: 19734
Epoch: [28]  [ 100/1251]  eta: 0:12:08  lr: 0.000017  loss: 3.7249 (3.4321)  time: 0.4594  data: 0.0005  max mem: 19734
Epoch: [28]  [ 110/1251]  eta: 0:11:46  lr: 0.000017  loss: 3.7881 (3.4614)  time: 0.4695  data: 0.0005  max mem: 19734
Epoch: [28]  [ 120/1251]  eta: 0:11:29  lr: 0.000017  loss: 3.6709 (3.4576)  time: 0.4889  data: 0.0005  max mem: 19734
Epoch: [28]  [ 130/1251]  eta: 0:11:12  lr: 0.000017  loss: 3.6709 (3.4708)  time: 0.4907  data: 0.0004  max mem: 19734
Epoch: [28]  [ 140/1251]  eta: 0:10:57  lr: 0.000017  loss: 3.7129 (3.4805)  time: 0.4849  data: 0.0004  max mem: 19734
Epoch: [28]  [ 150/1251]  eta: 0:10:42  lr: 0.000017  loss: 3.6269 (3.4788)  time: 0.4777  data: 0.0004  max mem: 19734
Epoch: [28]  [ 160/1251]  eta: 0:10:28  lr: 0.000017  loss: 3.7546 (3.4989)  time: 0.4656  data: 0.0004  max mem: 19734
Epoch: [28]  [ 170/1251]  eta: 0:10:15  lr: 0.000017  loss: 3.6097 (3.5012)  time: 0.4601  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5638, ratio_loss=0.0054, pruning_loss=0.1251, mse_loss=0.4587
Epoch: [28]  [ 180/1251]  eta: 0:10:03  lr: 0.000017  loss: 3.6097 (3.5110)  time: 0.4604  data: 0.0004  max mem: 19734
Epoch: [28]  [ 190/1251]  eta: 0:09:51  lr: 0.000017  loss: 3.7600 (3.5090)  time: 0.4589  data: 0.0005  max mem: 19734
Epoch: [28]  [ 200/1251]  eta: 0:09:40  lr: 0.000017  loss: 3.4233 (3.5052)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [28]  [ 210/1251]  eta: 0:09:30  lr: 0.000017  loss: 3.3191 (3.4979)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [28]  [ 220/1251]  eta: 0:09:21  lr: 0.000017  loss: 3.5776 (3.5147)  time: 0.4678  data: 0.0004  max mem: 19734
Epoch: [28]  [ 230/1251]  eta: 0:09:12  lr: 0.000017  loss: 3.8215 (3.5232)  time: 0.4663  data: 0.0007  max mem: 19734
Epoch: [28]  [ 240/1251]  eta: 0:09:03  lr: 0.000017  loss: 3.7684 (3.5199)  time: 0.4578  data: 0.0007  max mem: 19734
Epoch: [28]  [ 250/1251]  eta: 0:08:55  lr: 0.000017  loss: 3.4492 (3.5072)  time: 0.4652  data: 0.0004  max mem: 19734
Epoch: [28]  [ 260/1251]  eta: 0:08:48  lr: 0.000017  loss: 3.0726 (3.4921)  time: 0.4832  data: 0.0004  max mem: 19734
Epoch: [28]  [ 270/1251]  eta: 0:08:41  lr: 0.000017  loss: 3.1710 (3.4895)  time: 0.4858  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4351, ratio_loss=0.0051, pruning_loss=0.1302, mse_loss=0.4730
Epoch: [28]  [ 280/1251]  eta: 0:08:34  lr: 0.000017  loss: 3.3619 (3.4898)  time: 0.4848  data: 0.0004  max mem: 19734
Epoch: [28]  [ 290/1251]  eta: 0:08:27  lr: 0.000017  loss: 3.3093 (3.4831)  time: 0.4842  data: 0.0005  max mem: 19734
Epoch: [28]  [ 300/1251]  eta: 0:08:20  lr: 0.000017  loss: 3.4067 (3.4825)  time: 0.4652  data: 0.0004  max mem: 19734
Epoch: [28]  [ 310/1251]  eta: 0:08:12  lr: 0.000017  loss: 3.5639 (3.4807)  time: 0.4551  data: 0.0006  max mem: 19734
Epoch: [28]  [ 320/1251]  eta: 0:08:05  lr: 0.000017  loss: 3.3464 (3.4765)  time: 0.4553  data: 0.0006  max mem: 19734
Epoch: [28]  [ 330/1251]  eta: 0:07:58  lr: 0.000017  loss: 3.1108 (3.4650)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [28]  [ 340/1251]  eta: 0:07:51  lr: 0.000017  loss: 3.3388 (3.4670)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [28]  [ 350/1251]  eta: 0:07:44  lr: 0.000017  loss: 3.3069 (3.4562)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [28]  [ 360/1251]  eta: 0:07:38  lr: 0.000017  loss: 3.3069 (3.4564)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [28]  [ 370/1251]  eta: 0:07:31  lr: 0.000017  loss: 3.1158 (3.4432)  time: 0.4643  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3000, ratio_loss=0.0053, pruning_loss=0.1323, mse_loss=0.4741
Epoch: [28]  [ 380/1251]  eta: 0:07:25  lr: 0.000017  loss: 3.0150 (3.4375)  time: 0.4667  data: 0.0004  max mem: 19734
Epoch: [28]  [ 390/1251]  eta: 0:07:19  lr: 0.000017  loss: 3.3255 (3.4382)  time: 0.4607  data: 0.0004  max mem: 19734
Epoch: [28]  [ 400/1251]  eta: 0:07:13  lr: 0.000017  loss: 3.4312 (3.4408)  time: 0.4669  data: 0.0004  max mem: 19734
Epoch: [28]  [ 410/1251]  eta: 0:07:08  lr: 0.000017  loss: 3.5370 (3.4431)  time: 0.4867  data: 0.0003  max mem: 19734
Epoch: [28]  [ 420/1251]  eta: 0:07:02  lr: 0.000017  loss: 3.4820 (3.4416)  time: 0.4866  data: 0.0003  max mem: 19734
Epoch: [28]  [ 430/1251]  eta: 0:06:56  lr: 0.000017  loss: 3.3660 (3.4387)  time: 0.4721  data: 0.0004  max mem: 19734
Epoch: [28]  [ 440/1251]  eta: 0:06:50  lr: 0.000017  loss: 3.5215 (3.4421)  time: 0.4674  data: 0.0004  max mem: 19734
Epoch: [28]  [ 450/1251]  eta: 0:06:44  lr: 0.000017  loss: 3.6520 (3.4457)  time: 0.4603  data: 0.0004  max mem: 19734
Epoch: [28]  [ 460/1251]  eta: 0:06:38  lr: 0.000017  loss: 3.6383 (3.4457)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [28]  [ 470/1251]  eta: 0:06:32  lr: 0.000017  loss: 3.5448 (3.4455)  time: 0.4545  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4274, ratio_loss=0.0051, pruning_loss=0.1291, mse_loss=0.4704
Epoch: [28]  [ 480/1251]  eta: 0:06:27  lr: 0.000017  loss: 3.5357 (3.4428)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [28]  [ 490/1251]  eta: 0:06:21  lr: 0.000017  loss: 3.5772 (3.4465)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [28]  [ 500/1251]  eta: 0:06:15  lr: 0.000017  loss: 3.5772 (3.4434)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [28]  [ 510/1251]  eta: 0:06:10  lr: 0.000017  loss: 3.2272 (3.4373)  time: 0.4573  data: 0.0004  max mem: 19734
Epoch: [28]  [ 520/1251]  eta: 0:06:04  lr: 0.000017  loss: 3.4467 (3.4386)  time: 0.4668  data: 0.0005  max mem: 19734
Epoch: [28]  [ 530/1251]  eta: 0:05:59  lr: 0.000017  loss: 3.6299 (3.4383)  time: 0.4645  data: 0.0005  max mem: 19734
Epoch: [28]  [ 540/1251]  eta: 0:05:53  lr: 0.000017  loss: 3.7419 (3.4413)  time: 0.4631  data: 0.0004  max mem: 19734
Epoch: [28]  [ 550/1251]  eta: 0:05:48  lr: 0.000017  loss: 3.6684 (3.4432)  time: 0.4878  data: 0.0004  max mem: 19734
Epoch: [28]  [ 560/1251]  eta: 0:05:43  lr: 0.000017  loss: 3.7126 (3.4437)  time: 0.4882  data: 0.0004  max mem: 19734
Epoch: [28]  [ 570/1251]  eta: 0:05:38  lr: 0.000017  loss: 3.6932 (3.4463)  time: 0.4724  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4286, ratio_loss=0.0052, pruning_loss=0.1293, mse_loss=0.4597
Epoch: [28]  [ 580/1251]  eta: 0:05:33  lr: 0.000017  loss: 3.6932 (3.4522)  time: 0.4715  data: 0.0005  max mem: 19734
Epoch: [28]  [ 590/1251]  eta: 0:05:27  lr: 0.000017  loss: 3.6932 (3.4509)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [28]  [ 600/1251]  eta: 0:05:22  lr: 0.000017  loss: 3.7503 (3.4566)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [28]  [ 610/1251]  eta: 0:05:17  lr: 0.000017  loss: 3.7005 (3.4551)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [28]  [ 620/1251]  eta: 0:05:11  lr: 0.000017  loss: 3.2505 (3.4476)  time: 0.4623  data: 0.0004  max mem: 19734
Epoch: [28]  [ 630/1251]  eta: 0:05:06  lr: 0.000017  loss: 3.5598 (3.4506)  time: 0.4627  data: 0.0005  max mem: 19734
Epoch: [28]  [ 640/1251]  eta: 0:05:01  lr: 0.000017  loss: 3.5598 (3.4452)  time: 0.4631  data: 0.0005  max mem: 19734
Epoch: [28]  [ 650/1251]  eta: 0:04:56  lr: 0.000017  loss: 3.0741 (3.4434)  time: 0.4602  data: 0.0005  max mem: 19734
Epoch: [28]  [ 660/1251]  eta: 0:04:50  lr: 0.000017  loss: 3.5715 (3.4402)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [28]  [ 670/1251]  eta: 0:04:45  lr: 0.000017  loss: 3.5715 (3.4413)  time: 0.4689  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3853, ratio_loss=0.0048, pruning_loss=0.1297, mse_loss=0.4835
Epoch: [28]  [ 680/1251]  eta: 0:04:40  lr: 0.000017  loss: 3.6666 (3.4401)  time: 0.4667  data: 0.0005  max mem: 19734
Epoch: [28]  [ 690/1251]  eta: 0:04:35  lr: 0.000017  loss: 3.0132 (3.4334)  time: 0.4687  data: 0.0005  max mem: 19734
Epoch: [28]  [ 700/1251]  eta: 0:04:30  lr: 0.000017  loss: 3.3237 (3.4350)  time: 0.4711  data: 0.0005  max mem: 19734
Epoch: [28]  [ 710/1251]  eta: 0:04:25  lr: 0.000017  loss: 3.4556 (3.4322)  time: 0.4639  data: 0.0005  max mem: 19734
Epoch: [28]  [ 720/1251]  eta: 0:04:20  lr: 0.000017  loss: 3.6262 (3.4351)  time: 0.4684  data: 0.0005  max mem: 19734
Epoch: [28]  [ 730/1251]  eta: 0:04:15  lr: 0.000017  loss: 3.7664 (3.4391)  time: 0.4745  data: 0.0005  max mem: 19734
Epoch: [28]  [ 740/1251]  eta: 0:04:10  lr: 0.000017  loss: 3.4691 (3.4317)  time: 0.4711  data: 0.0005  max mem: 19734
Epoch: [28]  [ 750/1251]  eta: 0:04:05  lr: 0.000017  loss: 2.9735 (3.4303)  time: 0.4566  data: 0.0006  max mem: 19734
Epoch: [28]  [ 760/1251]  eta: 0:03:59  lr: 0.000017  loss: 3.2299 (3.4301)  time: 0.4530  data: 0.0007  max mem: 19734
Epoch: [28]  [ 770/1251]  eta: 0:03:54  lr: 0.000017  loss: 3.5138 (3.4333)  time: 0.4539  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3547, ratio_loss=0.0054, pruning_loss=0.1297, mse_loss=0.4624
Epoch: [28]  [ 780/1251]  eta: 0:03:49  lr: 0.000017  loss: 3.7838 (3.4332)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [28]  [ 790/1251]  eta: 0:03:44  lr: 0.000017  loss: 3.2767 (3.4320)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [28]  [ 800/1251]  eta: 0:03:39  lr: 0.000017  loss: 3.4667 (3.4334)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [28]  [ 810/1251]  eta: 0:03:34  lr: 0.000017  loss: 3.5171 (3.4338)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [28]  [ 820/1251]  eta: 0:03:29  lr: 0.000017  loss: 3.5171 (3.4339)  time: 0.4671  data: 0.0005  max mem: 19734
Epoch: [28]  [ 830/1251]  eta: 0:03:24  lr: 0.000017  loss: 3.3293 (3.4296)  time: 0.4803  data: 0.0006  max mem: 19734
Epoch: [28]  [ 840/1251]  eta: 0:03:19  lr: 0.000017  loss: 3.3293 (3.4270)  time: 0.4869  data: 0.0005  max mem: 19734
Epoch: [28]  [ 850/1251]  eta: 0:03:14  lr: 0.000017  loss: 3.4138 (3.4270)  time: 0.4696  data: 0.0004  max mem: 19734
Epoch: [28]  [ 860/1251]  eta: 0:03:10  lr: 0.000017  loss: 3.3864 (3.4226)  time: 0.4675  data: 0.0005  max mem: 19734
Epoch: [28]  [ 870/1251]  eta: 0:03:05  lr: 0.000017  loss: 2.9800 (3.4177)  time: 0.4750  data: 0.0007  max mem: 19734
loss info: cls_loss=3.2679, ratio_loss=0.0054, pruning_loss=0.1309, mse_loss=0.4708
Epoch: [28]  [ 880/1251]  eta: 0:03:00  lr: 0.000017  loss: 3.5117 (3.4198)  time: 0.4583  data: 0.0007  max mem: 19734
Epoch: [28]  [ 890/1251]  eta: 0:02:55  lr: 0.000017  loss: 3.6712 (3.4182)  time: 0.4523  data: 0.0008  max mem: 19734
Epoch: [28]  [ 900/1251]  eta: 0:02:50  lr: 0.000017  loss: 3.4883 (3.4195)  time: 0.4522  data: 0.0012  max mem: 19734
Epoch: [28]  [ 910/1251]  eta: 0:02:45  lr: 0.000017  loss: 3.4952 (3.4190)  time: 0.4543  data: 0.0008  max mem: 19734
Epoch: [28]  [ 920/1251]  eta: 0:02:40  lr: 0.000017  loss: 3.4445 (3.4200)  time: 0.4599  data: 0.0004  max mem: 19734
Epoch: [28]  [ 930/1251]  eta: 0:02:35  lr: 0.000017  loss: 3.5978 (3.4221)  time: 0.4610  data: 0.0005  max mem: 19734
Epoch: [28]  [ 940/1251]  eta: 0:02:30  lr: 0.000017  loss: 3.6000 (3.4211)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [28]  [ 950/1251]  eta: 0:02:25  lr: 0.000017  loss: 3.0479 (3.4172)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [28]  [ 960/1251]  eta: 0:02:20  lr: 0.000017  loss: 3.0682 (3.4159)  time: 0.4623  data: 0.0004  max mem: 19734
Epoch: [28]  [ 970/1251]  eta: 0:02:15  lr: 0.000017  loss: 3.5021 (3.4144)  time: 0.4682  data: 0.0003  max mem: 19734
loss info: cls_loss=3.3581, ratio_loss=0.0052, pruning_loss=0.1303, mse_loss=0.4697
Epoch: [28]  [ 980/1251]  eta: 0:02:10  lr: 0.000017  loss: 3.4401 (3.4129)  time: 0.4761  data: 0.0004  max mem: 19734
Epoch: [28]  [ 990/1251]  eta: 0:02:06  lr: 0.000017  loss: 3.4596 (3.4132)  time: 0.4807  data: 0.0004  max mem: 19734
Epoch: [28]  [1000/1251]  eta: 0:02:01  lr: 0.000017  loss: 3.4913 (3.4133)  time: 0.4828  data: 0.0005  max mem: 19734
Epoch: [28]  [1010/1251]  eta: 0:01:56  lr: 0.000017  loss: 3.5806 (3.4139)  time: 0.4721  data: 0.0007  max mem: 19734
Epoch: [28]  [1020/1251]  eta: 0:01:51  lr: 0.000017  loss: 3.7758 (3.4172)  time: 0.4684  data: 0.0006  max mem: 19734
Epoch: [28]  [1030/1251]  eta: 0:01:46  lr: 0.000017  loss: 3.4494 (3.4156)  time: 0.4684  data: 0.0005  max mem: 19734
Epoch: [28]  [1040/1251]  eta: 0:01:41  lr: 0.000017  loss: 3.4494 (3.4170)  time: 0.4532  data: 0.0006  max mem: 19734
Epoch: [28]  [1050/1251]  eta: 0:01:36  lr: 0.000017  loss: 3.4718 (3.4151)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [28]  [1060/1251]  eta: 0:01:31  lr: 0.000017  loss: 3.2728 (3.4145)  time: 0.4502  data: 0.0004  max mem: 19734
Epoch: [28]  [1070/1251]  eta: 0:01:27  lr: 0.000017  loss: 3.3143 (3.4138)  time: 0.4499  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3777, ratio_loss=0.0049, pruning_loss=0.1289, mse_loss=0.4606
Epoch: [28]  [1080/1251]  eta: 0:01:22  lr: 0.000017  loss: 3.1845 (3.4136)  time: 0.4510  data: 0.0004  max mem: 19734
Epoch: [28]  [1090/1251]  eta: 0:01:17  lr: 0.000017  loss: 3.2360 (3.4116)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [28]  [1100/1251]  eta: 0:01:12  lr: 0.000017  loss: 3.3046 (3.4114)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [28]  [1110/1251]  eta: 0:01:07  lr: 0.000017  loss: 3.4085 (3.4107)  time: 0.4618  data: 0.0004  max mem: 19734
Epoch: [28]  [1120/1251]  eta: 0:01:02  lr: 0.000017  loss: 3.4558 (3.4100)  time: 0.4762  data: 0.0004  max mem: 19734
Epoch: [28]  [1130/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.5550 (3.4117)  time: 0.4904  data: 0.0005  max mem: 19734
Epoch: [28]  [1140/1251]  eta: 0:00:53  lr: 0.000017  loss: 3.5550 (3.4118)  time: 0.4852  data: 0.0005  max mem: 19734
Epoch: [28]  [1150/1251]  eta: 0:00:48  lr: 0.000017  loss: 3.4765 (3.4121)  time: 0.4711  data: 0.0005  max mem: 19734
Epoch: [28]  [1160/1251]  eta: 0:00:43  lr: 0.000017  loss: 3.4224 (3.4122)  time: 0.4673  data: 0.0006  max mem: 19734
Epoch: [28]  [1170/1251]  eta: 0:00:38  lr: 0.000017  loss: 3.5913 (3.4125)  time: 0.4595  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3740, ratio_loss=0.0052, pruning_loss=0.1276, mse_loss=0.4510
Epoch: [28]  [1180/1251]  eta: 0:00:34  lr: 0.000017  loss: 3.5913 (3.4128)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [28]  [1190/1251]  eta: 0:00:29  lr: 0.000017  loss: 3.5320 (3.4137)  time: 0.4542  data: 0.0008  max mem: 19734
Epoch: [28]  [1200/1251]  eta: 0:00:24  lr: 0.000017  loss: 3.4679 (3.4129)  time: 0.4510  data: 0.0007  max mem: 19734
Epoch: [28]  [1210/1251]  eta: 0:00:19  lr: 0.000017  loss: 3.3351 (3.4124)  time: 0.4520  data: 0.0002  max mem: 19734
Epoch: [28]  [1220/1251]  eta: 0:00:14  lr: 0.000017  loss: 3.3702 (3.4131)  time: 0.4493  data: 0.0002  max mem: 19734
Epoch: [28]  [1230/1251]  eta: 0:00:10  lr: 0.000017  loss: 3.6317 (3.4137)  time: 0.4458  data: 0.0002  max mem: 19734
Epoch: [28]  [1240/1251]  eta: 0:00:05  lr: 0.000017  loss: 3.5354 (3.4146)  time: 0.4465  data: 0.0002  max mem: 19734
Epoch: [28]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.6559 (3.4170)  time: 0.4448  data: 0.0002  max mem: 19734
Epoch: [28] Total time: 0:09:58 (0.4786 s / it)
Averaged stats: lr: 0.000017  loss: 3.6559 (3.4316)
Test:  [  0/261]  eta: 1:39:52  loss: 0.7156 (0.7156)  acc1: 84.3750 (84.3750)  acc5: 96.3542 (96.3542)  time: 22.9609  data: 22.5011  max mem: 19734
Test:  [ 10/261]  eta: 0:12:41  loss: 0.7156 (0.7248)  acc1: 85.4167 (84.4224)  acc5: 96.3542 (96.0227)  time: 3.0329  data: 2.7545  max mem: 19734
Test:  [ 20/261]  eta: 0:06:47  loss: 0.9096 (0.8977)  acc1: 80.2083 (79.5883)  acc5: 93.2292 (94.3948)  time: 0.6253  data: 0.4012  max mem: 19734
Test:  [ 30/261]  eta: 0:04:47  loss: 0.8356 (0.8170)  acc1: 82.2917 (82.3757)  acc5: 93.2292 (94.8757)  time: 0.2590  data: 0.0201  max mem: 19734
Test:  [ 40/261]  eta: 0:03:48  loss: 0.5799 (0.7840)  acc1: 89.0625 (83.3206)  acc5: 96.3542 (95.1982)  time: 0.3419  data: 0.0847  max mem: 19734
Test:  [ 50/261]  eta: 0:03:06  loss: 0.9224 (0.8452)  acc1: 79.1667 (81.4440)  acc5: 94.7917 (94.8325)  time: 0.3290  data: 0.0828  max mem: 19734
Test:  [ 60/261]  eta: 0:02:32  loss: 0.9607 (0.8545)  acc1: 76.5625 (80.8999)  acc5: 94.2708 (94.9197)  time: 0.1967  data: 0.0115  max mem: 19734
Test:  [ 70/261]  eta: 0:02:11  loss: 0.9262 (0.8560)  acc1: 77.0833 (80.4064)  acc5: 96.3542 (95.1144)  time: 0.1968  data: 0.0513  max mem: 19734
Test:  [ 80/261]  eta: 0:01:57  loss: 0.8634 (0.8580)  acc1: 79.1667 (80.4591)  acc5: 96.3542 (95.2225)  time: 0.3146  data: 0.1193  max mem: 19734
Test:  [ 90/261]  eta: 0:01:45  loss: 0.7979 (0.8424)  acc1: 83.3333 (80.8951)  acc5: 95.8333 (95.3297)  time: 0.3611  data: 0.1639  max mem: 19734
Test:  [100/261]  eta: 0:01:35  loss: 0.7936 (0.8464)  acc1: 83.8542 (80.7962)  acc5: 95.8333 (95.4156)  time: 0.3565  data: 0.2008  max mem: 19734
Test:  [110/261]  eta: 0:01:25  loss: 0.8944 (0.8701)  acc1: 76.5625 (80.3022)  acc5: 94.7917 (95.1530)  time: 0.3245  data: 0.2233  max mem: 19734
Test:  [120/261]  eta: 0:01:14  loss: 1.2093 (0.9104)  acc1: 70.8333 (79.3216)  acc5: 90.6250 (94.6023)  time: 0.2178  data: 0.1184  max mem: 19734
Test:  [130/261]  eta: 0:01:09  loss: 1.3658 (0.9557)  acc1: 68.2292 (78.3755)  acc5: 86.9792 (93.9528)  time: 0.3485  data: 0.2317  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: 1.3687 (0.9814)  acc1: 68.7500 (77.7371)  acc5: 88.5417 (93.6687)  time: 0.5821  data: 0.4290  max mem: 19734
Test:  [150/261]  eta: 0:00:56  loss: 1.1899 (0.9870)  acc1: 72.9167 (77.6697)  acc5: 90.6250 (93.5396)  time: 0.3857  data: 0.2084  max mem: 19734
Test:  [160/261]  eta: 0:00:50  loss: 1.0064 (1.0058)  acc1: 77.6042 (77.3421)  acc5: 91.6667 (93.3003)  time: 0.2102  data: 0.0458  max mem: 19734
Test:  [170/261]  eta: 0:00:44  loss: 1.2842 (1.0353)  acc1: 67.1875 (76.6082)  acc5: 88.0208 (92.9703)  time: 0.3175  data: 0.1443  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4657 (1.0529)  acc1: 65.1042 (76.1999)  acc5: 88.0208 (92.8062)  time: 0.3044  data: 0.1119  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3899 (1.0662)  acc1: 67.1875 (75.9217)  acc5: 90.1042 (92.6538)  time: 0.2028  data: 0.0306  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3886 (1.0818)  acc1: 70.3125 (75.6038)  acc5: 89.0625 (92.4129)  time: 0.1998  data: 0.0889  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3886 (1.0961)  acc1: 69.7917 (75.3332)  acc5: 87.5000 (92.2122)  time: 0.1353  data: 0.0654  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4234 (1.1163)  acc1: 68.2292 (74.8445)  acc5: 87.5000 (91.9825)  time: 0.0616  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4256 (1.1265)  acc1: 66.1458 (74.6054)  acc5: 89.0625 (91.8741)  time: 0.0617  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3094 (1.1365)  acc1: 69.2708 (74.3538)  acc5: 90.6250 (91.8028)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0792 (1.1291)  acc1: 75.5208 (74.5290)  acc5: 94.2708 (91.9260)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9082 (1.1283)  acc1: 77.0833 (74.5240)  acc5: 94.7917 (91.9960)  time: 0.0602  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3602 s / it)
* Acc@1 74.524 Acc@5 91.996 loss 1.128
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.52%
Epoch: [29]  [   0/1251]  eta: 5:23:27  lr: 0.000017  loss: 3.7371 (3.7371)  time: 15.5133  data: 15.0369  max mem: 19734
Epoch: [29]  [  10/1251]  eta: 0:41:02  lr: 0.000017  loss: 3.4867 (3.5781)  time: 1.9839  data: 1.4080  max mem: 19734
loss info: cls_loss=3.4513, ratio_loss=0.0050, pruning_loss=0.1275, mse_loss=0.4847
Epoch: [29]  [  20/1251]  eta: 0:26:15  lr: 0.000017  loss: 3.3741 (3.4367)  time: 0.5685  data: 0.0229  max mem: 19734
Epoch: [29]  [  30/1251]  eta: 0:20:50  lr: 0.000017  loss: 2.9678 (3.4102)  time: 0.4960  data: 0.0005  max mem: 19734
Epoch: [29]  [  40/1251]  eta: 0:18:09  lr: 0.000017  loss: 3.0105 (3.3902)  time: 0.5000  data: 0.0004  max mem: 19734
Epoch: [29]  [  50/1251]  eta: 0:16:16  lr: 0.000017  loss: 3.0105 (3.3913)  time: 0.4856  data: 0.0004  max mem: 19734
Epoch: [29]  [  60/1251]  eta: 0:14:58  lr: 0.000017  loss: 3.0001 (3.3381)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [29]  [  70/1251]  eta: 0:14:03  lr: 0.000017  loss: 3.0001 (3.3264)  time: 0.4639  data: 0.0006  max mem: 19734
Epoch: [29]  [  80/1251]  eta: 0:13:19  lr: 0.000017  loss: 3.5454 (3.3587)  time: 0.4653  data: 0.0006  max mem: 19734
Epoch: [29]  [  90/1251]  eta: 0:12:44  lr: 0.000017  loss: 3.6832 (3.3752)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [29]  [ 100/1251]  eta: 0:12:15  lr: 0.000017  loss: 3.6247 (3.3808)  time: 0.4594  data: 0.0006  max mem: 19734
Epoch: [29]  [ 110/1251]  eta: 0:11:52  lr: 0.000017  loss: 3.3427 (3.3627)  time: 0.4676  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3514, ratio_loss=0.0053, pruning_loss=0.1307, mse_loss=0.4599
Epoch: [29]  [ 120/1251]  eta: 0:11:31  lr: 0.000017  loss: 3.5154 (3.3875)  time: 0.4709  data: 0.0005  max mem: 19734
Epoch: [29]  [ 130/1251]  eta: 0:11:11  lr: 0.000017  loss: 3.7140 (3.3998)  time: 0.4623  data: 0.0005  max mem: 19734
Epoch: [29]  [ 140/1251]  eta: 0:10:54  lr: 0.000017  loss: 3.6967 (3.3827)  time: 0.4587  data: 0.0005  max mem: 19734
Epoch: [29]  [ 150/1251]  eta: 0:10:41  lr: 0.000017  loss: 3.3026 (3.3800)  time: 0.4723  data: 0.0006  max mem: 19734
Epoch: [29]  [ 160/1251]  eta: 0:10:30  lr: 0.000017  loss: 3.6670 (3.4050)  time: 0.4976  data: 0.0006  max mem: 19734
Epoch: [29]  [ 170/1251]  eta: 0:10:18  lr: 0.000017  loss: 3.6888 (3.4034)  time: 0.4958  data: 0.0005  max mem: 19734
Epoch: [29]  [ 180/1251]  eta: 0:10:08  lr: 0.000017  loss: 3.4257 (3.4063)  time: 0.4926  data: 0.0004  max mem: 19734
Epoch: [29]  [ 190/1251]  eta: 0:09:58  lr: 0.000017  loss: 3.2617 (3.3977)  time: 0.4902  data: 0.0004  max mem: 19734
Epoch: [29]  [ 200/1251]  eta: 0:09:47  lr: 0.000017  loss: 3.1758 (3.3842)  time: 0.4701  data: 0.0004  max mem: 19734
Epoch: [29]  [ 210/1251]  eta: 0:09:37  lr: 0.000017  loss: 3.3637 (3.3874)  time: 0.4642  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3798, ratio_loss=0.0052, pruning_loss=0.1287, mse_loss=0.4716
Epoch: [29]  [ 220/1251]  eta: 0:09:27  lr: 0.000017  loss: 3.6981 (3.3957)  time: 0.4609  data: 0.0004  max mem: 19734
Epoch: [29]  [ 230/1251]  eta: 0:09:17  lr: 0.000017  loss: 3.6755 (3.4039)  time: 0.4600  data: 0.0005  max mem: 19734
Epoch: [29]  [ 240/1251]  eta: 0:09:08  lr: 0.000017  loss: 3.6755 (3.4104)  time: 0.4612  data: 0.0005  max mem: 19734
Epoch: [29]  [ 250/1251]  eta: 0:08:59  lr: 0.000017  loss: 3.4511 (3.4126)  time: 0.4584  data: 0.0004  max mem: 19734
Epoch: [29]  [ 260/1251]  eta: 0:08:52  lr: 0.000017  loss: 3.3450 (3.4065)  time: 0.4680  data: 0.0004  max mem: 19734
Epoch: [29]  [ 270/1251]  eta: 0:08:43  lr: 0.000017  loss: 3.4368 (3.4140)  time: 0.4656  data: 0.0005  max mem: 19734
Epoch: [29]  [ 280/1251]  eta: 0:08:35  lr: 0.000017  loss: 3.4892 (3.4118)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [29]  [ 290/1251]  eta: 0:08:27  lr: 0.000017  loss: 3.2970 (3.4088)  time: 0.4524  data: 0.0005  max mem: 19734
Epoch: [29]  [ 300/1251]  eta: 0:08:21  lr: 0.000017  loss: 3.3303 (3.4070)  time: 0.4730  data: 0.0006  max mem: 19734
Epoch: [29]  [ 310/1251]  eta: 0:08:14  lr: 0.000017  loss: 3.6682 (3.4148)  time: 0.4860  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4398, ratio_loss=0.0053, pruning_loss=0.1277, mse_loss=0.4605
Epoch: [29]  [ 320/1251]  eta: 0:08:07  lr: 0.000017  loss: 3.6799 (3.4174)  time: 0.4731  data: 0.0005  max mem: 19734
Epoch: [29]  [ 330/1251]  eta: 0:08:01  lr: 0.000017  loss: 3.5894 (3.4207)  time: 0.4786  data: 0.0005  max mem: 19734
Epoch: [29]  [ 340/1251]  eta: 0:07:54  lr: 0.000017  loss: 3.5894 (3.4180)  time: 0.4736  data: 0.0004  max mem: 19734
Epoch: [29]  [ 350/1251]  eta: 0:07:47  lr: 0.000017  loss: 3.3773 (3.4128)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [29]  [ 360/1251]  eta: 0:07:40  lr: 0.000017  loss: 3.4395 (3.4150)  time: 0.4594  data: 0.0005  max mem: 19734
Epoch: [29]  [ 370/1251]  eta: 0:07:34  lr: 0.000017  loss: 3.5091 (3.4137)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [29]  [ 380/1251]  eta: 0:07:27  lr: 0.000017  loss: 3.5915 (3.4197)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [29]  [ 390/1251]  eta: 0:07:21  lr: 0.000017  loss: 3.5915 (3.4211)  time: 0.4619  data: 0.0005  max mem: 19734
Epoch: [29]  [ 400/1251]  eta: 0:07:15  lr: 0.000017  loss: 3.4136 (3.4152)  time: 0.4755  data: 0.0005  max mem: 19734
Epoch: [29]  [ 410/1251]  eta: 0:07:09  lr: 0.000017  loss: 3.4136 (3.4156)  time: 0.4701  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3663, ratio_loss=0.0053, pruning_loss=0.1287, mse_loss=0.4660
Epoch: [29]  [ 420/1251]  eta: 0:07:03  lr: 0.000017  loss: 3.3718 (3.4114)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [29]  [ 430/1251]  eta: 0:06:57  lr: 0.000017  loss: 3.3820 (3.4128)  time: 0.4621  data: 0.0004  max mem: 19734
Epoch: [29]  [ 440/1251]  eta: 0:06:51  lr: 0.000017  loss: 3.4495 (3.4090)  time: 0.4711  data: 0.0004  max mem: 19734
Epoch: [29]  [ 450/1251]  eta: 0:06:46  lr: 0.000017  loss: 3.4495 (3.4092)  time: 0.4984  data: 0.0004  max mem: 19734
Epoch: [29]  [ 460/1251]  eta: 0:06:40  lr: 0.000017  loss: 3.4180 (3.4053)  time: 0.4878  data: 0.0005  max mem: 19734
Epoch: [29]  [ 470/1251]  eta: 0:06:35  lr: 0.000017  loss: 3.4744 (3.4068)  time: 0.4735  data: 0.0005  max mem: 19734
Epoch: [29]  [ 480/1251]  eta: 0:06:29  lr: 0.000017  loss: 3.6602 (3.4159)  time: 0.4802  data: 0.0005  max mem: 19734
Epoch: [29]  [ 490/1251]  eta: 0:06:24  lr: 0.000017  loss: 3.8005 (3.4180)  time: 0.4633  data: 0.0005  max mem: 19734
Epoch: [29]  [ 500/1251]  eta: 0:06:18  lr: 0.000017  loss: 3.5653 (3.4169)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [29]  [ 510/1251]  eta: 0:06:12  lr: 0.000017  loss: 3.5090 (3.4142)  time: 0.4542  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3823, ratio_loss=0.0054, pruning_loss=0.1278, mse_loss=0.4587
Epoch: [29]  [ 520/1251]  eta: 0:06:06  lr: 0.000017  loss: 3.3037 (3.4107)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [29]  [ 530/1251]  eta: 0:06:01  lr: 0.000017  loss: 3.2988 (3.4079)  time: 0.4526  data: 0.0006  max mem: 19734
Epoch: [29]  [ 540/1251]  eta: 0:05:55  lr: 0.000017  loss: 3.2988 (3.4037)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [29]  [ 550/1251]  eta: 0:05:50  lr: 0.000017  loss: 3.5874 (3.4063)  time: 0.4635  data: 0.0004  max mem: 19734
Epoch: [29]  [ 560/1251]  eta: 0:05:44  lr: 0.000017  loss: 3.5874 (3.4048)  time: 0.4615  data: 0.0005  max mem: 19734
Epoch: [29]  [ 570/1251]  eta: 0:05:39  lr: 0.000017  loss: 3.4970 (3.4075)  time: 0.4520  data: 0.0005  max mem: 19734
Epoch: [29]  [ 580/1251]  eta: 0:05:33  lr: 0.000017  loss: 3.6965 (3.4062)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [29]  [ 590/1251]  eta: 0:05:28  lr: 0.000017  loss: 3.4474 (3.4086)  time: 0.4837  data: 0.0005  max mem: 19734
Epoch: [29]  [ 600/1251]  eta: 0:05:23  lr: 0.000017  loss: 3.4886 (3.4091)  time: 0.4945  data: 0.0005  max mem: 19734
Epoch: [29]  [ 610/1251]  eta: 0:05:18  lr: 0.000017  loss: 3.5514 (3.4135)  time: 0.4843  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3880, ratio_loss=0.0052, pruning_loss=0.1285, mse_loss=0.4589
Epoch: [29]  [ 620/1251]  eta: 0:05:13  lr: 0.000017  loss: 3.5514 (3.4111)  time: 0.4738  data: 0.0005  max mem: 19734
Epoch: [29]  [ 630/1251]  eta: 0:05:08  lr: 0.000017  loss: 3.4337 (3.4097)  time: 0.4662  data: 0.0005  max mem: 19734
Epoch: [29]  [ 640/1251]  eta: 0:05:02  lr: 0.000017  loss: 3.4586 (3.4116)  time: 0.4682  data: 0.0005  max mem: 19734
Epoch: [29]  [ 650/1251]  eta: 0:04:57  lr: 0.000017  loss: 3.4267 (3.4121)  time: 0.4567  data: 0.0007  max mem: 19734
Epoch: [29]  [ 660/1251]  eta: 0:04:52  lr: 0.000017  loss: 3.4252 (3.4139)  time: 0.4585  data: 0.0006  max mem: 19734
Epoch: [29]  [ 670/1251]  eta: 0:04:46  lr: 0.000017  loss: 3.3345 (3.4107)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [29]  [ 680/1251]  eta: 0:04:41  lr: 0.000017  loss: 3.4613 (3.4136)  time: 0.4612  data: 0.0005  max mem: 19734
Epoch: [29]  [ 690/1251]  eta: 0:04:36  lr: 0.000017  loss: 3.6337 (3.4159)  time: 0.4610  data: 0.0005  max mem: 19734
Epoch: [29]  [ 700/1251]  eta: 0:04:31  lr: 0.000017  loss: 3.5324 (3.4176)  time: 0.4685  data: 0.0004  max mem: 19734
Epoch: [29]  [ 710/1251]  eta: 0:04:26  lr: 0.000017  loss: 3.5324 (3.4206)  time: 0.4665  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4595, ratio_loss=0.0052, pruning_loss=0.1272, mse_loss=0.4616
Epoch: [29]  [ 720/1251]  eta: 0:04:21  lr: 0.000017  loss: 3.5278 (3.4214)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [29]  [ 730/1251]  eta: 0:04:16  lr: 0.000017  loss: 3.5078 (3.4199)  time: 0.4715  data: 0.0007  max mem: 19734
Epoch: [29]  [ 740/1251]  eta: 0:04:11  lr: 0.000017  loss: 3.4324 (3.4193)  time: 0.4859  data: 0.0006  max mem: 19734
Epoch: [29]  [ 750/1251]  eta: 0:04:05  lr: 0.000017  loss: 3.4324 (3.4175)  time: 0.4690  data: 0.0004  max mem: 19734
Epoch: [29]  [ 760/1251]  eta: 0:04:01  lr: 0.000017  loss: 3.5358 (3.4181)  time: 0.4723  data: 0.0005  max mem: 19734
Epoch: [29]  [ 770/1251]  eta: 0:03:55  lr: 0.000017  loss: 3.4532 (3.4152)  time: 0.4796  data: 0.0005  max mem: 19734
Epoch: [29]  [ 780/1251]  eta: 0:03:50  lr: 0.000017  loss: 3.4653 (3.4167)  time: 0.4624  data: 0.0004  max mem: 19734
Epoch: [29]  [ 790/1251]  eta: 0:03:45  lr: 0.000017  loss: 3.5534 (3.4155)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [29]  [ 800/1251]  eta: 0:03:40  lr: 0.000017  loss: 3.5787 (3.4170)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [29]  [ 810/1251]  eta: 0:03:35  lr: 0.000017  loss: 3.4572 (3.4137)  time: 0.4562  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3330, ratio_loss=0.0049, pruning_loss=0.1288, mse_loss=0.4787
Epoch: [29]  [ 820/1251]  eta: 0:03:30  lr: 0.000017  loss: 3.3815 (3.4137)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [29]  [ 830/1251]  eta: 0:03:25  lr: 0.000017  loss: 3.3815 (3.4134)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [29]  [ 840/1251]  eta: 0:03:20  lr: 0.000017  loss: 3.3168 (3.4138)  time: 0.4648  data: 0.0004  max mem: 19734
Epoch: [29]  [ 850/1251]  eta: 0:03:15  lr: 0.000017  loss: 3.3954 (3.4130)  time: 0.4804  data: 0.0006  max mem: 19734
Epoch: [29]  [ 860/1251]  eta: 0:03:10  lr: 0.000017  loss: 3.4679 (3.4144)  time: 0.4757  data: 0.0006  max mem: 19734
Epoch: [29]  [ 870/1251]  eta: 0:03:05  lr: 0.000017  loss: 3.5474 (3.4142)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [29]  [ 880/1251]  eta: 0:03:00  lr: 0.000017  loss: 3.5418 (3.4138)  time: 0.4726  data: 0.0004  max mem: 19734
Epoch: [29]  [ 890/1251]  eta: 0:02:55  lr: 0.000017  loss: 3.5418 (3.4135)  time: 0.4808  data: 0.0004  max mem: 19734
Epoch: [29]  [ 900/1251]  eta: 0:02:50  lr: 0.000017  loss: 3.5949 (3.4171)  time: 0.4756  data: 0.0004  max mem: 19734
Epoch: [29]  [ 910/1251]  eta: 0:02:46  lr: 0.000017  loss: 3.5703 (3.4150)  time: 0.4850  data: 0.0008  max mem: 19734
loss info: cls_loss=3.3729, ratio_loss=0.0050, pruning_loss=0.1277, mse_loss=0.4401
Epoch: [29]  [ 920/1251]  eta: 0:02:41  lr: 0.000017  loss: 3.2737 (3.4120)  time: 0.4744  data: 0.0008  max mem: 19734
Epoch: [29]  [ 930/1251]  eta: 0:02:36  lr: 0.000017  loss: 3.1438 (3.4091)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [29]  [ 940/1251]  eta: 0:02:31  lr: 0.000017  loss: 3.3094 (3.4110)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [29]  [ 950/1251]  eta: 0:02:26  lr: 0.000017  loss: 3.6261 (3.4106)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [29]  [ 960/1251]  eta: 0:02:21  lr: 0.000017  loss: 3.5534 (3.4124)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [29]  [ 970/1251]  eta: 0:02:16  lr: 0.000017  loss: 3.4824 (3.4119)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [29]  [ 980/1251]  eta: 0:02:11  lr: 0.000017  loss: 3.2203 (3.4114)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [29]  [ 990/1251]  eta: 0:02:06  lr: 0.000017  loss: 3.3267 (3.4118)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [29]  [1000/1251]  eta: 0:02:01  lr: 0.000017  loss: 3.5855 (3.4129)  time: 0.4645  data: 0.0005  max mem: 19734
Epoch: [29]  [1010/1251]  eta: 0:01:56  lr: 0.000017  loss: 3.6787 (3.4148)  time: 0.4675  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4065, ratio_loss=0.0053, pruning_loss=0.1290, mse_loss=0.4617
Epoch: [29]  [1020/1251]  eta: 0:01:51  lr: 0.000017  loss: 3.5699 (3.4140)  time: 0.4706  data: 0.0007  max mem: 19734
Epoch: [29]  [1030/1251]  eta: 0:01:47  lr: 0.000017  loss: 3.4541 (3.4158)  time: 0.4943  data: 0.0007  max mem: 19734
Epoch: [29]  [1040/1251]  eta: 0:01:42  lr: 0.000017  loss: 3.5643 (3.4163)  time: 0.4804  data: 0.0004  max mem: 19734
Epoch: [29]  [1050/1251]  eta: 0:01:37  lr: 0.000017  loss: 3.5700 (3.4196)  time: 0.4752  data: 0.0005  max mem: 19734
Epoch: [29]  [1060/1251]  eta: 0:01:32  lr: 0.000017  loss: 3.5700 (3.4178)  time: 0.4830  data: 0.0005  max mem: 19734
Epoch: [29]  [1070/1251]  eta: 0:01:27  lr: 0.000017  loss: 3.5977 (3.4199)  time: 0.4623  data: 0.0007  max mem: 19734
Epoch: [29]  [1080/1251]  eta: 0:01:22  lr: 0.000017  loss: 3.6766 (3.4215)  time: 0.4558  data: 0.0007  max mem: 19734
Epoch: [29]  [1090/1251]  eta: 0:01:17  lr: 0.000017  loss: 3.5990 (3.4203)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [29]  [1100/1251]  eta: 0:01:12  lr: 0.000017  loss: 3.0814 (3.4187)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [29]  [1110/1251]  eta: 0:01:08  lr: 0.000017  loss: 3.5117 (3.4206)  time: 0.4573  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4662, ratio_loss=0.0052, pruning_loss=0.1263, mse_loss=0.4613
Epoch: [29]  [1120/1251]  eta: 0:01:03  lr: 0.000017  loss: 3.6084 (3.4209)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [29]  [1130/1251]  eta: 0:00:58  lr: 0.000017  loss: 3.3611 (3.4204)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [29]  [1140/1251]  eta: 0:00:53  lr: 0.000017  loss: 3.4385 (3.4221)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [29]  [1150/1251]  eta: 0:00:48  lr: 0.000017  loss: 3.5495 (3.4218)  time: 0.4642  data: 0.0005  max mem: 19734
Epoch: [29]  [1160/1251]  eta: 0:00:43  lr: 0.000017  loss: 3.3258 (3.4205)  time: 0.4651  data: 0.0005  max mem: 19734
Epoch: [29]  [1170/1251]  eta: 0:00:39  lr: 0.000017  loss: 3.2671 (3.4200)  time: 0.4826  data: 0.0004  max mem: 19734
Epoch: [29]  [1180/1251]  eta: 0:00:34  lr: 0.000017  loss: 3.4378 (3.4199)  time: 0.4941  data: 0.0004  max mem: 19734
Epoch: [29]  [1190/1251]  eta: 0:00:29  lr: 0.000017  loss: 3.5219 (3.4200)  time: 0.4786  data: 0.0008  max mem: 19734
Epoch: [29]  [1200/1251]  eta: 0:00:24  lr: 0.000017  loss: 3.4618 (3.4187)  time: 0.4772  data: 0.0007  max mem: 19734
Epoch: [29]  [1210/1251]  eta: 0:00:19  lr: 0.000017  loss: 3.3560 (3.4183)  time: 0.4617  data: 0.0002  max mem: 19734
loss info: cls_loss=3.3375, ratio_loss=0.0052, pruning_loss=0.1293, mse_loss=0.4589
Epoch: [29]  [1220/1251]  eta: 0:00:14  lr: 0.000017  loss: 3.2119 (3.4161)  time: 0.4465  data: 0.0002  max mem: 19734
Epoch: [29]  [1230/1251]  eta: 0:00:10  lr: 0.000017  loss: 3.1625 (3.4155)  time: 0.4483  data: 0.0002  max mem: 19734
Epoch: [29]  [1240/1251]  eta: 0:00:05  lr: 0.000017  loss: 3.3644 (3.4152)  time: 0.4498  data: 0.0002  max mem: 19734
Epoch: [29]  [1250/1251]  eta: 0:00:00  lr: 0.000017  loss: 3.3644 (3.4147)  time: 0.4513  data: 0.0002  max mem: 19734
Epoch: [29] Total time: 0:10:02 (0.4813 s / it)
Averaged stats: lr: 0.000017  loss: 3.3644 (3.4246)
Test:  [  0/261]  eta: 1:44:08  loss: 0.7218 (0.7218)  acc1: 83.8542 (83.8542)  acc5: 96.3542 (96.3542)  time: 23.9415  data: 23.7667  max mem: 19734
Test:  [ 10/261]  eta: 0:09:58  loss: 0.7218 (0.7406)  acc1: 83.8542 (84.1383)  acc5: 96.3542 (95.9280)  time: 2.3831  data: 2.1723  max mem: 19734
Test:  [ 20/261]  eta: 0:05:26  loss: 0.9335 (0.9055)  acc1: 80.2083 (79.6131)  acc5: 93.7500 (94.6429)  time: 0.2240  data: 0.0202  max mem: 19734
Test:  [ 30/261]  eta: 0:03:45  loss: 0.8257 (0.8220)  acc1: 83.8542 (82.3085)  acc5: 93.7500 (95.0941)  time: 0.2051  data: 0.0210  max mem: 19734
Test:  [ 40/261]  eta: 0:03:22  loss: 0.5874 (0.7886)  acc1: 88.0208 (83.2444)  acc5: 96.8750 (95.4395)  time: 0.4537  data: 0.2986  max mem: 19734
Test:  [ 50/261]  eta: 0:02:40  loss: 0.9383 (0.8535)  acc1: 78.1250 (81.1683)  acc5: 95.3125 (95.0470)  time: 0.4196  data: 0.2971  max mem: 19734
Test:  [ 60/261]  eta: 0:02:12  loss: 0.9734 (0.8609)  acc1: 77.0833 (80.7036)  acc5: 94.2708 (95.1417)  time: 0.1311  data: 0.0118  max mem: 19734
Test:  [ 70/261]  eta: 0:02:13  loss: 0.9130 (0.8610)  acc1: 77.6042 (80.3257)  acc5: 96.3542 (95.3272)  time: 0.5428  data: 0.4028  max mem: 19734
Test:  [ 80/261]  eta: 0:01:55  loss: 0.8467 (0.8611)  acc1: 79.1667 (80.4205)  acc5: 96.8750 (95.4154)  time: 0.5839  data: 0.4061  max mem: 19734
Test:  [ 90/261]  eta: 0:01:41  loss: 0.8198 (0.8479)  acc1: 84.3750 (80.7921)  acc5: 96.3542 (95.5128)  time: 0.2103  data: 0.0166  max mem: 19734
Test:  [100/261]  eta: 0:01:39  loss: 0.7987 (0.8513)  acc1: 84.3750 (80.7807)  acc5: 95.8333 (95.5600)  time: 0.5266  data: 0.3391  max mem: 19734
Test:  [110/261]  eta: 0:01:26  loss: 0.8886 (0.8740)  acc1: 76.5625 (80.3163)  acc5: 94.2708 (95.2280)  time: 0.5043  data: 0.3401  max mem: 19734
Test:  [120/261]  eta: 0:01:16  loss: 1.2145 (0.9137)  acc1: 71.8750 (79.4163)  acc5: 89.5833 (94.6711)  time: 0.1788  data: 0.0195  max mem: 19734
Test:  [130/261]  eta: 0:01:07  loss: 1.3694 (0.9584)  acc1: 68.7500 (78.4470)  acc5: 86.9792 (94.0522)  time: 0.2067  data: 0.0209  max mem: 19734
Test:  [140/261]  eta: 0:01:00  loss: 1.3465 (0.9857)  acc1: 69.2708 (77.7778)  acc5: 89.0625 (93.7722)  time: 0.2117  data: 0.0567  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: 1.2592 (0.9910)  acc1: 72.3958 (77.7870)  acc5: 91.1458 (93.6362)  time: 0.3116  data: 0.1772  max mem: 19734
Test:  [160/261]  eta: 0:00:47  loss: 1.0439 (1.0118)  acc1: 78.1250 (77.4198)  acc5: 91.1458 (93.3100)  time: 0.2727  data: 0.1369  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: 1.3073 (1.0418)  acc1: 65.6250 (76.6234)  acc5: 88.0208 (92.9946)  time: 0.3235  data: 0.1896  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4534 (1.0587)  acc1: 65.1042 (76.2431)  acc5: 89.0625 (92.8551)  time: 0.4897  data: 0.3586  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4002 (1.0721)  acc1: 67.7083 (75.9680)  acc5: 90.6250 (92.7138)  time: 0.2802  data: 0.1882  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3947 (1.0864)  acc1: 71.3542 (75.6815)  acc5: 89.0625 (92.4751)  time: 0.0766  data: 0.0127  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3554 (1.1002)  acc1: 68.2292 (75.3875)  acc5: 88.5417 (92.2961)  time: 0.0922  data: 0.0307  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4420 (1.1196)  acc1: 67.1875 (74.9222)  acc5: 88.5417 (92.0956)  time: 0.1025  data: 0.0410  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4586 (1.1287)  acc1: 65.6250 (74.6663)  acc5: 89.0625 (92.0026)  time: 0.0721  data: 0.0107  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.2960 (1.1378)  acc1: 68.7500 (74.4468)  acc5: 91.1458 (91.9195)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0323 (1.1312)  acc1: 76.0417 (74.6120)  acc5: 94.7917 (92.0402)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9530 (1.1302)  acc1: 76.5625 (74.6160)  acc5: 95.3125 (92.1140)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:31 (0.3510 s / it)
* Acc@1 74.616 Acc@5 92.114 loss 1.130
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.62%
Epoch: [30]  [   0/1251]  eta: 4:34:51  lr: 0.000016  loss: 3.8289 (3.8289)  time: 13.1828  data: 12.6436  max mem: 19734
Epoch: [30]  [  10/1251]  eta: 0:36:39  lr: 0.000016  loss: 3.6734 (3.4906)  time: 1.7721  data: 1.1626  max mem: 19734
Epoch: [30]  [  20/1251]  eta: 0:23:29  lr: 0.000016  loss: 3.5542 (3.4619)  time: 0.5431  data: 0.0075  max mem: 19734
Epoch: [30]  [  30/1251]  eta: 0:18:45  lr: 0.000016  loss: 3.3257 (3.4476)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [30]  [  40/1251]  eta: 0:16:17  lr: 0.000016  loss: 3.3587 (3.4353)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [30]  [  50/1251]  eta: 0:14:51  lr: 0.000016  loss: 3.5597 (3.4453)  time: 0.4637  data: 0.0004  max mem: 19734
Epoch: [30]  [  60/1251]  eta: 0:13:55  lr: 0.000016  loss: 3.5922 (3.4647)  time: 0.4859  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3723, ratio_loss=0.0052, pruning_loss=0.1286, mse_loss=0.4554
Epoch: [30]  [  70/1251]  eta: 0:13:13  lr: 0.000016  loss: 3.3992 (3.4112)  time: 0.4944  data: 0.0004  max mem: 19734
Epoch: [30]  [  80/1251]  eta: 0:12:38  lr: 0.000016  loss: 3.4548 (3.4272)  time: 0.4839  data: 0.0004  max mem: 19734
Epoch: [30]  [  90/1251]  eta: 0:12:10  lr: 0.000016  loss: 3.4541 (3.4199)  time: 0.4761  data: 0.0004  max mem: 19734
Epoch: [30]  [ 100/1251]  eta: 0:11:44  lr: 0.000016  loss: 3.4182 (3.4047)  time: 0.4661  data: 0.0004  max mem: 19734
Epoch: [30]  [ 110/1251]  eta: 0:11:22  lr: 0.000016  loss: 3.6613 (3.4314)  time: 0.4593  data: 0.0003  max mem: 19734
Epoch: [30]  [ 120/1251]  eta: 0:11:03  lr: 0.000016  loss: 3.7171 (3.4429)  time: 0.4610  data: 0.0003  max mem: 19734
Epoch: [30]  [ 130/1251]  eta: 0:10:47  lr: 0.000016  loss: 3.6016 (3.4481)  time: 0.4630  data: 0.0004  max mem: 19734
Epoch: [30]  [ 140/1251]  eta: 0:10:34  lr: 0.000016  loss: 3.6016 (3.4432)  time: 0.4763  data: 0.0004  max mem: 19734
Epoch: [30]  [ 150/1251]  eta: 0:10:21  lr: 0.000016  loss: 3.1599 (3.4262)  time: 0.4746  data: 0.0004  max mem: 19734
Epoch: [30]  [ 160/1251]  eta: 0:10:08  lr: 0.000016  loss: 3.4859 (3.4462)  time: 0.4630  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4489, ratio_loss=0.0052, pruning_loss=0.1258, mse_loss=0.4417
Epoch: [30]  [ 170/1251]  eta: 0:09:56  lr: 0.000016  loss: 3.7126 (3.4517)  time: 0.4619  data: 0.0005  max mem: 19734
Epoch: [30]  [ 180/1251]  eta: 0:09:45  lr: 0.000016  loss: 3.5089 (3.4476)  time: 0.4589  data: 0.0005  max mem: 19734
Epoch: [30]  [ 190/1251]  eta: 0:09:35  lr: 0.000016  loss: 3.5916 (3.4595)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [30]  [ 200/1251]  eta: 0:09:27  lr: 0.000016  loss: 3.6088 (3.4607)  time: 0.4806  data: 0.0004  max mem: 19734
Epoch: [30]  [ 210/1251]  eta: 0:09:21  lr: 0.000016  loss: 3.4663 (3.4521)  time: 0.5068  data: 0.0004  max mem: 19734
Epoch: [30]  [ 220/1251]  eta: 0:09:12  lr: 0.000016  loss: 2.9596 (3.4272)  time: 0.4938  data: 0.0004  max mem: 19734
Epoch: [30]  [ 230/1251]  eta: 0:09:04  lr: 0.000016  loss: 3.2819 (3.4257)  time: 0.4790  data: 0.0004  max mem: 19734
Epoch: [30]  [ 240/1251]  eta: 0:08:56  lr: 0.000016  loss: 3.5029 (3.4317)  time: 0.4684  data: 0.0004  max mem: 19734
Epoch: [30]  [ 250/1251]  eta: 0:08:48  lr: 0.000016  loss: 3.5866 (3.4295)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [30]  [ 260/1251]  eta: 0:08:40  lr: 0.000016  loss: 3.1444 (3.4207)  time: 0.4559  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3756, ratio_loss=0.0048, pruning_loss=0.1275, mse_loss=0.4593
Epoch: [30]  [ 270/1251]  eta: 0:08:32  lr: 0.000016  loss: 3.5335 (3.4322)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [30]  [ 280/1251]  eta: 0:08:25  lr: 0.000016  loss: 3.5867 (3.4247)  time: 0.4608  data: 0.0005  max mem: 19734
Epoch: [30]  [ 290/1251]  eta: 0:08:17  lr: 0.000016  loss: 3.3647 (3.4188)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [30]  [ 300/1251]  eta: 0:08:10  lr: 0.000016  loss: 3.3674 (3.4173)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [30]  [ 310/1251]  eta: 0:08:03  lr: 0.000016  loss: 3.3585 (3.4132)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [30]  [ 320/1251]  eta: 0:07:56  lr: 0.000016  loss: 3.3585 (3.4101)  time: 0.4533  data: 0.0003  max mem: 19734
Epoch: [30]  [ 330/1251]  eta: 0:07:50  lr: 0.000016  loss: 3.5990 (3.4182)  time: 0.4557  data: 0.0003  max mem: 19734
Epoch: [30]  [ 340/1251]  eta: 0:07:44  lr: 0.000016  loss: 3.6414 (3.4152)  time: 0.4664  data: 0.0004  max mem: 19734
Epoch: [30]  [ 350/1251]  eta: 0:07:38  lr: 0.000016  loss: 3.5523 (3.4198)  time: 0.4820  data: 0.0004  max mem: 19734
Epoch: [30]  [ 360/1251]  eta: 0:07:32  lr: 0.000016  loss: 3.5933 (3.4213)  time: 0.4795  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3580, ratio_loss=0.0051, pruning_loss=0.1284, mse_loss=0.4606
Epoch: [30]  [ 370/1251]  eta: 0:07:26  lr: 0.000016  loss: 3.4540 (3.4167)  time: 0.4801  data: 0.0004  max mem: 19734
Epoch: [30]  [ 380/1251]  eta: 0:07:20  lr: 0.000016  loss: 3.5218 (3.4175)  time: 0.4800  data: 0.0004  max mem: 19734
Epoch: [30]  [ 390/1251]  eta: 0:07:14  lr: 0.000016  loss: 3.5275 (3.4171)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [30]  [ 400/1251]  eta: 0:07:08  lr: 0.000016  loss: 3.6598 (3.4204)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [30]  [ 410/1251]  eta: 0:07:02  lr: 0.000016  loss: 3.5989 (3.4137)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [30]  [ 420/1251]  eta: 0:06:56  lr: 0.000016  loss: 2.9892 (3.4082)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [30]  [ 430/1251]  eta: 0:06:51  lr: 0.000016  loss: 3.5603 (3.4103)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [30]  [ 440/1251]  eta: 0:06:45  lr: 0.000016  loss: 3.3582 (3.4094)  time: 0.4649  data: 0.0004  max mem: 19734
Epoch: [30]  [ 450/1251]  eta: 0:06:39  lr: 0.000016  loss: 3.2575 (3.4070)  time: 0.4583  data: 0.0004  max mem: 19734
Epoch: [30]  [ 460/1251]  eta: 0:06:34  lr: 0.000016  loss: 3.3005 (3.4062)  time: 0.4599  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3603, ratio_loss=0.0052, pruning_loss=0.1278, mse_loss=0.4494
Epoch: [30]  [ 470/1251]  eta: 0:06:28  lr: 0.000016  loss: 3.5211 (3.4094)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [30]  [ 480/1251]  eta: 0:06:22  lr: 0.000016  loss: 3.2998 (3.4039)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [30]  [ 490/1251]  eta: 0:06:17  lr: 0.000016  loss: 3.1975 (3.4032)  time: 0.4796  data: 0.0005  max mem: 19734
Epoch: [30]  [ 500/1251]  eta: 0:06:12  lr: 0.000016  loss: 3.4331 (3.4003)  time: 0.4948  data: 0.0005  max mem: 19734
Epoch: [30]  [ 510/1251]  eta: 0:06:07  lr: 0.000016  loss: 3.4331 (3.3986)  time: 0.4829  data: 0.0004  max mem: 19734
Epoch: [30]  [ 520/1251]  eta: 0:06:02  lr: 0.000016  loss: 3.7048 (3.3991)  time: 0.4865  data: 0.0004  max mem: 19734
Epoch: [30]  [ 530/1251]  eta: 0:05:57  lr: 0.000016  loss: 3.5704 (3.4006)  time: 0.4740  data: 0.0006  max mem: 19734
Epoch: [30]  [ 540/1251]  eta: 0:05:51  lr: 0.000016  loss: 3.4274 (3.3959)  time: 0.4531  data: 0.0006  max mem: 19734
Epoch: [30]  [ 550/1251]  eta: 0:05:45  lr: 0.000016  loss: 3.3063 (3.3962)  time: 0.4513  data: 0.0005  max mem: 19734
Epoch: [30]  [ 560/1251]  eta: 0:05:40  lr: 0.000016  loss: 3.3228 (3.3958)  time: 0.4517  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3168, ratio_loss=0.0051, pruning_loss=0.1291, mse_loss=0.4483
Epoch: [30]  [ 570/1251]  eta: 0:05:35  lr: 0.000016  loss: 3.5091 (3.4000)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [30]  [ 580/1251]  eta: 0:05:30  lr: 0.000016  loss: 3.6915 (3.4038)  time: 0.4629  data: 0.0008  max mem: 19734
Epoch: [30]  [ 590/1251]  eta: 0:05:24  lr: 0.000016  loss: 3.7041 (3.4064)  time: 0.4623  data: 0.0007  max mem: 19734
Epoch: [30]  [ 600/1251]  eta: 0:05:19  lr: 0.000016  loss: 3.7588 (3.4081)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [30]  [ 610/1251]  eta: 0:05:14  lr: 0.000016  loss: 3.6879 (3.4094)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [30]  [ 620/1251]  eta: 0:05:08  lr: 0.000016  loss: 3.6843 (3.4116)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [30]  [ 630/1251]  eta: 0:05:04  lr: 0.000016  loss: 3.5841 (3.4130)  time: 0.4779  data: 0.0003  max mem: 19734
Epoch: [30]  [ 640/1251]  eta: 0:04:59  lr: 0.000016  loss: 3.4000 (3.4131)  time: 0.4956  data: 0.0004  max mem: 19734
Epoch: [30]  [ 650/1251]  eta: 0:04:54  lr: 0.000016  loss: 3.3638 (3.4105)  time: 0.4834  data: 0.0004  max mem: 19734
Epoch: [30]  [ 660/1251]  eta: 0:04:49  lr: 0.000016  loss: 3.3638 (3.4129)  time: 0.4763  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4600, ratio_loss=0.0054, pruning_loss=0.1269, mse_loss=0.4557
Epoch: [30]  [ 670/1251]  eta: 0:04:44  lr: 0.000016  loss: 3.5357 (3.4124)  time: 0.4762  data: 0.0005  max mem: 19734
Epoch: [30]  [ 680/1251]  eta: 0:04:38  lr: 0.000016  loss: 3.6043 (3.4152)  time: 0.4662  data: 0.0005  max mem: 19734
Epoch: [30]  [ 690/1251]  eta: 0:04:33  lr: 0.000016  loss: 3.6132 (3.4148)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [30]  [ 700/1251]  eta: 0:04:28  lr: 0.000016  loss: 3.4442 (3.4131)  time: 0.4577  data: 0.0004  max mem: 19734
Epoch: [30]  [ 710/1251]  eta: 0:04:23  lr: 0.000016  loss: 3.5483 (3.4155)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [30]  [ 720/1251]  eta: 0:04:18  lr: 0.000016  loss: 3.4300 (3.4134)  time: 0.4650  data: 0.0003  max mem: 19734
Epoch: [30]  [ 730/1251]  eta: 0:04:13  lr: 0.000016  loss: 3.4223 (3.4180)  time: 0.4642  data: 0.0004  max mem: 19734
Epoch: [30]  [ 740/1251]  eta: 0:04:08  lr: 0.000016  loss: 3.7781 (3.4197)  time: 0.4557  data: 0.0006  max mem: 19734
Epoch: [30]  [ 750/1251]  eta: 0:04:03  lr: 0.000016  loss: 3.6791 (3.4205)  time: 0.4561  data: 0.0006  max mem: 19734
Epoch: [30]  [ 760/1251]  eta: 0:03:58  lr: 0.000016  loss: 3.6171 (3.4211)  time: 0.4555  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4381, ratio_loss=0.0050, pruning_loss=0.1260, mse_loss=0.4599
Epoch: [30]  [ 770/1251]  eta: 0:03:53  lr: 0.000016  loss: 3.3725 (3.4195)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [30]  [ 780/1251]  eta: 0:03:48  lr: 0.000016  loss: 3.2573 (3.4154)  time: 0.4693  data: 0.0005  max mem: 19734
Epoch: [30]  [ 790/1251]  eta: 0:03:43  lr: 0.000016  loss: 3.3753 (3.4132)  time: 0.4762  data: 0.0006  max mem: 19734
Epoch: [30]  [ 800/1251]  eta: 0:03:38  lr: 0.000016  loss: 3.3917 (3.4123)  time: 0.4645  data: 0.0007  max mem: 19734
Epoch: [30]  [ 810/1251]  eta: 0:03:33  lr: 0.000016  loss: 3.4013 (3.4105)  time: 0.4717  data: 0.0005  max mem: 19734
Epoch: [30]  [ 820/1251]  eta: 0:03:28  lr: 0.000016  loss: 3.3353 (3.4088)  time: 0.4658  data: 0.0005  max mem: 19734
Epoch: [30]  [ 830/1251]  eta: 0:03:23  lr: 0.000016  loss: 3.4259 (3.4108)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [30]  [ 840/1251]  eta: 0:03:18  lr: 0.000016  loss: 3.4873 (3.4116)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [30]  [ 850/1251]  eta: 0:03:13  lr: 0.000016  loss: 3.4919 (3.4109)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [30]  [ 860/1251]  eta: 0:03:08  lr: 0.000016  loss: 3.7198 (3.4137)  time: 0.4550  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3318, ratio_loss=0.0049, pruning_loss=0.1285, mse_loss=0.4530
Epoch: [30]  [ 870/1251]  eta: 0:03:03  lr: 0.000016  loss: 3.6867 (3.4108)  time: 0.4649  data: 0.0005  max mem: 19734
Epoch: [30]  [ 880/1251]  eta: 0:02:58  lr: 0.000016  loss: 3.1792 (3.4109)  time: 0.4641  data: 0.0005  max mem: 19734
Epoch: [30]  [ 890/1251]  eta: 0:02:53  lr: 0.000016  loss: 3.2799 (3.4083)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [30]  [ 900/1251]  eta: 0:02:48  lr: 0.000016  loss: 3.4627 (3.4115)  time: 0.4522  data: 0.0005  max mem: 19734
Epoch: [30]  [ 910/1251]  eta: 0:02:44  lr: 0.000016  loss: 3.7797 (3.4139)  time: 0.4507  data: 0.0005  max mem: 19734
Epoch: [30]  [ 920/1251]  eta: 0:02:39  lr: 0.000016  loss: 3.5186 (3.4114)  time: 0.4666  data: 0.0005  max mem: 19734
Epoch: [30]  [ 930/1251]  eta: 0:02:34  lr: 0.000016  loss: 3.0839 (3.4105)  time: 0.4769  data: 0.0005  max mem: 19734
Epoch: [30]  [ 940/1251]  eta: 0:02:29  lr: 0.000016  loss: 3.3546 (3.4103)  time: 0.4724  data: 0.0005  max mem: 19734
Epoch: [30]  [ 950/1251]  eta: 0:02:24  lr: 0.000016  loss: 3.4602 (3.4114)  time: 0.4803  data: 0.0004  max mem: 19734
Epoch: [30]  [ 960/1251]  eta: 0:02:19  lr: 0.000016  loss: 3.4895 (3.4104)  time: 0.4687  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3644, ratio_loss=0.0053, pruning_loss=0.1293, mse_loss=0.4671
Epoch: [30]  [ 970/1251]  eta: 0:02:15  lr: 0.000016  loss: 3.2831 (3.4099)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [30]  [ 980/1251]  eta: 0:02:10  lr: 0.000016  loss: 3.6389 (3.4109)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [30]  [ 990/1251]  eta: 0:02:05  lr: 0.000016  loss: 3.8298 (3.4144)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [30]  [1000/1251]  eta: 0:02:00  lr: 0.000016  loss: 3.6432 (3.4141)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [30]  [1010/1251]  eta: 0:01:55  lr: 0.000016  loss: 3.2585 (3.4124)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [30]  [1020/1251]  eta: 0:01:50  lr: 0.000016  loss: 3.1435 (3.4094)  time: 0.4622  data: 0.0004  max mem: 19734
Epoch: [30]  [1030/1251]  eta: 0:01:45  lr: 0.000016  loss: 3.1700 (3.4087)  time: 0.4615  data: 0.0004  max mem: 19734
Epoch: [30]  [1040/1251]  eta: 0:01:41  lr: 0.000016  loss: 3.3765 (3.4088)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [30]  [1050/1251]  eta: 0:01:36  lr: 0.000016  loss: 3.5055 (3.4101)  time: 0.4566  data: 0.0007  max mem: 19734
Epoch: [30]  [1060/1251]  eta: 0:01:31  lr: 0.000016  loss: 3.6165 (3.4099)  time: 0.4560  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3747, ratio_loss=0.0052, pruning_loss=0.1284, mse_loss=0.4592
Epoch: [30]  [1070/1251]  eta: 0:01:26  lr: 0.000016  loss: 3.4982 (3.4089)  time: 0.4749  data: 0.0004  max mem: 19734
Epoch: [30]  [1080/1251]  eta: 0:01:21  lr: 0.000016  loss: 3.3170 (3.4086)  time: 0.4889  data: 0.0005  max mem: 19734
Epoch: [30]  [1090/1251]  eta: 0:01:17  lr: 0.000016  loss: 3.6243 (3.4098)  time: 0.4776  data: 0.0005  max mem: 19734
Epoch: [30]  [1100/1251]  eta: 0:01:12  lr: 0.000016  loss: 3.5178 (3.4085)  time: 0.4836  data: 0.0005  max mem: 19734
Epoch: [30]  [1110/1251]  eta: 0:01:07  lr: 0.000016  loss: 3.3952 (3.4092)  time: 0.4754  data: 0.0006  max mem: 19734
Epoch: [30]  [1120/1251]  eta: 0:01:02  lr: 0.000016  loss: 3.4799 (3.4085)  time: 0.4580  data: 0.0006  max mem: 19734
Epoch: [30]  [1130/1251]  eta: 0:00:57  lr: 0.000016  loss: 3.3886 (3.4079)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [30]  [1140/1251]  eta: 0:00:53  lr: 0.000016  loss: 3.2926 (3.4068)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [30]  [1150/1251]  eta: 0:00:48  lr: 0.000016  loss: 3.3607 (3.4066)  time: 0.4513  data: 0.0005  max mem: 19734
Epoch: [30]  [1160/1251]  eta: 0:00:43  lr: 0.000016  loss: 3.4652 (3.4072)  time: 0.4507  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3727, ratio_loss=0.0054, pruning_loss=0.1284, mse_loss=0.4419
Epoch: [30]  [1170/1251]  eta: 0:00:38  lr: 0.000016  loss: 3.4800 (3.4071)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [30]  [1180/1251]  eta: 0:00:33  lr: 0.000016  loss: 3.5070 (3.4082)  time: 0.4586  data: 0.0005  max mem: 19734
Epoch: [30]  [1190/1251]  eta: 0:00:29  lr: 0.000016  loss: 3.5070 (3.4069)  time: 0.4506  data: 0.0006  max mem: 19734
Epoch: [30]  [1200/1251]  eta: 0:00:24  lr: 0.000016  loss: 3.5015 (3.4072)  time: 0.4483  data: 0.0005  max mem: 19734
Epoch: [30]  [1210/1251]  eta: 0:00:19  lr: 0.000016  loss: 3.5544 (3.4091)  time: 0.4559  data: 0.0002  max mem: 19734
Epoch: [30]  [1220/1251]  eta: 0:00:14  lr: 0.000016  loss: 3.6666 (3.4095)  time: 0.4739  data: 0.0002  max mem: 19734
Epoch: [30]  [1230/1251]  eta: 0:00:10  lr: 0.000016  loss: 3.5147 (3.4080)  time: 0.4752  data: 0.0002  max mem: 19734
Epoch: [30]  [1240/1251]  eta: 0:00:05  lr: 0.000016  loss: 3.2361 (3.4061)  time: 0.4569  data: 0.0002  max mem: 19734
Epoch: [30]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.3380 (3.4064)  time: 0.4630  data: 0.0001  max mem: 19734
Epoch: [30] Total time: 0:09:56 (0.4769 s / it)
Averaged stats: lr: 0.000016  loss: 3.3380 (3.4219)
Test:  [  0/261]  eta: 1:17:21  loss: 0.7096 (0.7096)  acc1: 84.3750 (84.3750)  acc5: 95.3125 (95.3125)  time: 17.7823  data: 17.5984  max mem: 19734
Test:  [ 10/261]  eta: 0:10:54  loss: 0.7096 (0.7280)  acc1: 84.3750 (83.8068)  acc5: 96.8750 (96.3068)  time: 2.6092  data: 2.4714  max mem: 19734
Test:  [ 20/261]  eta: 0:05:43  loss: 0.9334 (0.9023)  acc1: 78.6458 (79.4147)  acc5: 94.2708 (94.7669)  time: 0.6090  data: 0.4872  max mem: 19734
Test:  [ 30/261]  eta: 0:03:54  loss: 0.8118 (0.8176)  acc1: 83.3333 (82.2581)  acc5: 93.2292 (95.1109)  time: 0.1357  data: 0.0143  max mem: 19734
Test:  [ 40/261]  eta: 0:03:35  loss: 0.5493 (0.7828)  acc1: 88.5417 (83.1301)  acc5: 96.3542 (95.4395)  time: 0.5031  data: 0.3612  max mem: 19734
Test:  [ 50/261]  eta: 0:02:53  loss: 0.9748 (0.8492)  acc1: 77.6042 (81.0662)  acc5: 94.7917 (95.0368)  time: 0.5272  data: 0.3605  max mem: 19734
Test:  [ 60/261]  eta: 0:02:23  loss: 0.9754 (0.8560)  acc1: 76.0417 (80.6267)  acc5: 94.7917 (95.1417)  time: 0.1766  data: 0.0129  max mem: 19734
Test:  [ 70/261]  eta: 0:02:03  loss: 0.9292 (0.8568)  acc1: 77.0833 (80.1643)  acc5: 95.8333 (95.2978)  time: 0.2030  data: 0.0786  max mem: 19734
Test:  [ 80/261]  eta: 0:01:53  loss: 0.8472 (0.8594)  acc1: 78.6458 (80.2276)  acc5: 96.8750 (95.4090)  time: 0.3568  data: 0.2118  max mem: 19734
Test:  [ 90/261]  eta: 0:01:39  loss: 0.8409 (0.8480)  acc1: 82.8125 (80.5689)  acc5: 95.8333 (95.4785)  time: 0.3566  data: 0.1478  max mem: 19734
Test:  [100/261]  eta: 0:01:31  loss: 0.8409 (0.8516)  acc1: 83.3333 (80.5642)  acc5: 95.3125 (95.5188)  time: 0.3329  data: 0.1509  max mem: 19734
Test:  [110/261]  eta: 0:01:21  loss: 0.8768 (0.8746)  acc1: 77.6042 (80.1614)  acc5: 94.7917 (95.1858)  time: 0.3453  data: 0.1660  max mem: 19734
Test:  [120/261]  eta: 0:01:12  loss: 1.2031 (0.9143)  acc1: 72.9167 (79.2700)  acc5: 89.0625 (94.6281)  time: 0.2571  data: 0.0306  max mem: 19734
Test:  [130/261]  eta: 0:01:05  loss: 1.4303 (0.9599)  acc1: 66.6667 (78.2840)  acc5: 87.5000 (94.0323)  time: 0.2510  data: 0.0444  max mem: 19734
Test:  [140/261]  eta: 0:01:01  loss: 1.3502 (0.9868)  acc1: 67.7083 (77.5857)  acc5: 90.1042 (93.7722)  time: 0.4781  data: 0.3105  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: 1.2476 (0.9920)  acc1: 71.8750 (77.5938)  acc5: 91.1458 (93.6396)  time: 0.4812  data: 0.2855  max mem: 19734
Test:  [160/261]  eta: 0:00:48  loss: 1.0373 (1.0116)  acc1: 78.1250 (77.2548)  acc5: 91.6667 (93.3683)  time: 0.2695  data: 0.0215  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: 1.2725 (1.0412)  acc1: 63.5417 (76.4955)  acc5: 88.0208 (93.0251)  time: 0.3850  data: 0.1886  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: 1.4814 (1.0578)  acc1: 64.0625 (76.0704)  acc5: 88.5417 (92.8983)  time: 0.2967  data: 0.1832  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: 1.3599 (1.0714)  acc1: 67.1875 (75.7908)  acc5: 91.6667 (92.7492)  time: 0.0996  data: 0.0127  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.3599 (1.0863)  acc1: 70.8333 (75.5053)  acc5: 89.5833 (92.5140)  time: 0.0920  data: 0.0140  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.3571 (1.0997)  acc1: 69.2708 (75.2542)  acc5: 87.5000 (92.3085)  time: 0.1151  data: 0.0418  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4234 (1.1187)  acc1: 68.2292 (74.8115)  acc5: 87.5000 (92.1027)  time: 0.1377  data: 0.0688  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4513 (1.1287)  acc1: 66.1458 (74.5288)  acc5: 89.0625 (91.9981)  time: 0.0947  data: 0.0332  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3310 (1.1384)  acc1: 66.6667 (74.2847)  acc5: 91.1458 (91.9174)  time: 0.0716  data: 0.0100  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0073 (1.1310)  acc1: 76.0417 (74.4605)  acc5: 93.7500 (92.0443)  time: 0.0715  data: 0.0100  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9589 (1.1307)  acc1: 77.6042 (74.4640)  acc5: 95.3125 (92.1020)  time: 0.0602  data: 0.0002  max mem: 19734
Test: Total time: 0:01:30 (0.3482 s / it)
* Acc@1 74.464 Acc@5 92.102 loss 1.131
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.62%
Epoch: [31]  [   0/1251]  eta: 5:38:32  lr: 0.000016  loss: 2.6244 (2.6244)  time: 16.2373  data: 15.4210  max mem: 19734
Epoch: [31]  [  10/1251]  eta: 0:43:30  lr: 0.000016  loss: 3.7621 (3.5071)  time: 2.1039  data: 1.4025  max mem: 19734
loss info: cls_loss=3.3757, ratio_loss=0.0053, pruning_loss=0.1286, mse_loss=0.4522
Epoch: [31]  [  20/1251]  eta: 0:27:04  lr: 0.000016  loss: 3.5741 (3.4485)  time: 0.5734  data: 0.0005  max mem: 19734
Epoch: [31]  [  30/1251]  eta: 0:21:12  lr: 0.000016  loss: 3.5163 (3.3957)  time: 0.4577  data: 0.0004  max mem: 19734
Epoch: [31]  [  40/1251]  eta: 0:18:12  lr: 0.000016  loss: 3.1927 (3.3246)  time: 0.4640  data: 0.0005  max mem: 19734
Epoch: [31]  [  50/1251]  eta: 0:16:18  lr: 0.000016  loss: 3.1739 (3.2897)  time: 0.4624  data: 0.0005  max mem: 19734
Epoch: [31]  [  60/1251]  eta: 0:15:00  lr: 0.000016  loss: 3.1739 (3.2741)  time: 0.4568  data: 0.0010  max mem: 19734
Epoch: [31]  [  70/1251]  eta: 0:14:03  lr: 0.000016  loss: 3.2728 (3.2621)  time: 0.4585  data: 0.0010  max mem: 19734
Epoch: [31]  [  80/1251]  eta: 0:13:20  lr: 0.000016  loss: 3.2728 (3.2618)  time: 0.4628  data: 0.0006  max mem: 19734
Epoch: [31]  [  90/1251]  eta: 0:12:46  lr: 0.000016  loss: 3.3264 (3.2950)  time: 0.4663  data: 0.0006  max mem: 19734
Epoch: [31]  [ 100/1251]  eta: 0:12:21  lr: 0.000016  loss: 3.7449 (3.3331)  time: 0.4839  data: 0.0005  max mem: 19734
Epoch: [31]  [ 110/1251]  eta: 0:11:59  lr: 0.000016  loss: 3.6210 (3.3566)  time: 0.4979  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3202, ratio_loss=0.0053, pruning_loss=0.1306, mse_loss=0.4577
Epoch: [31]  [ 120/1251]  eta: 0:11:39  lr: 0.000016  loss: 3.6171 (3.3795)  time: 0.4882  data: 0.0005  max mem: 19734
Epoch: [31]  [ 130/1251]  eta: 0:11:21  lr: 0.000016  loss: 3.5341 (3.3813)  time: 0.4839  data: 0.0004  max mem: 19734
Epoch: [31]  [ 140/1251]  eta: 0:11:03  lr: 0.000016  loss: 3.4266 (3.3671)  time: 0.4714  data: 0.0004  max mem: 19734
Epoch: [31]  [ 150/1251]  eta: 0:10:47  lr: 0.000016  loss: 3.3159 (3.3691)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [31]  [ 160/1251]  eta: 0:10:33  lr: 0.000016  loss: 3.2441 (3.3582)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [31]  [ 170/1251]  eta: 0:10:19  lr: 0.000016  loss: 3.4730 (3.3743)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [31]  [ 180/1251]  eta: 0:10:08  lr: 0.000016  loss: 3.7946 (3.3878)  time: 0.4713  data: 0.0004  max mem: 19734
Epoch: [31]  [ 190/1251]  eta: 0:09:56  lr: 0.000016  loss: 3.6224 (3.3744)  time: 0.4716  data: 0.0004  max mem: 19734
Epoch: [31]  [ 200/1251]  eta: 0:09:45  lr: 0.000016  loss: 3.3588 (3.3861)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [31]  [ 210/1251]  eta: 0:09:35  lr: 0.000016  loss: 3.6681 (3.3933)  time: 0.4615  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3967, ratio_loss=0.0051, pruning_loss=0.1279, mse_loss=0.4533
Epoch: [31]  [ 220/1251]  eta: 0:09:25  lr: 0.000016  loss: 3.5513 (3.3897)  time: 0.4610  data: 0.0004  max mem: 19734
Epoch: [31]  [ 230/1251]  eta: 0:09:16  lr: 0.000016  loss: 3.1295 (3.3797)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [31]  [ 240/1251]  eta: 0:09:08  lr: 0.000016  loss: 3.1295 (3.3782)  time: 0.4702  data: 0.0004  max mem: 19734
Epoch: [31]  [ 250/1251]  eta: 0:09:00  lr: 0.000016  loss: 3.6026 (3.3816)  time: 0.4873  data: 0.0004  max mem: 19734
Epoch: [31]  [ 260/1251]  eta: 0:08:52  lr: 0.000016  loss: 3.6026 (3.3825)  time: 0.4841  data: 0.0004  max mem: 19734
Epoch: [31]  [ 270/1251]  eta: 0:08:44  lr: 0.000016  loss: 3.7616 (3.3914)  time: 0.4673  data: 0.0004  max mem: 19734
Epoch: [31]  [ 280/1251]  eta: 0:08:38  lr: 0.000016  loss: 3.6027 (3.3895)  time: 0.4794  data: 0.0004  max mem: 19734
Epoch: [31]  [ 290/1251]  eta: 0:08:30  lr: 0.000016  loss: 3.6027 (3.3942)  time: 0.4777  data: 0.0005  max mem: 19734
Epoch: [31]  [ 300/1251]  eta: 0:08:22  lr: 0.000016  loss: 3.6143 (3.4043)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [31]  [ 310/1251]  eta: 0:08:15  lr: 0.000016  loss: 3.6706 (3.4105)  time: 0.4554  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4143, ratio_loss=0.0050, pruning_loss=0.1286, mse_loss=0.4505
Epoch: [31]  [ 320/1251]  eta: 0:08:07  lr: 0.000016  loss: 3.6036 (3.4060)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [31]  [ 330/1251]  eta: 0:08:01  lr: 0.000016  loss: 3.5051 (3.4143)  time: 0.4622  data: 0.0007  max mem: 19734
Epoch: [31]  [ 340/1251]  eta: 0:07:54  lr: 0.000016  loss: 3.8158 (3.4191)  time: 0.4617  data: 0.0007  max mem: 19734
Epoch: [31]  [ 350/1251]  eta: 0:07:47  lr: 0.000016  loss: 3.5555 (3.4160)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [31]  [ 360/1251]  eta: 0:07:40  lr: 0.000016  loss: 3.3772 (3.4121)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [31]  [ 370/1251]  eta: 0:07:33  lr: 0.000016  loss: 3.2208 (3.4061)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [31]  [ 380/1251]  eta: 0:07:27  lr: 0.000016  loss: 3.4474 (3.4050)  time: 0.4633  data: 0.0005  max mem: 19734
Epoch: [31]  [ 390/1251]  eta: 0:07:21  lr: 0.000016  loss: 3.2811 (3.4017)  time: 0.4724  data: 0.0005  max mem: 19734
Epoch: [31]  [ 400/1251]  eta: 0:07:15  lr: 0.000016  loss: 3.3760 (3.4122)  time: 0.4736  data: 0.0005  max mem: 19734
Epoch: [31]  [ 410/1251]  eta: 0:07:09  lr: 0.000016  loss: 3.5172 (3.4122)  time: 0.4739  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4270, ratio_loss=0.0052, pruning_loss=0.1282, mse_loss=0.4544
Epoch: [31]  [ 420/1251]  eta: 0:07:04  lr: 0.000016  loss: 3.5172 (3.4163)  time: 0.4842  data: 0.0005  max mem: 19734
Epoch: [31]  [ 430/1251]  eta: 0:06:58  lr: 0.000016  loss: 3.6265 (3.4186)  time: 0.4770  data: 0.0005  max mem: 19734
Epoch: [31]  [ 440/1251]  eta: 0:06:52  lr: 0.000016  loss: 3.6145 (3.4178)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [31]  [ 450/1251]  eta: 0:06:46  lr: 0.000016  loss: 3.5812 (3.4191)  time: 0.4604  data: 0.0004  max mem: 19734
Epoch: [31]  [ 460/1251]  eta: 0:06:40  lr: 0.000016  loss: 3.5168 (3.4160)  time: 0.4593  data: 0.0004  max mem: 19734
Epoch: [31]  [ 470/1251]  eta: 0:06:34  lr: 0.000016  loss: 3.2749 (3.4143)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [31]  [ 480/1251]  eta: 0:06:28  lr: 0.000016  loss: 3.6140 (3.4191)  time: 0.4653  data: 0.0004  max mem: 19734
Epoch: [31]  [ 490/1251]  eta: 0:06:22  lr: 0.000016  loss: 3.5628 (3.4160)  time: 0.4643  data: 0.0004  max mem: 19734
Epoch: [31]  [ 500/1251]  eta: 0:06:17  lr: 0.000016  loss: 3.4168 (3.4155)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [31]  [ 510/1251]  eta: 0:06:11  lr: 0.000016  loss: 3.3021 (3.4127)  time: 0.4570  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3573, ratio_loss=0.0053, pruning_loss=0.1303, mse_loss=0.4484
Epoch: [31]  [ 520/1251]  eta: 0:06:05  lr: 0.000016  loss: 3.2300 (3.4116)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [31]  [ 530/1251]  eta: 0:06:00  lr: 0.000016  loss: 3.3157 (3.4133)  time: 0.4742  data: 0.0008  max mem: 19734
Epoch: [31]  [ 540/1251]  eta: 0:05:56  lr: 0.000016  loss: 3.5235 (3.4164)  time: 0.5055  data: 0.0008  max mem: 19734
Epoch: [31]  [ 550/1251]  eta: 0:05:50  lr: 0.000016  loss: 3.2919 (3.4094)  time: 0.4996  data: 0.0004  max mem: 19734
Epoch: [31]  [ 560/1251]  eta: 0:05:45  lr: 0.000016  loss: 3.0065 (3.4081)  time: 0.4832  data: 0.0004  max mem: 19734
Epoch: [31]  [ 570/1251]  eta: 0:05:40  lr: 0.000016  loss: 3.4395 (3.4092)  time: 0.4742  data: 0.0004  max mem: 19734
Epoch: [31]  [ 580/1251]  eta: 0:05:34  lr: 0.000016  loss: 3.6297 (3.4126)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [31]  [ 590/1251]  eta: 0:05:29  lr: 0.000016  loss: 3.5618 (3.4122)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [31]  [ 600/1251]  eta: 0:05:23  lr: 0.000016  loss: 3.5153 (3.4119)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [31]  [ 610/1251]  eta: 0:05:18  lr: 0.000016  loss: 3.4653 (3.4124)  time: 0.4548  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4305, ratio_loss=0.0051, pruning_loss=0.1280, mse_loss=0.4360
Epoch: [31]  [ 620/1251]  eta: 0:05:13  lr: 0.000016  loss: 3.7655 (3.4198)  time: 0.4657  data: 0.0004  max mem: 19734
Epoch: [31]  [ 630/1251]  eta: 0:05:07  lr: 0.000016  loss: 3.7655 (3.4217)  time: 0.4670  data: 0.0004  max mem: 19734
Epoch: [31]  [ 640/1251]  eta: 0:05:02  lr: 0.000016  loss: 3.5916 (3.4221)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [31]  [ 650/1251]  eta: 0:04:57  lr: 0.000016  loss: 3.7395 (3.4225)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [31]  [ 660/1251]  eta: 0:04:51  lr: 0.000016  loss: 3.6644 (3.4229)  time: 0.4552  data: 0.0005  max mem: 19734
Epoch: [31]  [ 670/1251]  eta: 0:04:46  lr: 0.000016  loss: 3.6132 (3.4228)  time: 0.4680  data: 0.0004  max mem: 19734
Epoch: [31]  [ 680/1251]  eta: 0:04:41  lr: 0.000016  loss: 3.5160 (3.4218)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [31]  [ 690/1251]  eta: 0:04:36  lr: 0.000016  loss: 3.4102 (3.4223)  time: 0.4885  data: 0.0006  max mem: 19734
Epoch: [31]  [ 700/1251]  eta: 0:04:31  lr: 0.000016  loss: 3.5983 (3.4244)  time: 0.4765  data: 0.0007  max mem: 19734
Epoch: [31]  [ 710/1251]  eta: 0:04:26  lr: 0.000016  loss: 3.5715 (3.4220)  time: 0.4669  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4353, ratio_loss=0.0049, pruning_loss=0.1269, mse_loss=0.4580
Epoch: [31]  [ 720/1251]  eta: 0:04:21  lr: 0.000016  loss: 3.5400 (3.4246)  time: 0.4660  data: 0.0005  max mem: 19734
Epoch: [31]  [ 730/1251]  eta: 0:04:16  lr: 0.000016  loss: 3.6580 (3.4247)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [31]  [ 740/1251]  eta: 0:04:10  lr: 0.000016  loss: 3.6802 (3.4293)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [31]  [ 750/1251]  eta: 0:04:05  lr: 0.000016  loss: 3.7027 (3.4308)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [31]  [ 760/1251]  eta: 0:04:00  lr: 0.000016  loss: 3.3357 (3.4271)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [31]  [ 770/1251]  eta: 0:03:55  lr: 0.000016  loss: 3.1899 (3.4249)  time: 0.4662  data: 0.0006  max mem: 19734
Epoch: [31]  [ 780/1251]  eta: 0:03:50  lr: 0.000016  loss: 3.2513 (3.4233)  time: 0.4650  data: 0.0009  max mem: 19734
Epoch: [31]  [ 790/1251]  eta: 0:03:45  lr: 0.000016  loss: 3.5188 (3.4249)  time: 0.4510  data: 0.0008  max mem: 19734
Epoch: [31]  [ 800/1251]  eta: 0:03:40  lr: 0.000016  loss: 3.5798 (3.4273)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [31]  [ 810/1251]  eta: 0:03:35  lr: 0.000016  loss: 3.5798 (3.4296)  time: 0.4525  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4557, ratio_loss=0.0050, pruning_loss=0.1252, mse_loss=0.4517
Epoch: [31]  [ 820/1251]  eta: 0:03:30  lr: 0.000016  loss: 3.6443 (3.4312)  time: 0.4618  data: 0.0005  max mem: 19734
Epoch: [31]  [ 830/1251]  eta: 0:03:25  lr: 0.000016  loss: 3.5858 (3.4306)  time: 0.4866  data: 0.0008  max mem: 19734
Epoch: [31]  [ 840/1251]  eta: 0:03:20  lr: 0.000016  loss: 3.5468 (3.4327)  time: 0.4870  data: 0.0008  max mem: 19734
Epoch: [31]  [ 850/1251]  eta: 0:03:15  lr: 0.000016  loss: 3.5468 (3.4345)  time: 0.4725  data: 0.0006  max mem: 19734
Epoch: [31]  [ 860/1251]  eta: 0:03:10  lr: 0.000016  loss: 3.4451 (3.4329)  time: 0.4640  data: 0.0005  max mem: 19734
Epoch: [31]  [ 870/1251]  eta: 0:03:05  lr: 0.000016  loss: 3.6302 (3.4351)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [31]  [ 880/1251]  eta: 0:03:00  lr: 0.000016  loss: 3.7342 (3.4357)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [31]  [ 890/1251]  eta: 0:02:55  lr: 0.000016  loss: 3.4616 (3.4345)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [31]  [ 900/1251]  eta: 0:02:50  lr: 0.000016  loss: 3.5725 (3.4352)  time: 0.4538  data: 0.0006  max mem: 19734
Epoch: [31]  [ 910/1251]  eta: 0:02:45  lr: 0.000016  loss: 3.6576 (3.4371)  time: 0.4544  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4728, ratio_loss=0.0048, pruning_loss=0.1253, mse_loss=0.4673
Epoch: [31]  [ 920/1251]  eta: 0:02:40  lr: 0.000016  loss: 3.5366 (3.4370)  time: 0.4637  data: 0.0004  max mem: 19734
Epoch: [31]  [ 930/1251]  eta: 0:02:35  lr: 0.000016  loss: 3.4314 (3.4362)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [31]  [ 940/1251]  eta: 0:02:30  lr: 0.000016  loss: 3.5722 (3.4369)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [31]  [ 950/1251]  eta: 0:02:25  lr: 0.000016  loss: 3.6941 (3.4385)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [31]  [ 960/1251]  eta: 0:02:20  lr: 0.000016  loss: 3.7144 (3.4390)  time: 0.4651  data: 0.0005  max mem: 19734
Epoch: [31]  [ 970/1251]  eta: 0:02:16  lr: 0.000016  loss: 3.6150 (3.4380)  time: 0.4727  data: 0.0004  max mem: 19734
Epoch: [31]  [ 980/1251]  eta: 0:02:11  lr: 0.000016  loss: 3.6150 (3.4393)  time: 0.4891  data: 0.0005  max mem: 19734
Epoch: [31]  [ 990/1251]  eta: 0:02:06  lr: 0.000016  loss: 3.5354 (3.4379)  time: 0.4814  data: 0.0004  max mem: 19734
Epoch: [31]  [1000/1251]  eta: 0:02:01  lr: 0.000016  loss: 3.1337 (3.4351)  time: 0.4718  data: 0.0004  max mem: 19734
Epoch: [31]  [1010/1251]  eta: 0:01:56  lr: 0.000016  loss: 3.3602 (3.4333)  time: 0.4713  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3599, ratio_loss=0.0051, pruning_loss=0.1279, mse_loss=0.4481
Epoch: [31]  [1020/1251]  eta: 0:01:51  lr: 0.000016  loss: 3.4081 (3.4323)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [31]  [1030/1251]  eta: 0:01:46  lr: 0.000016  loss: 3.4081 (3.4326)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [31]  [1040/1251]  eta: 0:01:41  lr: 0.000016  loss: 3.4587 (3.4341)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [31]  [1050/1251]  eta: 0:01:37  lr: 0.000016  loss: 3.6084 (3.4343)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [31]  [1060/1251]  eta: 0:01:32  lr: 0.000016  loss: 3.4868 (3.4329)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [31]  [1070/1251]  eta: 0:01:27  lr: 0.000016  loss: 3.4474 (3.4320)  time: 0.4641  data: 0.0007  max mem: 19734
Epoch: [31]  [1080/1251]  eta: 0:01:22  lr: 0.000016  loss: 3.5323 (3.4327)  time: 0.4631  data: 0.0007  max mem: 19734
Epoch: [31]  [1090/1251]  eta: 0:01:17  lr: 0.000016  loss: 3.5838 (3.4328)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [31]  [1100/1251]  eta: 0:01:12  lr: 0.000016  loss: 3.4147 (3.4315)  time: 0.4616  data: 0.0005  max mem: 19734
Epoch: [31]  [1110/1251]  eta: 0:01:07  lr: 0.000016  loss: 3.4147 (3.4300)  time: 0.4615  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3750, ratio_loss=0.0051, pruning_loss=0.1272, mse_loss=0.4646
Epoch: [31]  [1120/1251]  eta: 0:01:03  lr: 0.000016  loss: 3.3984 (3.4293)  time: 0.4721  data: 0.0004  max mem: 19734
Epoch: [31]  [1130/1251]  eta: 0:00:58  lr: 0.000016  loss: 3.4698 (3.4290)  time: 0.4774  data: 0.0004  max mem: 19734
Epoch: [31]  [1140/1251]  eta: 0:00:53  lr: 0.000016  loss: 3.5576 (3.4292)  time: 0.4767  data: 0.0004  max mem: 19734
Epoch: [31]  [1150/1251]  eta: 0:00:48  lr: 0.000016  loss: 3.4130 (3.4282)  time: 0.4709  data: 0.0006  max mem: 19734
Epoch: [31]  [1160/1251]  eta: 0:00:43  lr: 0.000016  loss: 3.5912 (3.4301)  time: 0.4531  data: 0.0006  max mem: 19734
Epoch: [31]  [1170/1251]  eta: 0:00:38  lr: 0.000016  loss: 3.6492 (3.4303)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [31]  [1180/1251]  eta: 0:00:34  lr: 0.000016  loss: 3.2159 (3.4274)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [31]  [1190/1251]  eta: 0:00:29  lr: 0.000016  loss: 3.0248 (3.4276)  time: 0.4502  data: 0.0009  max mem: 19734
Epoch: [31]  [1200/1251]  eta: 0:00:24  lr: 0.000016  loss: 3.6453 (3.4286)  time: 0.4465  data: 0.0007  max mem: 19734
Epoch: [31]  [1210/1251]  eta: 0:00:19  lr: 0.000016  loss: 3.7294 (3.4311)  time: 0.4439  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4060, ratio_loss=0.0053, pruning_loss=0.1283, mse_loss=0.4495
Epoch: [31]  [1220/1251]  eta: 0:00:14  lr: 0.000016  loss: 3.6163 (3.4301)  time: 0.4527  data: 0.0002  max mem: 19734
Epoch: [31]  [1230/1251]  eta: 0:00:10  lr: 0.000016  loss: 3.6098 (3.4308)  time: 0.4525  data: 0.0002  max mem: 19734
Epoch: [31]  [1240/1251]  eta: 0:00:05  lr: 0.000016  loss: 3.3663 (3.4291)  time: 0.4435  data: 0.0002  max mem: 19734
Epoch: [31]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.1154 (3.4274)  time: 0.4549  data: 0.0002  max mem: 19734
Epoch: [31] Total time: 0:09:59 (0.4794 s / it)
Averaged stats: lr: 0.000016  loss: 3.1154 (3.4282)
Test:  [  0/261]  eta: 1:33:10  loss: 0.6887 (0.6887)  acc1: 83.8542 (83.8542)  acc5: 95.8333 (95.8333)  time: 21.4178  data: 21.0914  max mem: 19734
Test:  [ 10/261]  eta: 0:13:41  loss: 0.6887 (0.7133)  acc1: 86.4583 (84.5170)  acc5: 96.8750 (96.3542)  time: 3.2712  data: 3.0621  max mem: 19734
Test:  [ 20/261]  eta: 0:07:15  loss: 0.9058 (0.8940)  acc1: 80.2083 (79.4147)  acc5: 94.7917 (94.9653)  time: 0.8259  data: 0.6355  max mem: 19734
Test:  [ 30/261]  eta: 0:05:03  loss: 0.8025 (0.8104)  acc1: 81.7708 (82.1909)  acc5: 94.7917 (95.4133)  time: 0.2393  data: 0.0125  max mem: 19734
Test:  [ 40/261]  eta: 0:04:19  loss: 0.5609 (0.7792)  acc1: 88.0208 (83.1555)  acc5: 96.8750 (95.6936)  time: 0.5137  data: 0.2603  max mem: 19734
Test:  [ 50/261]  eta: 0:03:34  loss: 0.9102 (0.8411)  acc1: 78.1250 (81.2398)  acc5: 95.3125 (95.2614)  time: 0.5533  data: 0.2602  max mem: 19734
Test:  [ 60/261]  eta: 0:03:00  loss: 0.9844 (0.8510)  acc1: 76.5625 (80.8060)  acc5: 93.7500 (95.2357)  time: 0.3219  data: 0.0118  max mem: 19734
Test:  [ 70/261]  eta: 0:02:36  loss: 0.9523 (0.8508)  acc1: 77.6042 (80.4137)  acc5: 95.8333 (95.4445)  time: 0.3126  data: 0.1160  max mem: 19734
Test:  [ 80/261]  eta: 0:02:12  loss: 0.8570 (0.8521)  acc1: 80.2083 (80.4784)  acc5: 97.3958 (95.5247)  time: 0.2399  data: 0.1173  max mem: 19734
Test:  [ 90/261]  eta: 0:01:53  loss: 0.8247 (0.8407)  acc1: 83.3333 (80.7750)  acc5: 95.8333 (95.5586)  time: 0.1263  data: 0.0119  max mem: 19734
Test:  [100/261]  eta: 0:01:39  loss: 0.8268 (0.8459)  acc1: 83.3333 (80.6467)  acc5: 94.7917 (95.5858)  time: 0.1630  data: 0.0640  max mem: 19734
Test:  [110/261]  eta: 0:01:26  loss: 0.8782 (0.8712)  acc1: 77.0833 (80.1380)  acc5: 94.2708 (95.2468)  time: 0.1532  data: 0.0642  max mem: 19734
Test:  [120/261]  eta: 0:01:16  loss: 1.2276 (0.9104)  acc1: 70.8333 (79.1796)  acc5: 90.6250 (94.7615)  time: 0.1583  data: 0.0646  max mem: 19734
Test:  [130/261]  eta: 0:01:09  loss: 1.3628 (0.9550)  acc1: 68.2292 (78.2642)  acc5: 88.0208 (94.1436)  time: 0.2786  data: 0.1792  max mem: 19734
Test:  [140/261]  eta: 0:01:03  loss: 1.3011 (0.9829)  acc1: 68.2292 (77.5857)  acc5: 89.0625 (93.8423)  time: 0.4059  data: 0.2970  max mem: 19734
Test:  [150/261]  eta: 0:01:01  loss: 1.2700 (0.9876)  acc1: 71.8750 (77.5904)  acc5: 91.1458 (93.7086)  time: 0.7075  data: 0.5965  max mem: 19734
Test:  [160/261]  eta: 0:00:53  loss: 1.0050 (1.0076)  acc1: 78.1250 (77.2580)  acc5: 92.1875 (93.3942)  time: 0.5490  data: 0.4270  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.3627 (1.0387)  acc1: 65.6250 (76.4711)  acc5: 87.5000 (93.0647)  time: 0.1282  data: 0.0124  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4712 (1.0574)  acc1: 64.0625 (76.0100)  acc5: 89.0625 (92.9155)  time: 0.1028  data: 0.0131  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3765 (1.0709)  acc1: 67.1875 (75.7553)  acc5: 91.1458 (92.7629)  time: 0.0991  data: 0.0094  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3599 (1.0857)  acc1: 71.3542 (75.4509)  acc5: 89.5833 (92.5321)  time: 0.1390  data: 0.0468  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3703 (1.1003)  acc1: 70.8333 (75.1753)  acc5: 88.5417 (92.3060)  time: 0.1249  data: 0.0456  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4411 (1.1191)  acc1: 66.6667 (74.7219)  acc5: 87.5000 (92.0932)  time: 0.0626  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4507 (1.1293)  acc1: 65.6250 (74.4679)  acc5: 89.0625 (91.9936)  time: 0.0697  data: 0.0082  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3247 (1.1388)  acc1: 67.7083 (74.2371)  acc5: 89.5833 (91.9109)  time: 0.0696  data: 0.0082  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0715 (1.1321)  acc1: 75.0000 (74.4231)  acc5: 93.7500 (92.0277)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9213 (1.1318)  acc1: 78.1250 (74.4220)  acc5: 94.7917 (92.0840)  time: 0.0602  data: 0.0002  max mem: 19734
Test: Total time: 0:01:33 (0.3597 s / it)
* Acc@1 74.421 Acc@5 92.084 loss 1.132
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.62%
Epoch: [32]  [   0/1251]  eta: 5:53:06  lr: 0.000016  loss: 3.2799 (3.2799)  time: 16.9360  data: 10.8688  max mem: 19734
Epoch: [32]  [  10/1251]  eta: 0:44:23  lr: 0.000016  loss: 3.4361 (3.2510)  time: 2.1464  data: 1.0187  max mem: 19734
Epoch: [32]  [  20/1251]  eta: 0:27:33  lr: 0.000016  loss: 3.4455 (3.3312)  time: 0.5639  data: 0.0171  max mem: 19734
Epoch: [32]  [  30/1251]  eta: 0:21:47  lr: 0.000016  loss: 3.4455 (3.3606)  time: 0.4798  data: 0.0004  max mem: 19734
Epoch: [32]  [  40/1251]  eta: 0:18:37  lr: 0.000016  loss: 3.4880 (3.4025)  time: 0.4806  data: 0.0005  max mem: 19734
Epoch: [32]  [  50/1251]  eta: 0:16:39  lr: 0.000016  loss: 3.6151 (3.4054)  time: 0.4609  data: 0.0006  max mem: 19734
Epoch: [32]  [  60/1251]  eta: 0:15:19  lr: 0.000016  loss: 3.6425 (3.4404)  time: 0.4622  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3674, ratio_loss=0.0055, pruning_loss=0.1281, mse_loss=0.4479
Epoch: [32]  [  70/1251]  eta: 0:14:19  lr: 0.000016  loss: 3.6043 (3.4179)  time: 0.4630  data: 0.0004  max mem: 19734
Epoch: [32]  [  80/1251]  eta: 0:13:36  lr: 0.000016  loss: 3.6043 (3.4196)  time: 0.4713  data: 0.0004  max mem: 19734
Epoch: [32]  [  90/1251]  eta: 0:12:59  lr: 0.000016  loss: 3.4144 (3.3955)  time: 0.4711  data: 0.0004  max mem: 19734
Epoch: [32]  [ 100/1251]  eta: 0:12:28  lr: 0.000016  loss: 3.4144 (3.3961)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [32]  [ 110/1251]  eta: 0:12:02  lr: 0.000016  loss: 3.4824 (3.4101)  time: 0.4601  data: 0.0005  max mem: 19734
Epoch: [32]  [ 120/1251]  eta: 0:11:40  lr: 0.000016  loss: 3.6601 (3.4217)  time: 0.4604  data: 0.0005  max mem: 19734
Epoch: [32]  [ 130/1251]  eta: 0:11:21  lr: 0.000016  loss: 3.6852 (3.4213)  time: 0.4662  data: 0.0004  max mem: 19734
Epoch: [32]  [ 140/1251]  eta: 0:11:03  lr: 0.000016  loss: 3.5015 (3.4258)  time: 0.4660  data: 0.0004  max mem: 19734
Epoch: [32]  [ 150/1251]  eta: 0:10:49  lr: 0.000016  loss: 3.4514 (3.4228)  time: 0.4750  data: 0.0004  max mem: 19734
Epoch: [32]  [ 160/1251]  eta: 0:10:36  lr: 0.000016  loss: 3.2427 (3.4035)  time: 0.4822  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3682, ratio_loss=0.0052, pruning_loss=0.1266, mse_loss=0.4494
Epoch: [32]  [ 170/1251]  eta: 0:10:23  lr: 0.000016  loss: 3.3157 (3.4099)  time: 0.4727  data: 0.0004  max mem: 19734
Epoch: [32]  [ 180/1251]  eta: 0:10:11  lr: 0.000016  loss: 3.4152 (3.4041)  time: 0.4716  data: 0.0004  max mem: 19734
Epoch: [32]  [ 190/1251]  eta: 0:09:58  lr: 0.000016  loss: 3.4203 (3.4086)  time: 0.4622  data: 0.0004  max mem: 19734
Epoch: [32]  [ 200/1251]  eta: 0:09:47  lr: 0.000016  loss: 3.5262 (3.4142)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [32]  [ 210/1251]  eta: 0:09:36  lr: 0.000016  loss: 3.3134 (3.3995)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [32]  [ 220/1251]  eta: 0:09:26  lr: 0.000016  loss: 3.3007 (3.3971)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [32]  [ 230/1251]  eta: 0:09:18  lr: 0.000016  loss: 3.6429 (3.4082)  time: 0.4661  data: 0.0005  max mem: 19734
Epoch: [32]  [ 240/1251]  eta: 0:09:08  lr: 0.000016  loss: 3.7639 (3.4243)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [32]  [ 250/1251]  eta: 0:08:59  lr: 0.000016  loss: 3.6999 (3.4321)  time: 0.4556  data: 0.0007  max mem: 19734
Epoch: [32]  [ 260/1251]  eta: 0:08:51  lr: 0.000016  loss: 3.6936 (3.4451)  time: 0.4564  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4815, ratio_loss=0.0053, pruning_loss=0.1257, mse_loss=0.4650
Epoch: [32]  [ 270/1251]  eta: 0:08:43  lr: 0.000016  loss: 3.6936 (3.4488)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [32]  [ 280/1251]  eta: 0:08:35  lr: 0.000016  loss: 3.7280 (3.4501)  time: 0.4676  data: 0.0005  max mem: 19734
Epoch: [32]  [ 290/1251]  eta: 0:08:28  lr: 0.000016  loss: 3.3317 (3.4440)  time: 0.4796  data: 0.0005  max mem: 19734
Epoch: [32]  [ 300/1251]  eta: 0:08:22  lr: 0.000016  loss: 3.3568 (3.4441)  time: 0.4889  data: 0.0005  max mem: 19734
Epoch: [32]  [ 310/1251]  eta: 0:08:15  lr: 0.000016  loss: 3.4584 (3.4395)  time: 0.4782  data: 0.0005  max mem: 19734
Epoch: [32]  [ 320/1251]  eta: 0:08:09  lr: 0.000016  loss: 3.3774 (3.4319)  time: 0.4799  data: 0.0005  max mem: 19734
Epoch: [32]  [ 330/1251]  eta: 0:08:01  lr: 0.000016  loss: 3.3940 (3.4285)  time: 0.4786  data: 0.0005  max mem: 19734
Epoch: [32]  [ 340/1251]  eta: 0:07:54  lr: 0.000016  loss: 3.4756 (3.4302)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [32]  [ 350/1251]  eta: 0:07:48  lr: 0.000016  loss: 3.6008 (3.4340)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [32]  [ 360/1251]  eta: 0:07:41  lr: 0.000016  loss: 3.6008 (3.4311)  time: 0.4590  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3914, ratio_loss=0.0053, pruning_loss=0.1263, mse_loss=0.4487
Epoch: [32]  [ 370/1251]  eta: 0:07:34  lr: 0.000016  loss: 3.4700 (3.4296)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [32]  [ 380/1251]  eta: 0:07:28  lr: 0.000016  loss: 3.4325 (3.4297)  time: 0.4679  data: 0.0004  max mem: 19734
Epoch: [32]  [ 390/1251]  eta: 0:07:22  lr: 0.000016  loss: 3.4325 (3.4306)  time: 0.4704  data: 0.0005  max mem: 19734
Epoch: [32]  [ 400/1251]  eta: 0:07:16  lr: 0.000016  loss: 3.3550 (3.4272)  time: 0.4583  data: 0.0005  max mem: 19734
Epoch: [32]  [ 410/1251]  eta: 0:07:09  lr: 0.000016  loss: 3.3522 (3.4275)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [32]  [ 420/1251]  eta: 0:07:03  lr: 0.000016  loss: 3.7042 (3.4324)  time: 0.4635  data: 0.0004  max mem: 19734
Epoch: [32]  [ 430/1251]  eta: 0:06:57  lr: 0.000016  loss: 3.6091 (3.4352)  time: 0.4628  data: 0.0004  max mem: 19734
Epoch: [32]  [ 440/1251]  eta: 0:06:52  lr: 0.000016  loss: 3.5968 (3.4381)  time: 0.4767  data: 0.0004  max mem: 19734
Epoch: [32]  [ 450/1251]  eta: 0:06:46  lr: 0.000016  loss: 3.4315 (3.4394)  time: 0.4845  data: 0.0005  max mem: 19734
Epoch: [32]  [ 460/1251]  eta: 0:06:41  lr: 0.000016  loss: 3.4315 (3.4394)  time: 0.4771  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4235, ratio_loss=0.0056, pruning_loss=0.1259, mse_loss=0.4522
Epoch: [32]  [ 470/1251]  eta: 0:06:35  lr: 0.000016  loss: 3.4141 (3.4396)  time: 0.4705  data: 0.0006  max mem: 19734
Epoch: [32]  [ 480/1251]  eta: 0:06:29  lr: 0.000016  loss: 3.5375 (3.4407)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [32]  [ 490/1251]  eta: 0:06:23  lr: 0.000016  loss: 3.6960 (3.4441)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [32]  [ 500/1251]  eta: 0:06:17  lr: 0.000016  loss: 3.6920 (3.4442)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [32]  [ 510/1251]  eta: 0:06:12  lr: 0.000016  loss: 3.1400 (3.4356)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [32]  [ 520/1251]  eta: 0:06:06  lr: 0.000016  loss: 3.1255 (3.4335)  time: 0.4574  data: 0.0005  max mem: 19734
Epoch: [32]  [ 530/1251]  eta: 0:06:01  lr: 0.000016  loss: 3.1887 (3.4311)  time: 0.4661  data: 0.0004  max mem: 19734
Epoch: [32]  [ 540/1251]  eta: 0:05:55  lr: 0.000016  loss: 3.3726 (3.4353)  time: 0.4677  data: 0.0004  max mem: 19734
Epoch: [32]  [ 550/1251]  eta: 0:05:49  lr: 0.000016  loss: 3.6034 (3.4362)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [32]  [ 560/1251]  eta: 0:05:44  lr: 0.000016  loss: 3.4778 (3.4356)  time: 0.4599  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3775, ratio_loss=0.0052, pruning_loss=0.1271, mse_loss=0.4485
Epoch: [32]  [ 570/1251]  eta: 0:05:39  lr: 0.000016  loss: 3.5633 (3.4335)  time: 0.4675  data: 0.0004  max mem: 19734
Epoch: [32]  [ 580/1251]  eta: 0:05:34  lr: 0.000016  loss: 3.5067 (3.4283)  time: 0.4857  data: 0.0004  max mem: 19734
Epoch: [32]  [ 590/1251]  eta: 0:05:29  lr: 0.000016  loss: 3.5067 (3.4295)  time: 0.4987  data: 0.0004  max mem: 19734
Epoch: [32]  [ 600/1251]  eta: 0:05:24  lr: 0.000016  loss: 3.4330 (3.4249)  time: 0.4873  data: 0.0004  max mem: 19734
Epoch: [32]  [ 610/1251]  eta: 0:05:18  lr: 0.000016  loss: 3.4429 (3.4278)  time: 0.4725  data: 0.0004  max mem: 19734
Epoch: [32]  [ 620/1251]  eta: 0:05:13  lr: 0.000016  loss: 3.5425 (3.4250)  time: 0.4641  data: 0.0004  max mem: 19734
Epoch: [32]  [ 630/1251]  eta: 0:05:08  lr: 0.000016  loss: 3.5027 (3.4248)  time: 0.4572  data: 0.0004  max mem: 19734
Epoch: [32]  [ 640/1251]  eta: 0:05:02  lr: 0.000016  loss: 3.4911 (3.4266)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [32]  [ 650/1251]  eta: 0:04:57  lr: 0.000016  loss: 3.4721 (3.4239)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [32]  [ 660/1251]  eta: 0:04:52  lr: 0.000016  loss: 3.2239 (3.4207)  time: 0.4550  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3200, ratio_loss=0.0052, pruning_loss=0.1279, mse_loss=0.4649
Epoch: [32]  [ 670/1251]  eta: 0:04:46  lr: 0.000016  loss: 3.2891 (3.4198)  time: 0.4509  data: 0.0004  max mem: 19734
Epoch: [32]  [ 680/1251]  eta: 0:04:41  lr: 0.000016  loss: 3.4178 (3.4174)  time: 0.4632  data: 0.0004  max mem: 19734
Epoch: [32]  [ 690/1251]  eta: 0:04:36  lr: 0.000016  loss: 3.5457 (3.4209)  time: 0.4625  data: 0.0004  max mem: 19734
Epoch: [32]  [ 700/1251]  eta: 0:04:31  lr: 0.000016  loss: 3.5760 (3.4197)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [32]  [ 710/1251]  eta: 0:04:26  lr: 0.000016  loss: 3.4196 (3.4181)  time: 0.4600  data: 0.0004  max mem: 19734
Epoch: [32]  [ 720/1251]  eta: 0:04:20  lr: 0.000016  loss: 3.4467 (3.4187)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [32]  [ 730/1251]  eta: 0:04:16  lr: 0.000016  loss: 3.4467 (3.4166)  time: 0.4837  data: 0.0004  max mem: 19734
Epoch: [32]  [ 740/1251]  eta: 0:04:11  lr: 0.000016  loss: 3.2276 (3.4131)  time: 0.4944  data: 0.0005  max mem: 19734
Epoch: [32]  [ 750/1251]  eta: 0:04:06  lr: 0.000016  loss: 3.2205 (3.4125)  time: 0.4846  data: 0.0004  max mem: 19734
Epoch: [32]  [ 760/1251]  eta: 0:04:01  lr: 0.000016  loss: 3.4234 (3.4105)  time: 0.4772  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3173, ratio_loss=0.0053, pruning_loss=0.1275, mse_loss=0.4406
Epoch: [32]  [ 770/1251]  eta: 0:03:55  lr: 0.000016  loss: 3.2797 (3.4084)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [32]  [ 780/1251]  eta: 0:03:50  lr: 0.000016  loss: 3.4486 (3.4095)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [32]  [ 790/1251]  eta: 0:03:45  lr: 0.000016  loss: 3.4682 (3.4089)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [32]  [ 800/1251]  eta: 0:03:40  lr: 0.000016  loss: 3.2443 (3.4052)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [32]  [ 810/1251]  eta: 0:03:35  lr: 0.000016  loss: 3.0597 (3.4026)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [32]  [ 820/1251]  eta: 0:03:30  lr: 0.000016  loss: 3.1840 (3.4006)  time: 0.4701  data: 0.0006  max mem: 19734
Epoch: [32]  [ 830/1251]  eta: 0:03:25  lr: 0.000016  loss: 3.4450 (3.4033)  time: 0.4655  data: 0.0005  max mem: 19734
Epoch: [32]  [ 840/1251]  eta: 0:03:20  lr: 0.000016  loss: 3.6821 (3.4031)  time: 0.4519  data: 0.0005  max mem: 19734
Epoch: [32]  [ 850/1251]  eta: 0:03:15  lr: 0.000016  loss: 3.5425 (3.4035)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [32]  [ 860/1251]  eta: 0:03:10  lr: 0.000016  loss: 3.3958 (3.4012)  time: 0.4629  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3310, ratio_loss=0.0050, pruning_loss=0.1281, mse_loss=0.4602
Epoch: [32]  [ 870/1251]  eta: 0:03:05  lr: 0.000016  loss: 3.6397 (3.4046)  time: 0.4827  data: 0.0005  max mem: 19734
Epoch: [32]  [ 880/1251]  eta: 0:03:00  lr: 0.000016  loss: 3.6405 (3.4019)  time: 0.4837  data: 0.0005  max mem: 19734
Epoch: [32]  [ 890/1251]  eta: 0:02:55  lr: 0.000016  loss: 3.3749 (3.4026)  time: 0.4760  data: 0.0005  max mem: 19734
Epoch: [32]  [ 900/1251]  eta: 0:02:50  lr: 0.000016  loss: 3.3826 (3.4018)  time: 0.4814  data: 0.0005  max mem: 19734
Epoch: [32]  [ 910/1251]  eta: 0:02:45  lr: 0.000016  loss: 3.1971 (3.3992)  time: 0.4704  data: 0.0005  max mem: 19734
Epoch: [32]  [ 920/1251]  eta: 0:02:40  lr: 0.000016  loss: 3.3979 (3.3981)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [32]  [ 930/1251]  eta: 0:02:35  lr: 0.000016  loss: 3.4497 (3.3977)  time: 0.4526  data: 0.0005  max mem: 19734
Epoch: [32]  [ 940/1251]  eta: 0:02:31  lr: 0.000016  loss: 3.5404 (3.3991)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [32]  [ 950/1251]  eta: 0:02:26  lr: 0.000016  loss: 3.4160 (3.3997)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [32]  [ 960/1251]  eta: 0:02:21  lr: 0.000016  loss: 3.4160 (3.4009)  time: 0.4524  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3465, ratio_loss=0.0051, pruning_loss=0.1275, mse_loss=0.4361
Epoch: [32]  [ 970/1251]  eta: 0:02:16  lr: 0.000016  loss: 3.7107 (3.4009)  time: 0.4626  data: 0.0007  max mem: 19734
Epoch: [32]  [ 980/1251]  eta: 0:02:11  lr: 0.000016  loss: 3.5386 (3.4014)  time: 0.4652  data: 0.0008  max mem: 19734
Epoch: [32]  [ 990/1251]  eta: 0:02:06  lr: 0.000016  loss: 3.5388 (3.4030)  time: 0.4572  data: 0.0006  max mem: 19734
Epoch: [32]  [1000/1251]  eta: 0:02:01  lr: 0.000016  loss: 3.6203 (3.4058)  time: 0.4686  data: 0.0006  max mem: 19734
Epoch: [32]  [1010/1251]  eta: 0:01:56  lr: 0.000016  loss: 3.6203 (3.4057)  time: 0.4687  data: 0.0006  max mem: 19734
Epoch: [32]  [1020/1251]  eta: 0:01:51  lr: 0.000016  loss: 3.5538 (3.4069)  time: 0.4778  data: 0.0005  max mem: 19734
Epoch: [32]  [1030/1251]  eta: 0:01:46  lr: 0.000016  loss: 3.4395 (3.4060)  time: 0.4869  data: 0.0005  max mem: 19734
Epoch: [32]  [1040/1251]  eta: 0:01:42  lr: 0.000016  loss: 3.4443 (3.4078)  time: 0.4751  data: 0.0005  max mem: 19734
Epoch: [32]  [1050/1251]  eta: 0:01:37  lr: 0.000016  loss: 3.6235 (3.4091)  time: 0.4806  data: 0.0004  max mem: 19734
Epoch: [32]  [1060/1251]  eta: 0:01:32  lr: 0.000016  loss: 3.7268 (3.4103)  time: 0.4723  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5069, ratio_loss=0.0051, pruning_loss=0.1246, mse_loss=0.4391
Epoch: [32]  [1070/1251]  eta: 0:01:27  lr: 0.000016  loss: 3.7268 (3.4118)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [32]  [1080/1251]  eta: 0:01:22  lr: 0.000016  loss: 3.7161 (3.4124)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [32]  [1090/1251]  eta: 0:01:17  lr: 0.000016  loss: 3.7916 (3.4136)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [32]  [1100/1251]  eta: 0:01:12  lr: 0.000016  loss: 3.6883 (3.4149)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [32]  [1110/1251]  eta: 0:01:08  lr: 0.000016  loss: 3.6408 (3.4178)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [32]  [1120/1251]  eta: 0:01:03  lr: 0.000016  loss: 3.8215 (3.4196)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [32]  [1130/1251]  eta: 0:00:58  lr: 0.000016  loss: 3.6134 (3.4194)  time: 0.4642  data: 0.0004  max mem: 19734
Epoch: [32]  [1140/1251]  eta: 0:00:53  lr: 0.000016  loss: 3.5052 (3.4197)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [32]  [1150/1251]  eta: 0:00:48  lr: 0.000016  loss: 3.3804 (3.4198)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [32]  [1160/1251]  eta: 0:00:43  lr: 0.000016  loss: 3.3804 (3.4200)  time: 0.4888  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4822, ratio_loss=0.0051, pruning_loss=0.1243, mse_loss=0.4382
Epoch: [32]  [1170/1251]  eta: 0:00:39  lr: 0.000016  loss: 3.3174 (3.4192)  time: 0.4895  data: 0.0005  max mem: 19734
Epoch: [32]  [1180/1251]  eta: 0:00:34  lr: 0.000016  loss: 3.3174 (3.4192)  time: 0.4752  data: 0.0004  max mem: 19734
Epoch: [32]  [1190/1251]  eta: 0:00:29  lr: 0.000016  loss: 3.4473 (3.4187)  time: 0.4762  data: 0.0009  max mem: 19734
Epoch: [32]  [1200/1251]  eta: 0:00:24  lr: 0.000016  loss: 3.2676 (3.4171)  time: 0.4639  data: 0.0008  max mem: 19734
Epoch: [32]  [1210/1251]  eta: 0:00:19  lr: 0.000016  loss: 3.1770 (3.4149)  time: 0.4466  data: 0.0002  max mem: 19734
Epoch: [32]  [1220/1251]  eta: 0:00:14  lr: 0.000016  loss: 3.4744 (3.4152)  time: 0.4462  data: 0.0002  max mem: 19734
Epoch: [32]  [1230/1251]  eta: 0:00:10  lr: 0.000016  loss: 3.4408 (3.4143)  time: 0.4485  data: 0.0002  max mem: 19734
Epoch: [32]  [1240/1251]  eta: 0:00:05  lr: 0.000016  loss: 3.5131 (3.4150)  time: 0.4478  data: 0.0002  max mem: 19734
Epoch: [32]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.6590 (3.4176)  time: 0.4480  data: 0.0002  max mem: 19734
Epoch: [32] Total time: 0:10:01 (0.4809 s / it)
Averaged stats: lr: 0.000016  loss: 3.6590 (3.4243)
Test:  [  0/261]  eta: 1:44:18  loss: 0.7484 (0.7484)  acc1: 82.8125 (82.8125)  acc5: 96.3542 (96.3542)  time: 23.9789  data: 23.8310  max mem: 19734
Test:  [ 10/261]  eta: 0:09:32  loss: 0.7484 (0.7436)  acc1: 85.4167 (83.5701)  acc5: 96.8750 (96.4015)  time: 2.2799  data: 2.1923  max mem: 19734
Test:  [ 20/261]  eta: 0:04:57  loss: 0.9480 (0.9129)  acc1: 79.1667 (78.8194)  acc5: 94.2708 (94.7421)  time: 0.0988  data: 0.0203  max mem: 19734
Test:  [ 30/261]  eta: 0:03:21  loss: 0.8359 (0.8322)  acc1: 80.7292 (81.7036)  acc5: 93.2292 (95.1109)  time: 0.0967  data: 0.0102  max mem: 19734
Test:  [ 40/261]  eta: 0:03:19  loss: 0.5994 (0.7960)  acc1: 87.5000 (82.6982)  acc5: 96.8750 (95.4522)  time: 0.5568  data: 0.4083  max mem: 19734
Test:  [ 50/261]  eta: 0:02:45  loss: 0.9075 (0.8610)  acc1: 77.6042 (80.7394)  acc5: 94.7917 (95.0266)  time: 0.6541  data: 0.4297  max mem: 19734
Test:  [ 60/261]  eta: 0:02:21  loss: 0.9960 (0.8706)  acc1: 76.0417 (80.3279)  acc5: 93.7500 (95.0564)  time: 0.2969  data: 0.0353  max mem: 19734
Test:  [ 70/261]  eta: 0:02:03  loss: 0.9346 (0.8701)  acc1: 76.5625 (79.8856)  acc5: 95.8333 (95.2612)  time: 0.2830  data: 0.0769  max mem: 19734
Test:  [ 80/261]  eta: 0:01:51  loss: 0.8253 (0.8717)  acc1: 79.6875 (80.0347)  acc5: 96.8750 (95.3575)  time: 0.3467  data: 0.2178  max mem: 19734
Test:  [ 90/261]  eta: 0:01:37  loss: 0.8154 (0.8581)  acc1: 83.3333 (80.4430)  acc5: 96.3542 (95.4613)  time: 0.2965  data: 0.1537  max mem: 19734
Test:  [100/261]  eta: 0:01:24  loss: 0.8341 (0.8624)  acc1: 83.8542 (80.3321)  acc5: 95.3125 (95.4930)  time: 0.1634  data: 0.0179  max mem: 19734
Test:  [110/261]  eta: 0:01:17  loss: 0.8642 (0.8856)  acc1: 76.0417 (79.8987)  acc5: 94.7917 (95.1670)  time: 0.2580  data: 0.1382  max mem: 19734
Test:  [120/261]  eta: 0:01:08  loss: 1.2225 (0.9255)  acc1: 71.3542 (78.9773)  acc5: 91.1458 (94.6410)  time: 0.2599  data: 0.1331  max mem: 19734
Test:  [130/261]  eta: 0:01:03  loss: 1.4211 (0.9699)  acc1: 66.1458 (78.0375)  acc5: 86.9792 (94.0243)  time: 0.3179  data: 0.1752  max mem: 19734
Test:  [140/261]  eta: 0:00:59  loss: 1.3331 (0.9962)  acc1: 67.1875 (77.3715)  acc5: 89.0625 (93.7500)  time: 0.5513  data: 0.4088  max mem: 19734
Test:  [150/261]  eta: 0:00:52  loss: 1.2295 (1.0013)  acc1: 72.3958 (77.4041)  acc5: 90.6250 (93.5879)  time: 0.4072  data: 0.2621  max mem: 19734
Test:  [160/261]  eta: 0:00:45  loss: 0.9935 (1.0200)  acc1: 79.1667 (77.1125)  acc5: 92.1875 (93.2745)  time: 0.1904  data: 0.0446  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.3093 (1.0499)  acc1: 65.1042 (76.3402)  acc5: 87.5000 (92.9672)  time: 0.6997  data: 0.5558  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4940 (1.0682)  acc1: 65.1042 (75.9381)  acc5: 88.5417 (92.7918)  time: 0.6478  data: 0.5336  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.4314 (1.0816)  acc1: 67.7083 (75.6872)  acc5: 90.6250 (92.6156)  time: 0.0839  data: 0.0074  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3791 (1.0980)  acc1: 71.8750 (75.3498)  acc5: 88.5417 (92.3793)  time: 0.0766  data: 0.0059  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.4062 (1.1122)  acc1: 69.7917 (75.0444)  acc5: 88.0208 (92.1727)  time: 0.1479  data: 0.0827  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4634 (1.1309)  acc1: 66.6667 (74.5805)  acc5: 88.0208 (91.9778)  time: 0.1421  data: 0.0806  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4120 (1.1398)  acc1: 66.6667 (74.3574)  acc5: 88.5417 (91.8786)  time: 0.0617  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3531 (1.1490)  acc1: 68.2292 (74.1075)  acc5: 90.1042 (91.7942)  time: 0.0617  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.1115 (1.1423)  acc1: 75.0000 (74.2820)  acc5: 92.7083 (91.8991)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9560 (1.1417)  acc1: 76.5625 (74.2880)  acc5: 94.7917 (91.9760)  time: 0.0596  data: 0.0001  max mem: 19734
Test: Total time: 0:01:33 (0.3576 s / it)
* Acc@1 74.288 Acc@5 91.976 loss 1.142
Accuracy of the network on the 50000 test images: 74.3%
Max accuracy: 74.62%
Epoch: [33]  [   0/1251]  eta: 4:59:58  lr: 0.000016  loss: 3.6548 (3.6548)  time: 14.3871  data: 12.6245  max mem: 19734
Epoch: [33]  [  10/1251]  eta: 0:43:29  lr: 0.000016  loss: 3.6707 (3.5252)  time: 2.1025  data: 1.1488  max mem: 19734
loss info: cls_loss=3.3899, ratio_loss=0.0050, pruning_loss=0.1260, mse_loss=0.4392
Epoch: [33]  [  20/1251]  eta: 0:27:03  lr: 0.000016  loss: 3.6647 (3.5499)  time: 0.6656  data: 0.0009  max mem: 19734
Epoch: [33]  [  30/1251]  eta: 0:21:11  lr: 0.000016  loss: 3.5742 (3.4912)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [33]  [  40/1251]  eta: 0:18:13  lr: 0.000016  loss: 3.5054 (3.4299)  time: 0.4662  data: 0.0004  max mem: 19734
Epoch: [33]  [  50/1251]  eta: 0:16:35  lr: 0.000016  loss: 3.5041 (3.4085)  time: 0.4989  data: 0.0004  max mem: 19734
Epoch: [33]  [  60/1251]  eta: 0:15:20  lr: 0.000016  loss: 3.4864 (3.3717)  time: 0.5064  data: 0.0004  max mem: 19734
Epoch: [33]  [  70/1251]  eta: 0:14:24  lr: 0.000016  loss: 3.3848 (3.3780)  time: 0.4872  data: 0.0005  max mem: 19734
Epoch: [33]  [  80/1251]  eta: 0:13:42  lr: 0.000016  loss: 3.6664 (3.4076)  time: 0.4863  data: 0.0005  max mem: 19734
Epoch: [33]  [  90/1251]  eta: 0:13:04  lr: 0.000016  loss: 3.7357 (3.4063)  time: 0.4742  data: 0.0005  max mem: 19734
Epoch: [33]  [ 100/1251]  eta: 0:12:33  lr: 0.000016  loss: 3.5107 (3.4124)  time: 0.4599  data: 0.0005  max mem: 19734
Epoch: [33]  [ 110/1251]  eta: 0:12:06  lr: 0.000016  loss: 3.4944 (3.4122)  time: 0.4606  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3583, ratio_loss=0.0051, pruning_loss=0.1264, mse_loss=0.4294
Epoch: [33]  [ 120/1251]  eta: 0:11:44  lr: 0.000016  loss: 3.4398 (3.4103)  time: 0.4622  data: 0.0005  max mem: 19734
Epoch: [33]  [ 130/1251]  eta: 0:11:23  lr: 0.000016  loss: 3.4398 (3.4083)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [33]  [ 140/1251]  eta: 0:11:05  lr: 0.000016  loss: 3.2362 (3.3967)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [33]  [ 150/1251]  eta: 0:10:51  lr: 0.000016  loss: 3.3488 (3.3995)  time: 0.4693  data: 0.0004  max mem: 19734
Epoch: [33]  [ 160/1251]  eta: 0:10:36  lr: 0.000016  loss: 3.4388 (3.4022)  time: 0.4688  data: 0.0004  max mem: 19734
Epoch: [33]  [ 170/1251]  eta: 0:10:22  lr: 0.000016  loss: 3.6811 (3.4047)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [33]  [ 180/1251]  eta: 0:10:11  lr: 0.000016  loss: 3.6154 (3.3989)  time: 0.4709  data: 0.0004  max mem: 19734
Epoch: [33]  [ 190/1251]  eta: 0:10:01  lr: 0.000016  loss: 3.4969 (3.3950)  time: 0.4870  data: 0.0004  max mem: 19734
Epoch: [33]  [ 200/1251]  eta: 0:09:50  lr: 0.000016  loss: 3.4969 (3.3972)  time: 0.4861  data: 0.0004  max mem: 19734
Epoch: [33]  [ 210/1251]  eta: 0:09:42  lr: 0.000016  loss: 3.5231 (3.3973)  time: 0.4886  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3303, ratio_loss=0.0052, pruning_loss=0.1281, mse_loss=0.4182
Epoch: [33]  [ 220/1251]  eta: 0:09:32  lr: 0.000016  loss: 3.5231 (3.3851)  time: 0.4880  data: 0.0004  max mem: 19734
Epoch: [33]  [ 230/1251]  eta: 0:09:22  lr: 0.000016  loss: 3.5390 (3.3946)  time: 0.4663  data: 0.0004  max mem: 19734
Epoch: [33]  [ 240/1251]  eta: 0:09:13  lr: 0.000016  loss: 3.5706 (3.3911)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [33]  [ 250/1251]  eta: 0:09:04  lr: 0.000016  loss: 3.5479 (3.3914)  time: 0.4602  data: 0.0005  max mem: 19734
Epoch: [33]  [ 260/1251]  eta: 0:08:55  lr: 0.000016  loss: 3.4645 (3.3897)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [33]  [ 270/1251]  eta: 0:08:47  lr: 0.000016  loss: 3.4953 (3.3896)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [33]  [ 280/1251]  eta: 0:08:38  lr: 0.000016  loss: 3.7518 (3.4031)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [33]  [ 290/1251]  eta: 0:08:31  lr: 0.000016  loss: 3.7518 (3.4082)  time: 0.4611  data: 0.0004  max mem: 19734
Epoch: [33]  [ 300/1251]  eta: 0:08:23  lr: 0.000016  loss: 3.5961 (3.4179)  time: 0.4593  data: 0.0004  max mem: 19734
Epoch: [33]  [ 310/1251]  eta: 0:08:15  lr: 0.000016  loss: 3.6879 (3.4191)  time: 0.4530  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4694, ratio_loss=0.0048, pruning_loss=0.1253, mse_loss=0.4455
Epoch: [33]  [ 320/1251]  eta: 0:08:08  lr: 0.000016  loss: 3.3925 (3.4180)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [33]  [ 330/1251]  eta: 0:08:02  lr: 0.000016  loss: 3.3855 (3.4102)  time: 0.4682  data: 0.0004  max mem: 19734
Epoch: [33]  [ 340/1251]  eta: 0:07:56  lr: 0.000016  loss: 3.3855 (3.4103)  time: 0.5008  data: 0.0004  max mem: 19734
Epoch: [33]  [ 350/1251]  eta: 0:07:50  lr: 0.000016  loss: 3.2476 (3.4081)  time: 0.5090  data: 0.0004  max mem: 19734
Epoch: [33]  [ 360/1251]  eta: 0:07:44  lr: 0.000016  loss: 3.2377 (3.3992)  time: 0.4863  data: 0.0005  max mem: 19734
Epoch: [33]  [ 370/1251]  eta: 0:07:37  lr: 0.000016  loss: 3.3261 (3.3978)  time: 0.4640  data: 0.0005  max mem: 19734
Epoch: [33]  [ 380/1251]  eta: 0:07:31  lr: 0.000016  loss: 3.4298 (3.3990)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [33]  [ 390/1251]  eta: 0:07:24  lr: 0.000016  loss: 3.6182 (3.3979)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [33]  [ 400/1251]  eta: 0:07:17  lr: 0.000016  loss: 3.5760 (3.3950)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [33]  [ 410/1251]  eta: 0:07:11  lr: 0.000016  loss: 3.2990 (3.3963)  time: 0.4543  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2840, ratio_loss=0.0050, pruning_loss=0.1292, mse_loss=0.4511
Epoch: [33]  [ 420/1251]  eta: 0:07:05  lr: 0.000016  loss: 3.2990 (3.3921)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [33]  [ 430/1251]  eta: 0:06:59  lr: 0.000016  loss: 3.5133 (3.3924)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [33]  [ 440/1251]  eta: 0:06:53  lr: 0.000016  loss: 3.1362 (3.3873)  time: 0.4599  data: 0.0004  max mem: 19734
Epoch: [33]  [ 450/1251]  eta: 0:06:47  lr: 0.000016  loss: 3.2434 (3.3888)  time: 0.4609  data: 0.0004  max mem: 19734
Epoch: [33]  [ 460/1251]  eta: 0:06:41  lr: 0.000016  loss: 3.5771 (3.3908)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [33]  [ 470/1251]  eta: 0:06:35  lr: 0.000016  loss: 3.4958 (3.3846)  time: 0.4667  data: 0.0006  max mem: 19734
Epoch: [33]  [ 480/1251]  eta: 0:06:30  lr: 0.000016  loss: 3.3911 (3.3862)  time: 0.4853  data: 0.0006  max mem: 19734
Epoch: [33]  [ 490/1251]  eta: 0:06:24  lr: 0.000016  loss: 3.3911 (3.3821)  time: 0.4855  data: 0.0005  max mem: 19734
Epoch: [33]  [ 500/1251]  eta: 0:06:19  lr: 0.000016  loss: 3.5744 (3.3831)  time: 0.4758  data: 0.0004  max mem: 19734
Epoch: [33]  [ 510/1251]  eta: 0:06:13  lr: 0.000016  loss: 3.6697 (3.3857)  time: 0.4691  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3195, ratio_loss=0.0050, pruning_loss=0.1272, mse_loss=0.4472
Epoch: [33]  [ 520/1251]  eta: 0:06:07  lr: 0.000016  loss: 3.3739 (3.3845)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [33]  [ 530/1251]  eta: 0:06:02  lr: 0.000016  loss: 3.4436 (3.3844)  time: 0.4519  data: 0.0009  max mem: 19734
Epoch: [33]  [ 540/1251]  eta: 0:05:56  lr: 0.000016  loss: 3.4436 (3.3797)  time: 0.4519  data: 0.0009  max mem: 19734
Epoch: [33]  [ 550/1251]  eta: 0:05:50  lr: 0.000016  loss: 3.2392 (3.3796)  time: 0.4526  data: 0.0006  max mem: 19734
Epoch: [33]  [ 560/1251]  eta: 0:05:45  lr: 0.000016  loss: 3.1217 (3.3739)  time: 0.4537  data: 0.0007  max mem: 19734
Epoch: [33]  [ 570/1251]  eta: 0:05:39  lr: 0.000016  loss: 3.1840 (3.3741)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [33]  [ 580/1251]  eta: 0:05:34  lr: 0.000016  loss: 3.5244 (3.3735)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [33]  [ 590/1251]  eta: 0:05:28  lr: 0.000016  loss: 3.4612 (3.3739)  time: 0.4622  data: 0.0005  max mem: 19734
Epoch: [33]  [ 600/1251]  eta: 0:05:23  lr: 0.000016  loss: 3.4015 (3.3747)  time: 0.4620  data: 0.0005  max mem: 19734
Epoch: [33]  [ 610/1251]  eta: 0:05:18  lr: 0.000016  loss: 3.5315 (3.3758)  time: 0.4651  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2970, ratio_loss=0.0050, pruning_loss=0.1280, mse_loss=0.4517
Epoch: [33]  [ 620/1251]  eta: 0:05:13  lr: 0.000016  loss: 3.5315 (3.3769)  time: 0.4719  data: 0.0004  max mem: 19734
Epoch: [33]  [ 630/1251]  eta: 0:05:08  lr: 0.000016  loss: 3.7262 (3.3838)  time: 0.4793  data: 0.0004  max mem: 19734
Epoch: [33]  [ 640/1251]  eta: 0:05:02  lr: 0.000016  loss: 3.4194 (3.3782)  time: 0.4878  data: 0.0005  max mem: 19734
Epoch: [33]  [ 650/1251]  eta: 0:04:57  lr: 0.000016  loss: 3.2939 (3.3784)  time: 0.4785  data: 0.0005  max mem: 19734
Epoch: [33]  [ 660/1251]  eta: 0:04:52  lr: 0.000016  loss: 3.4115 (3.3796)  time: 0.4633  data: 0.0004  max mem: 19734
Epoch: [33]  [ 670/1251]  eta: 0:04:47  lr: 0.000016  loss: 3.5521 (3.3839)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [33]  [ 680/1251]  eta: 0:04:41  lr: 0.000016  loss: 3.6257 (3.3868)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [33]  [ 690/1251]  eta: 0:04:36  lr: 0.000016  loss: 3.4146 (3.3844)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [33]  [ 700/1251]  eta: 0:04:31  lr: 0.000016  loss: 3.5544 (3.3876)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [33]  [ 710/1251]  eta: 0:04:26  lr: 0.000016  loss: 3.7826 (3.3910)  time: 0.4552  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4730, ratio_loss=0.0054, pruning_loss=0.1247, mse_loss=0.4223
Epoch: [33]  [ 720/1251]  eta: 0:04:20  lr: 0.000016  loss: 3.6455 (3.3919)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [33]  [ 730/1251]  eta: 0:04:15  lr: 0.000016  loss: 3.4422 (3.3925)  time: 0.4631  data: 0.0004  max mem: 19734
Epoch: [33]  [ 740/1251]  eta: 0:04:10  lr: 0.000016  loss: 3.2615 (3.3903)  time: 0.4630  data: 0.0004  max mem: 19734
Epoch: [33]  [ 750/1251]  eta: 0:04:05  lr: 0.000016  loss: 3.5949 (3.3922)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [33]  [ 760/1251]  eta: 0:04:00  lr: 0.000016  loss: 3.6729 (3.3948)  time: 0.4618  data: 0.0004  max mem: 19734
Epoch: [33]  [ 770/1251]  eta: 0:03:55  lr: 0.000016  loss: 3.4930 (3.3972)  time: 0.4879  data: 0.0004  max mem: 19734
Epoch: [33]  [ 780/1251]  eta: 0:03:50  lr: 0.000016  loss: 3.4930 (3.3973)  time: 0.4866  data: 0.0005  max mem: 19734
Epoch: [33]  [ 790/1251]  eta: 0:03:45  lr: 0.000016  loss: 3.7325 (3.3985)  time: 0.4792  data: 0.0005  max mem: 19734
Epoch: [33]  [ 800/1251]  eta: 0:03:40  lr: 0.000016  loss: 3.5183 (3.3964)  time: 0.4812  data: 0.0004  max mem: 19734
Epoch: [33]  [ 810/1251]  eta: 0:03:35  lr: 0.000016  loss: 3.5183 (3.3954)  time: 0.4629  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4018, ratio_loss=0.0049, pruning_loss=0.1269, mse_loss=0.4454
Epoch: [33]  [ 820/1251]  eta: 0:03:30  lr: 0.000016  loss: 3.5787 (3.3967)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [33]  [ 830/1251]  eta: 0:03:25  lr: 0.000016  loss: 3.3465 (3.3957)  time: 0.4537  data: 0.0006  max mem: 19734
Epoch: [33]  [ 840/1251]  eta: 0:03:20  lr: 0.000016  loss: 3.5218 (3.3986)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [33]  [ 850/1251]  eta: 0:03:15  lr: 0.000016  loss: 3.5339 (3.3978)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [33]  [ 860/1251]  eta: 0:03:10  lr: 0.000016  loss: 3.4766 (3.3987)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [33]  [ 870/1251]  eta: 0:03:05  lr: 0.000016  loss: 3.4353 (3.3974)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [33]  [ 880/1251]  eta: 0:03:00  lr: 0.000016  loss: 3.2599 (3.3950)  time: 0.4617  data: 0.0005  max mem: 19734
Epoch: [33]  [ 890/1251]  eta: 0:02:55  lr: 0.000016  loss: 3.5243 (3.3952)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [33]  [ 900/1251]  eta: 0:02:50  lr: 0.000016  loss: 3.7041 (3.3984)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [33]  [ 910/1251]  eta: 0:02:45  lr: 0.000016  loss: 3.8002 (3.4004)  time: 0.4699  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4404, ratio_loss=0.0051, pruning_loss=0.1254, mse_loss=0.4510
Epoch: [33]  [ 920/1251]  eta: 0:02:40  lr: 0.000016  loss: 3.6394 (3.4032)  time: 0.4765  data: 0.0006  max mem: 19734
Epoch: [33]  [ 930/1251]  eta: 0:02:36  lr: 0.000016  loss: 3.4972 (3.4019)  time: 0.4905  data: 0.0004  max mem: 19734
Epoch: [33]  [ 940/1251]  eta: 0:02:31  lr: 0.000016  loss: 3.4104 (3.4026)  time: 0.4941  data: 0.0004  max mem: 19734
Epoch: [33]  [ 950/1251]  eta: 0:02:26  lr: 0.000016  loss: 3.4667 (3.4038)  time: 0.4659  data: 0.0004  max mem: 19734
Epoch: [33]  [ 960/1251]  eta: 0:02:21  lr: 0.000016  loss: 3.5473 (3.4035)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [33]  [ 970/1251]  eta: 0:02:16  lr: 0.000016  loss: 3.3827 (3.4040)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [33]  [ 980/1251]  eta: 0:02:11  lr: 0.000016  loss: 3.5920 (3.4055)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [33]  [ 990/1251]  eta: 0:02:06  lr: 0.000016  loss: 3.4189 (3.4015)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [33]  [1000/1251]  eta: 0:02:01  lr: 0.000016  loss: 3.4189 (3.4025)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [33]  [1010/1251]  eta: 0:01:56  lr: 0.000016  loss: 3.5212 (3.3996)  time: 0.4561  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3403, ratio_loss=0.0049, pruning_loss=0.1273, mse_loss=0.4378
Epoch: [33]  [1020/1251]  eta: 0:01:51  lr: 0.000016  loss: 3.5176 (3.4007)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [33]  [1030/1251]  eta: 0:01:46  lr: 0.000016  loss: 3.5857 (3.4013)  time: 0.4620  data: 0.0004  max mem: 19734
Epoch: [33]  [1040/1251]  eta: 0:01:41  lr: 0.000016  loss: 3.6970 (3.4039)  time: 0.4626  data: 0.0005  max mem: 19734
Epoch: [33]  [1050/1251]  eta: 0:01:37  lr: 0.000016  loss: 3.5747 (3.4049)  time: 0.4671  data: 0.0005  max mem: 19734
Epoch: [33]  [1060/1251]  eta: 0:01:32  lr: 0.000016  loss: 3.4975 (3.4061)  time: 0.4881  data: 0.0004  max mem: 19734
Epoch: [33]  [1070/1251]  eta: 0:01:27  lr: 0.000016  loss: 3.4254 (3.4035)  time: 0.4802  data: 0.0004  max mem: 19734
Epoch: [33]  [1080/1251]  eta: 0:01:22  lr: 0.000016  loss: 3.4530 (3.4040)  time: 0.4674  data: 0.0004  max mem: 19734
Epoch: [33]  [1090/1251]  eta: 0:01:17  lr: 0.000016  loss: 3.5028 (3.4034)  time: 0.4772  data: 0.0005  max mem: 19734
Epoch: [33]  [1100/1251]  eta: 0:01:12  lr: 0.000016  loss: 3.1354 (3.4017)  time: 0.4700  data: 0.0005  max mem: 19734
Epoch: [33]  [1110/1251]  eta: 0:01:08  lr: 0.000016  loss: 3.5531 (3.4028)  time: 0.4542  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4084, ratio_loss=0.0050, pruning_loss=0.1253, mse_loss=0.4462
Epoch: [33]  [1120/1251]  eta: 0:01:03  lr: 0.000016  loss: 3.6198 (3.4039)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [33]  [1130/1251]  eta: 0:00:58  lr: 0.000016  loss: 3.4291 (3.4043)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [33]  [1140/1251]  eta: 0:00:53  lr: 0.000016  loss: 3.4271 (3.4040)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [33]  [1150/1251]  eta: 0:00:48  lr: 0.000016  loss: 3.4626 (3.4041)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [33]  [1160/1251]  eta: 0:00:43  lr: 0.000016  loss: 3.5142 (3.4040)  time: 0.4515  data: 0.0004  max mem: 19734
Epoch: [33]  [1170/1251]  eta: 0:00:38  lr: 0.000016  loss: 3.5046 (3.4051)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [33]  [1180/1251]  eta: 0:00:34  lr: 0.000016  loss: 3.5949 (3.4058)  time: 0.4646  data: 0.0004  max mem: 19734
Epoch: [33]  [1190/1251]  eta: 0:00:29  lr: 0.000016  loss: 3.5949 (3.4064)  time: 0.4645  data: 0.0008  max mem: 19734
Epoch: [33]  [1200/1251]  eta: 0:00:24  lr: 0.000016  loss: 3.6052 (3.4079)  time: 0.4645  data: 0.0007  max mem: 19734
Epoch: [33]  [1210/1251]  eta: 0:00:19  lr: 0.000016  loss: 3.5749 (3.4077)  time: 0.4692  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4584, ratio_loss=0.0048, pruning_loss=0.1252, mse_loss=0.4373
Epoch: [33]  [1220/1251]  eta: 0:00:14  lr: 0.000016  loss: 3.6136 (3.4105)  time: 0.4701  data: 0.0002  max mem: 19734
Epoch: [33]  [1230/1251]  eta: 0:00:10  lr: 0.000016  loss: 3.6136 (3.4100)  time: 0.4672  data: 0.0001  max mem: 19734
Epoch: [33]  [1240/1251]  eta: 0:00:05  lr: 0.000016  loss: 3.5383 (3.4105)  time: 0.4579  data: 0.0001  max mem: 19734
Epoch: [33]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.4680 (3.4101)  time: 0.4542  data: 0.0001  max mem: 19734
Epoch: [33] Total time: 0:10:00 (0.4804 s / it)
Averaged stats: lr: 0.000016  loss: 3.4680 (3.4159)
Test:  [  0/261]  eta: 1:59:15  loss: 0.7422 (0.7422)  acc1: 84.3750 (84.3750)  acc5: 95.8333 (95.8333)  time: 27.4148  data: 27.1741  max mem: 19734
Test:  [ 10/261]  eta: 0:12:01  loss: 0.7024 (0.7181)  acc1: 84.3750 (84.2803)  acc5: 97.3958 (96.3542)  time: 2.8741  data: 2.7061  max mem: 19734
Test:  [ 20/261]  eta: 0:06:20  loss: 0.8944 (0.8967)  acc1: 80.7292 (79.4147)  acc5: 93.7500 (94.8165)  time: 0.2884  data: 0.1340  max mem: 19734
Test:  [ 30/261]  eta: 0:04:28  loss: 0.8188 (0.8142)  acc1: 81.2500 (82.1573)  acc5: 94.2708 (95.3125)  time: 0.2221  data: 0.0133  max mem: 19734
Test:  [ 40/261]  eta: 0:03:44  loss: 0.5762 (0.7810)  acc1: 88.0208 (83.0793)  acc5: 96.8750 (95.5158)  time: 0.4225  data: 0.2353  max mem: 19734
Test:  [ 50/261]  eta: 0:02:55  loss: 0.9047 (0.8417)  acc1: 77.0833 (81.2909)  acc5: 94.2708 (95.0980)  time: 0.3239  data: 0.2288  max mem: 19734
Test:  [ 60/261]  eta: 0:02:23  loss: 0.9660 (0.8501)  acc1: 75.5208 (80.9085)  acc5: 94.2708 (95.1674)  time: 0.0961  data: 0.0084  max mem: 19734
Test:  [ 70/261]  eta: 0:02:30  loss: 0.8979 (0.8529)  acc1: 76.5625 (80.3771)  acc5: 95.8333 (95.3418)  time: 0.6675  data: 0.5536  max mem: 19734
Test:  [ 80/261]  eta: 0:02:08  loss: 0.8510 (0.8548)  acc1: 79.1667 (80.4848)  acc5: 96.8750 (95.4475)  time: 0.7033  data: 0.5531  max mem: 19734
Test:  [ 90/261]  eta: 0:01:52  loss: 0.8031 (0.8416)  acc1: 83.8542 (80.8494)  acc5: 96.3542 (95.5243)  time: 0.1934  data: 0.0148  max mem: 19734
Test:  [100/261]  eta: 0:01:41  loss: 0.7981 (0.8465)  acc1: 84.8958 (80.8323)  acc5: 95.3125 (95.5652)  time: 0.2931  data: 0.1446  max mem: 19734
Test:  [110/261]  eta: 0:01:28  loss: 0.8823 (0.8693)  acc1: 77.0833 (80.3585)  acc5: 94.7917 (95.2374)  time: 0.2508  data: 0.1427  max mem: 19734
Test:  [120/261]  eta: 0:01:16  loss: 1.1478 (0.9104)  acc1: 70.3125 (79.3862)  acc5: 90.1042 (94.6841)  time: 0.1073  data: 0.0124  max mem: 19734
Test:  [130/261]  eta: 0:01:12  loss: 1.3694 (0.9567)  acc1: 66.6667 (78.3954)  acc5: 86.9792 (94.0879)  time: 0.3762  data: 0.2896  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: 1.3467 (0.9845)  acc1: 67.1875 (77.7113)  acc5: 88.5417 (93.8017)  time: 0.4676  data: 0.3694  max mem: 19734
Test:  [150/261]  eta: 0:00:55  loss: 1.2528 (0.9901)  acc1: 72.9167 (77.6628)  acc5: 91.6667 (93.6431)  time: 0.1782  data: 0.0885  max mem: 19734
Test:  [160/261]  eta: 0:00:50  loss: 1.0326 (1.0080)  acc1: 78.1250 (77.3421)  acc5: 91.6667 (93.3521)  time: 0.2619  data: 0.1712  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: 1.2387 (1.0384)  acc1: 64.0625 (76.5747)  acc5: 88.0208 (93.0099)  time: 0.2688  data: 0.1703  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4424 (1.0565)  acc1: 64.0625 (76.1510)  acc5: 89.5833 (92.8234)  time: 0.0986  data: 0.0137  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: 1.3845 (1.0691)  acc1: 67.7083 (75.9026)  acc5: 91.1458 (92.6756)  time: 0.1253  data: 0.0534  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.3845 (1.0845)  acc1: 71.8750 (75.5804)  acc5: 89.5833 (92.4596)  time: 0.1081  data: 0.0426  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.4275 (1.0989)  acc1: 70.3125 (75.2789)  acc5: 88.5417 (92.2616)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.4694 (1.1196)  acc1: 67.1875 (74.7573)  acc5: 88.5417 (92.0390)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4552 (1.1289)  acc1: 67.1875 (74.5378)  acc5: 89.0625 (91.9463)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.2886 (1.1382)  acc1: 67.1875 (74.2695)  acc5: 90.6250 (91.8698)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0696 (1.1311)  acc1: 73.4375 (74.4356)  acc5: 93.7500 (91.9904)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9468 (1.1310)  acc1: 76.0417 (74.4540)  acc5: 95.3125 (92.0560)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:28 (0.3380 s / it)
* Acc@1 74.454 Acc@5 92.056 loss 1.131
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.62%
Epoch: [34]  [   0/1251]  eta: 4:20:32  lr: 0.000015  loss: 3.5888 (3.5888)  time: 12.4960  data: 7.6045  max mem: 19734
Epoch: [34]  [  10/1251]  eta: 0:43:45  lr: 0.000015  loss: 3.6346 (3.4801)  time: 2.1160  data: 1.2031  max mem: 19734
Epoch: [34]  [  20/1251]  eta: 0:27:11  lr: 0.000015  loss: 3.3674 (3.3537)  time: 0.7667  data: 0.2817  max mem: 19734
Epoch: [34]  [  30/1251]  eta: 0:21:14  lr: 0.000015  loss: 3.0154 (3.2129)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [34]  [  40/1251]  eta: 0:18:15  lr: 0.000015  loss: 3.4522 (3.3104)  time: 0.4635  data: 0.0004  max mem: 19734
Epoch: [34]  [  50/1251]  eta: 0:16:21  lr: 0.000015  loss: 3.4991 (3.3290)  time: 0.4659  data: 0.0004  max mem: 19734
Epoch: [34]  [  60/1251]  eta: 0:15:02  lr: 0.000015  loss: 3.4919 (3.3468)  time: 0.4560  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3313, ratio_loss=0.0047, pruning_loss=0.1272, mse_loss=0.4430
Epoch: [34]  [  70/1251]  eta: 0:14:05  lr: 0.000015  loss: 3.4219 (3.3030)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [34]  [  80/1251]  eta: 0:13:24  lr: 0.000015  loss: 3.1020 (3.2970)  time: 0.4706  data: 0.0004  max mem: 19734
Epoch: [34]  [  90/1251]  eta: 0:12:52  lr: 0.000015  loss: 3.2694 (3.2961)  time: 0.4845  data: 0.0005  max mem: 19734
Epoch: [34]  [ 100/1251]  eta: 0:12:26  lr: 0.000015  loss: 3.7308 (3.3265)  time: 0.4943  data: 0.0005  max mem: 19734
Epoch: [34]  [ 110/1251]  eta: 0:12:03  lr: 0.000015  loss: 3.8183 (3.3537)  time: 0.4945  data: 0.0004  max mem: 19734
Epoch: [34]  [ 120/1251]  eta: 0:11:43  lr: 0.000015  loss: 3.6331 (3.3611)  time: 0.4893  data: 0.0004  max mem: 19734
Epoch: [34]  [ 130/1251]  eta: 0:11:24  lr: 0.000015  loss: 3.5944 (3.3520)  time: 0.4783  data: 0.0005  max mem: 19734
Epoch: [34]  [ 140/1251]  eta: 0:11:06  lr: 0.000015  loss: 3.5029 (3.3559)  time: 0.4643  data: 0.0004  max mem: 19734
Epoch: [34]  [ 150/1251]  eta: 0:10:50  lr: 0.000015  loss: 3.4307 (3.3447)  time: 0.4643  data: 0.0004  max mem: 19734
Epoch: [34]  [ 160/1251]  eta: 0:10:35  lr: 0.000015  loss: 3.3125 (3.3384)  time: 0.4622  data: 0.0008  max mem: 19734
loss info: cls_loss=3.3368, ratio_loss=0.0048, pruning_loss=0.1292, mse_loss=0.4440
Epoch: [34]  [ 170/1251]  eta: 0:10:22  lr: 0.000015  loss: 3.3125 (3.3363)  time: 0.4592  data: 0.0009  max mem: 19734
Epoch: [34]  [ 180/1251]  eta: 0:10:09  lr: 0.000015  loss: 3.4297 (3.3429)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [34]  [ 190/1251]  eta: 0:09:59  lr: 0.000015  loss: 3.4297 (3.3384)  time: 0.4701  data: 0.0004  max mem: 19734
Epoch: [34]  [ 200/1251]  eta: 0:09:47  lr: 0.000015  loss: 3.6666 (3.3594)  time: 0.4687  data: 0.0004  max mem: 19734
Epoch: [34]  [ 210/1251]  eta: 0:09:37  lr: 0.000015  loss: 3.7046 (3.3719)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [34]  [ 220/1251]  eta: 0:09:26  lr: 0.000015  loss: 3.4467 (3.3637)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [34]  [ 230/1251]  eta: 0:09:18  lr: 0.000015  loss: 3.4431 (3.3665)  time: 0.4677  data: 0.0004  max mem: 19734
Epoch: [34]  [ 240/1251]  eta: 0:09:10  lr: 0.000015  loss: 3.4804 (3.3654)  time: 0.4904  data: 0.0005  max mem: 19734
Epoch: [34]  [ 250/1251]  eta: 0:09:03  lr: 0.000015  loss: 3.2918 (3.3583)  time: 0.5009  data: 0.0005  max mem: 19734
Epoch: [34]  [ 260/1251]  eta: 0:08:55  lr: 0.000015  loss: 3.3067 (3.3593)  time: 0.4907  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3727, ratio_loss=0.0049, pruning_loss=0.1269, mse_loss=0.4381
Epoch: [34]  [ 270/1251]  eta: 0:08:47  lr: 0.000015  loss: 3.4725 (3.3701)  time: 0.4676  data: 0.0004  max mem: 19734
Epoch: [34]  [ 280/1251]  eta: 0:08:39  lr: 0.000015  loss: 3.5228 (3.3644)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [34]  [ 290/1251]  eta: 0:08:31  lr: 0.000015  loss: 3.4678 (3.3735)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [34]  [ 300/1251]  eta: 0:08:23  lr: 0.000015  loss: 3.4678 (3.3718)  time: 0.4570  data: 0.0007  max mem: 19734
Epoch: [34]  [ 310/1251]  eta: 0:08:16  lr: 0.000015  loss: 3.4493 (3.3723)  time: 0.4577  data: 0.0006  max mem: 19734
Epoch: [34]  [ 320/1251]  eta: 0:08:08  lr: 0.000015  loss: 3.3589 (3.3688)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [34]  [ 330/1251]  eta: 0:08:01  lr: 0.000015  loss: 3.5798 (3.3774)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [34]  [ 340/1251]  eta: 0:07:54  lr: 0.000015  loss: 3.6676 (3.3829)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [34]  [ 350/1251]  eta: 0:07:47  lr: 0.000015  loss: 3.5594 (3.3776)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [34]  [ 360/1251]  eta: 0:07:41  lr: 0.000015  loss: 3.1279 (3.3695)  time: 0.4537  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3757, ratio_loss=0.0050, pruning_loss=0.1268, mse_loss=0.4430
Epoch: [34]  [ 370/1251]  eta: 0:07:34  lr: 0.000015  loss: 3.1967 (3.3726)  time: 0.4642  data: 0.0005  max mem: 19734
Epoch: [34]  [ 380/1251]  eta: 0:07:28  lr: 0.000015  loss: 3.5577 (3.3750)  time: 0.4718  data: 0.0005  max mem: 19734
Epoch: [34]  [ 390/1251]  eta: 0:07:23  lr: 0.000015  loss: 3.2806 (3.3697)  time: 0.4846  data: 0.0004  max mem: 19734
Epoch: [34]  [ 400/1251]  eta: 0:07:17  lr: 0.000015  loss: 3.1602 (3.3667)  time: 0.4946  data: 0.0004  max mem: 19734
Epoch: [34]  [ 410/1251]  eta: 0:07:11  lr: 0.000015  loss: 3.0551 (3.3575)  time: 0.4799  data: 0.0004  max mem: 19734
Epoch: [34]  [ 420/1251]  eta: 0:07:05  lr: 0.000015  loss: 3.0394 (3.3590)  time: 0.4625  data: 0.0004  max mem: 19734
Epoch: [34]  [ 430/1251]  eta: 0:06:58  lr: 0.000015  loss: 3.4208 (3.3567)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [34]  [ 440/1251]  eta: 0:06:52  lr: 0.000015  loss: 3.3946 (3.3582)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [34]  [ 450/1251]  eta: 0:06:46  lr: 0.000015  loss: 3.4495 (3.3600)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [34]  [ 460/1251]  eta: 0:06:40  lr: 0.000015  loss: 3.4495 (3.3573)  time: 0.4530  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2790, ratio_loss=0.0051, pruning_loss=0.1273, mse_loss=0.4447
Epoch: [34]  [ 470/1251]  eta: 0:06:34  lr: 0.000015  loss: 3.5021 (3.3590)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [34]  [ 480/1251]  eta: 0:06:28  lr: 0.000015  loss: 3.5306 (3.3620)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [34]  [ 490/1251]  eta: 0:06:23  lr: 0.000015  loss: 3.5052 (3.3597)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [34]  [ 500/1251]  eta: 0:06:17  lr: 0.000015  loss: 3.5263 (3.3659)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [34]  [ 510/1251]  eta: 0:06:11  lr: 0.000015  loss: 3.6354 (3.3688)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [34]  [ 520/1251]  eta: 0:06:06  lr: 0.000015  loss: 3.5472 (3.3709)  time: 0.4609  data: 0.0006  max mem: 19734
Epoch: [34]  [ 530/1251]  eta: 0:06:01  lr: 0.000015  loss: 3.5472 (3.3756)  time: 0.4886  data: 0.0005  max mem: 19734
Epoch: [34]  [ 540/1251]  eta: 0:05:55  lr: 0.000015  loss: 3.6192 (3.3760)  time: 0.4847  data: 0.0006  max mem: 19734
Epoch: [34]  [ 550/1251]  eta: 0:05:50  lr: 0.000015  loss: 3.5844 (3.3806)  time: 0.4794  data: 0.0006  max mem: 19734
Epoch: [34]  [ 560/1251]  eta: 0:05:45  lr: 0.000015  loss: 3.5591 (3.3834)  time: 0.4756  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4751, ratio_loss=0.0050, pruning_loss=0.1229, mse_loss=0.4332
Epoch: [34]  [ 570/1251]  eta: 0:05:39  lr: 0.000015  loss: 3.5352 (3.3825)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [34]  [ 580/1251]  eta: 0:05:34  lr: 0.000015  loss: 3.6344 (3.3847)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [34]  [ 590/1251]  eta: 0:05:28  lr: 0.000015  loss: 3.4087 (3.3847)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [34]  [ 600/1251]  eta: 0:05:23  lr: 0.000015  loss: 3.2213 (3.3777)  time: 0.4540  data: 0.0006  max mem: 19734
Epoch: [34]  [ 610/1251]  eta: 0:05:17  lr: 0.000015  loss: 3.3884 (3.3763)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [34]  [ 620/1251]  eta: 0:05:12  lr: 0.000015  loss: 3.4500 (3.3757)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [34]  [ 630/1251]  eta: 0:05:07  lr: 0.000015  loss: 3.5773 (3.3778)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [34]  [ 640/1251]  eta: 0:05:01  lr: 0.000015  loss: 3.6537 (3.3805)  time: 0.4677  data: 0.0004  max mem: 19734
Epoch: [34]  [ 650/1251]  eta: 0:04:56  lr: 0.000015  loss: 3.6680 (3.3856)  time: 0.4685  data: 0.0005  max mem: 19734
Epoch: [34]  [ 660/1251]  eta: 0:04:51  lr: 0.000015  loss: 3.6539 (3.3862)  time: 0.4669  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3904, ratio_loss=0.0048, pruning_loss=0.1257, mse_loss=0.4576
Epoch: [34]  [ 670/1251]  eta: 0:04:46  lr: 0.000015  loss: 3.5778 (3.3874)  time: 0.4792  data: 0.0004  max mem: 19734
Epoch: [34]  [ 680/1251]  eta: 0:04:41  lr: 0.000015  loss: 3.5703 (3.3904)  time: 0.4914  data: 0.0005  max mem: 19734
Epoch: [34]  [ 690/1251]  eta: 0:04:36  lr: 0.000015  loss: 3.5378 (3.3916)  time: 0.4942  data: 0.0006  max mem: 19734
Epoch: [34]  [ 700/1251]  eta: 0:04:31  lr: 0.000015  loss: 3.5342 (3.3936)  time: 0.4842  data: 0.0005  max mem: 19734
Epoch: [34]  [ 710/1251]  eta: 0:04:26  lr: 0.000015  loss: 3.6369 (3.3978)  time: 0.4663  data: 0.0004  max mem: 19734
Epoch: [34]  [ 720/1251]  eta: 0:04:21  lr: 0.000015  loss: 3.6934 (3.4005)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [34]  [ 730/1251]  eta: 0:04:16  lr: 0.000015  loss: 3.6178 (3.4013)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [34]  [ 740/1251]  eta: 0:04:10  lr: 0.000015  loss: 3.5660 (3.4028)  time: 0.4534  data: 0.0007  max mem: 19734
Epoch: [34]  [ 750/1251]  eta: 0:04:05  lr: 0.000015  loss: 3.6364 (3.4048)  time: 0.4518  data: 0.0007  max mem: 19734
Epoch: [34]  [ 760/1251]  eta: 0:04:00  lr: 0.000015  loss: 3.7101 (3.4085)  time: 0.4539  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5202, ratio_loss=0.0050, pruning_loss=0.1228, mse_loss=0.4376
Epoch: [34]  [ 770/1251]  eta: 0:03:55  lr: 0.000015  loss: 3.4366 (3.4039)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [34]  [ 780/1251]  eta: 0:03:50  lr: 0.000015  loss: 3.3728 (3.4049)  time: 0.4646  data: 0.0004  max mem: 19734
Epoch: [34]  [ 790/1251]  eta: 0:03:45  lr: 0.000015  loss: 3.6002 (3.4045)  time: 0.4655  data: 0.0005  max mem: 19734
Epoch: [34]  [ 800/1251]  eta: 0:03:40  lr: 0.000015  loss: 3.4939 (3.4058)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [34]  [ 810/1251]  eta: 0:03:35  lr: 0.000015  loss: 3.4939 (3.4073)  time: 0.4602  data: 0.0004  max mem: 19734
Epoch: [34]  [ 820/1251]  eta: 0:03:30  lr: 0.000015  loss: 3.4684 (3.4056)  time: 0.4878  data: 0.0004  max mem: 19734
Epoch: [34]  [ 830/1251]  eta: 0:03:25  lr: 0.000015  loss: 3.4684 (3.4063)  time: 0.4971  data: 0.0005  max mem: 19734
Epoch: [34]  [ 840/1251]  eta: 0:03:20  lr: 0.000015  loss: 3.5326 (3.4072)  time: 0.4788  data: 0.0006  max mem: 19734
Epoch: [34]  [ 850/1251]  eta: 0:03:15  lr: 0.000015  loss: 3.6011 (3.4083)  time: 0.4620  data: 0.0005  max mem: 19734
Epoch: [34]  [ 860/1251]  eta: 0:03:10  lr: 0.000015  loss: 3.5828 (3.4073)  time: 0.4515  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3797, ratio_loss=0.0049, pruning_loss=0.1247, mse_loss=0.4356
Epoch: [34]  [ 870/1251]  eta: 0:03:05  lr: 0.000015  loss: 3.5828 (3.4088)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [34]  [ 880/1251]  eta: 0:03:00  lr: 0.000015  loss: 3.5217 (3.4096)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [34]  [ 890/1251]  eta: 0:02:55  lr: 0.000015  loss: 3.6459 (3.4120)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [34]  [ 900/1251]  eta: 0:02:50  lr: 0.000015  loss: 3.6693 (3.4136)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [34]  [ 910/1251]  eta: 0:02:45  lr: 0.000015  loss: 3.4728 (3.4141)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [34]  [ 920/1251]  eta: 0:02:40  lr: 0.000015  loss: 3.4728 (3.4156)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [34]  [ 930/1251]  eta: 0:02:35  lr: 0.000015  loss: 3.5202 (3.4156)  time: 0.4656  data: 0.0004  max mem: 19734
Epoch: [34]  [ 940/1251]  eta: 0:02:30  lr: 0.000015  loss: 3.4441 (3.4149)  time: 0.4635  data: 0.0005  max mem: 19734
Epoch: [34]  [ 950/1251]  eta: 0:02:25  lr: 0.000015  loss: 3.3784 (3.4158)  time: 0.4684  data: 0.0004  max mem: 19734
Epoch: [34]  [ 960/1251]  eta: 0:02:21  lr: 0.000015  loss: 3.3202 (3.4124)  time: 0.4868  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4176, ratio_loss=0.0046, pruning_loss=0.1244, mse_loss=0.4327
Epoch: [34]  [ 970/1251]  eta: 0:02:16  lr: 0.000015  loss: 3.0552 (3.4110)  time: 0.4806  data: 0.0005  max mem: 19734
Epoch: [34]  [ 980/1251]  eta: 0:02:11  lr: 0.000015  loss: 3.3566 (3.4120)  time: 0.4827  data: 0.0004  max mem: 19734
Epoch: [34]  [ 990/1251]  eta: 0:02:06  lr: 0.000015  loss: 3.5330 (3.4135)  time: 0.4806  data: 0.0004  max mem: 19734
Epoch: [34]  [1000/1251]  eta: 0:02:01  lr: 0.000015  loss: 3.3310 (3.4113)  time: 0.4634  data: 0.0005  max mem: 19734
Epoch: [34]  [1010/1251]  eta: 0:01:56  lr: 0.000015  loss: 3.1981 (3.4111)  time: 0.4555  data: 0.0006  max mem: 19734
Epoch: [34]  [1020/1251]  eta: 0:01:51  lr: 0.000015  loss: 3.4431 (3.4129)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [34]  [1030/1251]  eta: 0:01:46  lr: 0.000015  loss: 3.4431 (3.4121)  time: 0.4535  data: 0.0009  max mem: 19734
Epoch: [34]  [1040/1251]  eta: 0:01:41  lr: 0.000015  loss: 3.3875 (3.4113)  time: 0.4531  data: 0.0008  max mem: 19734
Epoch: [34]  [1050/1251]  eta: 0:01:37  lr: 0.000015  loss: 3.3875 (3.4094)  time: 0.4515  data: 0.0004  max mem: 19734
Epoch: [34]  [1060/1251]  eta: 0:01:32  lr: 0.000015  loss: 3.4330 (3.4107)  time: 0.4516  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3994, ratio_loss=0.0050, pruning_loss=0.1251, mse_loss=0.4465
Epoch: [34]  [1070/1251]  eta: 0:01:27  lr: 0.000015  loss: 3.6011 (3.4122)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [34]  [1080/1251]  eta: 0:01:22  lr: 0.000015  loss: 3.5471 (3.4116)  time: 0.4753  data: 0.0005  max mem: 19734
Epoch: [34]  [1090/1251]  eta: 0:01:17  lr: 0.000015  loss: 3.0605 (3.4093)  time: 0.4719  data: 0.0005  max mem: 19734
Epoch: [34]  [1100/1251]  eta: 0:01:12  lr: 0.000015  loss: 3.4454 (3.4107)  time: 0.4640  data: 0.0005  max mem: 19734
Epoch: [34]  [1110/1251]  eta: 0:01:07  lr: 0.000015  loss: 3.5055 (3.4096)  time: 0.4870  data: 0.0008  max mem: 19734
Epoch: [34]  [1120/1251]  eta: 0:01:03  lr: 0.000015  loss: 3.4920 (3.4113)  time: 0.4868  data: 0.0007  max mem: 19734
Epoch: [34]  [1130/1251]  eta: 0:00:58  lr: 0.000015  loss: 3.4372 (3.4105)  time: 0.4703  data: 0.0004  max mem: 19734
Epoch: [34]  [1140/1251]  eta: 0:00:53  lr: 0.000015  loss: 3.2762 (3.4105)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [34]  [1150/1251]  eta: 0:00:48  lr: 0.000015  loss: 3.4900 (3.4115)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [34]  [1160/1251]  eta: 0:00:43  lr: 0.000015  loss: 3.6534 (3.4144)  time: 0.4550  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4112, ratio_loss=0.0050, pruning_loss=0.1252, mse_loss=0.4514
Epoch: [34]  [1170/1251]  eta: 0:00:38  lr: 0.000015  loss: 3.5744 (3.4135)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [34]  [1180/1251]  eta: 0:00:34  lr: 0.000015  loss: 3.4771 (3.4145)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [34]  [1190/1251]  eta: 0:00:29  lr: 0.000015  loss: 3.6110 (3.4156)  time: 0.4522  data: 0.0008  max mem: 19734
Epoch: [34]  [1200/1251]  eta: 0:00:24  lr: 0.000015  loss: 3.4851 (3.4121)  time: 0.4489  data: 0.0006  max mem: 19734
Epoch: [34]  [1210/1251]  eta: 0:00:19  lr: 0.000015  loss: 3.3518 (3.4138)  time: 0.4461  data: 0.0001  max mem: 19734
Epoch: [34]  [1220/1251]  eta: 0:00:14  lr: 0.000015  loss: 3.5916 (3.4144)  time: 0.4456  data: 0.0002  max mem: 19734
Epoch: [34]  [1230/1251]  eta: 0:00:10  lr: 0.000015  loss: 3.5525 (3.4136)  time: 0.4545  data: 0.0001  max mem: 19734
Epoch: [34]  [1240/1251]  eta: 0:00:05  lr: 0.000015  loss: 3.4168 (3.4111)  time: 0.4558  data: 0.0001  max mem: 19734
Epoch: [34]  [1250/1251]  eta: 0:00:00  lr: 0.000015  loss: 3.4810 (3.4113)  time: 0.4637  data: 0.0001  max mem: 19734
Epoch: [34] Total time: 0:10:00 (0.4799 s / it)
Averaged stats: lr: 0.000015  loss: 3.4810 (3.4214)
Test:  [  0/261]  eta: 1:58:51  loss: 0.7315 (0.7315)  acc1: 80.7292 (80.7292)  acc5: 96.3542 (96.3542)  time: 27.3228  data: 27.1332  max mem: 19734
Test:  [ 10/261]  eta: 0:11:08  loss: 0.7315 (0.7280)  acc1: 83.8542 (83.9015)  acc5: 96.8750 (96.4015)  time: 2.6615  data: 2.4796  max mem: 19734
Test:  [ 20/261]  eta: 0:05:55  loss: 0.9401 (0.9076)  acc1: 79.1667 (79.1915)  acc5: 93.7500 (94.7421)  time: 0.1838  data: 0.0203  max mem: 19734
Test:  [ 30/261]  eta: 0:04:03  loss: 0.8704 (0.8252)  acc1: 80.2083 (81.9893)  acc5: 93.7500 (95.2285)  time: 0.1698  data: 0.0243  max mem: 19734
Test:  [ 40/261]  eta: 0:03:34  loss: 0.5516 (0.7875)  acc1: 88.5417 (82.9522)  acc5: 96.8750 (95.4903)  time: 0.4401  data: 0.3034  max mem: 19734
Test:  [ 50/261]  eta: 0:02:55  loss: 0.9140 (0.8530)  acc1: 77.6042 (80.9028)  acc5: 95.3125 (95.1083)  time: 0.4856  data: 0.3448  max mem: 19734
Test:  [ 60/261]  eta: 0:02:24  loss: 0.9922 (0.8646)  acc1: 74.4792 (80.4218)  acc5: 94.2708 (95.1503)  time: 0.2077  data: 0.0577  max mem: 19734
Test:  [ 70/261]  eta: 0:02:18  loss: 0.9261 (0.8629)  acc1: 76.5625 (80.0836)  acc5: 96.8750 (95.3565)  time: 0.4626  data: 0.3181  max mem: 19734
Test:  [ 80/261]  eta: 0:01:59  loss: 0.8564 (0.8639)  acc1: 79.6875 (80.1183)  acc5: 96.8750 (95.4411)  time: 0.4727  data: 0.3225  max mem: 19734
Test:  [ 90/261]  eta: 0:01:44  loss: 0.8408 (0.8505)  acc1: 83.3333 (80.5288)  acc5: 96.3542 (95.5300)  time: 0.1871  data: 0.0150  max mem: 19734
Test:  [100/261]  eta: 0:01:38  loss: 0.8307 (0.8537)  acc1: 83.3333 (80.4868)  acc5: 95.3125 (95.5703)  time: 0.4205  data: 0.2280  max mem: 19734
Test:  [110/261]  eta: 0:01:29  loss: 0.8729 (0.8773)  acc1: 77.6042 (79.9831)  acc5: 94.7917 (95.2656)  time: 0.5048  data: 0.2503  max mem: 19734
Test:  [120/261]  eta: 0:01:18  loss: 1.2128 (0.9180)  acc1: 71.3542 (78.9730)  acc5: 90.1042 (94.7615)  time: 0.2827  data: 0.0327  max mem: 19734
Test:  [130/261]  eta: 0:01:10  loss: 1.3422 (0.9620)  acc1: 65.1042 (78.0813)  acc5: 88.0208 (94.1953)  time: 0.2375  data: 0.0525  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: 1.2943 (0.9885)  acc1: 66.6667 (77.4084)  acc5: 89.0625 (93.8904)  time: 0.3621  data: 0.2123  max mem: 19734
Test:  [150/261]  eta: 0:00:56  loss: 1.1935 (0.9912)  acc1: 72.9167 (77.4352)  acc5: 91.1458 (93.7741)  time: 0.3436  data: 0.2301  max mem: 19734
Test:  [160/261]  eta: 0:00:49  loss: 0.9718 (1.0106)  acc1: 78.6458 (77.1222)  acc5: 91.6667 (93.4556)  time: 0.1797  data: 0.0699  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: 1.2812 (1.0403)  acc1: 65.6250 (76.3919)  acc5: 86.9792 (93.1408)  time: 0.1398  data: 0.0448  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4624 (1.0580)  acc1: 65.1042 (75.9784)  acc5: 89.5833 (93.0047)  time: 0.1123  data: 0.0419  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: 1.4114 (1.0714)  acc1: 66.6667 (75.7117)  acc5: 90.6250 (92.8420)  time: 0.0703  data: 0.0080  max mem: 19734
Test:  [200/261]  eta: 0:00:24  loss: 1.3256 (1.0861)  acc1: 72.3958 (75.4120)  acc5: 89.5833 (92.6021)  time: 0.0682  data: 0.0066  max mem: 19734
Test:  [210/261]  eta: 0:00:19  loss: 1.3348 (1.0992)  acc1: 68.2292 (75.1432)  acc5: 88.5417 (92.3948)  time: 0.0648  data: 0.0028  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.3846 (1.1186)  acc1: 67.1875 (74.6347)  acc5: 88.5417 (92.1663)  time: 0.0646  data: 0.0026  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4527 (1.1280)  acc1: 67.1875 (74.4228)  acc5: 89.0625 (92.0770)  time: 0.0638  data: 0.0023  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.2791 (1.1374)  acc1: 69.7917 (74.2198)  acc5: 91.6667 (92.0016)  time: 0.0642  data: 0.0028  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0529 (1.1299)  acc1: 75.0000 (74.3837)  acc5: 93.7500 (92.1087)  time: 0.0620  data: 0.0006  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9673 (1.1298)  acc1: 76.0417 (74.3820)  acc5: 95.3125 (92.1660)  time: 0.0600  data: 0.0002  max mem: 19734
Test: Total time: 0:01:25 (0.3291 s / it)
* Acc@1 74.382 Acc@5 92.166 loss 1.130
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.62%
Epoch: [35]  [   0/1251]  eta: 4:57:57  lr: 0.000015  loss: 3.7811 (3.7811)  time: 14.2909  data: 13.1607  max mem: 19734
Epoch: [35]  [  10/1251]  eta: 0:47:10  lr: 0.000015  loss: 3.7196 (3.4179)  time: 2.2807  data: 1.2422  max mem: 19734
loss info: cls_loss=3.3222, ratio_loss=0.0049, pruning_loss=0.1272, mse_loss=0.4611
Epoch: [35]  [  20/1251]  eta: 0:28:58  lr: 0.000015  loss: 3.6539 (3.3072)  time: 0.7680  data: 0.0253  max mem: 19734
Epoch: [35]  [  30/1251]  eta: 0:22:30  lr: 0.000015  loss: 3.4433 (3.3754)  time: 0.4595  data: 0.0003  max mem: 19734
Epoch: [35]  [  40/1251]  eta: 0:19:08  lr: 0.000015  loss: 3.5543 (3.4279)  time: 0.4625  data: 0.0003  max mem: 19734
Epoch: [35]  [  50/1251]  eta: 0:17:04  lr: 0.000015  loss: 3.5105 (3.4036)  time: 0.4608  data: 0.0004  max mem: 19734
Epoch: [35]  [  60/1251]  eta: 0:15:39  lr: 0.000015  loss: 3.4879 (3.4136)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [35]  [  70/1251]  eta: 0:14:38  lr: 0.000015  loss: 3.6352 (3.4344)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [35]  [  80/1251]  eta: 0:13:50  lr: 0.000015  loss: 3.5169 (3.4147)  time: 0.4638  data: 0.0005  max mem: 19734
Epoch: [35]  [  90/1251]  eta: 0:13:13  lr: 0.000015  loss: 3.3718 (3.4068)  time: 0.4677  data: 0.0004  max mem: 19734
Epoch: [35]  [ 100/1251]  eta: 0:12:40  lr: 0.000015  loss: 3.4777 (3.4164)  time: 0.4669  data: 0.0004  max mem: 19734
Epoch: [35]  [ 110/1251]  eta: 0:12:12  lr: 0.000015  loss: 3.4748 (3.3978)  time: 0.4570  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4048, ratio_loss=0.0046, pruning_loss=0.1252, mse_loss=0.4240
Epoch: [35]  [ 120/1251]  eta: 0:11:49  lr: 0.000015  loss: 3.5038 (3.4133)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [35]  [ 130/1251]  eta: 0:11:30  lr: 0.000015  loss: 3.5403 (3.4140)  time: 0.4682  data: 0.0005  max mem: 19734
Epoch: [35]  [ 140/1251]  eta: 0:11:15  lr: 0.000015  loss: 3.5403 (3.4173)  time: 0.4884  data: 0.0005  max mem: 19734
Epoch: [35]  [ 150/1251]  eta: 0:11:00  lr: 0.000015  loss: 3.6091 (3.4284)  time: 0.4952  data: 0.0005  max mem: 19734
Epoch: [35]  [ 160/1251]  eta: 0:10:45  lr: 0.000015  loss: 3.4891 (3.4154)  time: 0.4789  data: 0.0005  max mem: 19734
Epoch: [35]  [ 170/1251]  eta: 0:10:30  lr: 0.000015  loss: 3.4891 (3.4139)  time: 0.4604  data: 0.0005  max mem: 19734
Epoch: [35]  [ 180/1251]  eta: 0:10:17  lr: 0.000015  loss: 3.5441 (3.4106)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [35]  [ 190/1251]  eta: 0:10:04  lr: 0.000015  loss: 3.5916 (3.4198)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [35]  [ 200/1251]  eta: 0:09:53  lr: 0.000015  loss: 3.4967 (3.4147)  time: 0.4545  data: 0.0005  max mem: 19734
Epoch: [35]  [ 210/1251]  eta: 0:09:41  lr: 0.000015  loss: 3.4338 (3.4197)  time: 0.4539  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4188, ratio_loss=0.0048, pruning_loss=0.1239, mse_loss=0.4448
Epoch: [35]  [ 220/1251]  eta: 0:09:31  lr: 0.000015  loss: 3.6539 (3.4283)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [35]  [ 230/1251]  eta: 0:09:22  lr: 0.000015  loss: 3.3232 (3.4210)  time: 0.4628  data: 0.0004  max mem: 19734
Epoch: [35]  [ 240/1251]  eta: 0:09:12  lr: 0.000015  loss: 3.2687 (3.4152)  time: 0.4639  data: 0.0005  max mem: 19734
Epoch: [35]  [ 250/1251]  eta: 0:09:03  lr: 0.000015  loss: 3.3996 (3.4208)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [35]  [ 260/1251]  eta: 0:08:55  lr: 0.000015  loss: 3.4950 (3.4196)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [35]  [ 270/1251]  eta: 0:08:47  lr: 0.000015  loss: 3.6049 (3.4315)  time: 0.4679  data: 0.0004  max mem: 19734
Epoch: [35]  [ 280/1251]  eta: 0:08:40  lr: 0.000015  loss: 3.4195 (3.4241)  time: 0.4838  data: 0.0005  max mem: 19734
Epoch: [35]  [ 290/1251]  eta: 0:08:33  lr: 0.000015  loss: 3.4035 (3.4263)  time: 0.4816  data: 0.0006  max mem: 19734
Epoch: [35]  [ 300/1251]  eta: 0:08:26  lr: 0.000015  loss: 3.5018 (3.4233)  time: 0.4897  data: 0.0007  max mem: 19734
Epoch: [35]  [ 310/1251]  eta: 0:08:19  lr: 0.000015  loss: 3.4024 (3.4196)  time: 0.4834  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3915, ratio_loss=0.0048, pruning_loss=0.1253, mse_loss=0.4460
Epoch: [35]  [ 320/1251]  eta: 0:08:11  lr: 0.000015  loss: 3.4815 (3.4201)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [35]  [ 330/1251]  eta: 0:08:04  lr: 0.000015  loss: 3.5799 (3.4291)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [35]  [ 340/1251]  eta: 0:07:57  lr: 0.000015  loss: 3.6277 (3.4359)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [35]  [ 350/1251]  eta: 0:07:50  lr: 0.000015  loss: 3.6127 (3.4315)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [35]  [ 360/1251]  eta: 0:07:43  lr: 0.000015  loss: 3.4778 (3.4334)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [35]  [ 370/1251]  eta: 0:07:36  lr: 0.000015  loss: 3.6319 (3.4371)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [35]  [ 380/1251]  eta: 0:07:30  lr: 0.000015  loss: 3.6260 (3.4409)  time: 0.4654  data: 0.0004  max mem: 19734
Epoch: [35]  [ 390/1251]  eta: 0:07:24  lr: 0.000015  loss: 3.5599 (3.4406)  time: 0.4643  data: 0.0004  max mem: 19734
Epoch: [35]  [ 400/1251]  eta: 0:07:17  lr: 0.000015  loss: 3.5723 (3.4455)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [35]  [ 410/1251]  eta: 0:07:11  lr: 0.000015  loss: 3.7668 (3.4495)  time: 0.4574  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5313, ratio_loss=0.0052, pruning_loss=0.1210, mse_loss=0.4343
Epoch: [35]  [ 420/1251]  eta: 0:07:05  lr: 0.000015  loss: 3.6143 (3.4494)  time: 0.4647  data: 0.0004  max mem: 19734
Epoch: [35]  [ 430/1251]  eta: 0:07:00  lr: 0.000015  loss: 3.4258 (3.4448)  time: 0.4917  data: 0.0005  max mem: 19734
Epoch: [35]  [ 440/1251]  eta: 0:06:54  lr: 0.000015  loss: 3.5406 (3.4462)  time: 0.4974  data: 0.0005  max mem: 19734
Epoch: [35]  [ 450/1251]  eta: 0:06:49  lr: 0.000015  loss: 3.5494 (3.4453)  time: 0.4819  data: 0.0004  max mem: 19734
Epoch: [35]  [ 460/1251]  eta: 0:06:42  lr: 0.000015  loss: 3.5494 (3.4499)  time: 0.4661  data: 0.0004  max mem: 19734
Epoch: [35]  [ 470/1251]  eta: 0:06:37  lr: 0.000015  loss: 3.7242 (3.4532)  time: 0.4581  data: 0.0005  max mem: 19734
Epoch: [35]  [ 480/1251]  eta: 0:06:31  lr: 0.000015  loss: 3.4767 (3.4498)  time: 0.4603  data: 0.0004  max mem: 19734
Epoch: [35]  [ 490/1251]  eta: 0:06:25  lr: 0.000015  loss: 3.3274 (3.4512)  time: 0.4588  data: 0.0004  max mem: 19734
Epoch: [35]  [ 500/1251]  eta: 0:06:19  lr: 0.000015  loss: 3.5062 (3.4477)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [35]  [ 510/1251]  eta: 0:06:13  lr: 0.000015  loss: 3.3232 (3.4454)  time: 0.4614  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3883, ratio_loss=0.0048, pruning_loss=0.1259, mse_loss=0.4419
Epoch: [35]  [ 520/1251]  eta: 0:06:08  lr: 0.000015  loss: 3.5508 (3.4465)  time: 0.4632  data: 0.0004  max mem: 19734
Epoch: [35]  [ 530/1251]  eta: 0:06:02  lr: 0.000015  loss: 3.5451 (3.4454)  time: 0.4670  data: 0.0004  max mem: 19734
Epoch: [35]  [ 540/1251]  eta: 0:05:57  lr: 0.000015  loss: 3.3714 (3.4445)  time: 0.4659  data: 0.0004  max mem: 19734
Epoch: [35]  [ 550/1251]  eta: 0:05:51  lr: 0.000015  loss: 3.3714 (3.4428)  time: 0.4608  data: 0.0004  max mem: 19734
Epoch: [35]  [ 560/1251]  eta: 0:05:46  lr: 0.000015  loss: 3.5024 (3.4426)  time: 0.4667  data: 0.0004  max mem: 19734
Epoch: [35]  [ 570/1251]  eta: 0:05:41  lr: 0.000015  loss: 3.5466 (3.4467)  time: 0.4911  data: 0.0004  max mem: 19734
Epoch: [35]  [ 580/1251]  eta: 0:05:36  lr: 0.000015  loss: 3.7092 (3.4489)  time: 0.4945  data: 0.0004  max mem: 19734
Epoch: [35]  [ 590/1251]  eta: 0:05:31  lr: 0.000015  loss: 3.5976 (3.4479)  time: 0.5006  data: 0.0004  max mem: 19734
Epoch: [35]  [ 600/1251]  eta: 0:05:25  lr: 0.000015  loss: 3.5976 (3.4495)  time: 0.4890  data: 0.0004  max mem: 19734
Epoch: [35]  [ 610/1251]  eta: 0:05:20  lr: 0.000015  loss: 3.6082 (3.4493)  time: 0.4582  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4410, ratio_loss=0.0050, pruning_loss=0.1240, mse_loss=0.4563
Epoch: [35]  [ 620/1251]  eta: 0:05:14  lr: 0.000015  loss: 3.5219 (3.4470)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [35]  [ 630/1251]  eta: 0:05:09  lr: 0.000015  loss: 3.3284 (3.4408)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [35]  [ 640/1251]  eta: 0:05:04  lr: 0.000015  loss: 3.1910 (3.4381)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [35]  [ 650/1251]  eta: 0:04:58  lr: 0.000015  loss: 3.5548 (3.4399)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [35]  [ 660/1251]  eta: 0:04:53  lr: 0.000015  loss: 3.5405 (3.4386)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [35]  [ 670/1251]  eta: 0:04:48  lr: 0.000015  loss: 3.2384 (3.4338)  time: 0.4659  data: 0.0004  max mem: 19734
Epoch: [35]  [ 680/1251]  eta: 0:04:42  lr: 0.000015  loss: 3.4338 (3.4357)  time: 0.4646  data: 0.0004  max mem: 19734
Epoch: [35]  [ 690/1251]  eta: 0:04:37  lr: 0.000015  loss: 3.5411 (3.4340)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [35]  [ 700/1251]  eta: 0:04:32  lr: 0.000015  loss: 3.4512 (3.4308)  time: 0.4680  data: 0.0006  max mem: 19734
Epoch: [35]  [ 710/1251]  eta: 0:04:27  lr: 0.000015  loss: 3.5169 (3.4333)  time: 0.4698  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3158, ratio_loss=0.0053, pruning_loss=0.1263, mse_loss=0.4514
Epoch: [35]  [ 720/1251]  eta: 0:04:22  lr: 0.000015  loss: 3.6669 (3.4331)  time: 0.4917  data: 0.0007  max mem: 19734
Epoch: [35]  [ 730/1251]  eta: 0:04:17  lr: 0.000015  loss: 3.5209 (3.4317)  time: 0.5147  data: 0.0006  max mem: 19734
Epoch: [35]  [ 740/1251]  eta: 0:04:12  lr: 0.000015  loss: 3.3516 (3.4274)  time: 0.4799  data: 0.0004  max mem: 19734
Epoch: [35]  [ 750/1251]  eta: 0:04:07  lr: 0.000015  loss: 2.8978 (3.4228)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [35]  [ 760/1251]  eta: 0:04:02  lr: 0.000015  loss: 3.7214 (3.4280)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [35]  [ 770/1251]  eta: 0:03:56  lr: 0.000015  loss: 3.5894 (3.4245)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [35]  [ 780/1251]  eta: 0:03:51  lr: 0.000015  loss: 3.3925 (3.4255)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [35]  [ 790/1251]  eta: 0:03:46  lr: 0.000015  loss: 3.5771 (3.4237)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [35]  [ 800/1251]  eta: 0:03:41  lr: 0.000015  loss: 3.6954 (3.4259)  time: 0.4512  data: 0.0004  max mem: 19734
Epoch: [35]  [ 810/1251]  eta: 0:03:36  lr: 0.000015  loss: 3.4693 (3.4250)  time: 0.4522  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3358, ratio_loss=0.0052, pruning_loss=0.1266, mse_loss=0.4267
Epoch: [35]  [ 820/1251]  eta: 0:03:31  lr: 0.000015  loss: 3.4693 (3.4269)  time: 0.4627  data: 0.0006  max mem: 19734
Epoch: [35]  [ 830/1251]  eta: 0:03:26  lr: 0.000015  loss: 3.5980 (3.4273)  time: 0.4629  data: 0.0006  max mem: 19734
Epoch: [35]  [ 840/1251]  eta: 0:03:21  lr: 0.000015  loss: 3.5980 (3.4276)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [35]  [ 850/1251]  eta: 0:03:16  lr: 0.000015  loss: 3.6240 (3.4264)  time: 0.4645  data: 0.0004  max mem: 19734
Epoch: [35]  [ 860/1251]  eta: 0:03:11  lr: 0.000015  loss: 3.5069 (3.4278)  time: 0.4888  data: 0.0005  max mem: 19734
Epoch: [35]  [ 870/1251]  eta: 0:03:06  lr: 0.000015  loss: 3.6217 (3.4283)  time: 0.4754  data: 0.0005  max mem: 19734
Epoch: [35]  [ 880/1251]  eta: 0:03:01  lr: 0.000015  loss: 3.6217 (3.4281)  time: 0.4742  data: 0.0005  max mem: 19734
Epoch: [35]  [ 890/1251]  eta: 0:02:56  lr: 0.000015  loss: 3.3698 (3.4259)  time: 0.4775  data: 0.0005  max mem: 19734
Epoch: [35]  [ 900/1251]  eta: 0:02:51  lr: 0.000015  loss: 3.3001 (3.4256)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [35]  [ 910/1251]  eta: 0:02:46  lr: 0.000015  loss: 3.3053 (3.4250)  time: 0.4529  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3997, ratio_loss=0.0047, pruning_loss=0.1251, mse_loss=0.4328
Epoch: [35]  [ 920/1251]  eta: 0:02:41  lr: 0.000015  loss: 3.4392 (3.4250)  time: 0.4542  data: 0.0006  max mem: 19734
Epoch: [35]  [ 930/1251]  eta: 0:02:36  lr: 0.000015  loss: 3.5726 (3.4265)  time: 0.4547  data: 0.0006  max mem: 19734
Epoch: [35]  [ 940/1251]  eta: 0:02:31  lr: 0.000015  loss: 3.6546 (3.4286)  time: 0.4565  data: 0.0005  max mem: 19734
Epoch: [35]  [ 950/1251]  eta: 0:02:26  lr: 0.000015  loss: 3.6649 (3.4304)  time: 0.4555  data: 0.0005  max mem: 19734
Epoch: [35]  [ 960/1251]  eta: 0:02:21  lr: 0.000015  loss: 3.6581 (3.4308)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [35]  [ 970/1251]  eta: 0:02:16  lr: 0.000015  loss: 3.4974 (3.4297)  time: 0.4648  data: 0.0006  max mem: 19734
Epoch: [35]  [ 980/1251]  eta: 0:02:11  lr: 0.000015  loss: 3.3226 (3.4285)  time: 0.4648  data: 0.0006  max mem: 19734
Epoch: [35]  [ 990/1251]  eta: 0:02:06  lr: 0.000015  loss: 3.3759 (3.4284)  time: 0.4675  data: 0.0004  max mem: 19734
Epoch: [35]  [1000/1251]  eta: 0:02:01  lr: 0.000015  loss: 3.6005 (3.4291)  time: 0.4717  data: 0.0004  max mem: 19734
Epoch: [35]  [1010/1251]  eta: 0:01:57  lr: 0.000015  loss: 3.5318 (3.4289)  time: 0.4753  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4536, ratio_loss=0.0050, pruning_loss=0.1237, mse_loss=0.4317
Epoch: [35]  [1020/1251]  eta: 0:01:52  lr: 0.000015  loss: 3.5893 (3.4316)  time: 0.4890  data: 0.0005  max mem: 19734
Epoch: [35]  [1030/1251]  eta: 0:01:47  lr: 0.000015  loss: 3.5893 (3.4282)  time: 0.4746  data: 0.0005  max mem: 19734
Epoch: [35]  [1040/1251]  eta: 0:01:42  lr: 0.000015  loss: 3.2179 (3.4264)  time: 0.4566  data: 0.0006  max mem: 19734
Epoch: [35]  [1050/1251]  eta: 0:01:37  lr: 0.000015  loss: 3.5455 (3.4277)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [35]  [1060/1251]  eta: 0:01:32  lr: 0.000015  loss: 3.4237 (3.4254)  time: 0.4588  data: 0.0005  max mem: 19734
Epoch: [35]  [1070/1251]  eta: 0:01:27  lr: 0.000015  loss: 3.2137 (3.4237)  time: 0.4565  data: 0.0005  max mem: 19734
Epoch: [35]  [1080/1251]  eta: 0:01:22  lr: 0.000015  loss: 3.4713 (3.4214)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [35]  [1090/1251]  eta: 0:01:17  lr: 0.000015  loss: 3.2046 (3.4200)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [35]  [1100/1251]  eta: 0:01:13  lr: 0.000015  loss: 3.6085 (3.4218)  time: 0.4536  data: 0.0006  max mem: 19734
Epoch: [35]  [1110/1251]  eta: 0:01:08  lr: 0.000015  loss: 3.6395 (3.4228)  time: 0.4535  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3080, ratio_loss=0.0048, pruning_loss=0.1263, mse_loss=0.4216
Epoch: [35]  [1120/1251]  eta: 0:01:03  lr: 0.000015  loss: 3.5458 (3.4203)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [35]  [1130/1251]  eta: 0:00:58  lr: 0.000015  loss: 3.2186 (3.4203)  time: 0.4613  data: 0.0004  max mem: 19734
Epoch: [35]  [1140/1251]  eta: 0:00:53  lr: 0.000015  loss: 3.4893 (3.4202)  time: 0.4646  data: 0.0005  max mem: 19734
Epoch: [35]  [1150/1251]  eta: 0:00:48  lr: 0.000015  loss: 3.4893 (3.4187)  time: 0.4829  data: 0.0005  max mem: 19734
Epoch: [35]  [1160/1251]  eta: 0:00:43  lr: 0.000015  loss: 3.6060 (3.4203)  time: 0.4799  data: 0.0005  max mem: 19734
Epoch: [35]  [1170/1251]  eta: 0:00:39  lr: 0.000015  loss: 3.7146 (3.4213)  time: 0.4720  data: 0.0005  max mem: 19734
Epoch: [35]  [1180/1251]  eta: 0:00:34  lr: 0.000015  loss: 3.5732 (3.4205)  time: 0.4640  data: 0.0005  max mem: 19734
Epoch: [35]  [1190/1251]  eta: 0:00:29  lr: 0.000015  loss: 3.4807 (3.4219)  time: 0.4545  data: 0.0010  max mem: 19734
Epoch: [35]  [1200/1251]  eta: 0:00:24  lr: 0.000015  loss: 3.4098 (3.4187)  time: 0.4491  data: 0.0008  max mem: 19734
Epoch: [35]  [1210/1251]  eta: 0:00:19  lr: 0.000015  loss: 3.3237 (3.4191)  time: 0.4442  data: 0.0002  max mem: 19734
loss info: cls_loss=3.3908, ratio_loss=0.0051, pruning_loss=0.1239, mse_loss=0.4230
Epoch: [35]  [1220/1251]  eta: 0:00:14  lr: 0.000015  loss: 3.5546 (3.4208)  time: 0.4440  data: 0.0002  max mem: 19734
Epoch: [35]  [1230/1251]  eta: 0:00:10  lr: 0.000015  loss: 3.5334 (3.4206)  time: 0.4438  data: 0.0002  max mem: 19734
Epoch: [35]  [1240/1251]  eta: 0:00:05  lr: 0.000015  loss: 3.5219 (3.4201)  time: 0.4438  data: 0.0002  max mem: 19734
Epoch: [35]  [1250/1251]  eta: 0:00:00  lr: 0.000015  loss: 3.5219 (3.4218)  time: 0.4446  data: 0.0002  max mem: 19734
Epoch: [35] Total time: 0:10:01 (0.4810 s / it)
Averaged stats: lr: 0.000015  loss: 3.5219 (3.4206)
Test:  [  0/261]  eta: 1:16:56  loss: 0.7289 (0.7289)  acc1: 84.3750 (84.3750)  acc5: 96.3542 (96.3542)  time: 17.6861  data: 17.5393  max mem: 19734
Test:  [ 10/261]  eta: 0:09:39  loss: 0.7198 (0.7293)  acc1: 84.8958 (84.1856)  acc5: 97.3958 (96.5436)  time: 2.3070  data: 2.1750  max mem: 19734
Test:  [ 20/261]  eta: 0:05:25  loss: 0.9330 (0.9008)  acc1: 78.1250 (79.0179)  acc5: 94.7917 (94.8165)  time: 0.5322  data: 0.3311  max mem: 19734
Test:  [ 30/261]  eta: 0:03:57  loss: 0.8305 (0.8182)  acc1: 81.2500 (82.0229)  acc5: 94.7917 (95.2621)  time: 0.3232  data: 0.0226  max mem: 19734
Test:  [ 40/261]  eta: 0:03:33  loss: 0.5865 (0.7813)  acc1: 88.0208 (82.9649)  acc5: 96.8750 (95.5920)  time: 0.5676  data: 0.3351  max mem: 19734
Test:  [ 50/261]  eta: 0:02:52  loss: 0.8988 (0.8424)  acc1: 76.5625 (81.1275)  acc5: 94.7917 (95.2002)  time: 0.4955  data: 0.3607  max mem: 19734
Test:  [ 60/261]  eta: 0:02:23  loss: 0.9964 (0.8521)  acc1: 75.5208 (80.6950)  acc5: 94.2708 (95.1674)  time: 0.1950  data: 0.0464  max mem: 19734
Test:  [ 70/261]  eta: 0:02:11  loss: 0.9282 (0.8537)  acc1: 77.6042 (80.2377)  acc5: 96.3542 (95.3418)  time: 0.3519  data: 0.2026  max mem: 19734
Test:  [ 80/261]  eta: 0:01:57  loss: 0.8475 (0.8546)  acc1: 80.7292 (80.2791)  acc5: 96.8750 (95.4733)  time: 0.4433  data: 0.2734  max mem: 19734
Test:  [ 90/261]  eta: 0:01:41  loss: 0.8218 (0.8418)  acc1: 83.3333 (80.6605)  acc5: 95.8333 (95.5586)  time: 0.2628  data: 0.0860  max mem: 19734
Test:  [100/261]  eta: 0:01:32  loss: 0.8218 (0.8457)  acc1: 83.3333 (80.5796)  acc5: 95.3125 (95.6322)  time: 0.2731  data: 0.1240  max mem: 19734
Test:  [110/261]  eta: 0:01:26  loss: 0.8941 (0.8671)  acc1: 77.0833 (80.1849)  acc5: 94.7917 (95.3735)  time: 0.4669  data: 0.3167  max mem: 19734
Test:  [120/261]  eta: 0:01:15  loss: 1.2095 (0.9079)  acc1: 71.8750 (79.2657)  acc5: 90.1042 (94.8433)  time: 0.3518  data: 0.2023  max mem: 19734
Test:  [130/261]  eta: 0:01:06  loss: 1.3643 (0.9539)  acc1: 67.1875 (78.3198)  acc5: 86.9792 (94.2510)  time: 0.1726  data: 0.0303  max mem: 19734
Test:  [140/261]  eta: 0:00:58  loss: 1.3140 (0.9821)  acc1: 69.7917 (77.6522)  acc5: 89.5833 (93.9642)  time: 0.1713  data: 0.0313  max mem: 19734
Test:  [150/261]  eta: 0:00:55  loss: 1.2253 (0.9876)  acc1: 72.3958 (77.6456)  acc5: 91.1458 (93.8086)  time: 0.4020  data: 0.2668  max mem: 19734
Test:  [160/261]  eta: 0:00:48  loss: 1.0048 (1.0076)  acc1: 78.6458 (77.3033)  acc5: 91.6667 (93.4944)  time: 0.4470  data: 0.3181  max mem: 19734
Test:  [170/261]  eta: 0:00:41  loss: 1.3309 (1.0368)  acc1: 64.5833 (76.5595)  acc5: 88.0208 (93.1865)  time: 0.1931  data: 0.0628  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4773 (1.0550)  acc1: 64.5833 (76.1050)  acc5: 88.5417 (93.0162)  time: 0.2089  data: 0.0948  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: 1.3619 (1.0675)  acc1: 66.6667 (75.8644)  acc5: 90.6250 (92.8638)  time: 0.1898  data: 0.1099  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.3309 (1.0828)  acc1: 72.3958 (75.5545)  acc5: 89.5833 (92.6176)  time: 0.0831  data: 0.0213  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.3625 (1.0958)  acc1: 71.3542 (75.3061)  acc5: 88.5417 (92.4047)  time: 0.0654  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.3991 (1.1159)  acc1: 67.7083 (74.8492)  acc5: 88.0208 (92.1663)  time: 0.0736  data: 0.0080  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4123 (1.1249)  acc1: 66.1458 (74.6641)  acc5: 88.5417 (92.0883)  time: 0.0698  data: 0.0080  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3203 (1.1349)  acc1: 68.7500 (74.4295)  acc5: 91.1458 (91.9952)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0189 (1.1278)  acc1: 76.0417 (74.5995)  acc5: 93.2292 (92.1211)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9833 (1.1285)  acc1: 76.5625 (74.5820)  acc5: 95.3125 (92.1700)  time: 0.0602  data: 0.0002  max mem: 19734
Test: Total time: 0:01:27 (0.3345 s / it)
* Acc@1 74.582 Acc@5 92.170 loss 1.128
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.62%
Epoch: [36]  [   0/1251]  eta: 6:13:45  lr: 0.000015  loss: 3.9744 (3.9744)  time: 17.9257  data: 7.8270  max mem: 19734
Epoch: [36]  [  10/1251]  eta: 0:44:52  lr: 0.000015  loss: 3.5102 (3.3994)  time: 2.1699  data: 0.7454  max mem: 19734
Epoch: [36]  [  20/1251]  eta: 0:28:04  lr: 0.000015  loss: 3.3797 (3.3510)  time: 0.5408  data: 0.0188  max mem: 19734
Epoch: [36]  [  30/1251]  eta: 0:21:53  lr: 0.000015  loss: 3.3672 (3.3707)  time: 0.4741  data: 0.0005  max mem: 19734
Epoch: [36]  [  40/1251]  eta: 0:18:56  lr: 0.000015  loss: 3.5435 (3.4308)  time: 0.4872  data: 0.0005  max mem: 19734
Epoch: [36]  [  50/1251]  eta: 0:16:59  lr: 0.000015  loss: 3.7172 (3.5034)  time: 0.4971  data: 0.0004  max mem: 19734
Epoch: [36]  [  60/1251]  eta: 0:15:40  lr: 0.000015  loss: 3.6149 (3.5111)  time: 0.4849  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4701, ratio_loss=0.0051, pruning_loss=0.1228, mse_loss=0.4521
Epoch: [36]  [  70/1251]  eta: 0:14:38  lr: 0.000015  loss: 3.5160 (3.4924)  time: 0.4747  data: 0.0005  max mem: 19734
Epoch: [36]  [  80/1251]  eta: 0:13:49  lr: 0.000015  loss: 3.5222 (3.4796)  time: 0.4587  data: 0.0005  max mem: 19734
Epoch: [36]  [  90/1251]  eta: 0:13:10  lr: 0.000015  loss: 3.5404 (3.4801)  time: 0.4583  data: 0.0004  max mem: 19734
Epoch: [36]  [ 100/1251]  eta: 0:12:38  lr: 0.000015  loss: 3.5404 (3.4799)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [36]  [ 110/1251]  eta: 0:12:11  lr: 0.000015  loss: 3.5667 (3.4755)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [36]  [ 120/1251]  eta: 0:11:47  lr: 0.000015  loss: 3.4942 (3.4633)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [36]  [ 130/1251]  eta: 0:11:28  lr: 0.000015  loss: 3.4942 (3.4641)  time: 0.4676  data: 0.0015  max mem: 19734
Epoch: [36]  [ 140/1251]  eta: 0:11:10  lr: 0.000015  loss: 3.6465 (3.4643)  time: 0.4682  data: 0.0015  max mem: 19734
Epoch: [36]  [ 150/1251]  eta: 0:10:53  lr: 0.000015  loss: 3.5669 (3.4706)  time: 0.4579  data: 0.0006  max mem: 19734
Epoch: [36]  [ 160/1251]  eta: 0:10:38  lr: 0.000015  loss: 3.3172 (3.4486)  time: 0.4578  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3895, ratio_loss=0.0055, pruning_loss=0.1254, mse_loss=0.4554
Epoch: [36]  [ 170/1251]  eta: 0:10:25  lr: 0.000015  loss: 3.4571 (3.4527)  time: 0.4638  data: 0.0005  max mem: 19734
Epoch: [36]  [ 180/1251]  eta: 0:10:15  lr: 0.000015  loss: 3.6279 (3.4525)  time: 0.4860  data: 0.0005  max mem: 19734
Epoch: [36]  [ 190/1251]  eta: 0:10:03  lr: 0.000015  loss: 3.3265 (3.4495)  time: 0.4890  data: 0.0005  max mem: 19734
Epoch: [36]  [ 200/1251]  eta: 0:09:55  lr: 0.000015  loss: 3.5060 (3.4513)  time: 0.4951  data: 0.0005  max mem: 19734
Epoch: [36]  [ 210/1251]  eta: 0:09:44  lr: 0.000015  loss: 3.5533 (3.4447)  time: 0.4850  data: 0.0004  max mem: 19734
Epoch: [36]  [ 220/1251]  eta: 0:09:33  lr: 0.000015  loss: 3.4624 (3.4394)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [36]  [ 230/1251]  eta: 0:09:23  lr: 0.000015  loss: 3.5786 (3.4497)  time: 0.4525  data: 0.0005  max mem: 19734
Epoch: [36]  [ 240/1251]  eta: 0:09:13  lr: 0.000015  loss: 3.6933 (3.4480)  time: 0.4508  data: 0.0005  max mem: 19734
Epoch: [36]  [ 250/1251]  eta: 0:09:04  lr: 0.000015  loss: 3.6336 (3.4523)  time: 0.4507  data: 0.0005  max mem: 19734
Epoch: [36]  [ 260/1251]  eta: 0:08:55  lr: 0.000015  loss: 3.6336 (3.4575)  time: 0.4506  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4553, ratio_loss=0.0049, pruning_loss=0.1241, mse_loss=0.4428
Epoch: [36]  [ 270/1251]  eta: 0:08:47  lr: 0.000015  loss: 3.5706 (3.4640)  time: 0.4616  data: 0.0005  max mem: 19734
Epoch: [36]  [ 280/1251]  eta: 0:08:39  lr: 0.000015  loss: 3.6554 (3.4684)  time: 0.4611  data: 0.0005  max mem: 19734
Epoch: [36]  [ 290/1251]  eta: 0:08:31  lr: 0.000015  loss: 3.5159 (3.4595)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [36]  [ 300/1251]  eta: 0:08:23  lr: 0.000015  loss: 3.3461 (3.4550)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [36]  [ 310/1251]  eta: 0:08:16  lr: 0.000015  loss: 3.5957 (3.4576)  time: 0.4592  data: 0.0004  max mem: 19734
Epoch: [36]  [ 320/1251]  eta: 0:08:09  lr: 0.000015  loss: 3.1622 (3.4422)  time: 0.4709  data: 0.0004  max mem: 19734
Epoch: [36]  [ 330/1251]  eta: 0:08:03  lr: 0.000015  loss: 3.0175 (3.4411)  time: 0.4839  data: 0.0005  max mem: 19734
Epoch: [36]  [ 340/1251]  eta: 0:07:56  lr: 0.000015  loss: 3.4504 (3.4378)  time: 0.4817  data: 0.0005  max mem: 19734
Epoch: [36]  [ 350/1251]  eta: 0:07:50  lr: 0.000015  loss: 3.4504 (3.4363)  time: 0.4829  data: 0.0005  max mem: 19734
Epoch: [36]  [ 360/1251]  eta: 0:07:43  lr: 0.000015  loss: 3.6975 (3.4388)  time: 0.4764  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3550, ratio_loss=0.0047, pruning_loss=0.1268, mse_loss=0.4428
Epoch: [36]  [ 370/1251]  eta: 0:07:36  lr: 0.000015  loss: 3.5655 (3.4384)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [36]  [ 380/1251]  eta: 0:07:30  lr: 0.000015  loss: 3.4438 (3.4382)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [36]  [ 390/1251]  eta: 0:07:23  lr: 0.000015  loss: 3.6878 (3.4405)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [36]  [ 400/1251]  eta: 0:07:17  lr: 0.000015  loss: 3.5861 (3.4385)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [36]  [ 410/1251]  eta: 0:07:10  lr: 0.000015  loss: 3.4809 (3.4374)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [36]  [ 420/1251]  eta: 0:07:04  lr: 0.000015  loss: 3.4809 (3.4348)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [36]  [ 430/1251]  eta: 0:06:58  lr: 0.000015  loss: 3.3968 (3.4343)  time: 0.4635  data: 0.0005  max mem: 19734
Epoch: [36]  [ 440/1251]  eta: 0:06:52  lr: 0.000015  loss: 3.2299 (3.4306)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [36]  [ 450/1251]  eta: 0:06:46  lr: 0.000015  loss: 3.5408 (3.4365)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [36]  [ 460/1251]  eta: 0:06:40  lr: 0.000015  loss: 3.6377 (3.4302)  time: 0.4638  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3554, ratio_loss=0.0048, pruning_loss=0.1238, mse_loss=0.4375
Epoch: [36]  [ 470/1251]  eta: 0:06:35  lr: 0.000015  loss: 3.5277 (3.4289)  time: 0.4836  data: 0.0004  max mem: 19734
Epoch: [36]  [ 480/1251]  eta: 0:06:29  lr: 0.000015  loss: 3.2777 (3.4224)  time: 0.4720  data: 0.0004  max mem: 19734
Epoch: [36]  [ 490/1251]  eta: 0:06:24  lr: 0.000015  loss: 2.9701 (3.4216)  time: 0.4866  data: 0.0005  max mem: 19734
Epoch: [36]  [ 500/1251]  eta: 0:06:19  lr: 0.000015  loss: 2.8809 (3.4122)  time: 0.4876  data: 0.0006  max mem: 19734
Epoch: [36]  [ 510/1251]  eta: 0:06:13  lr: 0.000015  loss: 3.3721 (3.4143)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [36]  [ 520/1251]  eta: 0:06:07  lr: 0.000015  loss: 3.5908 (3.4147)  time: 0.4543  data: 0.0005  max mem: 19734
Epoch: [36]  [ 530/1251]  eta: 0:06:01  lr: 0.000015  loss: 3.5908 (3.4140)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [36]  [ 540/1251]  eta: 0:05:56  lr: 0.000015  loss: 3.4283 (3.4107)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [36]  [ 550/1251]  eta: 0:05:50  lr: 0.000015  loss: 3.2865 (3.4089)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [36]  [ 560/1251]  eta: 0:05:45  lr: 0.000015  loss: 3.5129 (3.4123)  time: 0.4553  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3148, ratio_loss=0.0049, pruning_loss=0.1249, mse_loss=0.4568
Epoch: [36]  [ 570/1251]  eta: 0:05:39  lr: 0.000015  loss: 3.6084 (3.4118)  time: 0.4664  data: 0.0004  max mem: 19734
Epoch: [36]  [ 580/1251]  eta: 0:05:34  lr: 0.000015  loss: 3.4289 (3.4122)  time: 0.4658  data: 0.0004  max mem: 19734
Epoch: [36]  [ 590/1251]  eta: 0:05:28  lr: 0.000015  loss: 3.3945 (3.4088)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [36]  [ 600/1251]  eta: 0:05:23  lr: 0.000015  loss: 3.3851 (3.4081)  time: 0.4647  data: 0.0005  max mem: 19734
Epoch: [36]  [ 610/1251]  eta: 0:05:18  lr: 0.000015  loss: 3.3938 (3.4032)  time: 0.4737  data: 0.0005  max mem: 19734
Epoch: [36]  [ 620/1251]  eta: 0:05:13  lr: 0.000015  loss: 3.3281 (3.4042)  time: 0.4862  data: 0.0006  max mem: 19734
Epoch: [36]  [ 630/1251]  eta: 0:05:08  lr: 0.000015  loss: 3.4761 (3.4039)  time: 0.4917  data: 0.0007  max mem: 19734
Epoch: [36]  [ 640/1251]  eta: 0:05:03  lr: 0.000015  loss: 3.6370 (3.4084)  time: 0.4832  data: 0.0005  max mem: 19734
Epoch: [36]  [ 650/1251]  eta: 0:04:57  lr: 0.000015  loss: 3.6352 (3.4109)  time: 0.4676  data: 0.0004  max mem: 19734
Epoch: [36]  [ 660/1251]  eta: 0:04:52  lr: 0.000015  loss: 3.6352 (3.4128)  time: 0.4550  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4003, ratio_loss=0.0050, pruning_loss=0.1235, mse_loss=0.4421
Epoch: [36]  [ 670/1251]  eta: 0:04:47  lr: 0.000015  loss: 3.6677 (3.4119)  time: 0.4527  data: 0.0007  max mem: 19734
Epoch: [36]  [ 680/1251]  eta: 0:04:41  lr: 0.000015  loss: 3.3228 (3.4113)  time: 0.4517  data: 0.0006  max mem: 19734
Epoch: [36]  [ 690/1251]  eta: 0:04:36  lr: 0.000015  loss: 3.5318 (3.4154)  time: 0.4517  data: 0.0006  max mem: 19734
Epoch: [36]  [ 700/1251]  eta: 0:04:31  lr: 0.000015  loss: 3.5344 (3.4128)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [36]  [ 710/1251]  eta: 0:04:26  lr: 0.000015  loss: 3.4416 (3.4134)  time: 0.4523  data: 0.0006  max mem: 19734
Epoch: [36]  [ 720/1251]  eta: 0:04:21  lr: 0.000015  loss: 3.3036 (3.4116)  time: 0.4641  data: 0.0006  max mem: 19734
Epoch: [36]  [ 730/1251]  eta: 0:04:15  lr: 0.000015  loss: 3.3863 (3.4113)  time: 0.4653  data: 0.0005  max mem: 19734
Epoch: [36]  [ 740/1251]  eta: 0:04:10  lr: 0.000015  loss: 3.5299 (3.4131)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [36]  [ 750/1251]  eta: 0:04:05  lr: 0.000015  loss: 3.6154 (3.4147)  time: 0.4630  data: 0.0005  max mem: 19734
Epoch: [36]  [ 760/1251]  eta: 0:04:00  lr: 0.000015  loss: 3.4849 (3.4129)  time: 0.4814  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3474, ratio_loss=0.0046, pruning_loss=0.1263, mse_loss=0.4466
Epoch: [36]  [ 770/1251]  eta: 0:03:55  lr: 0.000015  loss: 3.0146 (3.4068)  time: 0.4835  data: 0.0004  max mem: 19734
Epoch: [36]  [ 780/1251]  eta: 0:03:50  lr: 0.000015  loss: 3.3419 (3.4072)  time: 0.4878  data: 0.0007  max mem: 19734
Epoch: [36]  [ 790/1251]  eta: 0:03:45  lr: 0.000015  loss: 3.6746 (3.4096)  time: 0.4766  data: 0.0007  max mem: 19734
Epoch: [36]  [ 800/1251]  eta: 0:03:40  lr: 0.000015  loss: 3.6799 (3.4103)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [36]  [ 810/1251]  eta: 0:03:35  lr: 0.000015  loss: 3.3158 (3.4071)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [36]  [ 820/1251]  eta: 0:03:30  lr: 0.000015  loss: 2.9640 (3.4046)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [36]  [ 830/1251]  eta: 0:03:25  lr: 0.000015  loss: 3.2149 (3.4011)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [36]  [ 840/1251]  eta: 0:03:20  lr: 0.000015  loss: 3.2149 (3.4013)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [36]  [ 850/1251]  eta: 0:03:15  lr: 0.000015  loss: 3.5059 (3.4003)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [36]  [ 860/1251]  eta: 0:03:10  lr: 0.000015  loss: 3.5059 (3.4006)  time: 0.4560  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3250, ratio_loss=0.0048, pruning_loss=0.1261, mse_loss=0.4485
Epoch: [36]  [ 870/1251]  eta: 0:03:05  lr: 0.000015  loss: 3.4548 (3.3999)  time: 0.4651  data: 0.0005  max mem: 19734
Epoch: [36]  [ 880/1251]  eta: 0:03:00  lr: 0.000015  loss: 3.4805 (3.4004)  time: 0.4683  data: 0.0004  max mem: 19734
Epoch: [36]  [ 890/1251]  eta: 0:02:55  lr: 0.000015  loss: 3.6280 (3.4005)  time: 0.4652  data: 0.0006  max mem: 19734
Epoch: [36]  [ 900/1251]  eta: 0:02:50  lr: 0.000015  loss: 3.5498 (3.4008)  time: 0.4816  data: 0.0006  max mem: 19734
Epoch: [36]  [ 910/1251]  eta: 0:02:45  lr: 0.000015  loss: 3.7715 (3.4054)  time: 0.4810  data: 0.0005  max mem: 19734
Epoch: [36]  [ 920/1251]  eta: 0:02:40  lr: 0.000015  loss: 3.7029 (3.4060)  time: 0.4783  data: 0.0004  max mem: 19734
Epoch: [36]  [ 930/1251]  eta: 0:02:36  lr: 0.000015  loss: 3.6121 (3.4067)  time: 0.4761  data: 0.0007  max mem: 19734
Epoch: [36]  [ 940/1251]  eta: 0:02:31  lr: 0.000015  loss: 3.6146 (3.4072)  time: 0.4593  data: 0.0008  max mem: 19734
Epoch: [36]  [ 950/1251]  eta: 0:02:26  lr: 0.000015  loss: 3.5648 (3.4067)  time: 0.4528  data: 0.0006  max mem: 19734
Epoch: [36]  [ 960/1251]  eta: 0:02:21  lr: 0.000015  loss: 3.5265 (3.4076)  time: 0.4546  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4369, ratio_loss=0.0048, pruning_loss=0.1235, mse_loss=0.4527
Epoch: [36]  [ 970/1251]  eta: 0:02:16  lr: 0.000015  loss: 3.6167 (3.4092)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [36]  [ 980/1251]  eta: 0:02:11  lr: 0.000015  loss: 3.6203 (3.4090)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [36]  [ 990/1251]  eta: 0:02:06  lr: 0.000015  loss: 3.6203 (3.4107)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [36]  [1000/1251]  eta: 0:02:01  lr: 0.000015  loss: 3.5236 (3.4106)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [36]  [1010/1251]  eta: 0:01:56  lr: 0.000015  loss: 3.3715 (3.4092)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [36]  [1020/1251]  eta: 0:01:51  lr: 0.000015  loss: 3.4438 (3.4094)  time: 0.4641  data: 0.0005  max mem: 19734
Epoch: [36]  [1030/1251]  eta: 0:01:46  lr: 0.000015  loss: 3.4972 (3.4098)  time: 0.4628  data: 0.0005  max mem: 19734
Epoch: [36]  [1040/1251]  eta: 0:01:41  lr: 0.000015  loss: 3.4198 (3.4087)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [36]  [1050/1251]  eta: 0:01:37  lr: 0.000015  loss: 3.4836 (3.4083)  time: 0.4919  data: 0.0004  max mem: 19734
Epoch: [36]  [1060/1251]  eta: 0:01:32  lr: 0.000015  loss: 3.5710 (3.4087)  time: 0.4910  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3924, ratio_loss=0.0052, pruning_loss=0.1238, mse_loss=0.4330
Epoch: [36]  [1070/1251]  eta: 0:01:27  lr: 0.000015  loss: 3.5786 (3.4087)  time: 0.4837  data: 0.0004  max mem: 19734
Epoch: [36]  [1080/1251]  eta: 0:01:22  lr: 0.000015  loss: 3.5823 (3.4095)  time: 0.4744  data: 0.0004  max mem: 19734
Epoch: [36]  [1090/1251]  eta: 0:01:17  lr: 0.000015  loss: 3.6469 (3.4096)  time: 0.4535  data: 0.0006  max mem: 19734
Epoch: [36]  [1100/1251]  eta: 0:01:12  lr: 0.000015  loss: 3.4790 (3.4089)  time: 0.4518  data: 0.0006  max mem: 19734
Epoch: [36]  [1110/1251]  eta: 0:01:08  lr: 0.000015  loss: 3.5082 (3.4104)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [36]  [1120/1251]  eta: 0:01:03  lr: 0.000015  loss: 3.6416 (3.4109)  time: 0.4516  data: 0.0006  max mem: 19734
Epoch: [36]  [1130/1251]  eta: 0:00:58  lr: 0.000015  loss: 3.4525 (3.4109)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [36]  [1140/1251]  eta: 0:00:53  lr: 0.000015  loss: 3.6476 (3.4135)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [36]  [1150/1251]  eta: 0:00:48  lr: 0.000015  loss: 3.5273 (3.4139)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [36]  [1160/1251]  eta: 0:00:43  lr: 0.000015  loss: 3.6608 (3.4177)  time: 0.4512  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4841, ratio_loss=0.0051, pruning_loss=0.1227, mse_loss=0.4135
Epoch: [36]  [1170/1251]  eta: 0:00:38  lr: 0.000015  loss: 3.4531 (3.4161)  time: 0.4611  data: 0.0004  max mem: 19734
Epoch: [36]  [1180/1251]  eta: 0:00:34  lr: 0.000015  loss: 3.3502 (3.4164)  time: 0.4732  data: 0.0004  max mem: 19734
Epoch: [36]  [1190/1251]  eta: 0:00:29  lr: 0.000015  loss: 3.5402 (3.4166)  time: 0.4708  data: 0.0008  max mem: 19734
Epoch: [36]  [1200/1251]  eta: 0:00:24  lr: 0.000015  loss: 3.5182 (3.4161)  time: 0.4703  data: 0.0006  max mem: 19734
Epoch: [36]  [1210/1251]  eta: 0:00:19  lr: 0.000015  loss: 3.3075 (3.4144)  time: 0.4705  data: 0.0001  max mem: 19734
Epoch: [36]  [1220/1251]  eta: 0:00:14  lr: 0.000015  loss: 3.4677 (3.4158)  time: 0.4630  data: 0.0001  max mem: 19734
Epoch: [36]  [1230/1251]  eta: 0:00:10  lr: 0.000015  loss: 3.4929 (3.4160)  time: 0.4528  data: 0.0001  max mem: 19734
Epoch: [36]  [1240/1251]  eta: 0:00:05  lr: 0.000015  loss: 3.4153 (3.4155)  time: 0.4468  data: 0.0001  max mem: 19734
Epoch: [36]  [1250/1251]  eta: 0:00:00  lr: 0.000015  loss: 3.6419 (3.4178)  time: 0.4465  data: 0.0001  max mem: 19734
Epoch: [36] Total time: 0:10:00 (0.4803 s / it)
Averaged stats: lr: 0.000015  loss: 3.6419 (3.4198)
Test:  [  0/261]  eta: 2:30:06  loss: 0.7021 (0.7021)  acc1: 82.8125 (82.8125)  acc5: 96.8750 (96.8750)  time: 34.5075  data: 34.2378  max mem: 19734
Test:  [ 10/261]  eta: 0:13:59  loss: 0.7021 (0.7201)  acc1: 84.8958 (83.7121)  acc5: 96.8750 (96.5436)  time: 3.3437  data: 3.1276  max mem: 19734
Test:  [ 20/261]  eta: 0:07:24  loss: 0.9045 (0.8959)  acc1: 78.6458 (78.8442)  acc5: 94.7917 (94.7669)  time: 0.2092  data: 0.0194  max mem: 19734
Test:  [ 30/261]  eta: 0:05:04  loss: 0.7902 (0.8173)  acc1: 82.2917 (81.5356)  acc5: 93.7500 (95.1277)  time: 0.2028  data: 0.0185  max mem: 19734
Test:  [ 40/261]  eta: 0:04:08  loss: 0.5811 (0.7832)  acc1: 86.4583 (82.5965)  acc5: 96.3542 (95.4522)  time: 0.3720  data: 0.2139  max mem: 19734
Test:  [ 50/261]  eta: 0:03:15  loss: 0.9202 (0.8459)  acc1: 77.0833 (80.6883)  acc5: 94.7917 (95.0878)  time: 0.3180  data: 0.2106  max mem: 19734
Test:  [ 60/261]  eta: 0:02:39  loss: 0.9786 (0.8525)  acc1: 76.5625 (80.3706)  acc5: 94.2708 (95.1759)  time: 0.1157  data: 0.0096  max mem: 19734
Test:  [ 70/261]  eta: 0:02:20  loss: 0.9161 (0.8517)  acc1: 77.0833 (80.1130)  acc5: 95.8333 (95.3565)  time: 0.2442  data: 0.1187  max mem: 19734
Test:  [ 80/261]  eta: 0:01:58  loss: 0.8114 (0.8521)  acc1: 79.6875 (80.2276)  acc5: 96.8750 (95.4475)  time: 0.2398  data: 0.1198  max mem: 19734
Test:  [ 90/261]  eta: 0:01:45  loss: 0.8083 (0.8392)  acc1: 83.8542 (80.5804)  acc5: 95.8333 (95.5243)  time: 0.1971  data: 0.0836  max mem: 19734
Test:  [100/261]  eta: 0:01:36  loss: 0.8115 (0.8429)  acc1: 83.8542 (80.5642)  acc5: 94.7917 (95.5858)  time: 0.3693  data: 0.2719  max mem: 19734
Test:  [110/261]  eta: 0:01:26  loss: 0.8798 (0.8662)  acc1: 77.6042 (80.0863)  acc5: 94.2708 (95.2703)  time: 0.3945  data: 0.3211  max mem: 19734
Test:  [120/261]  eta: 0:01:15  loss: 1.1719 (0.9066)  acc1: 71.3542 (79.1581)  acc5: 89.5833 (94.7142)  time: 0.2075  data: 0.1302  max mem: 19734
Test:  [130/261]  eta: 0:01:07  loss: 1.3605 (0.9532)  acc1: 67.1875 (78.1687)  acc5: 86.4583 (94.0601)  time: 0.1752  data: 0.0711  max mem: 19734
Test:  [140/261]  eta: 0:01:11  loss: 1.3058 (0.9805)  acc1: 69.7917 (77.5340)  acc5: 89.5833 (93.7537)  time: 0.9290  data: 0.7908  max mem: 19734
Test:  [150/261]  eta: 0:01:02  loss: 1.2462 (0.9861)  acc1: 71.8750 (77.4938)  acc5: 90.6250 (93.5948)  time: 0.8723  data: 0.7337  max mem: 19734
Test:  [160/261]  eta: 0:00:54  loss: 0.9972 (1.0072)  acc1: 77.6042 (77.1545)  acc5: 91.1458 (93.2745)  time: 0.1439  data: 0.0154  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: 1.2824 (1.0368)  acc1: 65.6250 (76.4254)  acc5: 86.4583 (92.9368)  time: 0.1103  data: 0.0112  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4121 (1.0542)  acc1: 65.6250 (75.9985)  acc5: 88.5417 (92.7975)  time: 0.0788  data: 0.0075  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3686 (1.0676)  acc1: 67.1875 (75.7799)  acc5: 90.6250 (92.6347)  time: 0.0726  data: 0.0038  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3686 (1.0841)  acc1: 69.7917 (75.4353)  acc5: 90.1042 (92.3896)  time: 0.1241  data: 0.0565  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3649 (1.0980)  acc1: 67.1875 (75.1432)  acc5: 87.5000 (92.1998)  time: 0.1195  data: 0.0546  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.3882 (1.1177)  acc1: 66.1458 (74.6465)  acc5: 88.0208 (91.9966)  time: 0.0630  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.3942 (1.1266)  acc1: 65.6250 (74.3935)  acc5: 88.5417 (91.9034)  time: 0.0629  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3460 (1.1359)  acc1: 66.6667 (74.1896)  acc5: 90.6250 (91.8266)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0730 (1.1277)  acc1: 76.0417 (74.3858)  acc5: 93.2292 (91.9530)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9271 (1.1266)  acc1: 78.1250 (74.4080)  acc5: 95.3125 (92.0340)  time: 0.0595  data: 0.0001  max mem: 19734
Test: Total time: 0:01:34 (0.3608 s / it)
* Acc@1 74.408 Acc@5 92.034 loss 1.127
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.62%
Epoch: [37]  [   0/1251]  eta: 5:25:31  lr: 0.000015  loss: 3.3055 (3.3055)  time: 15.6128  data: 8.5236  max mem: 19734
Epoch: [37]  [  10/1251]  eta: 0:44:43  lr: 0.000015  loss: 3.8080 (3.6132)  time: 2.1622  data: 0.9347  max mem: 19734
loss info: cls_loss=3.4172, ratio_loss=0.0051, pruning_loss=0.1232, mse_loss=0.4199
Epoch: [37]  [  20/1251]  eta: 0:27:40  lr: 0.000015  loss: 3.4156 (3.3422)  time: 0.6354  data: 0.0880  max mem: 19734
Epoch: [37]  [  30/1251]  eta: 0:21:41  lr: 0.000015  loss: 3.4821 (3.4620)  time: 0.4631  data: 0.0003  max mem: 19734
Epoch: [37]  [  40/1251]  eta: 0:18:31  lr: 0.000015  loss: 3.5617 (3.4512)  time: 0.4655  data: 0.0003  max mem: 19734
Epoch: [37]  [  50/1251]  eta: 0:16:33  lr: 0.000015  loss: 3.5622 (3.4782)  time: 0.4566  data: 0.0004  max mem: 19734
Epoch: [37]  [  60/1251]  eta: 0:15:12  lr: 0.000015  loss: 3.5622 (3.4616)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [37]  [  70/1251]  eta: 0:14:17  lr: 0.000015  loss: 3.3800 (3.4285)  time: 0.4689  data: 0.0004  max mem: 19734
Epoch: [37]  [  80/1251]  eta: 0:13:34  lr: 0.000015  loss: 3.6403 (3.4703)  time: 0.4791  data: 0.0005  max mem: 19734
Epoch: [37]  [  90/1251]  eta: 0:13:04  lr: 0.000015  loss: 3.6934 (3.4580)  time: 0.4986  data: 0.0004  max mem: 19734
Epoch: [37]  [ 100/1251]  eta: 0:12:35  lr: 0.000015  loss: 3.4196 (3.4545)  time: 0.4984  data: 0.0004  max mem: 19734
Epoch: [37]  [ 110/1251]  eta: 0:12:08  lr: 0.000015  loss: 3.4550 (3.4562)  time: 0.4650  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4373, ratio_loss=0.0051, pruning_loss=0.1231, mse_loss=0.4329
Epoch: [37]  [ 120/1251]  eta: 0:11:44  lr: 0.000015  loss: 3.3542 (3.4609)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [37]  [ 130/1251]  eta: 0:11:25  lr: 0.000015  loss: 3.5170 (3.4574)  time: 0.4619  data: 0.0005  max mem: 19734
Epoch: [37]  [ 140/1251]  eta: 0:11:07  lr: 0.000015  loss: 3.5170 (3.4573)  time: 0.4655  data: 0.0007  max mem: 19734
Epoch: [37]  [ 150/1251]  eta: 0:10:51  lr: 0.000015  loss: 3.3563 (3.4415)  time: 0.4614  data: 0.0007  max mem: 19734
Epoch: [37]  [ 160/1251]  eta: 0:10:35  lr: 0.000015  loss: 3.0432 (3.4352)  time: 0.4554  data: 0.0005  max mem: 19734
Epoch: [37]  [ 170/1251]  eta: 0:10:22  lr: 0.000015  loss: 3.6242 (3.4440)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [37]  [ 180/1251]  eta: 0:10:10  lr: 0.000015  loss: 3.5761 (3.4404)  time: 0.4700  data: 0.0005  max mem: 19734
Epoch: [37]  [ 190/1251]  eta: 0:09:58  lr: 0.000015  loss: 3.3185 (3.4298)  time: 0.4672  data: 0.0005  max mem: 19734
Epoch: [37]  [ 200/1251]  eta: 0:09:47  lr: 0.000015  loss: 3.4500 (3.4265)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [37]  [ 210/1251]  eta: 0:09:36  lr: 0.000015  loss: 3.3889 (3.4215)  time: 0.4533  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3414, ratio_loss=0.0051, pruning_loss=0.1240, mse_loss=0.4439
Epoch: [37]  [ 220/1251]  eta: 0:09:26  lr: 0.000015  loss: 3.2141 (3.4177)  time: 0.4598  data: 0.0005  max mem: 19734
Epoch: [37]  [ 230/1251]  eta: 0:09:18  lr: 0.000015  loss: 3.1827 (3.4114)  time: 0.4725  data: 0.0005  max mem: 19734
Epoch: [37]  [ 240/1251]  eta: 0:09:11  lr: 0.000015  loss: 3.4544 (3.4105)  time: 0.5016  data: 0.0005  max mem: 19734
Epoch: [37]  [ 250/1251]  eta: 0:09:03  lr: 0.000015  loss: 3.4655 (3.4081)  time: 0.4978  data: 0.0005  max mem: 19734
Epoch: [37]  [ 260/1251]  eta: 0:08:54  lr: 0.000015  loss: 3.5921 (3.4056)  time: 0.4652  data: 0.0005  max mem: 19734
Epoch: [37]  [ 270/1251]  eta: 0:08:46  lr: 0.000015  loss: 3.4528 (3.4018)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [37]  [ 280/1251]  eta: 0:08:37  lr: 0.000015  loss: 3.6234 (3.4058)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [37]  [ 290/1251]  eta: 0:08:30  lr: 0.000015  loss: 3.8241 (3.4133)  time: 0.4544  data: 0.0006  max mem: 19734
Epoch: [37]  [ 300/1251]  eta: 0:08:22  lr: 0.000015  loss: 3.5705 (3.4116)  time: 0.4566  data: 0.0006  max mem: 19734
Epoch: [37]  [ 310/1251]  eta: 0:08:14  lr: 0.000015  loss: 3.5429 (3.4147)  time: 0.4570  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3691, ratio_loss=0.0049, pruning_loss=0.1253, mse_loss=0.4339
Epoch: [37]  [ 320/1251]  eta: 0:08:07  lr: 0.000015  loss: 3.5760 (3.4160)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [37]  [ 330/1251]  eta: 0:08:01  lr: 0.000015  loss: 3.5760 (3.4110)  time: 0.4639  data: 0.0004  max mem: 19734
Epoch: [37]  [ 340/1251]  eta: 0:07:54  lr: 0.000015  loss: 3.2885 (3.4075)  time: 0.4660  data: 0.0005  max mem: 19734
Epoch: [37]  [ 350/1251]  eta: 0:07:47  lr: 0.000015  loss: 3.4908 (3.4122)  time: 0.4571  data: 0.0005  max mem: 19734
Epoch: [37]  [ 360/1251]  eta: 0:07:41  lr: 0.000015  loss: 3.5814 (3.4166)  time: 0.4705  data: 0.0004  max mem: 19734
Epoch: [37]  [ 370/1251]  eta: 0:07:35  lr: 0.000015  loss: 3.5814 (3.4122)  time: 0.4876  data: 0.0005  max mem: 19734
Epoch: [37]  [ 380/1251]  eta: 0:07:29  lr: 0.000015  loss: 3.4317 (3.4104)  time: 0.4808  data: 0.0005  max mem: 19734
Epoch: [37]  [ 390/1251]  eta: 0:07:23  lr: 0.000015  loss: 3.4317 (3.4124)  time: 0.4900  data: 0.0004  max mem: 19734
Epoch: [37]  [ 400/1251]  eta: 0:07:17  lr: 0.000015  loss: 3.3077 (3.4051)  time: 0.4798  data: 0.0004  max mem: 19734
Epoch: [37]  [ 410/1251]  eta: 0:07:10  lr: 0.000015  loss: 3.2620 (3.4048)  time: 0.4514  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3573, ratio_loss=0.0048, pruning_loss=0.1240, mse_loss=0.4453
Epoch: [37]  [ 420/1251]  eta: 0:07:04  lr: 0.000015  loss: 3.0905 (3.3967)  time: 0.4521  data: 0.0006  max mem: 19734
Epoch: [37]  [ 430/1251]  eta: 0:06:58  lr: 0.000015  loss: 3.5552 (3.4006)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [37]  [ 440/1251]  eta: 0:06:52  lr: 0.000015  loss: 3.7424 (3.4009)  time: 0.4512  data: 0.0005  max mem: 19734
Epoch: [37]  [ 450/1251]  eta: 0:06:46  lr: 0.000015  loss: 3.3778 (3.3973)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [37]  [ 460/1251]  eta: 0:06:40  lr: 0.000015  loss: 3.0875 (3.3907)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [37]  [ 470/1251]  eta: 0:06:34  lr: 0.000015  loss: 3.0875 (3.3896)  time: 0.4622  data: 0.0006  max mem: 19734
Epoch: [37]  [ 480/1251]  eta: 0:06:28  lr: 0.000015  loss: 3.6683 (3.3955)  time: 0.4626  data: 0.0005  max mem: 19734
Epoch: [37]  [ 490/1251]  eta: 0:06:22  lr: 0.000015  loss: 3.6415 (3.3995)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [37]  [ 500/1251]  eta: 0:06:17  lr: 0.000015  loss: 3.6415 (3.4037)  time: 0.4566  data: 0.0006  max mem: 19734
Epoch: [37]  [ 510/1251]  eta: 0:06:11  lr: 0.000015  loss: 3.7411 (3.4087)  time: 0.4644  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4047, ratio_loss=0.0050, pruning_loss=0.1235, mse_loss=0.4266
Epoch: [37]  [ 520/1251]  eta: 0:06:06  lr: 0.000015  loss: 3.7411 (3.4078)  time: 0.4828  data: 0.0004  max mem: 19734
Epoch: [37]  [ 530/1251]  eta: 0:06:01  lr: 0.000015  loss: 3.6463 (3.4113)  time: 0.5103  data: 0.0004  max mem: 19734
Epoch: [37]  [ 540/1251]  eta: 0:05:56  lr: 0.000015  loss: 3.6935 (3.4161)  time: 0.4900  data: 0.0004  max mem: 19734
Epoch: [37]  [ 550/1251]  eta: 0:05:50  lr: 0.000015  loss: 3.6084 (3.4178)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [37]  [ 560/1251]  eta: 0:05:45  lr: 0.000015  loss: 3.4464 (3.4178)  time: 0.4542  data: 0.0007  max mem: 19734
Epoch: [37]  [ 570/1251]  eta: 0:05:39  lr: 0.000015  loss: 3.4464 (3.4158)  time: 0.4538  data: 0.0007  max mem: 19734
Epoch: [37]  [ 580/1251]  eta: 0:05:33  lr: 0.000015  loss: 3.4469 (3.4149)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [37]  [ 590/1251]  eta: 0:05:28  lr: 0.000015  loss: 3.4956 (3.4124)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [37]  [ 600/1251]  eta: 0:05:23  lr: 0.000015  loss: 3.7119 (3.4170)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [37]  [ 610/1251]  eta: 0:05:17  lr: 0.000015  loss: 3.5757 (3.4139)  time: 0.4551  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4019, ratio_loss=0.0050, pruning_loss=0.1233, mse_loss=0.4359
Epoch: [37]  [ 620/1251]  eta: 0:05:12  lr: 0.000015  loss: 3.3551 (3.4120)  time: 0.4691  data: 0.0007  max mem: 19734
Epoch: [37]  [ 630/1251]  eta: 0:05:07  lr: 0.000015  loss: 3.2233 (3.4075)  time: 0.4688  data: 0.0006  max mem: 19734
Epoch: [37]  [ 640/1251]  eta: 0:05:01  lr: 0.000015  loss: 3.6066 (3.4110)  time: 0.4582  data: 0.0009  max mem: 19734
Epoch: [37]  [ 650/1251]  eta: 0:04:56  lr: 0.000015  loss: 3.6791 (3.4141)  time: 0.4709  data: 0.0010  max mem: 19734
Epoch: [37]  [ 660/1251]  eta: 0:04:52  lr: 0.000015  loss: 3.6939 (3.4181)  time: 0.4915  data: 0.0006  max mem: 19734
Epoch: [37]  [ 670/1251]  eta: 0:04:46  lr: 0.000015  loss: 3.6939 (3.4205)  time: 0.4847  data: 0.0004  max mem: 19734
Epoch: [37]  [ 680/1251]  eta: 0:04:41  lr: 0.000015  loss: 3.5757 (3.4242)  time: 0.4825  data: 0.0005  max mem: 19734
Epoch: [37]  [ 690/1251]  eta: 0:04:36  lr: 0.000015  loss: 3.5416 (3.4229)  time: 0.4758  data: 0.0005  max mem: 19734
Epoch: [37]  [ 700/1251]  eta: 0:04:31  lr: 0.000015  loss: 3.0727 (3.4187)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [37]  [ 710/1251]  eta: 0:04:26  lr: 0.000015  loss: 3.1840 (3.4196)  time: 0.4546  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4488, ratio_loss=0.0051, pruning_loss=0.1227, mse_loss=0.4310
Epoch: [37]  [ 720/1251]  eta: 0:04:21  lr: 0.000015  loss: 3.7358 (3.4246)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [37]  [ 730/1251]  eta: 0:04:15  lr: 0.000015  loss: 3.8077 (3.4264)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [37]  [ 740/1251]  eta: 0:04:10  lr: 0.000015  loss: 3.7815 (3.4286)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [37]  [ 750/1251]  eta: 0:04:05  lr: 0.000015  loss: 3.5691 (3.4278)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [37]  [ 760/1251]  eta: 0:04:00  lr: 0.000015  loss: 3.4116 (3.4294)  time: 0.4513  data: 0.0005  max mem: 19734
Epoch: [37]  [ 770/1251]  eta: 0:03:55  lr: 0.000015  loss: 3.4112 (3.4277)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [37]  [ 780/1251]  eta: 0:03:50  lr: 0.000015  loss: 3.1096 (3.4242)  time: 0.4601  data: 0.0007  max mem: 19734
Epoch: [37]  [ 790/1251]  eta: 0:03:45  lr: 0.000015  loss: 3.2885 (3.4244)  time: 0.4606  data: 0.0007  max mem: 19734
Epoch: [37]  [ 800/1251]  eta: 0:03:40  lr: 0.000015  loss: 3.5307 (3.4259)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [37]  [ 810/1251]  eta: 0:03:35  lr: 0.000015  loss: 3.5070 (3.4242)  time: 0.4888  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4254, ratio_loss=0.0049, pruning_loss=0.1226, mse_loss=0.4298
Epoch: [37]  [ 820/1251]  eta: 0:03:30  lr: 0.000015  loss: 3.5593 (3.4250)  time: 0.5177  data: 0.0005  max mem: 19734
Epoch: [37]  [ 830/1251]  eta: 0:03:25  lr: 0.000015  loss: 3.6105 (3.4256)  time: 0.4837  data: 0.0004  max mem: 19734
Epoch: [37]  [ 840/1251]  eta: 0:03:20  lr: 0.000015  loss: 3.0318 (3.4196)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [37]  [ 850/1251]  eta: 0:03:15  lr: 0.000015  loss: 3.3834 (3.4212)  time: 0.4522  data: 0.0005  max mem: 19734
Epoch: [37]  [ 860/1251]  eta: 0:03:10  lr: 0.000015  loss: 3.4344 (3.4211)  time: 0.4505  data: 0.0005  max mem: 19734
Epoch: [37]  [ 870/1251]  eta: 0:03:05  lr: 0.000015  loss: 3.4345 (3.4212)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [37]  [ 880/1251]  eta: 0:03:00  lr: 0.000015  loss: 3.4629 (3.4217)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [37]  [ 890/1251]  eta: 0:02:55  lr: 0.000015  loss: 3.4225 (3.4213)  time: 0.4511  data: 0.0004  max mem: 19734
Epoch: [37]  [ 900/1251]  eta: 0:02:50  lr: 0.000015  loss: 3.4225 (3.4219)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [37]  [ 910/1251]  eta: 0:02:45  lr: 0.000015  loss: 3.3543 (3.4177)  time: 0.4540  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3389, ratio_loss=0.0053, pruning_loss=0.1241, mse_loss=0.4247
Epoch: [37]  [ 920/1251]  eta: 0:02:40  lr: 0.000015  loss: 3.0018 (3.4167)  time: 0.4690  data: 0.0005  max mem: 19734
Epoch: [37]  [ 930/1251]  eta: 0:02:35  lr: 0.000015  loss: 3.6021 (3.4186)  time: 0.4705  data: 0.0006  max mem: 19734
Epoch: [37]  [ 940/1251]  eta: 0:02:30  lr: 0.000015  loss: 3.6567 (3.4216)  time: 0.4667  data: 0.0006  max mem: 19734
Epoch: [37]  [ 950/1251]  eta: 0:02:25  lr: 0.000015  loss: 3.6530 (3.4218)  time: 0.4740  data: 0.0004  max mem: 19734
Epoch: [37]  [ 960/1251]  eta: 0:02:21  lr: 0.000015  loss: 3.5802 (3.4215)  time: 0.4809  data: 0.0006  max mem: 19734
Epoch: [37]  [ 970/1251]  eta: 0:02:16  lr: 0.000015  loss: 3.5150 (3.4228)  time: 0.4893  data: 0.0006  max mem: 19734
Epoch: [37]  [ 980/1251]  eta: 0:02:11  lr: 0.000015  loss: 3.4306 (3.4224)  time: 0.4707  data: 0.0005  max mem: 19734
Epoch: [37]  [ 990/1251]  eta: 0:02:06  lr: 0.000015  loss: 3.6432 (3.4246)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [37]  [1000/1251]  eta: 0:02:01  lr: 0.000015  loss: 3.6432 (3.4247)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [37]  [1010/1251]  eta: 0:01:56  lr: 0.000015  loss: 3.4563 (3.4236)  time: 0.4552  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4593, ratio_loss=0.0046, pruning_loss=0.1213, mse_loss=0.4190
Epoch: [37]  [1020/1251]  eta: 0:01:51  lr: 0.000015  loss: 3.5222 (3.4245)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [37]  [1030/1251]  eta: 0:01:46  lr: 0.000015  loss: 3.6065 (3.4239)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [37]  [1040/1251]  eta: 0:01:41  lr: 0.000015  loss: 3.6684 (3.4249)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [37]  [1050/1251]  eta: 0:01:36  lr: 0.000015  loss: 3.4678 (3.4234)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [37]  [1060/1251]  eta: 0:01:32  lr: 0.000015  loss: 3.2918 (3.4232)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [37]  [1070/1251]  eta: 0:01:27  lr: 0.000015  loss: 3.7123 (3.4262)  time: 0.4652  data: 0.0006  max mem: 19734
Epoch: [37]  [1080/1251]  eta: 0:01:22  lr: 0.000015  loss: 3.7123 (3.4280)  time: 0.4751  data: 0.0005  max mem: 19734
Epoch: [37]  [1090/1251]  eta: 0:01:17  lr: 0.000015  loss: 3.4774 (3.4268)  time: 0.4648  data: 0.0004  max mem: 19734
Epoch: [37]  [1100/1251]  eta: 0:01:12  lr: 0.000015  loss: 3.2681 (3.4269)  time: 0.4820  data: 0.0004  max mem: 19734
Epoch: [37]  [1110/1251]  eta: 0:01:07  lr: 0.000015  loss: 3.4008 (3.4279)  time: 0.5024  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4388, ratio_loss=0.0054, pruning_loss=0.1221, mse_loss=0.4194
Epoch: [37]  [1120/1251]  eta: 0:01:03  lr: 0.000015  loss: 3.4747 (3.4269)  time: 0.4885  data: 0.0004  max mem: 19734
Epoch: [37]  [1130/1251]  eta: 0:00:58  lr: 0.000015  loss: 3.7139 (3.4291)  time: 0.4665  data: 0.0004  max mem: 19734
Epoch: [37]  [1140/1251]  eta: 0:00:53  lr: 0.000015  loss: 3.7238 (3.4289)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [37]  [1150/1251]  eta: 0:00:48  lr: 0.000015  loss: 3.5584 (3.4305)  time: 0.4515  data: 0.0004  max mem: 19734
Epoch: [37]  [1160/1251]  eta: 0:00:43  lr: 0.000015  loss: 3.5768 (3.4296)  time: 0.4511  data: 0.0004  max mem: 19734
Epoch: [37]  [1170/1251]  eta: 0:00:38  lr: 0.000015  loss: 3.4397 (3.4282)  time: 0.4534  data: 0.0006  max mem: 19734
Epoch: [37]  [1180/1251]  eta: 0:00:34  lr: 0.000015  loss: 3.4455 (3.4292)  time: 0.4539  data: 0.0007  max mem: 19734
Epoch: [37]  [1190/1251]  eta: 0:00:29  lr: 0.000015  loss: 3.7940 (3.4315)  time: 0.4536  data: 0.0009  max mem: 19734
Epoch: [37]  [1200/1251]  eta: 0:00:24  lr: 0.000015  loss: 3.5768 (3.4311)  time: 0.4503  data: 0.0007  max mem: 19734
Epoch: [37]  [1210/1251]  eta: 0:00:19  lr: 0.000015  loss: 3.2846 (3.4301)  time: 0.4460  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4290, ratio_loss=0.0048, pruning_loss=0.1223, mse_loss=0.4230
Epoch: [37]  [1220/1251]  eta: 0:00:14  lr: 0.000015  loss: 3.2971 (3.4293)  time: 0.4524  data: 0.0002  max mem: 19734
Epoch: [37]  [1230/1251]  eta: 0:00:10  lr: 0.000015  loss: 3.3125 (3.4280)  time: 0.4597  data: 0.0002  max mem: 19734
Epoch: [37]  [1240/1251]  eta: 0:00:05  lr: 0.000015  loss: 3.5613 (3.4289)  time: 0.4645  data: 0.0002  max mem: 19734
Epoch: [37]  [1250/1251]  eta: 0:00:00  lr: 0.000015  loss: 3.6582 (3.4297)  time: 0.4628  data: 0.0002  max mem: 19734
Epoch: [37] Total time: 0:10:00 (0.4799 s / it)
Averaged stats: lr: 0.000015  loss: 3.6582 (3.4199)
Test:  [  0/261]  eta: 1:10:23  loss: 0.6603 (0.6603)  acc1: 84.3750 (84.3750)  acc5: 96.8750 (96.8750)  time: 16.1814  data: 16.0759  max mem: 19734
Test:  [ 10/261]  eta: 0:06:27  loss: 0.6603 (0.7233)  acc1: 84.3750 (83.8068)  acc5: 96.8750 (95.9754)  time: 1.5423  data: 1.4671  max mem: 19734
Test:  [ 20/261]  eta: 0:04:03  loss: 0.8704 (0.8961)  acc1: 79.1667 (79.1171)  acc5: 93.7500 (94.5933)  time: 0.2515  data: 0.1222  max mem: 19734
Test:  [ 30/261]  eta: 0:02:47  loss: 0.7987 (0.8131)  acc1: 85.4167 (81.8884)  acc5: 93.7500 (94.9765)  time: 0.2766  data: 0.1256  max mem: 19734
Test:  [ 40/261]  eta: 0:02:45  loss: 0.5562 (0.7801)  acc1: 88.5417 (82.8125)  acc5: 96.8750 (95.3125)  time: 0.4778  data: 0.3519  max mem: 19734
Test:  [ 50/261]  eta: 0:02:19  loss: 0.9157 (0.8434)  acc1: 78.6458 (80.9845)  acc5: 95.3125 (94.9755)  time: 0.5608  data: 0.4148  max mem: 19734
Test:  [ 60/261]  eta: 0:01:58  loss: 0.9539 (0.8498)  acc1: 75.5208 (80.6950)  acc5: 94.7917 (95.0222)  time: 0.2583  data: 0.0736  max mem: 19734
Test:  [ 70/261]  eta: 0:01:53  loss: 0.8980 (0.8533)  acc1: 77.6042 (80.2157)  acc5: 95.3125 (95.2245)  time: 0.4314  data: 0.2343  max mem: 19734
Test:  [ 80/261]  eta: 0:01:41  loss: 0.8365 (0.8540)  acc1: 79.6875 (80.3498)  acc5: 96.8750 (95.3125)  time: 0.4687  data: 0.2371  max mem: 19734
Test:  [ 90/261]  eta: 0:01:30  loss: 0.8231 (0.8404)  acc1: 83.8542 (80.6662)  acc5: 96.3542 (95.3926)  time: 0.2807  data: 0.0152  max mem: 19734
Test:  [100/261]  eta: 0:01:36  loss: 0.8273 (0.8434)  acc1: 83.3333 (80.6570)  acc5: 94.7917 (95.4724)  time: 0.7524  data: 0.5376  max mem: 19734
Test:  [110/261]  eta: 0:01:25  loss: 0.8736 (0.8670)  acc1: 77.0833 (80.1990)  acc5: 94.7917 (95.1483)  time: 0.7475  data: 0.5352  max mem: 19734
Test:  [120/261]  eta: 0:01:16  loss: 1.2109 (0.9080)  acc1: 70.8333 (79.2657)  acc5: 90.1042 (94.5980)  time: 0.2582  data: 0.0147  max mem: 19734
Test:  [130/261]  eta: 0:01:09  loss: 1.3963 (0.9549)  acc1: 68.2292 (78.3079)  acc5: 86.4583 (93.9687)  time: 0.3190  data: 0.0886  max mem: 19734
Test:  [140/261]  eta: 0:01:01  loss: 1.2766 (0.9811)  acc1: 68.2292 (77.6226)  acc5: 89.5833 (93.7278)  time: 0.3090  data: 0.0848  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: 1.1699 (0.9854)  acc1: 71.8750 (77.6421)  acc5: 91.1458 (93.5913)  time: 0.2649  data: 0.0144  max mem: 19734
Test:  [160/261]  eta: 0:00:47  loss: 0.9925 (1.0055)  acc1: 78.1250 (77.2936)  acc5: 91.1458 (93.3068)  time: 0.2215  data: 0.0135  max mem: 19734
Test:  [170/261]  eta: 0:00:41  loss: 1.2746 (1.0355)  acc1: 65.6250 (76.5412)  acc5: 88.0208 (92.9764)  time: 0.1997  data: 0.0406  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4349 (1.0513)  acc1: 66.6667 (76.1999)  acc5: 88.5417 (92.8551)  time: 0.3131  data: 0.1297  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: 1.3885 (1.0649)  acc1: 67.7083 (75.9435)  acc5: 91.1458 (92.6947)  time: 0.2488  data: 0.1028  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3397 (1.0808)  acc1: 71.8750 (75.5882)  acc5: 88.5417 (92.4363)  time: 0.2161  data: 0.1233  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: 1.3385 (1.0943)  acc1: 71.8750 (75.3110)  acc5: 88.0208 (92.2443)  time: 0.4002  data: 0.2920  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: 1.4258 (1.1145)  acc1: 67.7083 (74.8020)  acc5: 88.0208 (92.0131)  time: 0.2813  data: 0.1784  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4173 (1.1232)  acc1: 68.7500 (74.5806)  acc5: 88.0208 (91.9395)  time: 0.0698  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.2739 (1.1322)  acc1: 69.7917 (74.3473)  acc5: 90.6250 (91.8763)  time: 0.0651  data: 0.0036  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0156 (1.1255)  acc1: 75.5208 (74.5020)  acc5: 93.7500 (91.9924)  time: 0.0651  data: 0.0036  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9694 (1.1250)  acc1: 76.5625 (74.5080)  acc5: 94.7917 (92.0460)  time: 0.0597  data: 0.0001  max mem: 19734
Test: Total time: 0:01:35 (0.3644 s / it)
* Acc@1 74.508 Acc@5 92.046 loss 1.125
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.62%
Epoch: [38]  [   0/1251]  eta: 6:18:28  lr: 0.000014  loss: 3.5109 (3.5109)  time: 18.1525  data: 13.3971  max mem: 19734
Epoch: [38]  [  10/1251]  eta: 0:45:17  lr: 0.000014  loss: 3.4124 (3.3586)  time: 2.1896  data: 1.2298  max mem: 19734
Epoch: [38]  [  20/1251]  eta: 0:28:03  lr: 0.000014  loss: 3.5447 (3.4649)  time: 0.5282  data: 0.0068  max mem: 19734
Epoch: [38]  [  30/1251]  eta: 0:21:52  lr: 0.000014  loss: 3.5447 (3.4173)  time: 0.4621  data: 0.0005  max mem: 19734
Epoch: [38]  [  40/1251]  eta: 0:18:40  lr: 0.000014  loss: 3.5115 (3.3989)  time: 0.4606  data: 0.0005  max mem: 19734
Epoch: [38]  [  50/1251]  eta: 0:16:42  lr: 0.000014  loss: 3.4850 (3.3857)  time: 0.4615  data: 0.0005  max mem: 19734
Epoch: [38]  [  60/1251]  eta: 0:15:20  lr: 0.000014  loss: 3.4662 (3.4089)  time: 0.4622  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3956, ratio_loss=0.0048, pruning_loss=0.1221, mse_loss=0.4509
Epoch: [38]  [  70/1251]  eta: 0:14:21  lr: 0.000014  loss: 3.5706 (3.4311)  time: 0.4626  data: 0.0005  max mem: 19734
Epoch: [38]  [  80/1251]  eta: 0:13:39  lr: 0.000014  loss: 3.4314 (3.4042)  time: 0.4769  data: 0.0008  max mem: 19734
Epoch: [38]  [  90/1251]  eta: 0:13:02  lr: 0.000014  loss: 3.4314 (3.3899)  time: 0.4747  data: 0.0007  max mem: 19734
Epoch: [38]  [ 100/1251]  eta: 0:12:31  lr: 0.000014  loss: 3.4792 (3.3897)  time: 0.4616  data: 0.0004  max mem: 19734
Epoch: [38]  [ 110/1251]  eta: 0:12:04  lr: 0.000014  loss: 3.6452 (3.4075)  time: 0.4609  data: 0.0004  max mem: 19734
Epoch: [38]  [ 120/1251]  eta: 0:11:45  lr: 0.000014  loss: 3.6279 (3.4057)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [38]  [ 130/1251]  eta: 0:11:29  lr: 0.000014  loss: 3.6199 (3.4177)  time: 0.4999  data: 0.0005  max mem: 19734
Epoch: [38]  [ 140/1251]  eta: 0:11:12  lr: 0.000014  loss: 3.5489 (3.4063)  time: 0.4941  data: 0.0005  max mem: 19734
Epoch: [38]  [ 150/1251]  eta: 0:10:57  lr: 0.000014  loss: 3.5489 (3.4154)  time: 0.4804  data: 0.0005  max mem: 19734
Epoch: [38]  [ 160/1251]  eta: 0:10:41  lr: 0.000014  loss: 3.6136 (3.4145)  time: 0.4656  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3975, ratio_loss=0.0048, pruning_loss=0.1219, mse_loss=0.4304
Epoch: [38]  [ 170/1251]  eta: 0:10:27  lr: 0.000014  loss: 3.6885 (3.4238)  time: 0.4586  data: 0.0005  max mem: 19734
Epoch: [38]  [ 180/1251]  eta: 0:10:14  lr: 0.000014  loss: 3.6614 (3.4370)  time: 0.4579  data: 0.0006  max mem: 19734
Epoch: [38]  [ 190/1251]  eta: 0:10:02  lr: 0.000014  loss: 3.5799 (3.4379)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [38]  [ 200/1251]  eta: 0:09:50  lr: 0.000014  loss: 3.3900 (3.4326)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [38]  [ 210/1251]  eta: 0:09:39  lr: 0.000014  loss: 3.5934 (3.4432)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [38]  [ 220/1251]  eta: 0:09:29  lr: 0.000014  loss: 3.6640 (3.4509)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [38]  [ 230/1251]  eta: 0:09:20  lr: 0.000014  loss: 3.5632 (3.4486)  time: 0.4654  data: 0.0005  max mem: 19734
Epoch: [38]  [ 240/1251]  eta: 0:09:10  lr: 0.000014  loss: 3.5152 (3.4539)  time: 0.4660  data: 0.0005  max mem: 19734
Epoch: [38]  [ 250/1251]  eta: 0:09:01  lr: 0.000014  loss: 3.6142 (3.4608)  time: 0.4530  data: 0.0006  max mem: 19734
Epoch: [38]  [ 260/1251]  eta: 0:08:53  lr: 0.000014  loss: 3.7974 (3.4726)  time: 0.4601  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5403, ratio_loss=0.0050, pruning_loss=0.1194, mse_loss=0.4444
Epoch: [38]  [ 270/1251]  eta: 0:08:47  lr: 0.000014  loss: 3.5189 (3.4643)  time: 0.4905  data: 0.0005  max mem: 19734
Epoch: [38]  [ 280/1251]  eta: 0:08:39  lr: 0.000014  loss: 3.2207 (3.4573)  time: 0.4840  data: 0.0005  max mem: 19734
Epoch: [38]  [ 290/1251]  eta: 0:08:32  lr: 0.000014  loss: 3.3195 (3.4595)  time: 0.4768  data: 0.0005  max mem: 19734
Epoch: [38]  [ 300/1251]  eta: 0:08:24  lr: 0.000014  loss: 3.2600 (3.4521)  time: 0.4773  data: 0.0005  max mem: 19734
Epoch: [38]  [ 310/1251]  eta: 0:08:17  lr: 0.000014  loss: 3.2600 (3.4511)  time: 0.4556  data: 0.0006  max mem: 19734
Epoch: [38]  [ 320/1251]  eta: 0:08:09  lr: 0.000014  loss: 3.3530 (3.4451)  time: 0.4551  data: 0.0006  max mem: 19734
Epoch: [38]  [ 330/1251]  eta: 0:08:02  lr: 0.000014  loss: 3.5291 (3.4537)  time: 0.4545  data: 0.0005  max mem: 19734
Epoch: [38]  [ 340/1251]  eta: 0:07:55  lr: 0.000014  loss: 3.6798 (3.4540)  time: 0.4538  data: 0.0003  max mem: 19734
Epoch: [38]  [ 350/1251]  eta: 0:07:48  lr: 0.000014  loss: 3.4802 (3.4479)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [38]  [ 360/1251]  eta: 0:07:41  lr: 0.000014  loss: 3.6185 (3.4510)  time: 0.4523  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3690, ratio_loss=0.0048, pruning_loss=0.1218, mse_loss=0.4394
Epoch: [38]  [ 370/1251]  eta: 0:07:35  lr: 0.000014  loss: 3.6936 (3.4523)  time: 0.4651  data: 0.0004  max mem: 19734
Epoch: [38]  [ 380/1251]  eta: 0:07:28  lr: 0.000014  loss: 3.6194 (3.4530)  time: 0.4642  data: 0.0007  max mem: 19734
Epoch: [38]  [ 390/1251]  eta: 0:07:22  lr: 0.000014  loss: 3.3597 (3.4484)  time: 0.4513  data: 0.0008  max mem: 19734
Epoch: [38]  [ 400/1251]  eta: 0:07:15  lr: 0.000014  loss: 3.4134 (3.4497)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [38]  [ 410/1251]  eta: 0:07:10  lr: 0.000014  loss: 3.5771 (3.4487)  time: 0.4694  data: 0.0004  max mem: 19734
Epoch: [38]  [ 420/1251]  eta: 0:07:04  lr: 0.000014  loss: 3.3667 (3.4475)  time: 0.4918  data: 0.0004  max mem: 19734
Epoch: [38]  [ 430/1251]  eta: 0:06:59  lr: 0.000014  loss: 3.3667 (3.4438)  time: 0.4933  data: 0.0004  max mem: 19734
Epoch: [38]  [ 440/1251]  eta: 0:06:53  lr: 0.000014  loss: 3.2495 (3.4379)  time: 0.4829  data: 0.0005  max mem: 19734
Epoch: [38]  [ 450/1251]  eta: 0:06:47  lr: 0.000014  loss: 3.4547 (3.4427)  time: 0.4682  data: 0.0004  max mem: 19734
Epoch: [38]  [ 460/1251]  eta: 0:06:41  lr: 0.000014  loss: 3.6401 (3.4406)  time: 0.4584  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3747, ratio_loss=0.0048, pruning_loss=0.1234, mse_loss=0.4246
Epoch: [38]  [ 470/1251]  eta: 0:06:35  lr: 0.000014  loss: 3.6293 (3.4450)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [38]  [ 480/1251]  eta: 0:06:29  lr: 0.000014  loss: 3.8059 (3.4491)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [38]  [ 490/1251]  eta: 0:06:23  lr: 0.000014  loss: 3.4734 (3.4454)  time: 0.4583  data: 0.0004  max mem: 19734
Epoch: [38]  [ 500/1251]  eta: 0:06:18  lr: 0.000014  loss: 3.5136 (3.4504)  time: 0.4590  data: 0.0006  max mem: 19734
Epoch: [38]  [ 510/1251]  eta: 0:06:12  lr: 0.000014  loss: 3.5571 (3.4509)  time: 0.4581  data: 0.0006  max mem: 19734
Epoch: [38]  [ 520/1251]  eta: 0:06:07  lr: 0.000014  loss: 3.4901 (3.4498)  time: 0.4648  data: 0.0005  max mem: 19734
Epoch: [38]  [ 530/1251]  eta: 0:06:01  lr: 0.000014  loss: 3.4616 (3.4481)  time: 0.4656  data: 0.0005  max mem: 19734
Epoch: [38]  [ 540/1251]  eta: 0:05:55  lr: 0.000014  loss: 3.3878 (3.4442)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [38]  [ 550/1251]  eta: 0:05:50  lr: 0.000014  loss: 3.1668 (3.4413)  time: 0.4647  data: 0.0005  max mem: 19734
Epoch: [38]  [ 560/1251]  eta: 0:05:45  lr: 0.000014  loss: 3.1275 (3.4338)  time: 0.4960  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3725, ratio_loss=0.0049, pruning_loss=0.1232, mse_loss=0.4244
Epoch: [38]  [ 570/1251]  eta: 0:05:40  lr: 0.000014  loss: 3.2245 (3.4348)  time: 0.4852  data: 0.0005  max mem: 19734
Epoch: [38]  [ 580/1251]  eta: 0:05:35  lr: 0.000014  loss: 3.6086 (3.4332)  time: 0.4734  data: 0.0005  max mem: 19734
Epoch: [38]  [ 590/1251]  eta: 0:05:29  lr: 0.000014  loss: 3.2570 (3.4362)  time: 0.4732  data: 0.0004  max mem: 19734
Epoch: [38]  [ 600/1251]  eta: 0:05:24  lr: 0.000014  loss: 3.3554 (3.4344)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [38]  [ 610/1251]  eta: 0:05:18  lr: 0.000014  loss: 3.4425 (3.4325)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [38]  [ 620/1251]  eta: 0:05:13  lr: 0.000014  loss: 3.6151 (3.4383)  time: 0.4543  data: 0.0008  max mem: 19734
Epoch: [38]  [ 630/1251]  eta: 0:05:07  lr: 0.000014  loss: 3.6004 (3.4399)  time: 0.4544  data: 0.0008  max mem: 19734
Epoch: [38]  [ 640/1251]  eta: 0:05:02  lr: 0.000014  loss: 3.2848 (3.4355)  time: 0.4558  data: 0.0006  max mem: 19734
Epoch: [38]  [ 650/1251]  eta: 0:04:57  lr: 0.000014  loss: 3.5105 (3.4378)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [38]  [ 660/1251]  eta: 0:04:51  lr: 0.000014  loss: 3.4816 (3.4314)  time: 0.4546  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4048, ratio_loss=0.0052, pruning_loss=0.1235, mse_loss=0.4205
Epoch: [38]  [ 670/1251]  eta: 0:04:46  lr: 0.000014  loss: 3.2788 (3.4310)  time: 0.4625  data: 0.0005  max mem: 19734
Epoch: [38]  [ 680/1251]  eta: 0:04:41  lr: 0.000014  loss: 3.5931 (3.4336)  time: 0.4650  data: 0.0005  max mem: 19734
Epoch: [38]  [ 690/1251]  eta: 0:04:36  lr: 0.000014  loss: 3.5931 (3.4339)  time: 0.4567  data: 0.0007  max mem: 19734
Epoch: [38]  [ 700/1251]  eta: 0:04:31  lr: 0.000014  loss: 3.6865 (3.4394)  time: 0.4785  data: 0.0007  max mem: 19734
Epoch: [38]  [ 710/1251]  eta: 0:04:26  lr: 0.000014  loss: 3.7082 (3.4421)  time: 0.5001  data: 0.0005  max mem: 19734
Epoch: [38]  [ 720/1251]  eta: 0:04:21  lr: 0.000014  loss: 3.5535 (3.4396)  time: 0.4959  data: 0.0004  max mem: 19734
Epoch: [38]  [ 730/1251]  eta: 0:04:16  lr: 0.000014  loss: 3.5522 (3.4372)  time: 0.4875  data: 0.0004  max mem: 19734
Epoch: [38]  [ 740/1251]  eta: 0:04:11  lr: 0.000014  loss: 3.3954 (3.4369)  time: 0.4708  data: 0.0004  max mem: 19734
Epoch: [38]  [ 750/1251]  eta: 0:04:06  lr: 0.000014  loss: 3.8028 (3.4426)  time: 0.4596  data: 0.0008  max mem: 19734
Epoch: [38]  [ 760/1251]  eta: 0:04:01  lr: 0.000014  loss: 3.6096 (3.4402)  time: 0.4607  data: 0.0009  max mem: 19734
loss info: cls_loss=3.4610, ratio_loss=0.0052, pruning_loss=0.1209, mse_loss=0.4189
Epoch: [38]  [ 770/1251]  eta: 0:03:56  lr: 0.000014  loss: 3.2471 (3.4368)  time: 0.4611  data: 0.0005  max mem: 19734
Epoch: [38]  [ 780/1251]  eta: 0:03:51  lr: 0.000014  loss: 3.0118 (3.4339)  time: 0.4577  data: 0.0005  max mem: 19734
Epoch: [38]  [ 790/1251]  eta: 0:03:45  lr: 0.000014  loss: 3.3228 (3.4324)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [38]  [ 800/1251]  eta: 0:03:40  lr: 0.000014  loss: 3.4889 (3.4358)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [38]  [ 810/1251]  eta: 0:03:35  lr: 0.000014  loss: 3.5856 (3.4347)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [38]  [ 820/1251]  eta: 0:03:30  lr: 0.000014  loss: 3.5335 (3.4340)  time: 0.4598  data: 0.0005  max mem: 19734
Epoch: [38]  [ 830/1251]  eta: 0:03:25  lr: 0.000014  loss: 3.5809 (3.4349)  time: 0.4603  data: 0.0005  max mem: 19734
Epoch: [38]  [ 840/1251]  eta: 0:03:20  lr: 0.000014  loss: 3.6840 (3.4370)  time: 0.4635  data: 0.0008  max mem: 19734
Epoch: [38]  [ 850/1251]  eta: 0:03:15  lr: 0.000014  loss: 3.7247 (3.4369)  time: 0.4917  data: 0.0007  max mem: 19734
Epoch: [38]  [ 860/1251]  eta: 0:03:10  lr: 0.000014  loss: 3.6362 (3.4394)  time: 0.4815  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4264, ratio_loss=0.0049, pruning_loss=0.1230, mse_loss=0.4358
Epoch: [38]  [ 870/1251]  eta: 0:03:06  lr: 0.000014  loss: 3.6362 (3.4392)  time: 0.4851  data: 0.0006  max mem: 19734
Epoch: [38]  [ 880/1251]  eta: 0:03:01  lr: 0.000014  loss: 3.3979 (3.4376)  time: 0.4845  data: 0.0006  max mem: 19734
Epoch: [38]  [ 890/1251]  eta: 0:02:56  lr: 0.000014  loss: 3.3979 (3.4357)  time: 0.4533  data: 0.0006  max mem: 19734
Epoch: [38]  [ 900/1251]  eta: 0:02:51  lr: 0.000014  loss: 3.6493 (3.4398)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [38]  [ 910/1251]  eta: 0:02:46  lr: 0.000014  loss: 3.7714 (3.4425)  time: 0.4560  data: 0.0004  max mem: 19734
Epoch: [38]  [ 920/1251]  eta: 0:02:41  lr: 0.000014  loss: 3.5257 (3.4414)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [38]  [ 930/1251]  eta: 0:02:36  lr: 0.000014  loss: 3.4994 (3.4411)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [38]  [ 940/1251]  eta: 0:02:31  lr: 0.000014  loss: 3.5095 (3.4400)  time: 0.4523  data: 0.0006  max mem: 19734
Epoch: [38]  [ 950/1251]  eta: 0:02:26  lr: 0.000014  loss: 3.5903 (3.4403)  time: 0.4534  data: 0.0008  max mem: 19734
Epoch: [38]  [ 960/1251]  eta: 0:02:21  lr: 0.000014  loss: 3.3924 (3.4394)  time: 0.4538  data: 0.0008  max mem: 19734
loss info: cls_loss=3.4152, ratio_loss=0.0055, pruning_loss=0.1222, mse_loss=0.4230
Epoch: [38]  [ 970/1251]  eta: 0:02:16  lr: 0.000014  loss: 3.4768 (3.4415)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [38]  [ 980/1251]  eta: 0:02:11  lr: 0.000014  loss: 3.5198 (3.4414)  time: 0.4692  data: 0.0005  max mem: 19734
Epoch: [38]  [ 990/1251]  eta: 0:02:06  lr: 0.000014  loss: 3.2992 (3.4396)  time: 0.4782  data: 0.0004  max mem: 19734
Epoch: [38]  [1000/1251]  eta: 0:02:01  lr: 0.000014  loss: 3.2357 (3.4398)  time: 0.4760  data: 0.0004  max mem: 19734
Epoch: [38]  [1010/1251]  eta: 0:01:56  lr: 0.000014  loss: 3.2357 (3.4369)  time: 0.4767  data: 0.0004  max mem: 19734
Epoch: [38]  [1020/1251]  eta: 0:01:51  lr: 0.000014  loss: 3.3595 (3.4384)  time: 0.4811  data: 0.0005  max mem: 19734
Epoch: [38]  [1030/1251]  eta: 0:01:47  lr: 0.000014  loss: 3.5296 (3.4376)  time: 0.4663  data: 0.0005  max mem: 19734
Epoch: [38]  [1040/1251]  eta: 0:01:42  lr: 0.000014  loss: 3.4747 (3.4375)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [38]  [1050/1251]  eta: 0:01:37  lr: 0.000014  loss: 3.4747 (3.4382)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [38]  [1060/1251]  eta: 0:01:32  lr: 0.000014  loss: 3.6253 (3.4375)  time: 0.4557  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3782, ratio_loss=0.0050, pruning_loss=0.1231, mse_loss=0.4354
Epoch: [38]  [1070/1251]  eta: 0:01:27  lr: 0.000014  loss: 3.6253 (3.4373)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [38]  [1080/1251]  eta: 0:01:22  lr: 0.000014  loss: 3.7768 (3.4385)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [38]  [1090/1251]  eta: 0:01:17  lr: 0.000014  loss: 3.7119 (3.4394)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [38]  [1100/1251]  eta: 0:01:12  lr: 0.000014  loss: 3.4321 (3.4377)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [38]  [1110/1251]  eta: 0:01:07  lr: 0.000014  loss: 3.4027 (3.4384)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [38]  [1120/1251]  eta: 0:01:03  lr: 0.000014  loss: 3.3247 (3.4359)  time: 0.4622  data: 0.0004  max mem: 19734
Epoch: [38]  [1130/1251]  eta: 0:00:58  lr: 0.000014  loss: 3.3247 (3.4343)  time: 0.4703  data: 0.0005  max mem: 19734
Epoch: [38]  [1140/1251]  eta: 0:00:53  lr: 0.000014  loss: 3.5205 (3.4347)  time: 0.4945  data: 0.0006  max mem: 19734
Epoch: [38]  [1150/1251]  eta: 0:00:48  lr: 0.000014  loss: 3.5327 (3.4367)  time: 0.4847  data: 0.0006  max mem: 19734
Epoch: [38]  [1160/1251]  eta: 0:00:43  lr: 0.000014  loss: 3.4923 (3.4358)  time: 0.4813  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3970, ratio_loss=0.0052, pruning_loss=0.1229, mse_loss=0.4250
Epoch: [38]  [1170/1251]  eta: 0:00:39  lr: 0.000014  loss: 3.3673 (3.4354)  time: 0.4812  data: 0.0004  max mem: 19734
Epoch: [38]  [1180/1251]  eta: 0:00:34  lr: 0.000014  loss: 3.0814 (3.4322)  time: 0.4534  data: 0.0006  max mem: 19734
Epoch: [38]  [1190/1251]  eta: 0:00:29  lr: 0.000014  loss: 3.1343 (3.4328)  time: 0.4547  data: 0.0009  max mem: 19734
Epoch: [38]  [1200/1251]  eta: 0:00:24  lr: 0.000014  loss: 3.6019 (3.4313)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [38]  [1210/1251]  eta: 0:00:19  lr: 0.000014  loss: 3.6520 (3.4321)  time: 0.4517  data: 0.0002  max mem: 19734
Epoch: [38]  [1220/1251]  eta: 0:00:14  lr: 0.000014  loss: 3.6809 (3.4336)  time: 0.4507  data: 0.0002  max mem: 19734
Epoch: [38]  [1230/1251]  eta: 0:00:10  lr: 0.000014  loss: 3.6812 (3.4342)  time: 0.4499  data: 0.0002  max mem: 19734
Epoch: [38]  [1240/1251]  eta: 0:00:05  lr: 0.000014  loss: 3.5582 (3.4346)  time: 0.4516  data: 0.0002  max mem: 19734
Epoch: [38]  [1250/1251]  eta: 0:00:00  lr: 0.000014  loss: 3.4448 (3.4326)  time: 0.4520  data: 0.0002  max mem: 19734
Epoch: [38] Total time: 0:10:01 (0.4807 s / it)
Averaged stats: lr: 0.000014  loss: 3.4448 (3.4170)
Test:  [  0/261]  eta: 2:09:57  loss: 0.6731 (0.6731)  acc1: 83.3333 (83.3333)  acc5: 96.8750 (96.8750)  time: 29.8738  data: 29.4715  max mem: 19734
Test:  [ 10/261]  eta: 0:12:30  loss: 0.6731 (0.7330)  acc1: 83.8542 (83.3333)  acc5: 96.8750 (96.1648)  time: 2.9884  data: 2.7972  max mem: 19734
Test:  [ 20/261]  eta: 0:06:32  loss: 0.9319 (0.9020)  acc1: 78.1250 (78.6458)  acc5: 93.7500 (94.7173)  time: 0.2153  data: 0.0719  max mem: 19734
Test:  [ 30/261]  eta: 0:04:26  loss: 0.8213 (0.8223)  acc1: 82.8125 (81.5524)  acc5: 94.2708 (95.1277)  time: 0.1424  data: 0.0150  max mem: 19734
Test:  [ 40/261]  eta: 0:03:58  loss: 0.5790 (0.7857)  acc1: 86.4583 (82.7363)  acc5: 96.3542 (95.4649)  time: 0.5034  data: 0.3775  max mem: 19734
Test:  [ 50/261]  eta: 0:03:07  loss: 0.8887 (0.8442)  acc1: 78.1250 (80.9436)  acc5: 95.3125 (95.0572)  time: 0.4852  data: 0.3771  max mem: 19734
Test:  [ 60/261]  eta: 0:02:33  loss: 0.9743 (0.8503)  acc1: 75.5208 (80.5584)  acc5: 94.2708 (95.1161)  time: 0.1203  data: 0.0128  max mem: 19734
Test:  [ 70/261]  eta: 0:02:35  loss: 0.9103 (0.8519)  acc1: 76.5625 (80.2230)  acc5: 96.3542 (95.3052)  time: 0.6237  data: 0.5115  max mem: 19734
Test:  [ 80/261]  eta: 0:02:14  loss: 0.8151 (0.8532)  acc1: 78.6458 (80.2919)  acc5: 96.8750 (95.4090)  time: 0.6755  data: 0.5139  max mem: 19734
Test:  [ 90/261]  eta: 0:01:55  loss: 0.8137 (0.8383)  acc1: 83.8542 (80.7578)  acc5: 96.3542 (95.5185)  time: 0.1755  data: 0.0180  max mem: 19734
Test:  [100/261]  eta: 0:01:59  loss: 0.8159 (0.8418)  acc1: 83.8542 (80.7446)  acc5: 95.8333 (95.5703)  time: 0.7477  data: 0.6385  max mem: 19734
Test:  [110/261]  eta: 0:01:44  loss: 0.8670 (0.8646)  acc1: 76.5625 (80.2928)  acc5: 94.7917 (95.2797)  time: 0.7731  data: 0.6385  max mem: 19734
Test:  [120/261]  eta: 0:01:31  loss: 1.2448 (0.9050)  acc1: 70.3125 (79.3302)  acc5: 89.0625 (94.7400)  time: 0.1908  data: 0.0184  max mem: 19734
Test:  [130/261]  eta: 0:01:20  loss: 1.3857 (0.9507)  acc1: 66.6667 (78.3954)  acc5: 86.4583 (94.1476)  time: 0.1832  data: 0.0149  max mem: 19734
Test:  [140/261]  eta: 0:01:11  loss: 1.3142 (0.9769)  acc1: 67.7083 (77.7261)  acc5: 90.1042 (93.9273)  time: 0.1947  data: 0.0134  max mem: 19734
Test:  [150/261]  eta: 0:01:02  loss: 1.1971 (0.9812)  acc1: 72.3958 (77.7283)  acc5: 92.1875 (93.7983)  time: 0.1962  data: 0.0091  max mem: 19734
Test:  [160/261]  eta: 0:00:53  loss: 0.9736 (1.0008)  acc1: 78.6458 (77.3971)  acc5: 92.1875 (93.5203)  time: 0.1366  data: 0.0041  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: 1.2988 (1.0302)  acc1: 67.1875 (76.6904)  acc5: 89.5833 (93.1804)  time: 0.1575  data: 0.0725  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4124 (1.0473)  acc1: 66.1458 (76.3064)  acc5: 89.0625 (93.0191)  time: 0.1333  data: 0.0712  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: 1.3879 (1.0600)  acc1: 68.2292 (76.0798)  acc5: 91.1458 (92.8692)  time: 0.0630  data: 0.0013  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3351 (1.0759)  acc1: 71.3542 (75.7048)  acc5: 90.1042 (92.6176)  time: 0.1001  data: 0.0380  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3785 (1.0897)  acc1: 69.2708 (75.4295)  acc5: 88.0208 (92.4245)  time: 0.0989  data: 0.0370  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4109 (1.1080)  acc1: 68.7500 (74.9576)  acc5: 88.5417 (92.2229)  time: 0.0613  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.3782 (1.1165)  acc1: 68.2292 (74.7137)  acc5: 89.0625 (92.1582)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.2953 (1.1257)  acc1: 68.2292 (74.4727)  acc5: 91.6667 (92.0903)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0327 (1.1175)  acc1: 76.0417 (74.6763)  acc5: 93.7500 (92.2000)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9124 (1.1170)  acc1: 76.5625 (74.6860)  acc5: 95.3125 (92.2700)  time: 0.0596  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3606 s / it)
* Acc@1 74.686 Acc@5 92.270 loss 1.117
Accuracy of the network on the 50000 test images: 74.7%
Max accuracy: 74.69%
Epoch: [39]  [   0/1251]  eta: 5:41:41  lr: 0.000014  loss: 3.1847 (3.1847)  time: 16.3885  data: 15.8754  max mem: 19734
loss info: cls_loss=3.3854, ratio_loss=0.0052, pruning_loss=0.1222, mse_loss=0.4444
Epoch: [39]  [  10/1251]  eta: 0:41:59  lr: 0.000014  loss: 3.6461 (3.5073)  time: 2.0302  data: 1.4852  max mem: 19734
Epoch: [39]  [  20/1251]  eta: 0:26:43  lr: 0.000014  loss: 3.5651 (3.5433)  time: 0.5483  data: 0.0233  max mem: 19734
Epoch: [39]  [  30/1251]  eta: 0:21:12  lr: 0.000014  loss: 3.4803 (3.4890)  time: 0.4983  data: 0.0005  max mem: 19734
Epoch: [39]  [  40/1251]  eta: 0:18:14  lr: 0.000014  loss: 3.3528 (3.4402)  time: 0.4858  data: 0.0005  max mem: 19734
Epoch: [39]  [  50/1251]  eta: 0:16:31  lr: 0.000014  loss: 3.3364 (3.4095)  time: 0.4906  data: 0.0005  max mem: 19734
Epoch: [39]  [  60/1251]  eta: 0:15:13  lr: 0.000014  loss: 3.3509 (3.3870)  time: 0.4845  data: 0.0007  max mem: 19734
Epoch: [39]  [  70/1251]  eta: 0:14:15  lr: 0.000014  loss: 3.3201 (3.3797)  time: 0.4669  data: 0.0007  max mem: 19734
Epoch: [39]  [  80/1251]  eta: 0:13:30  lr: 0.000014  loss: 3.2421 (3.3678)  time: 0.4643  data: 0.0005  max mem: 19734
Epoch: [39]  [  90/1251]  eta: 0:12:54  lr: 0.000014  loss: 3.5919 (3.3963)  time: 0.4624  data: 0.0005  max mem: 19734
Epoch: [39]  [ 100/1251]  eta: 0:12:24  lr: 0.000014  loss: 3.7000 (3.4209)  time: 0.4622  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4114, ratio_loss=0.0051, pruning_loss=0.1213, mse_loss=0.4281
Epoch: [39]  [ 110/1251]  eta: 0:11:58  lr: 0.000014  loss: 3.7224 (3.4426)  time: 0.4609  data: 0.0004  max mem: 19734
Epoch: [39]  [ 120/1251]  eta: 0:11:38  lr: 0.000014  loss: 3.7533 (3.4421)  time: 0.4701  data: 0.0005  max mem: 19734
Epoch: [39]  [ 130/1251]  eta: 0:11:18  lr: 0.000014  loss: 3.6544 (3.4733)  time: 0.4708  data: 0.0005  max mem: 19734
Epoch: [39]  [ 140/1251]  eta: 0:11:02  lr: 0.000014  loss: 3.6095 (3.4621)  time: 0.4690  data: 0.0005  max mem: 19734
Epoch: [39]  [ 150/1251]  eta: 0:10:46  lr: 0.000014  loss: 3.4796 (3.4622)  time: 0.4690  data: 0.0005  max mem: 19734
Epoch: [39]  [ 160/1251]  eta: 0:10:33  lr: 0.000014  loss: 3.5229 (3.4582)  time: 0.4741  data: 0.0005  max mem: 19734
Epoch: [39]  [ 170/1251]  eta: 0:10:23  lr: 0.000014  loss: 3.3504 (3.4492)  time: 0.4959  data: 0.0005  max mem: 19734
Epoch: [39]  [ 180/1251]  eta: 0:10:11  lr: 0.000014  loss: 3.3504 (3.4481)  time: 0.4935  data: 0.0004  max mem: 19734
Epoch: [39]  [ 190/1251]  eta: 0:10:03  lr: 0.000014  loss: 3.6033 (3.4584)  time: 0.4988  data: 0.0004  max mem: 19734
Epoch: [39]  [ 200/1251]  eta: 0:09:51  lr: 0.000014  loss: 3.6173 (3.4526)  time: 0.4871  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4238, ratio_loss=0.0052, pruning_loss=0.1200, mse_loss=0.4221
Epoch: [39]  [ 210/1251]  eta: 0:09:41  lr: 0.000014  loss: 3.4591 (3.4450)  time: 0.4587  data: 0.0004  max mem: 19734
Epoch: [39]  [ 220/1251]  eta: 0:09:30  lr: 0.000014  loss: 3.6059 (3.4542)  time: 0.4583  data: 0.0004  max mem: 19734
Epoch: [39]  [ 230/1251]  eta: 0:09:21  lr: 0.000014  loss: 3.6059 (3.4477)  time: 0.4578  data: 0.0006  max mem: 19734
Epoch: [39]  [ 240/1251]  eta: 0:09:11  lr: 0.000014  loss: 3.4399 (3.4526)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [39]  [ 250/1251]  eta: 0:09:02  lr: 0.000014  loss: 3.4133 (3.4436)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [39]  [ 260/1251]  eta: 0:08:55  lr: 0.000014  loss: 3.4133 (3.4480)  time: 0.4696  data: 0.0004  max mem: 19734
Epoch: [39]  [ 270/1251]  eta: 0:08:46  lr: 0.000014  loss: 3.6338 (3.4494)  time: 0.4657  data: 0.0004  max mem: 19734
Epoch: [39]  [ 280/1251]  eta: 0:08:38  lr: 0.000014  loss: 3.5442 (3.4490)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [39]  [ 290/1251]  eta: 0:08:30  lr: 0.000014  loss: 3.2911 (3.4367)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [39]  [ 300/1251]  eta: 0:08:22  lr: 0.000014  loss: 3.2264 (3.4386)  time: 0.4540  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4035, ratio_loss=0.0049, pruning_loss=0.1210, mse_loss=0.4253
Epoch: [39]  [ 310/1251]  eta: 0:08:16  lr: 0.000014  loss: 3.5090 (3.4394)  time: 0.4832  data: 0.0006  max mem: 19734
Epoch: [39]  [ 320/1251]  eta: 0:08:10  lr: 0.000014  loss: 3.6319 (3.4446)  time: 0.4946  data: 0.0006  max mem: 19734
Epoch: [39]  [ 330/1251]  eta: 0:08:03  lr: 0.000014  loss: 3.6477 (3.4418)  time: 0.4863  data: 0.0004  max mem: 19734
Epoch: [39]  [ 340/1251]  eta: 0:07:57  lr: 0.000014  loss: 3.4413 (3.4388)  time: 0.4827  data: 0.0004  max mem: 19734
Epoch: [39]  [ 350/1251]  eta: 0:07:50  lr: 0.000014  loss: 3.5064 (3.4417)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [39]  [ 360/1251]  eta: 0:07:43  lr: 0.000014  loss: 3.5760 (3.4321)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [39]  [ 370/1251]  eta: 0:07:36  lr: 0.000014  loss: 3.5289 (3.4321)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [39]  [ 380/1251]  eta: 0:07:30  lr: 0.000014  loss: 3.6320 (3.4378)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [39]  [ 390/1251]  eta: 0:07:23  lr: 0.000014  loss: 3.5822 (3.4376)  time: 0.4573  data: 0.0008  max mem: 19734
Epoch: [39]  [ 400/1251]  eta: 0:07:17  lr: 0.000014  loss: 3.6323 (3.4412)  time: 0.4541  data: 0.0008  max mem: 19734
loss info: cls_loss=3.4292, ratio_loss=0.0050, pruning_loss=0.1207, mse_loss=0.4517
Epoch: [39]  [ 410/1251]  eta: 0:07:11  lr: 0.000014  loss: 3.6165 (3.4428)  time: 0.4640  data: 0.0006  max mem: 19734
Epoch: [39]  [ 420/1251]  eta: 0:07:04  lr: 0.000014  loss: 3.5565 (3.4445)  time: 0.4649  data: 0.0006  max mem: 19734
Epoch: [39]  [ 430/1251]  eta: 0:06:58  lr: 0.000014  loss: 3.7143 (3.4470)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [39]  [ 440/1251]  eta: 0:06:52  lr: 0.000014  loss: 3.6711 (3.4489)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [39]  [ 450/1251]  eta: 0:06:46  lr: 0.000014  loss: 3.3892 (3.4451)  time: 0.4635  data: 0.0004  max mem: 19734
Epoch: [39]  [ 460/1251]  eta: 0:06:41  lr: 0.000014  loss: 3.1945 (3.4380)  time: 0.4861  data: 0.0004  max mem: 19734
Epoch: [39]  [ 470/1251]  eta: 0:06:36  lr: 0.000014  loss: 3.4051 (3.4409)  time: 0.4943  data: 0.0004  max mem: 19734
Epoch: [39]  [ 480/1251]  eta: 0:06:30  lr: 0.000014  loss: 3.4751 (3.4449)  time: 0.4841  data: 0.0005  max mem: 19734
Epoch: [39]  [ 490/1251]  eta: 0:06:24  lr: 0.000014  loss: 3.5970 (3.4463)  time: 0.4660  data: 0.0005  max mem: 19734
Epoch: [39]  [ 500/1251]  eta: 0:06:18  lr: 0.000014  loss: 3.4878 (3.4405)  time: 0.4543  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4110, ratio_loss=0.0049, pruning_loss=0.1205, mse_loss=0.4321
Epoch: [39]  [ 510/1251]  eta: 0:06:13  lr: 0.000014  loss: 3.3806 (3.4413)  time: 0.4565  data: 0.0005  max mem: 19734
Epoch: [39]  [ 520/1251]  eta: 0:06:07  lr: 0.000014  loss: 3.5113 (3.4448)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [39]  [ 530/1251]  eta: 0:06:01  lr: 0.000014  loss: 3.4573 (3.4404)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [39]  [ 540/1251]  eta: 0:05:56  lr: 0.000014  loss: 3.2264 (3.4372)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [39]  [ 550/1251]  eta: 0:05:50  lr: 0.000014  loss: 3.4289 (3.4382)  time: 0.4555  data: 0.0005  max mem: 19734
Epoch: [39]  [ 560/1251]  eta: 0:05:45  lr: 0.000014  loss: 3.5327 (3.4380)  time: 0.4661  data: 0.0005  max mem: 19734
Epoch: [39]  [ 570/1251]  eta: 0:05:39  lr: 0.000014  loss: 3.6421 (3.4429)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [39]  [ 580/1251]  eta: 0:05:34  lr: 0.000014  loss: 3.8084 (3.4458)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [39]  [ 590/1251]  eta: 0:05:28  lr: 0.000014  loss: 3.8124 (3.4479)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [39]  [ 600/1251]  eta: 0:05:23  lr: 0.000014  loss: 3.7407 (3.4513)  time: 0.4761  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5124, ratio_loss=0.0047, pruning_loss=0.1189, mse_loss=0.4184
Epoch: [39]  [ 610/1251]  eta: 0:05:18  lr: 0.000014  loss: 3.7407 (3.4568)  time: 0.4937  data: 0.0006  max mem: 19734
Epoch: [39]  [ 620/1251]  eta: 0:05:13  lr: 0.000014  loss: 3.6797 (3.4544)  time: 0.4935  data: 0.0005  max mem: 19734
Epoch: [39]  [ 630/1251]  eta: 0:05:08  lr: 0.000014  loss: 3.6311 (3.4550)  time: 0.4779  data: 0.0005  max mem: 19734
Epoch: [39]  [ 640/1251]  eta: 0:05:03  lr: 0.000014  loss: 3.3143 (3.4496)  time: 0.4569  data: 0.0005  max mem: 19734
Epoch: [39]  [ 650/1251]  eta: 0:04:57  lr: 0.000014  loss: 3.2279 (3.4483)  time: 0.4540  data: 0.0005  max mem: 19734
Epoch: [39]  [ 660/1251]  eta: 0:04:52  lr: 0.000014  loss: 3.2697 (3.4462)  time: 0.4534  data: 0.0006  max mem: 19734
Epoch: [39]  [ 670/1251]  eta: 0:04:47  lr: 0.000014  loss: 3.6663 (3.4501)  time: 0.4540  data: 0.0006  max mem: 19734
Epoch: [39]  [ 680/1251]  eta: 0:04:41  lr: 0.000014  loss: 3.6603 (3.4474)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [39]  [ 690/1251]  eta: 0:04:36  lr: 0.000014  loss: 3.4222 (3.4438)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [39]  [ 700/1251]  eta: 0:04:31  lr: 0.000014  loss: 3.4420 (3.4405)  time: 0.4556  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3198, ratio_loss=0.0048, pruning_loss=0.1219, mse_loss=0.4287
Epoch: [39]  [ 710/1251]  eta: 0:04:26  lr: 0.000014  loss: 3.4420 (3.4409)  time: 0.4621  data: 0.0006  max mem: 19734
Epoch: [39]  [ 720/1251]  eta: 0:04:21  lr: 0.000014  loss: 3.4700 (3.4396)  time: 0.4618  data: 0.0006  max mem: 19734
Epoch: [39]  [ 730/1251]  eta: 0:04:15  lr: 0.000014  loss: 3.5650 (3.4414)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [39]  [ 740/1251]  eta: 0:04:11  lr: 0.000014  loss: 3.6063 (3.4380)  time: 0.4744  data: 0.0004  max mem: 19734
Epoch: [39]  [ 750/1251]  eta: 0:04:05  lr: 0.000014  loss: 3.4571 (3.4366)  time: 0.4817  data: 0.0006  max mem: 19734
Epoch: [39]  [ 760/1251]  eta: 0:04:00  lr: 0.000014  loss: 3.5996 (3.4353)  time: 0.4735  data: 0.0006  max mem: 19734
Epoch: [39]  [ 770/1251]  eta: 0:03:56  lr: 0.000014  loss: 3.6493 (3.4353)  time: 0.4898  data: 0.0005  max mem: 19734
Epoch: [39]  [ 780/1251]  eta: 0:03:51  lr: 0.000014  loss: 3.5997 (3.4346)  time: 0.4795  data: 0.0005  max mem: 19734
Epoch: [39]  [ 790/1251]  eta: 0:03:45  lr: 0.000014  loss: 3.3962 (3.4305)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [39]  [ 800/1251]  eta: 0:03:40  lr: 0.000014  loss: 3.4075 (3.4322)  time: 0.4580  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3438, ratio_loss=0.0046, pruning_loss=0.1210, mse_loss=0.4354
Epoch: [39]  [ 810/1251]  eta: 0:03:35  lr: 0.000014  loss: 3.4574 (3.4319)  time: 0.4562  data: 0.0008  max mem: 19734
Epoch: [39]  [ 820/1251]  eta: 0:03:30  lr: 0.000014  loss: 3.4574 (3.4316)  time: 0.4531  data: 0.0006  max mem: 19734
Epoch: [39]  [ 830/1251]  eta: 0:03:25  lr: 0.000014  loss: 3.4030 (3.4292)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [39]  [ 840/1251]  eta: 0:03:20  lr: 0.000014  loss: 3.3293 (3.4278)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [39]  [ 850/1251]  eta: 0:03:15  lr: 0.000014  loss: 3.2045 (3.4260)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [39]  [ 860/1251]  eta: 0:03:10  lr: 0.000014  loss: 3.5967 (3.4279)  time: 0.4617  data: 0.0004  max mem: 19734
Epoch: [39]  [ 870/1251]  eta: 0:03:05  lr: 0.000014  loss: 3.6401 (3.4273)  time: 0.4619  data: 0.0005  max mem: 19734
Epoch: [39]  [ 880/1251]  eta: 0:03:00  lr: 0.000014  loss: 3.5662 (3.4257)  time: 0.4547  data: 0.0007  max mem: 19734
Epoch: [39]  [ 890/1251]  eta: 0:02:55  lr: 0.000014  loss: 3.3794 (3.4254)  time: 0.4863  data: 0.0006  max mem: 19734
Epoch: [39]  [ 900/1251]  eta: 0:02:50  lr: 0.000014  loss: 3.4474 (3.4273)  time: 0.4932  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3658, ratio_loss=0.0051, pruning_loss=0.1209, mse_loss=0.4033
Epoch: [39]  [ 910/1251]  eta: 0:02:46  lr: 0.000014  loss: 3.5704 (3.4272)  time: 0.4819  data: 0.0005  max mem: 19734
Epoch: [39]  [ 920/1251]  eta: 0:02:41  lr: 0.000014  loss: 3.0991 (3.4242)  time: 0.4732  data: 0.0005  max mem: 19734
Epoch: [39]  [ 930/1251]  eta: 0:02:36  lr: 0.000014  loss: 3.0444 (3.4229)  time: 0.4512  data: 0.0005  max mem: 19734
Epoch: [39]  [ 940/1251]  eta: 0:02:31  lr: 0.000014  loss: 3.3692 (3.4213)  time: 0.4492  data: 0.0006  max mem: 19734
Epoch: [39]  [ 950/1251]  eta: 0:02:26  lr: 0.000014  loss: 3.4029 (3.4234)  time: 0.4508  data: 0.0005  max mem: 19734
Epoch: [39]  [ 960/1251]  eta: 0:02:21  lr: 0.000014  loss: 3.4357 (3.4220)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [39]  [ 970/1251]  eta: 0:02:16  lr: 0.000014  loss: 3.1502 (3.4196)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [39]  [ 980/1251]  eta: 0:02:11  lr: 0.000014  loss: 3.2928 (3.4201)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [39]  [ 990/1251]  eta: 0:02:06  lr: 0.000014  loss: 3.6833 (3.4197)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [39]  [1000/1251]  eta: 0:02:01  lr: 0.000014  loss: 3.4187 (3.4193)  time: 0.4568  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3165, ratio_loss=0.0049, pruning_loss=0.1210, mse_loss=0.4118
Epoch: [39]  [1010/1251]  eta: 0:01:56  lr: 0.000014  loss: 3.3671 (3.4186)  time: 0.4635  data: 0.0005  max mem: 19734
Epoch: [39]  [1020/1251]  eta: 0:01:51  lr: 0.000014  loss: 3.4579 (3.4187)  time: 0.4634  data: 0.0006  max mem: 19734
Epoch: [39]  [1030/1251]  eta: 0:01:46  lr: 0.000014  loss: 3.6741 (3.4217)  time: 0.4779  data: 0.0005  max mem: 19734
Epoch: [39]  [1040/1251]  eta: 0:01:42  lr: 0.000014  loss: 3.6444 (3.4216)  time: 0.4924  data: 0.0006  max mem: 19734
Epoch: [39]  [1050/1251]  eta: 0:01:37  lr: 0.000014  loss: 3.3799 (3.4219)  time: 0.4686  data: 0.0006  max mem: 19734
Epoch: [39]  [1060/1251]  eta: 0:01:32  lr: 0.000014  loss: 3.5187 (3.4214)  time: 0.4800  data: 0.0005  max mem: 19734
Epoch: [39]  [1070/1251]  eta: 0:01:27  lr: 0.000014  loss: 3.5425 (3.4208)  time: 0.4796  data: 0.0005  max mem: 19734
Epoch: [39]  [1080/1251]  eta: 0:01:22  lr: 0.000014  loss: 3.5447 (3.4223)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [39]  [1090/1251]  eta: 0:01:17  lr: 0.000014  loss: 3.6223 (3.4244)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [39]  [1100/1251]  eta: 0:01:12  lr: 0.000014  loss: 3.4730 (3.4247)  time: 0.4523  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4733, ratio_loss=0.0048, pruning_loss=0.1199, mse_loss=0.4551
Epoch: [39]  [1110/1251]  eta: 0:01:07  lr: 0.000014  loss: 3.5569 (3.4257)  time: 0.4571  data: 0.0005  max mem: 19734
Epoch: [39]  [1120/1251]  eta: 0:01:03  lr: 0.000014  loss: 3.6370 (3.4264)  time: 0.4588  data: 0.0005  max mem: 19734
Epoch: [39]  [1130/1251]  eta: 0:00:58  lr: 0.000014  loss: 3.6687 (3.4269)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [39]  [1140/1251]  eta: 0:00:53  lr: 0.000014  loss: 3.6996 (3.4278)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [39]  [1150/1251]  eta: 0:00:48  lr: 0.000014  loss: 3.7535 (3.4303)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [39]  [1160/1251]  eta: 0:00:43  lr: 0.000014  loss: 3.8299 (3.4324)  time: 0.4614  data: 0.0004  max mem: 19734
Epoch: [39]  [1170/1251]  eta: 0:00:38  lr: 0.000014  loss: 3.5530 (3.4300)  time: 0.4670  data: 0.0004  max mem: 19734
Epoch: [39]  [1180/1251]  eta: 0:00:34  lr: 0.000014  loss: 3.1962 (3.4291)  time: 0.4813  data: 0.0005  max mem: 19734
Epoch: [39]  [1190/1251]  eta: 0:00:29  lr: 0.000014  loss: 3.6569 (3.4320)  time: 0.4825  data: 0.0010  max mem: 19734
Epoch: [39]  [1200/1251]  eta: 0:00:24  lr: 0.000014  loss: 3.6470 (3.4334)  time: 0.4696  data: 0.0008  max mem: 19734
loss info: cls_loss=3.4745, ratio_loss=0.0053, pruning_loss=0.1202, mse_loss=0.4209
Epoch: [39]  [1210/1251]  eta: 0:00:19  lr: 0.000014  loss: 3.4839 (3.4317)  time: 0.4616  data: 0.0002  max mem: 19734
Epoch: [39]  [1220/1251]  eta: 0:00:14  lr: 0.000014  loss: 3.2742 (3.4298)  time: 0.4504  data: 0.0002  max mem: 19734
Epoch: [39]  [1230/1251]  eta: 0:00:10  lr: 0.000014  loss: 3.1444 (3.4288)  time: 0.4458  data: 0.0002  max mem: 19734
Epoch: [39]  [1240/1251]  eta: 0:00:05  lr: 0.000014  loss: 3.2863 (3.4284)  time: 0.4487  data: 0.0002  max mem: 19734
Epoch: [39]  [1250/1251]  eta: 0:00:00  lr: 0.000014  loss: 3.2863 (3.4251)  time: 0.4467  data: 0.0002  max mem: 19734
Epoch: [39] Total time: 0:10:00 (0.4803 s / it)
Averaged stats: lr: 0.000014  loss: 3.2863 (3.4143)
Test:  [  0/261]  eta: 2:31:47  loss: 0.6725 (0.6725)  acc1: 85.4167 (85.4167)  acc5: 96.8750 (96.8750)  time: 34.8934  data: 34.7059  max mem: 19734
Test:  [ 10/261]  eta: 0:13:58  loss: 0.6725 (0.7247)  acc1: 85.4167 (83.8068)  acc5: 96.8750 (96.2595)  time: 3.3413  data: 3.1718  max mem: 19734
Test:  [ 20/261]  eta: 0:07:27  loss: 0.9613 (0.8891)  acc1: 76.5625 (79.1667)  acc5: 94.2708 (94.7173)  time: 0.2038  data: 0.0187  max mem: 19734
Test:  [ 30/261]  eta: 0:05:09  loss: 0.7905 (0.8084)  acc1: 84.3750 (81.8884)  acc5: 94.2708 (95.0941)  time: 0.2406  data: 0.0179  max mem: 19734
Test:  [ 40/261]  eta: 0:04:29  loss: 0.5569 (0.7774)  acc1: 88.0208 (82.8633)  acc5: 96.8750 (95.3760)  time: 0.5549  data: 0.3369  max mem: 19734
Test:  [ 50/261]  eta: 0:03:34  loss: 0.9174 (0.8416)  acc1: 77.6042 (81.0253)  acc5: 95.3125 (95.0061)  time: 0.5098  data: 0.3367  max mem: 19734
Test:  [ 60/261]  eta: 0:02:57  loss: 0.9744 (0.8482)  acc1: 75.5208 (80.6694)  acc5: 94.2708 (95.0905)  time: 0.1938  data: 0.0144  max mem: 19734
Test:  [ 70/261]  eta: 0:02:39  loss: 0.9200 (0.8496)  acc1: 77.6042 (80.2303)  acc5: 96.8750 (95.3125)  time: 0.3707  data: 0.1645  max mem: 19734
Test:  [ 80/261]  eta: 0:02:16  loss: 0.8574 (0.8518)  acc1: 79.1667 (80.2726)  acc5: 96.8750 (95.3961)  time: 0.3558  data: 0.1647  max mem: 19734
Test:  [ 90/261]  eta: 0:01:57  loss: 0.8236 (0.8372)  acc1: 82.2917 (80.7234)  acc5: 95.8333 (95.4899)  time: 0.1732  data: 0.0132  max mem: 19734
Test:  [100/261]  eta: 0:01:50  loss: 0.8236 (0.8406)  acc1: 83.8542 (80.6776)  acc5: 95.3125 (95.5446)  time: 0.4062  data: 0.2577  max mem: 19734
Test:  [110/261]  eta: 0:01:36  loss: 0.8659 (0.8653)  acc1: 78.1250 (80.1520)  acc5: 94.2708 (95.2234)  time: 0.4115  data: 0.2609  max mem: 19734
Test:  [120/261]  eta: 0:01:24  loss: 1.1762 (0.9047)  acc1: 73.4375 (79.2657)  acc5: 90.1042 (94.6884)  time: 0.1642  data: 0.0168  max mem: 19734
Test:  [130/261]  eta: 0:01:14  loss: 1.3510 (0.9486)  acc1: 66.6667 (78.3397)  acc5: 88.0208 (94.0840)  time: 0.2040  data: 0.0589  max mem: 19734
Test:  [140/261]  eta: 0:01:10  loss: 1.3039 (0.9751)  acc1: 68.2292 (77.7445)  acc5: 89.5833 (93.8165)  time: 0.4867  data: 0.3077  max mem: 19734
Test:  [150/261]  eta: 0:01:01  loss: 1.2132 (0.9800)  acc1: 72.9167 (77.7111)  acc5: 91.6667 (93.6810)  time: 0.4171  data: 0.2610  max mem: 19734
Test:  [160/261]  eta: 0:00:53  loss: 0.9899 (0.9995)  acc1: 78.6458 (77.3777)  acc5: 91.6667 (93.3812)  time: 0.1409  data: 0.0528  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: 1.2687 (1.0294)  acc1: 66.6667 (76.6478)  acc5: 87.5000 (93.0312)  time: 0.1622  data: 0.0922  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4582 (1.0471)  acc1: 66.1458 (76.2460)  acc5: 87.5000 (92.8781)  time: 0.1078  data: 0.0459  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3760 (1.0608)  acc1: 68.2292 (75.9571)  acc5: 91.1458 (92.7056)  time: 0.0662  data: 0.0027  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3718 (1.0763)  acc1: 71.8750 (75.6970)  acc5: 88.0208 (92.4751)  time: 0.0792  data: 0.0129  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3718 (1.0896)  acc1: 68.7500 (75.4196)  acc5: 88.0208 (92.2813)  time: 0.0757  data: 0.0114  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4353 (1.1097)  acc1: 67.1875 (74.9387)  acc5: 88.0208 (92.0720)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4476 (1.1201)  acc1: 64.5833 (74.6460)  acc5: 88.5417 (91.9756)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3366 (1.1290)  acc1: 67.1875 (74.4165)  acc5: 90.6250 (91.9044)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0192 (1.1218)  acc1: 76.0417 (74.6140)  acc5: 92.7083 (92.0173)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9170 (1.1219)  acc1: 78.6458 (74.6180)  acc5: 95.8333 (92.0800)  time: 0.0596  data: 0.0002  max mem: 19734
Test: Total time: 0:01:32 (0.3548 s / it)
* Acc@1 74.618 Acc@5 92.080 loss 1.122
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.69%
Epoch: [40]  [   0/1251]  eta: 5:39:12  lr: 0.000014  loss: 2.5853 (2.5853)  time: 16.2686  data: 6.6586  max mem: 19734
Epoch: [40]  [  10/1251]  eta: 0:43:43  lr: 0.000014  loss: 3.5613 (3.4045)  time: 2.1138  data: 0.6107  max mem: 19734
Epoch: [40]  [  20/1251]  eta: 0:27:21  lr: 0.000014  loss: 3.6291 (3.4653)  time: 0.5865  data: 0.0031  max mem: 19734
Epoch: [40]  [  30/1251]  eta: 0:21:22  lr: 0.000014  loss: 3.5748 (3.5061)  time: 0.4659  data: 0.0004  max mem: 19734
Epoch: [40]  [  40/1251]  eta: 0:18:17  lr: 0.000014  loss: 3.4275 (3.4593)  time: 0.4585  data: 0.0005  max mem: 19734
Epoch: [40]  [  50/1251]  eta: 0:16:24  lr: 0.000014  loss: 3.4149 (3.4368)  time: 0.4625  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3321, ratio_loss=0.0051, pruning_loss=0.1227, mse_loss=0.4287
Epoch: [40]  [  60/1251]  eta: 0:15:11  lr: 0.000014  loss: 3.4302 (3.4271)  time: 0.4746  data: 0.0004  max mem: 19734
Epoch: [40]  [  70/1251]  eta: 0:14:16  lr: 0.000014  loss: 3.2366 (3.3706)  time: 0.4846  data: 0.0006  max mem: 19734
Epoch: [40]  [  80/1251]  eta: 0:13:34  lr: 0.000014  loss: 3.3866 (3.3963)  time: 0.4851  data: 0.0006  max mem: 19734
Epoch: [40]  [  90/1251]  eta: 0:13:05  lr: 0.000014  loss: 3.3866 (3.3577)  time: 0.5040  data: 0.0005  max mem: 19734
Epoch: [40]  [ 100/1251]  eta: 0:12:34  lr: 0.000014  loss: 3.2542 (3.3571)  time: 0.4912  data: 0.0005  max mem: 19734
Epoch: [40]  [ 110/1251]  eta: 0:12:07  lr: 0.000014  loss: 3.3480 (3.3559)  time: 0.4605  data: 0.0007  max mem: 19734
Epoch: [40]  [ 120/1251]  eta: 0:11:44  lr: 0.000014  loss: 3.5907 (3.3693)  time: 0.4598  data: 0.0007  max mem: 19734
Epoch: [40]  [ 130/1251]  eta: 0:11:23  lr: 0.000014  loss: 3.6661 (3.3741)  time: 0.4564  data: 0.0007  max mem: 19734
Epoch: [40]  [ 140/1251]  eta: 0:11:06  lr: 0.000014  loss: 3.3048 (3.3674)  time: 0.4582  data: 0.0009  max mem: 19734
Epoch: [40]  [ 150/1251]  eta: 0:10:49  lr: 0.000014  loss: 3.4226 (3.3760)  time: 0.4594  data: 0.0006  max mem: 19734
loss info: cls_loss=3.2962, ratio_loss=0.0047, pruning_loss=0.1230, mse_loss=0.4272
Epoch: [40]  [ 160/1251]  eta: 0:10:34  lr: 0.000014  loss: 3.4345 (3.3685)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [40]  [ 170/1251]  eta: 0:10:22  lr: 0.000014  loss: 3.4429 (3.3748)  time: 0.4671  data: 0.0005  max mem: 19734
Epoch: [40]  [ 180/1251]  eta: 0:10:09  lr: 0.000014  loss: 3.4231 (3.3718)  time: 0.4657  data: 0.0005  max mem: 19734
Epoch: [40]  [ 190/1251]  eta: 0:09:57  lr: 0.000014  loss: 3.4179 (3.3735)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [40]  [ 200/1251]  eta: 0:09:45  lr: 0.000014  loss: 3.4399 (3.3699)  time: 0.4525  data: 0.0007  max mem: 19734
Epoch: [40]  [ 210/1251]  eta: 0:09:37  lr: 0.000014  loss: 3.6611 (3.3769)  time: 0.4794  data: 0.0007  max mem: 19734
Epoch: [40]  [ 220/1251]  eta: 0:09:27  lr: 0.000014  loss: 3.6729 (3.3805)  time: 0.4797  data: 0.0005  max mem: 19734
Epoch: [40]  [ 230/1251]  eta: 0:09:18  lr: 0.000014  loss: 3.4465 (3.3847)  time: 0.4639  data: 0.0005  max mem: 19734
Epoch: [40]  [ 240/1251]  eta: 0:09:11  lr: 0.000014  loss: 3.4971 (3.3893)  time: 0.4967  data: 0.0005  max mem: 19734
Epoch: [40]  [ 250/1251]  eta: 0:09:02  lr: 0.000014  loss: 3.6360 (3.4011)  time: 0.4881  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4190, ratio_loss=0.0048, pruning_loss=0.1198, mse_loss=0.4332
Epoch: [40]  [ 260/1251]  eta: 0:08:54  lr: 0.000014  loss: 3.5651 (3.3960)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [40]  [ 270/1251]  eta: 0:08:45  lr: 0.000014  loss: 3.3820 (3.3915)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [40]  [ 280/1251]  eta: 0:08:37  lr: 0.000014  loss: 3.0606 (3.3872)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [40]  [ 290/1251]  eta: 0:08:29  lr: 0.000014  loss: 3.3634 (3.3915)  time: 0.4585  data: 0.0005  max mem: 19734
Epoch: [40]  [ 300/1251]  eta: 0:08:22  lr: 0.000014  loss: 3.5528 (3.3953)  time: 0.4585  data: 0.0005  max mem: 19734
Epoch: [40]  [ 310/1251]  eta: 0:08:14  lr: 0.000014  loss: 3.6472 (3.4045)  time: 0.4589  data: 0.0005  max mem: 19734
Epoch: [40]  [ 320/1251]  eta: 0:08:08  lr: 0.000014  loss: 3.4477 (3.3937)  time: 0.4657  data: 0.0005  max mem: 19734
Epoch: [40]  [ 330/1251]  eta: 0:08:01  lr: 0.000014  loss: 3.3412 (3.3970)  time: 0.4658  data: 0.0005  max mem: 19734
Epoch: [40]  [ 340/1251]  eta: 0:07:54  lr: 0.000014  loss: 3.6503 (3.4049)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [40]  [ 350/1251]  eta: 0:07:47  lr: 0.000014  loss: 3.6120 (3.4003)  time: 0.4658  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4317, ratio_loss=0.0049, pruning_loss=0.1203, mse_loss=0.4253
Epoch: [40]  [ 360/1251]  eta: 0:07:42  lr: 0.000014  loss: 3.6309 (3.4136)  time: 0.4872  data: 0.0004  max mem: 19734
Epoch: [40]  [ 370/1251]  eta: 0:07:35  lr: 0.000014  loss: 3.7601 (3.4200)  time: 0.4886  data: 0.0005  max mem: 19734
Epoch: [40]  [ 380/1251]  eta: 0:07:30  lr: 0.000014  loss: 3.6438 (3.4279)  time: 0.4986  data: 0.0005  max mem: 19734
Epoch: [40]  [ 390/1251]  eta: 0:07:24  lr: 0.000014  loss: 3.6438 (3.4315)  time: 0.4892  data: 0.0004  max mem: 19734
Epoch: [40]  [ 400/1251]  eta: 0:07:17  lr: 0.000014  loss: 3.6853 (3.4374)  time: 0.4581  data: 0.0005  max mem: 19734
Epoch: [40]  [ 410/1251]  eta: 0:07:11  lr: 0.000014  loss: 3.6800 (3.4384)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [40]  [ 420/1251]  eta: 0:07:05  lr: 0.000014  loss: 3.6958 (3.4424)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [40]  [ 430/1251]  eta: 0:06:59  lr: 0.000014  loss: 3.7653 (3.4456)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [40]  [ 440/1251]  eta: 0:06:53  lr: 0.000014  loss: 3.5795 (3.4463)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [40]  [ 450/1251]  eta: 0:06:47  lr: 0.000014  loss: 3.3112 (3.4347)  time: 0.4583  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5082, ratio_loss=0.0049, pruning_loss=0.1178, mse_loss=0.4150
Epoch: [40]  [ 460/1251]  eta: 0:06:41  lr: 0.000014  loss: 3.0068 (3.4373)  time: 0.4619  data: 0.0005  max mem: 19734
Epoch: [40]  [ 470/1251]  eta: 0:06:35  lr: 0.000014  loss: 3.5287 (3.4365)  time: 0.4754  data: 0.0005  max mem: 19734
Epoch: [40]  [ 480/1251]  eta: 0:06:29  lr: 0.000014  loss: 3.5251 (3.4369)  time: 0.4707  data: 0.0004  max mem: 19734
Epoch: [40]  [ 490/1251]  eta: 0:06:24  lr: 0.000014  loss: 3.5054 (3.4341)  time: 0.4535  data: 0.0004  max mem: 19734
Epoch: [40]  [ 500/1251]  eta: 0:06:19  lr: 0.000014  loss: 3.5397 (3.4386)  time: 0.4816  data: 0.0005  max mem: 19734
Epoch: [40]  [ 510/1251]  eta: 0:06:13  lr: 0.000014  loss: 3.6384 (3.4413)  time: 0.4899  data: 0.0005  max mem: 19734
Epoch: [40]  [ 520/1251]  eta: 0:06:08  lr: 0.000014  loss: 3.7617 (3.4438)  time: 0.4739  data: 0.0004  max mem: 19734
Epoch: [40]  [ 530/1251]  eta: 0:06:02  lr: 0.000014  loss: 3.6338 (3.4457)  time: 0.4863  data: 0.0005  max mem: 19734
Epoch: [40]  [ 540/1251]  eta: 0:05:57  lr: 0.000014  loss: 3.5602 (3.4447)  time: 0.4747  data: 0.0005  max mem: 19734
Epoch: [40]  [ 550/1251]  eta: 0:05:51  lr: 0.000014  loss: 3.6545 (3.4462)  time: 0.4573  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4193, ratio_loss=0.0048, pruning_loss=0.1193, mse_loss=0.4197
Epoch: [40]  [ 560/1251]  eta: 0:05:46  lr: 0.000014  loss: 3.2289 (3.4405)  time: 0.4597  data: 0.0006  max mem: 19734
Epoch: [40]  [ 570/1251]  eta: 0:05:40  lr: 0.000014  loss: 3.3136 (3.4397)  time: 0.4577  data: 0.0006  max mem: 19734
Epoch: [40]  [ 580/1251]  eta: 0:05:35  lr: 0.000014  loss: 3.4978 (3.4355)  time: 0.4552  data: 0.0005  max mem: 19734
Epoch: [40]  [ 590/1251]  eta: 0:05:29  lr: 0.000014  loss: 2.8610 (3.4272)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [40]  [ 600/1251]  eta: 0:05:24  lr: 0.000014  loss: 3.0917 (3.4300)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [40]  [ 610/1251]  eta: 0:05:18  lr: 0.000014  loss: 3.5344 (3.4280)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [40]  [ 620/1251]  eta: 0:05:13  lr: 0.000014  loss: 3.1463 (3.4255)  time: 0.4665  data: 0.0005  max mem: 19734
Epoch: [40]  [ 630/1251]  eta: 0:05:08  lr: 0.000014  loss: 3.6656 (3.4286)  time: 0.4656  data: 0.0005  max mem: 19734
Epoch: [40]  [ 640/1251]  eta: 0:05:02  lr: 0.000014  loss: 3.7013 (3.4272)  time: 0.4649  data: 0.0005  max mem: 19734
Epoch: [40]  [ 650/1251]  eta: 0:04:58  lr: 0.000014  loss: 3.6481 (3.4305)  time: 0.4866  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3679, ratio_loss=0.0048, pruning_loss=0.1216, mse_loss=0.4138
Epoch: [40]  [ 660/1251]  eta: 0:04:52  lr: 0.000014  loss: 3.6481 (3.4320)  time: 0.4837  data: 0.0006  max mem: 19734
Epoch: [40]  [ 670/1251]  eta: 0:04:47  lr: 0.000014  loss: 3.5507 (3.4307)  time: 0.4839  data: 0.0007  max mem: 19734
Epoch: [40]  [ 680/1251]  eta: 0:04:42  lr: 0.000014  loss: 3.5808 (3.4336)  time: 0.4888  data: 0.0006  max mem: 19734
Epoch: [40]  [ 690/1251]  eta: 0:04:37  lr: 0.000014  loss: 3.7940 (3.4366)  time: 0.4681  data: 0.0004  max mem: 19734
Epoch: [40]  [ 700/1251]  eta: 0:04:32  lr: 0.000014  loss: 3.6135 (3.4386)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [40]  [ 710/1251]  eta: 0:04:27  lr: 0.000014  loss: 3.4915 (3.4353)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [40]  [ 720/1251]  eta: 0:04:21  lr: 0.000014  loss: 3.2459 (3.4331)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [40]  [ 730/1251]  eta: 0:04:16  lr: 0.000014  loss: 3.5802 (3.4334)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [40]  [ 740/1251]  eta: 0:04:11  lr: 0.000014  loss: 3.3946 (3.4288)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [40]  [ 750/1251]  eta: 0:04:06  lr: 0.000014  loss: 3.0818 (3.4268)  time: 0.4525  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3668, ratio_loss=0.0046, pruning_loss=0.1209, mse_loss=0.4298
Epoch: [40]  [ 760/1251]  eta: 0:04:01  lr: 0.000014  loss: 3.2729 (3.4251)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [40]  [ 770/1251]  eta: 0:03:56  lr: 0.000014  loss: 3.4496 (3.4257)  time: 0.4622  data: 0.0007  max mem: 19734
Epoch: [40]  [ 780/1251]  eta: 0:03:50  lr: 0.000014  loss: 3.4633 (3.4243)  time: 0.4612  data: 0.0007  max mem: 19734
Epoch: [40]  [ 790/1251]  eta: 0:03:46  lr: 0.000014  loss: 3.6761 (3.4272)  time: 0.4822  data: 0.0004  max mem: 19734
Epoch: [40]  [ 800/1251]  eta: 0:03:41  lr: 0.000014  loss: 3.6907 (3.4268)  time: 0.4910  data: 0.0005  max mem: 19734
Epoch: [40]  [ 810/1251]  eta: 0:03:35  lr: 0.000014  loss: 3.6471 (3.4286)  time: 0.4626  data: 0.0004  max mem: 19734
Epoch: [40]  [ 820/1251]  eta: 0:03:31  lr: 0.000014  loss: 3.7073 (3.4307)  time: 0.4820  data: 0.0005  max mem: 19734
Epoch: [40]  [ 830/1251]  eta: 0:03:26  lr: 0.000014  loss: 3.6031 (3.4323)  time: 0.4815  data: 0.0005  max mem: 19734
Epoch: [40]  [ 840/1251]  eta: 0:03:21  lr: 0.000014  loss: 3.5133 (3.4349)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [40]  [ 850/1251]  eta: 0:03:15  lr: 0.000014  loss: 3.6243 (3.4349)  time: 0.4539  data: 0.0007  max mem: 19734
loss info: cls_loss=3.5087, ratio_loss=0.0052, pruning_loss=0.1183, mse_loss=0.4212
Epoch: [40]  [ 860/1251]  eta: 0:03:10  lr: 0.000014  loss: 3.7429 (3.4381)  time: 0.4535  data: 0.0007  max mem: 19734
Epoch: [40]  [ 870/1251]  eta: 0:03:05  lr: 0.000014  loss: 3.6263 (3.4380)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [40]  [ 880/1251]  eta: 0:03:00  lr: 0.000014  loss: 3.6263 (3.4409)  time: 0.4623  data: 0.0006  max mem: 19734
Epoch: [40]  [ 890/1251]  eta: 0:02:55  lr: 0.000014  loss: 3.5793 (3.4402)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [40]  [ 900/1251]  eta: 0:02:50  lr: 0.000014  loss: 3.5214 (3.4405)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [40]  [ 910/1251]  eta: 0:02:45  lr: 0.000014  loss: 3.6148 (3.4412)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [40]  [ 920/1251]  eta: 0:02:41  lr: 0.000014  loss: 3.5438 (3.4421)  time: 0.4692  data: 0.0004  max mem: 19734
Epoch: [40]  [ 930/1251]  eta: 0:02:36  lr: 0.000014  loss: 3.5438 (3.4422)  time: 0.4730  data: 0.0005  max mem: 19734
Epoch: [40]  [ 940/1251]  eta: 0:02:31  lr: 0.000014  loss: 3.5239 (3.4423)  time: 0.4734  data: 0.0005  max mem: 19734
Epoch: [40]  [ 950/1251]  eta: 0:02:26  lr: 0.000014  loss: 3.3923 (3.4382)  time: 0.4774  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3846, ratio_loss=0.0053, pruning_loss=0.1213, mse_loss=0.4260
Epoch: [40]  [ 960/1251]  eta: 0:02:21  lr: 0.000014  loss: 3.0366 (3.4363)  time: 0.4976  data: 0.0005  max mem: 19734
Epoch: [40]  [ 970/1251]  eta: 0:02:16  lr: 0.000014  loss: 3.3697 (3.4352)  time: 0.4853  data: 0.0005  max mem: 19734
Epoch: [40]  [ 980/1251]  eta: 0:02:11  lr: 0.000014  loss: 3.5251 (3.4354)  time: 0.4509  data: 0.0004  max mem: 19734
Epoch: [40]  [ 990/1251]  eta: 0:02:06  lr: 0.000014  loss: 3.5482 (3.4357)  time: 0.4543  data: 0.0005  max mem: 19734
Epoch: [40]  [1000/1251]  eta: 0:02:01  lr: 0.000014  loss: 3.6690 (3.4381)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [40]  [1010/1251]  eta: 0:01:56  lr: 0.000014  loss: 3.5857 (3.4373)  time: 0.4616  data: 0.0005  max mem: 19734
Epoch: [40]  [1020/1251]  eta: 0:01:51  lr: 0.000014  loss: 3.4366 (3.4378)  time: 0.4580  data: 0.0006  max mem: 19734
Epoch: [40]  [1030/1251]  eta: 0:01:47  lr: 0.000014  loss: 3.5811 (3.4365)  time: 0.4533  data: 0.0005  max mem: 19734
Epoch: [40]  [1040/1251]  eta: 0:01:42  lr: 0.000014  loss: 3.3035 (3.4353)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [40]  [1050/1251]  eta: 0:01:37  lr: 0.000014  loss: 3.5595 (3.4361)  time: 0.4544  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4109, ratio_loss=0.0046, pruning_loss=0.1199, mse_loss=0.4190
Epoch: [40]  [1060/1251]  eta: 0:01:32  lr: 0.000014  loss: 3.4906 (3.4357)  time: 0.4635  data: 0.0004  max mem: 19734
Epoch: [40]  [1070/1251]  eta: 0:01:27  lr: 0.000014  loss: 3.4095 (3.4350)  time: 0.4644  data: 0.0004  max mem: 19734
Epoch: [40]  [1080/1251]  eta: 0:01:22  lr: 0.000014  loss: 3.4609 (3.4351)  time: 0.4891  data: 0.0004  max mem: 19734
Epoch: [40]  [1090/1251]  eta: 0:01:17  lr: 0.000014  loss: 3.5551 (3.4321)  time: 0.4980  data: 0.0004  max mem: 19734
Epoch: [40]  [1100/1251]  eta: 0:01:13  lr: 0.000014  loss: 3.3001 (3.4298)  time: 0.4760  data: 0.0007  max mem: 19734
Epoch: [40]  [1110/1251]  eta: 0:01:08  lr: 0.000014  loss: 3.3705 (3.4289)  time: 0.4829  data: 0.0007  max mem: 19734
Epoch: [40]  [1120/1251]  eta: 0:01:03  lr: 0.000014  loss: 3.5363 (3.4295)  time: 0.4709  data: 0.0008  max mem: 19734
Epoch: [40]  [1130/1251]  eta: 0:00:58  lr: 0.000014  loss: 3.6432 (3.4300)  time: 0.4541  data: 0.0008  max mem: 19734
Epoch: [40]  [1140/1251]  eta: 0:00:53  lr: 0.000014  loss: 3.5446 (3.4282)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [40]  [1150/1251]  eta: 0:00:48  lr: 0.000014  loss: 3.3855 (3.4279)  time: 0.4524  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3228, ratio_loss=0.0049, pruning_loss=0.1209, mse_loss=0.4269
Epoch: [40]  [1160/1251]  eta: 0:00:43  lr: 0.000014  loss: 3.4931 (3.4285)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [40]  [1170/1251]  eta: 0:00:39  lr: 0.000014  loss: 3.3618 (3.4255)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [40]  [1180/1251]  eta: 0:00:34  lr: 0.000014  loss: 2.8483 (3.4221)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [40]  [1190/1251]  eta: 0:00:29  lr: 0.000014  loss: 2.8480 (3.4190)  time: 0.4576  data: 0.0007  max mem: 19734
Epoch: [40]  [1200/1251]  eta: 0:00:24  lr: 0.000014  loss: 3.1379 (3.4179)  time: 0.4486  data: 0.0006  max mem: 19734
Epoch: [40]  [1210/1251]  eta: 0:00:19  lr: 0.000014  loss: 2.9994 (3.4146)  time: 0.4553  data: 0.0002  max mem: 19734
Epoch: [40]  [1220/1251]  eta: 0:00:14  lr: 0.000014  loss: 3.4773 (3.4148)  time: 0.4605  data: 0.0002  max mem: 19734
Epoch: [40]  [1230/1251]  eta: 0:00:10  lr: 0.000014  loss: 3.4924 (3.4143)  time: 0.4680  data: 0.0002  max mem: 19734
Epoch: [40]  [1240/1251]  eta: 0:00:05  lr: 0.000014  loss: 3.4924 (3.4143)  time: 0.4686  data: 0.0002  max mem: 19734
Epoch: [40]  [1250/1251]  eta: 0:00:00  lr: 0.000014  loss: 3.4888 (3.4139)  time: 0.4637  data: 0.0002  max mem: 19734
Epoch: [40] Total time: 0:10:02 (0.4816 s / it)
Averaged stats: lr: 0.000014  loss: 3.4888 (3.4084)
Test:  [  0/261]  eta: 2:25:15  loss: 0.6438 (0.6438)  acc1: 84.8958 (84.8958)  acc5: 96.3542 (96.3542)  time: 33.3909  data: 33.1983  max mem: 19734
Test:  [ 10/261]  eta: 0:14:06  loss: 0.6438 (0.7230)  acc1: 84.8958 (83.6648)  acc5: 96.3542 (96.2595)  time: 3.3744  data: 3.0287  max mem: 19734
Test:  [ 20/261]  eta: 0:07:21  loss: 0.9300 (0.8982)  acc1: 77.6042 (79.1171)  acc5: 93.7500 (94.5933)  time: 0.2562  data: 0.0124  max mem: 19734
Test:  [ 30/261]  eta: 0:05:21  loss: 0.8390 (0.8178)  acc1: 82.8125 (81.8212)  acc5: 93.7500 (95.0605)  time: 0.2983  data: 0.0160  max mem: 19734
Test:  [ 40/261]  eta: 0:04:22  loss: 0.5801 (0.7814)  acc1: 86.4583 (82.8760)  acc5: 96.8750 (95.4522)  time: 0.5092  data: 0.1764  max mem: 19734
Test:  [ 50/261]  eta: 0:03:27  loss: 0.9045 (0.8405)  acc1: 78.1250 (81.1479)  acc5: 95.3125 (95.1287)  time: 0.3508  data: 0.1735  max mem: 19734
Test:  [ 60/261]  eta: 0:02:54  loss: 0.9772 (0.8476)  acc1: 77.0833 (80.7719)  acc5: 94.2708 (95.1759)  time: 0.2158  data: 0.0180  max mem: 19734
Test:  [ 70/261]  eta: 0:02:26  loss: 0.9087 (0.8518)  acc1: 77.6042 (80.3844)  acc5: 96.3542 (95.3345)  time: 0.2117  data: 0.0170  max mem: 19734
Test:  [ 80/261]  eta: 0:02:10  loss: 0.7851 (0.8520)  acc1: 79.6875 (80.4848)  acc5: 96.8750 (95.4154)  time: 0.2656  data: 0.1314  max mem: 19734
Test:  [ 90/261]  eta: 0:01:52  loss: 0.7851 (0.8387)  acc1: 83.3333 (80.7807)  acc5: 95.8333 (95.4899)  time: 0.2794  data: 0.1309  max mem: 19734
Test:  [100/261]  eta: 0:01:39  loss: 0.8200 (0.8419)  acc1: 82.2917 (80.7137)  acc5: 94.7917 (95.5291)  time: 0.2199  data: 0.0969  max mem: 19734
Test:  [110/261]  eta: 0:01:26  loss: 0.8731 (0.8659)  acc1: 77.6042 (80.1708)  acc5: 94.7917 (95.2234)  time: 0.1740  data: 0.0924  max mem: 19734
Test:  [120/261]  eta: 0:01:14  loss: 1.1889 (0.9064)  acc1: 71.8750 (79.2872)  acc5: 90.1042 (94.6798)  time: 0.0663  data: 0.0013  max mem: 19734
Test:  [130/261]  eta: 0:01:04  loss: 1.3841 (0.9527)  acc1: 66.6667 (78.3198)  acc5: 86.9792 (94.0561)  time: 0.0648  data: 0.0024  max mem: 19734
Test:  [140/261]  eta: 0:00:56  loss: 1.2556 (0.9789)  acc1: 67.1875 (77.6854)  acc5: 88.5417 (93.7759)  time: 0.0644  data: 0.0030  max mem: 19734
Test:  [150/261]  eta: 0:00:48  loss: 1.1983 (0.9841)  acc1: 72.9167 (77.6628)  acc5: 91.6667 (93.6396)  time: 0.0630  data: 0.0017  max mem: 19734
Test:  [160/261]  eta: 0:00:41  loss: 0.9728 (1.0029)  acc1: 78.1250 (77.3130)  acc5: 91.6667 (93.3715)  time: 0.0618  data: 0.0006  max mem: 19734
Test:  [170/261]  eta: 0:00:35  loss: 1.2532 (1.0324)  acc1: 64.5833 (76.5686)  acc5: 86.9792 (93.0586)  time: 0.0616  data: 0.0004  max mem: 19734
Test:  [180/261]  eta: 0:00:30  loss: 1.4128 (1.0490)  acc1: 64.5833 (76.1884)  acc5: 89.5833 (92.9328)  time: 0.0623  data: 0.0010  max mem: 19734
Test:  [190/261]  eta: 0:00:25  loss: 1.3424 (1.0616)  acc1: 68.2292 (75.9571)  acc5: 91.1458 (92.7547)  time: 0.0625  data: 0.0012  max mem: 19734
Test:  [200/261]  eta: 0:00:20  loss: 1.3162 (1.0765)  acc1: 72.3958 (75.6659)  acc5: 89.0625 (92.5114)  time: 0.0619  data: 0.0006  max mem: 19734
Test:  [210/261]  eta: 0:00:16  loss: 1.3532 (1.0897)  acc1: 69.7917 (75.3974)  acc5: 88.0208 (92.3183)  time: 0.0615  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:13  loss: 1.4173 (1.1087)  acc1: 67.1875 (74.9340)  acc5: 88.0208 (92.1121)  time: 0.0614  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:09  loss: 1.4173 (1.1173)  acc1: 66.1458 (74.6821)  acc5: 89.0625 (92.0297)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:06  loss: 1.2736 (1.1258)  acc1: 67.7083 (74.4597)  acc5: 91.6667 (91.9606)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0081 (1.1188)  acc1: 74.4792 (74.6369)  acc5: 92.7083 (92.0630)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9485 (1.1191)  acc1: 77.6042 (74.6460)  acc5: 94.7917 (92.1160)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:12 (0.2795 s / it)
* Acc@1 74.646 Acc@5 92.116 loss 1.119
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.69%
Epoch: [41]  [   0/1251]  eta: 5:26:39  lr: 0.000013  loss: 3.7384 (3.7384)  time: 15.6670  data: 8.6493  max mem: 19734
loss info: cls_loss=3.2605, ratio_loss=0.0048, pruning_loss=0.1244, mse_loss=0.4232
Epoch: [41]  [  10/1251]  eta: 0:43:49  lr: 0.000013  loss: 3.7384 (3.5731)  time: 2.1187  data: 0.8465  max mem: 19734
Epoch: [41]  [  20/1251]  eta: 0:27:16  lr: 0.000013  loss: 3.5415 (3.4704)  time: 0.6125  data: 0.0334  max mem: 19734
Epoch: [41]  [  30/1251]  eta: 0:21:21  lr: 0.000013  loss: 3.0972 (3.2759)  time: 0.4611  data: 0.0006  max mem: 19734
Epoch: [41]  [  40/1251]  eta: 0:18:15  lr: 0.000013  loss: 3.0902 (3.3417)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [41]  [  50/1251]  eta: 0:16:21  lr: 0.000013  loss: 3.7634 (3.4002)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [41]  [  60/1251]  eta: 0:15:04  lr: 0.000013  loss: 3.6229 (3.3992)  time: 0.4610  data: 0.0005  max mem: 19734
Epoch: [41]  [  70/1251]  eta: 0:14:10  lr: 0.000013  loss: 3.6229 (3.4313)  time: 0.4734  data: 0.0004  max mem: 19734
Epoch: [41]  [  80/1251]  eta: 0:13:26  lr: 0.000013  loss: 3.6166 (3.4303)  time: 0.4724  data: 0.0004  max mem: 19734
Epoch: [41]  [  90/1251]  eta: 0:12:50  lr: 0.000013  loss: 3.5109 (3.4115)  time: 0.4611  data: 0.0005  max mem: 19734
Epoch: [41]  [ 100/1251]  eta: 0:12:20  lr: 0.000013  loss: 3.5031 (3.4204)  time: 0.4591  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3567, ratio_loss=0.0051, pruning_loss=0.1220, mse_loss=0.4156
Epoch: [41]  [ 110/1251]  eta: 0:12:01  lr: 0.000013  loss: 3.5031 (3.4209)  time: 0.4931  data: 0.0005  max mem: 19734
Epoch: [41]  [ 120/1251]  eta: 0:11:41  lr: 0.000013  loss: 3.4723 (3.4175)  time: 0.5053  data: 0.0005  max mem: 19734
Epoch: [41]  [ 130/1251]  eta: 0:11:22  lr: 0.000013  loss: 3.6339 (3.4452)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [41]  [ 140/1251]  eta: 0:11:08  lr: 0.000013  loss: 3.7234 (3.4546)  time: 0.4901  data: 0.0005  max mem: 19734
Epoch: [41]  [ 150/1251]  eta: 0:10:52  lr: 0.000013  loss: 3.5927 (3.4605)  time: 0.4843  data: 0.0005  max mem: 19734
Epoch: [41]  [ 160/1251]  eta: 0:10:37  lr: 0.000013  loss: 3.5239 (3.4591)  time: 0.4602  data: 0.0004  max mem: 19734
Epoch: [41]  [ 170/1251]  eta: 0:10:23  lr: 0.000013  loss: 3.3661 (3.4518)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [41]  [ 180/1251]  eta: 0:10:10  lr: 0.000013  loss: 3.3638 (3.4510)  time: 0.4564  data: 0.0005  max mem: 19734
Epoch: [41]  [ 190/1251]  eta: 0:09:58  lr: 0.000013  loss: 3.1859 (3.4253)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [41]  [ 200/1251]  eta: 0:09:47  lr: 0.000013  loss: 3.3352 (3.4202)  time: 0.4569  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4151, ratio_loss=0.0048, pruning_loss=0.1210, mse_loss=0.4360
Epoch: [41]  [ 210/1251]  eta: 0:09:36  lr: 0.000013  loss: 3.5057 (3.4269)  time: 0.4569  data: 0.0005  max mem: 19734
Epoch: [41]  [ 220/1251]  eta: 0:09:27  lr: 0.000013  loss: 3.6198 (3.4312)  time: 0.4675  data: 0.0004  max mem: 19734
Epoch: [41]  [ 230/1251]  eta: 0:09:17  lr: 0.000013  loss: 3.5665 (3.4327)  time: 0.4661  data: 0.0005  max mem: 19734
Epoch: [41]  [ 240/1251]  eta: 0:09:08  lr: 0.000013  loss: 3.5997 (3.4404)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [41]  [ 250/1251]  eta: 0:09:00  lr: 0.000013  loss: 3.6492 (3.4490)  time: 0.4657  data: 0.0005  max mem: 19734
Epoch: [41]  [ 260/1251]  eta: 0:08:53  lr: 0.000013  loss: 3.3240 (3.4386)  time: 0.4848  data: 0.0005  max mem: 19734
Epoch: [41]  [ 270/1251]  eta: 0:08:45  lr: 0.000013  loss: 3.5761 (3.4448)  time: 0.4869  data: 0.0005  max mem: 19734
Epoch: [41]  [ 280/1251]  eta: 0:08:39  lr: 0.000013  loss: 3.5575 (3.4326)  time: 0.4967  data: 0.0004  max mem: 19734
Epoch: [41]  [ 290/1251]  eta: 0:08:31  lr: 0.000013  loss: 3.3441 (3.4341)  time: 0.4877  data: 0.0004  max mem: 19734
Epoch: [41]  [ 300/1251]  eta: 0:08:24  lr: 0.000013  loss: 3.5463 (3.4339)  time: 0.4592  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4346, ratio_loss=0.0050, pruning_loss=0.1198, mse_loss=0.4223
Epoch: [41]  [ 310/1251]  eta: 0:08:16  lr: 0.000013  loss: 3.5553 (3.4343)  time: 0.4590  data: 0.0005  max mem: 19734
Epoch: [41]  [ 320/1251]  eta: 0:08:09  lr: 0.000013  loss: 3.5760 (3.4416)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [41]  [ 330/1251]  eta: 0:08:02  lr: 0.000013  loss: 3.6191 (3.4428)  time: 0.4610  data: 0.0004  max mem: 19734
Epoch: [41]  [ 340/1251]  eta: 0:07:55  lr: 0.000013  loss: 3.4260 (3.4369)  time: 0.4608  data: 0.0006  max mem: 19734
Epoch: [41]  [ 350/1251]  eta: 0:07:48  lr: 0.000013  loss: 3.5106 (3.4472)  time: 0.4609  data: 0.0005  max mem: 19734
Epoch: [41]  [ 360/1251]  eta: 0:07:42  lr: 0.000013  loss: 3.6321 (3.4425)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [41]  [ 370/1251]  eta: 0:07:35  lr: 0.000013  loss: 3.2868 (3.4353)  time: 0.4676  data: 0.0004  max mem: 19734
Epoch: [41]  [ 380/1251]  eta: 0:07:29  lr: 0.000013  loss: 3.5095 (3.4401)  time: 0.4674  data: 0.0008  max mem: 19734
Epoch: [41]  [ 390/1251]  eta: 0:07:22  lr: 0.000013  loss: 3.5486 (3.4415)  time: 0.4580  data: 0.0007  max mem: 19734
Epoch: [41]  [ 400/1251]  eta: 0:07:18  lr: 0.000013  loss: 3.4914 (3.4382)  time: 0.4940  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3903, ratio_loss=0.0046, pruning_loss=0.1209, mse_loss=0.4255
Epoch: [41]  [ 410/1251]  eta: 0:07:11  lr: 0.000013  loss: 3.3305 (3.4324)  time: 0.4942  data: 0.0004  max mem: 19734
Epoch: [41]  [ 420/1251]  eta: 0:07:06  lr: 0.000013  loss: 3.2726 (3.4306)  time: 0.4754  data: 0.0004  max mem: 19734
Epoch: [41]  [ 430/1251]  eta: 0:07:00  lr: 0.000013  loss: 3.2472 (3.4252)  time: 0.4995  data: 0.0004  max mem: 19734
Epoch: [41]  [ 440/1251]  eta: 0:06:54  lr: 0.000013  loss: 3.1528 (3.4212)  time: 0.4823  data: 0.0005  max mem: 19734
Epoch: [41]  [ 450/1251]  eta: 0:06:48  lr: 0.000013  loss: 3.4653 (3.4247)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [41]  [ 460/1251]  eta: 0:06:42  lr: 0.000013  loss: 3.6701 (3.4268)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [41]  [ 470/1251]  eta: 0:06:36  lr: 0.000013  loss: 3.4926 (3.4226)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [41]  [ 480/1251]  eta: 0:06:30  lr: 0.000013  loss: 3.3215 (3.4179)  time: 0.4577  data: 0.0006  max mem: 19734
Epoch: [41]  [ 490/1251]  eta: 0:06:25  lr: 0.000013  loss: 3.3215 (3.4186)  time: 0.4565  data: 0.0006  max mem: 19734
Epoch: [41]  [ 500/1251]  eta: 0:06:19  lr: 0.000013  loss: 3.5261 (3.4185)  time: 0.4543  data: 0.0009  max mem: 19734
loss info: cls_loss=3.3460, ratio_loss=0.0045, pruning_loss=0.1235, mse_loss=0.4251
Epoch: [41]  [ 510/1251]  eta: 0:06:13  lr: 0.000013  loss: 3.3679 (3.4180)  time: 0.4547  data: 0.0009  max mem: 19734
Epoch: [41]  [ 520/1251]  eta: 0:06:08  lr: 0.000013  loss: 3.3733 (3.4186)  time: 0.4684  data: 0.0005  max mem: 19734
Epoch: [41]  [ 530/1251]  eta: 0:06:02  lr: 0.000013  loss: 3.2834 (3.4165)  time: 0.4672  data: 0.0006  max mem: 19734
Epoch: [41]  [ 540/1251]  eta: 0:05:57  lr: 0.000013  loss: 3.1798 (3.4149)  time: 0.4679  data: 0.0010  max mem: 19734
Epoch: [41]  [ 550/1251]  eta: 0:05:51  lr: 0.000013  loss: 3.4889 (3.4155)  time: 0.4794  data: 0.0008  max mem: 19734
Epoch: [41]  [ 560/1251]  eta: 0:05:46  lr: 0.000013  loss: 3.4889 (3.4151)  time: 0.4746  data: 0.0004  max mem: 19734
Epoch: [41]  [ 570/1251]  eta: 0:05:41  lr: 0.000013  loss: 3.4581 (3.4140)  time: 0.4835  data: 0.0005  max mem: 19734
Epoch: [41]  [ 580/1251]  eta: 0:05:36  lr: 0.000013  loss: 3.4581 (3.4131)  time: 0.4864  data: 0.0005  max mem: 19734
Epoch: [41]  [ 590/1251]  eta: 0:05:30  lr: 0.000013  loss: 3.4484 (3.4115)  time: 0.4691  data: 0.0004  max mem: 19734
Epoch: [41]  [ 600/1251]  eta: 0:05:25  lr: 0.000013  loss: 3.5760 (3.4153)  time: 0.4589  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3952, ratio_loss=0.0047, pruning_loss=0.1225, mse_loss=0.4256
Epoch: [41]  [ 610/1251]  eta: 0:05:19  lr: 0.000013  loss: 3.7200 (3.4172)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [41]  [ 620/1251]  eta: 0:05:14  lr: 0.000013  loss: 3.6171 (3.4180)  time: 0.4586  data: 0.0004  max mem: 19734
Epoch: [41]  [ 630/1251]  eta: 0:05:08  lr: 0.000013  loss: 3.6403 (3.4197)  time: 0.4583  data: 0.0007  max mem: 19734
Epoch: [41]  [ 640/1251]  eta: 0:05:03  lr: 0.000013  loss: 3.7297 (3.4232)  time: 0.4542  data: 0.0007  max mem: 19734
Epoch: [41]  [ 650/1251]  eta: 0:04:58  lr: 0.000013  loss: 3.6884 (3.4252)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [41]  [ 660/1251]  eta: 0:04:52  lr: 0.000013  loss: 3.5823 (3.4255)  time: 0.4625  data: 0.0005  max mem: 19734
Epoch: [41]  [ 670/1251]  eta: 0:04:47  lr: 0.000013  loss: 3.5273 (3.4274)  time: 0.4627  data: 0.0006  max mem: 19734
Epoch: [41]  [ 680/1251]  eta: 0:04:42  lr: 0.000013  loss: 3.3794 (3.4254)  time: 0.4556  data: 0.0006  max mem: 19734
Epoch: [41]  [ 690/1251]  eta: 0:04:37  lr: 0.000013  loss: 3.4486 (3.4262)  time: 0.4777  data: 0.0005  max mem: 19734
Epoch: [41]  [ 700/1251]  eta: 0:04:32  lr: 0.000013  loss: 3.4446 (3.4235)  time: 0.4764  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4269, ratio_loss=0.0046, pruning_loss=0.1209, mse_loss=0.4274
Epoch: [41]  [ 710/1251]  eta: 0:04:27  lr: 0.000013  loss: 3.1944 (3.4228)  time: 0.4727  data: 0.0006  max mem: 19734
Epoch: [41]  [ 720/1251]  eta: 0:04:22  lr: 0.000013  loss: 3.4859 (3.4242)  time: 0.4934  data: 0.0006  max mem: 19734
Epoch: [41]  [ 730/1251]  eta: 0:04:17  lr: 0.000013  loss: 3.6140 (3.4241)  time: 0.4759  data: 0.0005  max mem: 19734
Epoch: [41]  [ 740/1251]  eta: 0:04:11  lr: 0.000013  loss: 3.5533 (3.4226)  time: 0.4568  data: 0.0004  max mem: 19734
Epoch: [41]  [ 750/1251]  eta: 0:04:06  lr: 0.000013  loss: 3.6596 (3.4252)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [41]  [ 760/1251]  eta: 0:04:01  lr: 0.000013  loss: 3.3135 (3.4218)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [41]  [ 770/1251]  eta: 0:03:56  lr: 0.000013  loss: 3.1824 (3.4214)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [41]  [ 780/1251]  eta: 0:03:51  lr: 0.000013  loss: 3.4294 (3.4198)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [41]  [ 790/1251]  eta: 0:03:46  lr: 0.000013  loss: 3.4517 (3.4215)  time: 0.4535  data: 0.0004  max mem: 19734
Epoch: [41]  [ 800/1251]  eta: 0:03:41  lr: 0.000013  loss: 3.3908 (3.4185)  time: 0.4531  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3848, ratio_loss=0.0048, pruning_loss=0.1218, mse_loss=0.4111
Epoch: [41]  [ 810/1251]  eta: 0:03:36  lr: 0.000013  loss: 3.5139 (3.4206)  time: 0.4646  data: 0.0004  max mem: 19734
Epoch: [41]  [ 820/1251]  eta: 0:03:30  lr: 0.000013  loss: 3.5472 (3.4203)  time: 0.4654  data: 0.0004  max mem: 19734
Epoch: [41]  [ 830/1251]  eta: 0:03:26  lr: 0.000013  loss: 3.4827 (3.4224)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [41]  [ 840/1251]  eta: 0:03:21  lr: 0.000013  loss: 3.5573 (3.4250)  time: 0.4766  data: 0.0004  max mem: 19734
Epoch: [41]  [ 850/1251]  eta: 0:03:16  lr: 0.000013  loss: 3.7002 (3.4289)  time: 0.4667  data: 0.0007  max mem: 19734
Epoch: [41]  [ 860/1251]  eta: 0:03:11  lr: 0.000013  loss: 3.6516 (3.4300)  time: 0.4846  data: 0.0007  max mem: 19734
Epoch: [41]  [ 870/1251]  eta: 0:03:06  lr: 0.000013  loss: 3.5286 (3.4319)  time: 0.4815  data: 0.0004  max mem: 19734
Epoch: [41]  [ 880/1251]  eta: 0:03:01  lr: 0.000013  loss: 3.5286 (3.4328)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [41]  [ 890/1251]  eta: 0:02:56  lr: 0.000013  loss: 3.3412 (3.4323)  time: 0.4547  data: 0.0007  max mem: 19734
Epoch: [41]  [ 900/1251]  eta: 0:02:51  lr: 0.000013  loss: 3.2621 (3.4322)  time: 0.4548  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4798, ratio_loss=0.0049, pruning_loss=0.1186, mse_loss=0.4351
Epoch: [41]  [ 910/1251]  eta: 0:02:46  lr: 0.000013  loss: 3.2593 (3.4298)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [41]  [ 920/1251]  eta: 0:02:41  lr: 0.000013  loss: 3.4615 (3.4288)  time: 0.4534  data: 0.0006  max mem: 19734
Epoch: [41]  [ 930/1251]  eta: 0:02:36  lr: 0.000013  loss: 3.6897 (3.4312)  time: 0.4529  data: 0.0006  max mem: 19734
Epoch: [41]  [ 940/1251]  eta: 0:02:31  lr: 0.000013  loss: 3.7360 (3.4339)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [41]  [ 950/1251]  eta: 0:02:26  lr: 0.000013  loss: 3.6244 (3.4334)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [41]  [ 960/1251]  eta: 0:02:21  lr: 0.000013  loss: 3.7116 (3.4358)  time: 0.4675  data: 0.0005  max mem: 19734
Epoch: [41]  [ 970/1251]  eta: 0:02:16  lr: 0.000013  loss: 3.7050 (3.4348)  time: 0.4692  data: 0.0005  max mem: 19734
Epoch: [41]  [ 980/1251]  eta: 0:02:11  lr: 0.000013  loss: 3.4548 (3.4332)  time: 0.4808  data: 0.0004  max mem: 19734
Epoch: [41]  [ 990/1251]  eta: 0:02:06  lr: 0.000013  loss: 3.4654 (3.4324)  time: 0.4860  data: 0.0004  max mem: 19734
Epoch: [41]  [1000/1251]  eta: 0:02:01  lr: 0.000013  loss: 3.4654 (3.4309)  time: 0.4751  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4033, ratio_loss=0.0047, pruning_loss=0.1210, mse_loss=0.4154
Epoch: [41]  [1010/1251]  eta: 0:01:57  lr: 0.000013  loss: 3.4711 (3.4310)  time: 0.4861  data: 0.0004  max mem: 19734
Epoch: [41]  [1020/1251]  eta: 0:01:52  lr: 0.000013  loss: 3.6702 (3.4340)  time: 0.4727  data: 0.0005  max mem: 19734
Epoch: [41]  [1030/1251]  eta: 0:01:47  lr: 0.000013  loss: 3.6380 (3.4339)  time: 0.4548  data: 0.0010  max mem: 19734
Epoch: [41]  [1040/1251]  eta: 0:01:42  lr: 0.000013  loss: 3.5012 (3.4344)  time: 0.4560  data: 0.0010  max mem: 19734
Epoch: [41]  [1050/1251]  eta: 0:01:37  lr: 0.000013  loss: 3.5012 (3.4326)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [41]  [1060/1251]  eta: 0:01:32  lr: 0.000013  loss: 2.9413 (3.4292)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [41]  [1070/1251]  eta: 0:01:27  lr: 0.000013  loss: 3.5335 (3.4293)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [41]  [1080/1251]  eta: 0:01:22  lr: 0.000013  loss: 3.5816 (3.4316)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [41]  [1090/1251]  eta: 0:01:17  lr: 0.000013  loss: 3.5883 (3.4322)  time: 0.4552  data: 0.0005  max mem: 19734
Epoch: [41]  [1100/1251]  eta: 0:01:12  lr: 0.000013  loss: 3.6226 (3.4344)  time: 0.4566  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4501, ratio_loss=0.0049, pruning_loss=0.1207, mse_loss=0.4337
Epoch: [41]  [1110/1251]  eta: 0:01:08  lr: 0.000013  loss: 3.6226 (3.4332)  time: 0.4631  data: 0.0005  max mem: 19734
Epoch: [41]  [1120/1251]  eta: 0:01:03  lr: 0.000013  loss: 3.5325 (3.4358)  time: 0.4706  data: 0.0005  max mem: 19734
Epoch: [41]  [1130/1251]  eta: 0:00:58  lr: 0.000013  loss: 3.6268 (3.4361)  time: 0.4714  data: 0.0004  max mem: 19734
Epoch: [41]  [1140/1251]  eta: 0:00:53  lr: 0.000013  loss: 3.5385 (3.4352)  time: 0.4711  data: 0.0004  max mem: 19734
Epoch: [41]  [1150/1251]  eta: 0:00:48  lr: 0.000013  loss: 3.6318 (3.4371)  time: 0.4857  data: 0.0005  max mem: 19734
Epoch: [41]  [1160/1251]  eta: 0:00:43  lr: 0.000013  loss: 3.6551 (3.4373)  time: 0.4759  data: 0.0005  max mem: 19734
Epoch: [41]  [1170/1251]  eta: 0:00:39  lr: 0.000013  loss: 3.6341 (3.4377)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [41]  [1180/1251]  eta: 0:00:34  lr: 0.000013  loss: 3.6341 (3.4394)  time: 0.4537  data: 0.0007  max mem: 19734
Epoch: [41]  [1190/1251]  eta: 0:00:29  lr: 0.000013  loss: 3.5934 (3.4371)  time: 0.4550  data: 0.0010  max mem: 19734
Epoch: [41]  [1200/1251]  eta: 0:00:24  lr: 0.000013  loss: 3.1473 (3.4353)  time: 0.4500  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4303, ratio_loss=0.0048, pruning_loss=0.1193, mse_loss=0.4264
Epoch: [41]  [1210/1251]  eta: 0:00:19  lr: 0.000013  loss: 3.3259 (3.4356)  time: 0.4444  data: 0.0001  max mem: 19734
Epoch: [41]  [1220/1251]  eta: 0:00:14  lr: 0.000013  loss: 3.4589 (3.4351)  time: 0.4450  data: 0.0001  max mem: 19734
Epoch: [41]  [1230/1251]  eta: 0:00:10  lr: 0.000013  loss: 3.5304 (3.4349)  time: 0.4465  data: 0.0001  max mem: 19734
Epoch: [41]  [1240/1251]  eta: 0:00:05  lr: 0.000013  loss: 3.5267 (3.4355)  time: 0.4460  data: 0.0001  max mem: 19734
Epoch: [41]  [1250/1251]  eta: 0:00:00  lr: 0.000013  loss: 3.5048 (3.4358)  time: 0.4461  data: 0.0001  max mem: 19734
Epoch: [41] Total time: 0:10:01 (0.4809 s / it)
Averaged stats: lr: 0.000013  loss: 3.5048 (3.4223)
Test:  [  0/261]  eta: 1:11:43  loss: 0.7178 (0.7178)  acc1: 81.2500 (81.2500)  acc5: 95.3125 (95.3125)  time: 16.4874  data: 16.1466  max mem: 19734
Test:  [ 10/261]  eta: 0:07:12  loss: 0.6937 (0.7187)  acc1: 83.8542 (83.5227)  acc5: 96.8750 (96.4015)  time: 1.7225  data: 1.4825  max mem: 19734
Test:  [ 20/261]  eta: 0:04:01  loss: 0.9144 (0.8964)  acc1: 79.1667 (78.6954)  acc5: 93.7500 (94.7173)  time: 0.2276  data: 0.0121  max mem: 19734
Test:  [ 30/261]  eta: 0:02:50  loss: 0.8193 (0.8149)  acc1: 82.8125 (81.7204)  acc5: 93.2292 (95.1277)  time: 0.1999  data: 0.0064  max mem: 19734
Test:  [ 40/261]  eta: 0:03:13  loss: 0.5416 (0.7818)  acc1: 88.5417 (82.6728)  acc5: 96.8750 (95.4268)  time: 0.7385  data: 0.5162  max mem: 19734
Test:  [ 50/261]  eta: 0:02:44  loss: 0.8861 (0.8406)  acc1: 78.1250 (80.9538)  acc5: 95.3125 (95.0776)  time: 0.8431  data: 0.6021  max mem: 19734
Test:  [ 60/261]  eta: 0:02:21  loss: 0.9696 (0.8496)  acc1: 76.5625 (80.6694)  acc5: 94.2708 (95.0990)  time: 0.3614  data: 0.0972  max mem: 19734
Test:  [ 70/261]  eta: 0:02:06  loss: 0.9093 (0.8528)  acc1: 78.6458 (80.2450)  acc5: 96.3542 (95.3052)  time: 0.3615  data: 0.1424  max mem: 19734
Test:  [ 80/261]  eta: 0:01:48  loss: 0.8217 (0.8550)  acc1: 80.2083 (80.3305)  acc5: 96.3542 (95.4025)  time: 0.2836  data: 0.1410  max mem: 19734
Test:  [ 90/261]  eta: 0:01:33  loss: 0.8217 (0.8409)  acc1: 83.3333 (80.7063)  acc5: 95.8333 (95.4899)  time: 0.1371  data: 0.0121  max mem: 19734
Test:  [100/261]  eta: 0:01:21  loss: 0.8259 (0.8438)  acc1: 83.3333 (80.6621)  acc5: 95.3125 (95.5910)  time: 0.1128  data: 0.0069  max mem: 19734
Test:  [110/261]  eta: 0:01:15  loss: 0.8672 (0.8686)  acc1: 76.5625 (80.1755)  acc5: 95.3125 (95.2280)  time: 0.2768  data: 0.1472  max mem: 19734
Test:  [120/261]  eta: 0:01:06  loss: 1.1851 (0.9088)  acc1: 71.3542 (79.2743)  acc5: 89.0625 (94.6841)  time: 0.2963  data: 0.1529  max mem: 19734
Test:  [130/261]  eta: 0:00:58  loss: 1.3128 (0.9536)  acc1: 69.2708 (78.3397)  acc5: 86.4583 (94.0760)  time: 0.1770  data: 0.0254  max mem: 19734
Test:  [140/261]  eta: 0:00:58  loss: 1.3569 (0.9802)  acc1: 68.2292 (77.6817)  acc5: 89.5833 (93.7796)  time: 0.5449  data: 0.3986  max mem: 19734
Test:  [150/261]  eta: 0:00:51  loss: 1.2456 (0.9847)  acc1: 71.8750 (77.6249)  acc5: 91.1458 (93.6534)  time: 0.5332  data: 0.3906  max mem: 19734
Test:  [160/261]  eta: 0:00:44  loss: 0.9811 (1.0028)  acc1: 78.1250 (77.3518)  acc5: 92.1875 (93.3553)  time: 0.1755  data: 0.0188  max mem: 19734
Test:  [170/261]  eta: 0:00:40  loss: 1.2980 (1.0323)  acc1: 65.1042 (76.6051)  acc5: 87.5000 (93.0281)  time: 0.2860  data: 0.1436  max mem: 19734
Test:  [180/261]  eta: 0:00:34  loss: 1.4328 (1.0488)  acc1: 66.1458 (76.2402)  acc5: 89.0625 (92.8983)  time: 0.2389  data: 0.1373  max mem: 19734
Test:  [190/261]  eta: 0:00:28  loss: 1.3980 (1.0625)  acc1: 68.7500 (75.9680)  acc5: 91.1458 (92.7083)  time: 0.1031  data: 0.0269  max mem: 19734
Test:  [200/261]  eta: 0:00:23  loss: 1.3681 (1.0778)  acc1: 72.3958 (75.6582)  acc5: 88.5417 (92.4777)  time: 0.1082  data: 0.0369  max mem: 19734
Test:  [210/261]  eta: 0:00:19  loss: 1.3681 (1.0916)  acc1: 71.3542 (75.3875)  acc5: 88.5417 (92.2788)  time: 0.1023  data: 0.0345  max mem: 19734
Test:  [220/261]  eta: 0:00:14  loss: 1.4045 (1.1104)  acc1: 67.7083 (74.9222)  acc5: 88.0208 (92.0720)  time: 0.1212  data: 0.0527  max mem: 19734
Test:  [230/261]  eta: 0:00:10  loss: 1.4001 (1.1199)  acc1: 66.6667 (74.6528)  acc5: 88.5417 (92.0004)  time: 0.0970  data: 0.0312  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3085 (1.1286)  acc1: 66.6667 (74.4230)  acc5: 90.1042 (91.9368)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0698 (1.1219)  acc1: 75.0000 (74.5850)  acc5: 94.2708 (92.0506)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9121 (1.1221)  acc1: 76.5625 (74.6000)  acc5: 94.7917 (92.1140)  time: 0.0597  data: 0.0001  max mem: 19734
Test: Total time: 0:01:23 (0.3195 s / it)
* Acc@1 74.600 Acc@5 92.114 loss 1.122
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.69%
Epoch: [42]  [   0/1251]  eta: 4:27:36  lr: 0.000013  loss: 3.6307 (3.6307)  time: 12.8346  data: 11.2054  max mem: 19734
Epoch: [42]  [  10/1251]  eta: 0:44:25  lr: 0.000013  loss: 3.6979 (3.5987)  time: 2.1482  data: 1.3558  max mem: 19734
Epoch: [42]  [  20/1251]  eta: 0:27:38  lr: 0.000013  loss: 3.7233 (3.6088)  time: 0.7732  data: 0.1856  max mem: 19734
Epoch: [42]  [  30/1251]  eta: 0:21:44  lr: 0.000013  loss: 3.6924 (3.5729)  time: 0.4751  data: 0.0004  max mem: 19734
Epoch: [42]  [  40/1251]  eta: 0:18:45  lr: 0.000013  loss: 3.6019 (3.6044)  time: 0.4899  data: 0.0004  max mem: 19734
Epoch: [42]  [  50/1251]  eta: 0:16:51  lr: 0.000013  loss: 3.5446 (3.5921)  time: 0.4917  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4674, ratio_loss=0.0051, pruning_loss=0.1193, mse_loss=0.4383
Epoch: [42]  [  60/1251]  eta: 0:15:28  lr: 0.000013  loss: 3.4009 (3.5478)  time: 0.4737  data: 0.0005  max mem: 19734
Epoch: [42]  [  70/1251]  eta: 0:14:28  lr: 0.000013  loss: 3.4845 (3.5432)  time: 0.4612  data: 0.0005  max mem: 19734
Epoch: [42]  [  80/1251]  eta: 0:13:41  lr: 0.000013  loss: 3.5636 (3.5268)  time: 0.4618  data: 0.0005  max mem: 19734
Epoch: [42]  [  90/1251]  eta: 0:13:03  lr: 0.000013  loss: 3.3054 (3.4831)  time: 0.4608  data: 0.0004  max mem: 19734
Epoch: [42]  [ 100/1251]  eta: 0:12:32  lr: 0.000013  loss: 3.3138 (3.4867)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [42]  [ 110/1251]  eta: 0:12:05  lr: 0.000013  loss: 3.4317 (3.4576)  time: 0.4590  data: 0.0004  max mem: 19734
Epoch: [42]  [ 120/1251]  eta: 0:11:44  lr: 0.000013  loss: 2.9389 (3.4167)  time: 0.4680  data: 0.0005  max mem: 19734
Epoch: [42]  [ 130/1251]  eta: 0:11:24  lr: 0.000013  loss: 2.9389 (3.4027)  time: 0.4707  data: 0.0007  max mem: 19734
Epoch: [42]  [ 140/1251]  eta: 0:11:06  lr: 0.000013  loss: 3.2944 (3.3971)  time: 0.4610  data: 0.0006  max mem: 19734
Epoch: [42]  [ 150/1251]  eta: 0:10:51  lr: 0.000013  loss: 3.5323 (3.3965)  time: 0.4702  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2911, ratio_loss=0.0052, pruning_loss=0.1237, mse_loss=0.4258
Epoch: [42]  [ 160/1251]  eta: 0:10:38  lr: 0.000013  loss: 3.5532 (3.4020)  time: 0.4782  data: 0.0004  max mem: 19734
Epoch: [42]  [ 170/1251]  eta: 0:10:25  lr: 0.000013  loss: 3.4986 (3.3993)  time: 0.4738  data: 0.0004  max mem: 19734
Epoch: [42]  [ 180/1251]  eta: 0:10:14  lr: 0.000013  loss: 3.5965 (3.4151)  time: 0.4874  data: 0.0004  max mem: 19734
Epoch: [42]  [ 190/1251]  eta: 0:10:04  lr: 0.000013  loss: 3.5269 (3.4133)  time: 0.4932  data: 0.0004  max mem: 19734
Epoch: [42]  [ 200/1251]  eta: 0:09:52  lr: 0.000013  loss: 3.4748 (3.4031)  time: 0.4719  data: 0.0004  max mem: 19734
Epoch: [42]  [ 210/1251]  eta: 0:09:42  lr: 0.000013  loss: 3.5901 (3.4099)  time: 0.4616  data: 0.0004  max mem: 19734
Epoch: [42]  [ 220/1251]  eta: 0:09:31  lr: 0.000013  loss: 3.5961 (3.4185)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [42]  [ 230/1251]  eta: 0:09:21  lr: 0.000013  loss: 3.3843 (3.4102)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [42]  [ 240/1251]  eta: 0:09:12  lr: 0.000013  loss: 3.2232 (3.3941)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [42]  [ 250/1251]  eta: 0:09:03  lr: 0.000013  loss: 3.1965 (3.3939)  time: 0.4589  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3792, ratio_loss=0.0049, pruning_loss=0.1204, mse_loss=0.4284
Epoch: [42]  [ 260/1251]  eta: 0:08:55  lr: 0.000013  loss: 3.3099 (3.3931)  time: 0.4674  data: 0.0004  max mem: 19734
Epoch: [42]  [ 270/1251]  eta: 0:08:47  lr: 0.000013  loss: 3.3845 (3.3978)  time: 0.4638  data: 0.0004  max mem: 19734
Epoch: [42]  [ 280/1251]  eta: 0:08:38  lr: 0.000013  loss: 3.2618 (3.3895)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [42]  [ 290/1251]  eta: 0:08:31  lr: 0.000013  loss: 3.1975 (3.3896)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [42]  [ 300/1251]  eta: 0:08:24  lr: 0.000013  loss: 3.3330 (3.3858)  time: 0.4788  data: 0.0005  max mem: 19734
Epoch: [42]  [ 310/1251]  eta: 0:08:17  lr: 0.000013  loss: 3.4303 (3.3850)  time: 0.4762  data: 0.0005  max mem: 19734
Epoch: [42]  [ 320/1251]  eta: 0:08:10  lr: 0.000013  loss: 3.5210 (3.3892)  time: 0.4676  data: 0.0005  max mem: 19734
Epoch: [42]  [ 330/1251]  eta: 0:08:04  lr: 0.000013  loss: 3.5210 (3.3897)  time: 0.5004  data: 0.0005  max mem: 19734
Epoch: [42]  [ 340/1251]  eta: 0:07:57  lr: 0.000013  loss: 3.6411 (3.3957)  time: 0.4857  data: 0.0004  max mem: 19734
Epoch: [42]  [ 350/1251]  eta: 0:07:50  lr: 0.000013  loss: 3.5063 (3.3957)  time: 0.4524  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3370, ratio_loss=0.0048, pruning_loss=0.1216, mse_loss=0.4291
Epoch: [42]  [ 360/1251]  eta: 0:07:43  lr: 0.000013  loss: 3.1921 (3.3875)  time: 0.4542  data: 0.0005  max mem: 19734
Epoch: [42]  [ 370/1251]  eta: 0:07:36  lr: 0.000013  loss: 3.0372 (3.3790)  time: 0.4552  data: 0.0005  max mem: 19734
Epoch: [42]  [ 380/1251]  eta: 0:07:30  lr: 0.000013  loss: 3.4245 (3.3845)  time: 0.4553  data: 0.0004  max mem: 19734
Epoch: [42]  [ 390/1251]  eta: 0:07:23  lr: 0.000013  loss: 3.4931 (3.3881)  time: 0.4546  data: 0.0003  max mem: 19734
Epoch: [42]  [ 400/1251]  eta: 0:07:17  lr: 0.000013  loss: 3.6486 (3.3942)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [42]  [ 410/1251]  eta: 0:07:11  lr: 0.000013  loss: 3.6486 (3.3921)  time: 0.4665  data: 0.0005  max mem: 19734
Epoch: [42]  [ 420/1251]  eta: 0:07:05  lr: 0.000013  loss: 3.3631 (3.3872)  time: 0.4644  data: 0.0005  max mem: 19734
Epoch: [42]  [ 430/1251]  eta: 0:06:58  lr: 0.000013  loss: 3.2391 (3.3850)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [42]  [ 440/1251]  eta: 0:06:52  lr: 0.000013  loss: 3.5346 (3.3902)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [42]  [ 450/1251]  eta: 0:06:47  lr: 0.000013  loss: 3.6648 (3.3917)  time: 0.4675  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3764, ratio_loss=0.0047, pruning_loss=0.1214, mse_loss=0.4459
Epoch: [42]  [ 460/1251]  eta: 0:06:41  lr: 0.000013  loss: 3.6648 (3.3917)  time: 0.4735  data: 0.0004  max mem: 19734
Epoch: [42]  [ 470/1251]  eta: 0:06:36  lr: 0.000013  loss: 3.3211 (3.3915)  time: 0.4791  data: 0.0004  max mem: 19734
Epoch: [42]  [ 480/1251]  eta: 0:06:30  lr: 0.000013  loss: 3.4530 (3.3923)  time: 0.4794  data: 0.0003  max mem: 19734
Epoch: [42]  [ 490/1251]  eta: 0:06:24  lr: 0.000013  loss: 3.4530 (3.3937)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [42]  [ 500/1251]  eta: 0:06:18  lr: 0.000013  loss: 3.4094 (3.3949)  time: 0.4543  data: 0.0005  max mem: 19734
Epoch: [42]  [ 510/1251]  eta: 0:06:12  lr: 0.000013  loss: 3.5396 (3.3935)  time: 0.4569  data: 0.0005  max mem: 19734
Epoch: [42]  [ 520/1251]  eta: 0:06:07  lr: 0.000013  loss: 3.5396 (3.3985)  time: 0.4560  data: 0.0006  max mem: 19734
Epoch: [42]  [ 530/1251]  eta: 0:06:01  lr: 0.000013  loss: 3.4907 (3.3955)  time: 0.4523  data: 0.0006  max mem: 19734
Epoch: [42]  [ 540/1251]  eta: 0:05:55  lr: 0.000013  loss: 3.4344 (3.3972)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [42]  [ 550/1251]  eta: 0:05:50  lr: 0.000013  loss: 3.5112 (3.3950)  time: 0.4520  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4069, ratio_loss=0.0048, pruning_loss=0.1215, mse_loss=0.4189
Epoch: [42]  [ 560/1251]  eta: 0:05:44  lr: 0.000013  loss: 3.7864 (3.4004)  time: 0.4581  data: 0.0005  max mem: 19734
Epoch: [42]  [ 570/1251]  eta: 0:05:39  lr: 0.000013  loss: 3.7006 (3.4005)  time: 0.4589  data: 0.0005  max mem: 19734
Epoch: [42]  [ 580/1251]  eta: 0:05:33  lr: 0.000013  loss: 3.6625 (3.4059)  time: 0.4534  data: 0.0006  max mem: 19734
Epoch: [42]  [ 590/1251]  eta: 0:05:28  lr: 0.000013  loss: 3.5119 (3.4022)  time: 0.4688  data: 0.0005  max mem: 19734
Epoch: [42]  [ 600/1251]  eta: 0:05:23  lr: 0.000013  loss: 3.2094 (3.4006)  time: 0.4702  data: 0.0004  max mem: 19734
Epoch: [42]  [ 610/1251]  eta: 0:05:18  lr: 0.000013  loss: 3.4368 (3.4014)  time: 0.4782  data: 0.0006  max mem: 19734
Epoch: [42]  [ 620/1251]  eta: 0:05:13  lr: 0.000013  loss: 3.6281 (3.4078)  time: 0.4942  data: 0.0006  max mem: 19734
Epoch: [42]  [ 630/1251]  eta: 0:05:07  lr: 0.000013  loss: 3.5521 (3.4011)  time: 0.4706  data: 0.0006  max mem: 19734
Epoch: [42]  [ 640/1251]  eta: 0:05:02  lr: 0.000013  loss: 3.2292 (3.3977)  time: 0.4532  data: 0.0006  max mem: 19734
Epoch: [42]  [ 650/1251]  eta: 0:04:57  lr: 0.000013  loss: 3.5625 (3.4003)  time: 0.4544  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3756, ratio_loss=0.0048, pruning_loss=0.1210, mse_loss=0.4275
Epoch: [42]  [ 660/1251]  eta: 0:04:51  lr: 0.000013  loss: 3.5625 (3.3966)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [42]  [ 670/1251]  eta: 0:04:46  lr: 0.000013  loss: 3.0883 (3.3935)  time: 0.4605  data: 0.0006  max mem: 19734
Epoch: [42]  [ 680/1251]  eta: 0:04:41  lr: 0.000013  loss: 3.6138 (3.3940)  time: 0.4546  data: 0.0006  max mem: 19734
Epoch: [42]  [ 690/1251]  eta: 0:04:36  lr: 0.000013  loss: 3.6138 (3.3938)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [42]  [ 700/1251]  eta: 0:04:30  lr: 0.000013  loss: 3.5412 (3.3922)  time: 0.4521  data: 0.0005  max mem: 19734
Epoch: [42]  [ 710/1251]  eta: 0:04:25  lr: 0.000013  loss: 3.5408 (3.3907)  time: 0.4631  data: 0.0005  max mem: 19734
Epoch: [42]  [ 720/1251]  eta: 0:04:20  lr: 0.000013  loss: 3.5408 (3.3894)  time: 0.4638  data: 0.0007  max mem: 19734
Epoch: [42]  [ 730/1251]  eta: 0:04:15  lr: 0.000013  loss: 3.2268 (3.3874)  time: 0.4635  data: 0.0006  max mem: 19734
Epoch: [42]  [ 740/1251]  eta: 0:04:10  lr: 0.000013  loss: 3.4679 (3.3894)  time: 0.4762  data: 0.0004  max mem: 19734
Epoch: [42]  [ 750/1251]  eta: 0:04:05  lr: 0.000013  loss: 3.5591 (3.3887)  time: 0.4739  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3162, ratio_loss=0.0047, pruning_loss=0.1228, mse_loss=0.4194
Epoch: [42]  [ 760/1251]  eta: 0:04:00  lr: 0.000013  loss: 3.5025 (3.3901)  time: 0.4788  data: 0.0005  max mem: 19734
Epoch: [42]  [ 770/1251]  eta: 0:03:55  lr: 0.000013  loss: 3.5842 (3.3912)  time: 0.4833  data: 0.0005  max mem: 19734
Epoch: [42]  [ 780/1251]  eta: 0:03:50  lr: 0.000013  loss: 3.4299 (3.3866)  time: 0.4710  data: 0.0005  max mem: 19734
Epoch: [42]  [ 790/1251]  eta: 0:03:45  lr: 0.000013  loss: 3.2478 (3.3853)  time: 0.4611  data: 0.0005  max mem: 19734
Epoch: [42]  [ 800/1251]  eta: 0:03:40  lr: 0.000013  loss: 3.3515 (3.3864)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [42]  [ 810/1251]  eta: 0:03:35  lr: 0.000013  loss: 3.3544 (3.3864)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [42]  [ 820/1251]  eta: 0:03:30  lr: 0.000013  loss: 3.4876 (3.3881)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [42]  [ 830/1251]  eta: 0:03:25  lr: 0.000013  loss: 3.4876 (3.3886)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [42]  [ 840/1251]  eta: 0:03:20  lr: 0.000013  loss: 3.3654 (3.3854)  time: 0.4539  data: 0.0006  max mem: 19734
Epoch: [42]  [ 850/1251]  eta: 0:03:15  lr: 0.000013  loss: 3.2341 (3.3839)  time: 0.4541  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3090, ratio_loss=0.0051, pruning_loss=0.1231, mse_loss=0.4482
Epoch: [42]  [ 860/1251]  eta: 0:03:10  lr: 0.000013  loss: 3.2341 (3.3841)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [42]  [ 870/1251]  eta: 0:03:05  lr: 0.000013  loss: 3.5029 (3.3851)  time: 0.4604  data: 0.0004  max mem: 19734
Epoch: [42]  [ 880/1251]  eta: 0:03:00  lr: 0.000013  loss: 3.5162 (3.3861)  time: 0.4709  data: 0.0005  max mem: 19734
Epoch: [42]  [ 890/1251]  eta: 0:02:55  lr: 0.000013  loss: 3.3841 (3.3862)  time: 0.4871  data: 0.0005  max mem: 19734
Epoch: [42]  [ 900/1251]  eta: 0:02:50  lr: 0.000013  loss: 3.5564 (3.3861)  time: 0.4778  data: 0.0004  max mem: 19734
Epoch: [42]  [ 910/1251]  eta: 0:02:45  lr: 0.000013  loss: 3.5300 (3.3859)  time: 0.4717  data: 0.0004  max mem: 19734
Epoch: [42]  [ 920/1251]  eta: 0:02:40  lr: 0.000013  loss: 3.2179 (3.3835)  time: 0.4708  data: 0.0004  max mem: 19734
Epoch: [42]  [ 930/1251]  eta: 0:02:35  lr: 0.000013  loss: 3.2311 (3.3850)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [42]  [ 940/1251]  eta: 0:02:30  lr: 0.000013  loss: 3.2946 (3.3829)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [42]  [ 950/1251]  eta: 0:02:25  lr: 0.000013  loss: 3.3356 (3.3863)  time: 0.4531  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3846, ratio_loss=0.0053, pruning_loss=0.1224, mse_loss=0.4298
Epoch: [42]  [ 960/1251]  eta: 0:02:20  lr: 0.000013  loss: 3.7318 (3.3875)  time: 0.4553  data: 0.0006  max mem: 19734
Epoch: [42]  [ 970/1251]  eta: 0:02:16  lr: 0.000013  loss: 3.5584 (3.3879)  time: 0.4593  data: 0.0006  max mem: 19734
Epoch: [42]  [ 980/1251]  eta: 0:02:11  lr: 0.000013  loss: 3.5828 (3.3898)  time: 0.4565  data: 0.0005  max mem: 19734
Epoch: [42]  [ 990/1251]  eta: 0:02:06  lr: 0.000013  loss: 3.6877 (3.3929)  time: 0.4543  data: 0.0005  max mem: 19734
Epoch: [42]  [1000/1251]  eta: 0:02:01  lr: 0.000013  loss: 3.5591 (3.3909)  time: 0.4664  data: 0.0005  max mem: 19734
Epoch: [42]  [1010/1251]  eta: 0:01:56  lr: 0.000013  loss: 3.4617 (3.3920)  time: 0.4644  data: 0.0004  max mem: 19734
Epoch: [42]  [1020/1251]  eta: 0:01:51  lr: 0.000013  loss: 3.4993 (3.3909)  time: 0.4634  data: 0.0005  max mem: 19734
Epoch: [42]  [1030/1251]  eta: 0:01:46  lr: 0.000013  loss: 3.5493 (3.3914)  time: 0.4752  data: 0.0005  max mem: 19734
Epoch: [42]  [1040/1251]  eta: 0:01:41  lr: 0.000013  loss: 3.5493 (3.3926)  time: 0.4700  data: 0.0004  max mem: 19734
Epoch: [42]  [1050/1251]  eta: 0:01:37  lr: 0.000013  loss: 3.6816 (3.3954)  time: 0.4808  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4517, ratio_loss=0.0049, pruning_loss=0.1178, mse_loss=0.4257
Epoch: [42]  [1060/1251]  eta: 0:01:32  lr: 0.000013  loss: 3.6816 (3.3962)  time: 0.4814  data: 0.0005  max mem: 19734
Epoch: [42]  [1070/1251]  eta: 0:01:27  lr: 0.000013  loss: 3.4166 (3.3962)  time: 0.4581  data: 0.0005  max mem: 19734
Epoch: [42]  [1080/1251]  eta: 0:01:22  lr: 0.000013  loss: 3.4166 (3.3943)  time: 0.4507  data: 0.0006  max mem: 19734
Epoch: [42]  [1090/1251]  eta: 0:01:17  lr: 0.000013  loss: 3.3308 (3.3944)  time: 0.4527  data: 0.0006  max mem: 19734
Epoch: [42]  [1100/1251]  eta: 0:01:12  lr: 0.000013  loss: 3.2823 (3.3923)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [42]  [1110/1251]  eta: 0:01:07  lr: 0.000013  loss: 3.4050 (3.3926)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [42]  [1120/1251]  eta: 0:01:03  lr: 0.000013  loss: 3.1451 (3.3893)  time: 0.4575  data: 0.0004  max mem: 19734
Epoch: [42]  [1130/1251]  eta: 0:00:58  lr: 0.000013  loss: 3.1059 (3.3876)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [42]  [1140/1251]  eta: 0:00:53  lr: 0.000013  loss: 3.2456 (3.3874)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [42]  [1150/1251]  eta: 0:00:48  lr: 0.000013  loss: 3.5386 (3.3886)  time: 0.4617  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3089, ratio_loss=0.0049, pruning_loss=0.1219, mse_loss=0.4348
Epoch: [42]  [1160/1251]  eta: 0:00:43  lr: 0.000013  loss: 3.5956 (3.3902)  time: 0.4633  data: 0.0004  max mem: 19734
Epoch: [42]  [1170/1251]  eta: 0:00:38  lr: 0.000013  loss: 3.4158 (3.3900)  time: 0.4786  data: 0.0004  max mem: 19734
Epoch: [42]  [1180/1251]  eta: 0:00:34  lr: 0.000013  loss: 3.5295 (3.3900)  time: 0.4822  data: 0.0004  max mem: 19734
Epoch: [42]  [1190/1251]  eta: 0:00:29  lr: 0.000013  loss: 3.5295 (3.3907)  time: 0.4582  data: 0.0007  max mem: 19734
Epoch: [42]  [1200/1251]  eta: 0:00:24  lr: 0.000013  loss: 3.5392 (3.3913)  time: 0.4605  data: 0.0006  max mem: 19734
Epoch: [42]  [1210/1251]  eta: 0:00:19  lr: 0.000013  loss: 3.4990 (3.3911)  time: 0.4615  data: 0.0002  max mem: 19734
Epoch: [42]  [1220/1251]  eta: 0:00:14  lr: 0.000013  loss: 3.4366 (3.3916)  time: 0.4507  data: 0.0002  max mem: 19734
Epoch: [42]  [1230/1251]  eta: 0:00:10  lr: 0.000013  loss: 3.3690 (3.3905)  time: 0.4461  data: 0.0001  max mem: 19734
Epoch: [42]  [1240/1251]  eta: 0:00:05  lr: 0.000013  loss: 3.3928 (3.3905)  time: 0.4447  data: 0.0001  max mem: 19734
Epoch: [42]  [1250/1251]  eta: 0:00:00  lr: 0.000013  loss: 3.3928 (3.3897)  time: 0.4428  data: 0.0001  max mem: 19734
Epoch: [42] Total time: 0:09:59 (0.4796 s / it)
Averaged stats: lr: 0.000013  loss: 3.3928 (3.4063)
Test:  [  0/261]  eta: 1:34:55  loss: 0.7135 (0.7135)  acc1: 82.2917 (82.2917)  acc5: 96.3542 (96.3542)  time: 21.8204  data: 21.5660  max mem: 19734
Test:  [ 10/261]  eta: 0:09:51  loss: 0.7114 (0.7259)  acc1: 83.8542 (83.6174)  acc5: 96.8750 (96.4489)  time: 2.3570  data: 2.1748  max mem: 19734
Test:  [ 20/261]  eta: 0:05:27  loss: 0.9288 (0.9022)  acc1: 77.6042 (78.7450)  acc5: 94.2708 (94.8909)  time: 0.3337  data: 0.1587  max mem: 19734
Test:  [ 30/261]  eta: 0:03:47  loss: 0.8059 (0.8195)  acc1: 82.8125 (81.7204)  acc5: 94.2708 (95.3125)  time: 0.2285  data: 0.0506  max mem: 19734
Test:  [ 40/261]  eta: 0:03:24  loss: 0.5474 (0.7829)  acc1: 88.5417 (82.7109)  acc5: 96.3542 (95.5412)  time: 0.4715  data: 0.3140  max mem: 19734
Test:  [ 50/261]  eta: 0:02:46  loss: 0.9042 (0.8455)  acc1: 78.1250 (80.9436)  acc5: 94.2708 (95.1185)  time: 0.4881  data: 0.3075  max mem: 19734
Test:  [ 60/261]  eta: 0:02:19  loss: 0.9683 (0.8533)  acc1: 76.5625 (80.6096)  acc5: 93.7500 (95.1247)  time: 0.2134  data: 0.0116  max mem: 19734
Test:  [ 70/261]  eta: 0:02:18  loss: 0.9317 (0.8544)  acc1: 78.6458 (80.2083)  acc5: 96.3542 (95.3125)  time: 0.5597  data: 0.3548  max mem: 19734
Test:  [ 80/261]  eta: 0:01:58  loss: 0.8157 (0.8565)  acc1: 79.1667 (80.2919)  acc5: 96.8750 (95.3575)  time: 0.5509  data: 0.3523  max mem: 19734
Test:  [ 90/261]  eta: 0:01:43  loss: 0.8157 (0.8425)  acc1: 83.8542 (80.6834)  acc5: 95.8333 (95.4270)  time: 0.1876  data: 0.0129  max mem: 19734
Test:  [100/261]  eta: 0:01:34  loss: 0.8307 (0.8466)  acc1: 83.3333 (80.6570)  acc5: 95.3125 (95.4981)  time: 0.3088  data: 0.1376  max mem: 19734
Test:  [110/261]  eta: 0:01:25  loss: 0.8825 (0.8682)  acc1: 77.0833 (80.2130)  acc5: 94.7917 (95.2280)  time: 0.3797  data: 0.2313  max mem: 19734
Test:  [120/261]  eta: 0:01:15  loss: 1.2057 (0.9073)  acc1: 71.3542 (79.3518)  acc5: 90.1042 (94.7314)  time: 0.2756  data: 0.1076  max mem: 19734
Test:  [130/261]  eta: 0:01:07  loss: 1.3406 (0.9518)  acc1: 68.7500 (78.4113)  acc5: 88.0208 (94.1595)  time: 0.2196  data: 0.0142  max mem: 19734
Test:  [140/261]  eta: 0:01:00  loss: 1.3288 (0.9795)  acc1: 68.7500 (77.7335)  acc5: 89.5833 (93.8978)  time: 0.2973  data: 0.1021  max mem: 19734
Test:  [150/261]  eta: 0:00:55  loss: 1.2304 (0.9850)  acc1: 73.4375 (77.6801)  acc5: 92.1875 (93.7569)  time: 0.3943  data: 0.2186  max mem: 19734
Test:  [160/261]  eta: 0:00:48  loss: 0.9758 (1.0051)  acc1: 78.6458 (77.3486)  acc5: 92.1875 (93.4491)  time: 0.3294  data: 0.1321  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: 1.3189 (1.0348)  acc1: 66.1458 (76.5991)  acc5: 88.0208 (93.1073)  time: 0.2286  data: 0.0157  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: 1.4522 (1.0504)  acc1: 66.1458 (76.2402)  acc5: 89.0625 (92.9990)  time: 0.3738  data: 0.2049  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.2932 (1.0633)  acc1: 69.2708 (75.9953)  acc5: 91.1458 (92.8392)  time: 0.3576  data: 0.2419  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.2932 (1.0795)  acc1: 72.3958 (75.6634)  acc5: 89.0625 (92.5891)  time: 0.1761  data: 0.0907  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3993 (1.0931)  acc1: 69.7917 (75.4172)  acc5: 88.0208 (92.3998)  time: 0.1286  data: 0.0625  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4586 (1.1132)  acc1: 67.7083 (74.8987)  acc5: 86.9792 (92.1663)  time: 0.0780  data: 0.0163  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4586 (1.1227)  acc1: 65.1042 (74.6438)  acc5: 87.5000 (92.0725)  time: 0.0619  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3215 (1.1315)  acc1: 66.6667 (74.4143)  acc5: 90.6250 (91.9995)  time: 0.0618  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0405 (1.1249)  acc1: 75.5208 (74.6016)  acc5: 93.7500 (92.0921)  time: 0.0631  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9679 (1.1252)  acc1: 76.0417 (74.6060)  acc5: 94.2708 (92.1460)  time: 0.0612  data: 0.0002  max mem: 19734
Test: Total time: 0:01:32 (0.3552 s / it)
* Acc@1 74.606 Acc@5 92.146 loss 1.125
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.69%
Epoch: [43]  [   0/1251]  eta: 6:07:56  lr: 0.000013  loss: 3.6655 (3.6655)  time: 17.6469  data: 14.4916  max mem: 19734
loss info: cls_loss=3.3692, ratio_loss=0.0048, pruning_loss=0.1204, mse_loss=0.4241
Epoch: [43]  [  10/1251]  eta: 0:43:34  lr: 0.000013  loss: 3.4791 (3.3982)  time: 2.1070  data: 1.3226  max mem: 19734
Epoch: [43]  [  20/1251]  eta: 0:27:23  lr: 0.000013  loss: 3.4791 (3.4191)  time: 0.5192  data: 0.0031  max mem: 19734
Epoch: [43]  [  30/1251]  eta: 0:21:23  lr: 0.000013  loss: 3.6428 (3.5259)  time: 0.4710  data: 0.0007  max mem: 19734
Epoch: [43]  [  40/1251]  eta: 0:18:19  lr: 0.000013  loss: 3.6473 (3.4961)  time: 0.4589  data: 0.0007  max mem: 19734
Epoch: [43]  [  50/1251]  eta: 0:16:24  lr: 0.000013  loss: 3.6473 (3.5290)  time: 0.4612  data: 0.0005  max mem: 19734
Epoch: [43]  [  60/1251]  eta: 0:15:15  lr: 0.000013  loss: 3.6485 (3.5195)  time: 0.4849  data: 0.0005  max mem: 19734
Epoch: [43]  [  70/1251]  eta: 0:14:20  lr: 0.000013  loss: 3.5385 (3.4961)  time: 0.4968  data: 0.0005  max mem: 19734
Epoch: [43]  [  80/1251]  eta: 0:13:40  lr: 0.000013  loss: 3.5385 (3.4961)  time: 0.4927  data: 0.0005  max mem: 19734
Epoch: [43]  [  90/1251]  eta: 0:13:06  lr: 0.000013  loss: 3.7178 (3.5129)  time: 0.4945  data: 0.0005  max mem: 19734
Epoch: [43]  [ 100/1251]  eta: 0:12:35  lr: 0.000013  loss: 3.7265 (3.5083)  time: 0.4753  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4839, ratio_loss=0.0047, pruning_loss=0.1193, mse_loss=0.4124
Epoch: [43]  [ 110/1251]  eta: 0:12:08  lr: 0.000013  loss: 3.5866 (3.5013)  time: 0.4618  data: 0.0006  max mem: 19734
Epoch: [43]  [ 120/1251]  eta: 0:11:45  lr: 0.000013  loss: 3.5510 (3.4919)  time: 0.4605  data: 0.0006  max mem: 19734
Epoch: [43]  [ 130/1251]  eta: 0:11:25  lr: 0.000013  loss: 3.4537 (3.4683)  time: 0.4598  data: 0.0006  max mem: 19734
Epoch: [43]  [ 140/1251]  eta: 0:11:07  lr: 0.000013  loss: 3.4537 (3.4639)  time: 0.4624  data: 0.0006  max mem: 19734
Epoch: [43]  [ 150/1251]  eta: 0:10:51  lr: 0.000013  loss: 3.6373 (3.4781)  time: 0.4622  data: 0.0005  max mem: 19734
Epoch: [43]  [ 160/1251]  eta: 0:10:36  lr: 0.000013  loss: 3.5583 (3.4703)  time: 0.4606  data: 0.0005  max mem: 19734
Epoch: [43]  [ 170/1251]  eta: 0:10:23  lr: 0.000013  loss: 3.1814 (3.4664)  time: 0.4628  data: 0.0005  max mem: 19734
Epoch: [43]  [ 180/1251]  eta: 0:10:10  lr: 0.000013  loss: 3.2836 (3.4569)  time: 0.4617  data: 0.0006  max mem: 19734
Epoch: [43]  [ 190/1251]  eta: 0:09:58  lr: 0.000013  loss: 3.2836 (3.4452)  time: 0.4585  data: 0.0006  max mem: 19734
Epoch: [43]  [ 200/1251]  eta: 0:09:50  lr: 0.000013  loss: 3.5275 (3.4446)  time: 0.4860  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3540, ratio_loss=0.0049, pruning_loss=0.1209, mse_loss=0.4251
Epoch: [43]  [ 210/1251]  eta: 0:09:39  lr: 0.000013  loss: 3.5766 (3.4337)  time: 0.4881  data: 0.0005  max mem: 19734
Epoch: [43]  [ 220/1251]  eta: 0:09:30  lr: 0.000013  loss: 3.5548 (3.4297)  time: 0.4700  data: 0.0005  max mem: 19734
Epoch: [43]  [ 230/1251]  eta: 0:09:22  lr: 0.000013  loss: 3.3212 (3.4239)  time: 0.4873  data: 0.0006  max mem: 19734
Epoch: [43]  [ 240/1251]  eta: 0:09:14  lr: 0.000013  loss: 3.6182 (3.4363)  time: 0.4918  data: 0.0005  max mem: 19734
Epoch: [43]  [ 250/1251]  eta: 0:09:05  lr: 0.000013  loss: 3.6140 (3.4343)  time: 0.4751  data: 0.0005  max mem: 19734
Epoch: [43]  [ 260/1251]  eta: 0:08:57  lr: 0.000013  loss: 3.3270 (3.4297)  time: 0.4654  data: 0.0005  max mem: 19734
Epoch: [43]  [ 270/1251]  eta: 0:08:48  lr: 0.000013  loss: 3.4797 (3.4375)  time: 0.4646  data: 0.0004  max mem: 19734
Epoch: [43]  [ 280/1251]  eta: 0:08:40  lr: 0.000013  loss: 3.6928 (3.4387)  time: 0.4605  data: 0.0004  max mem: 19734
Epoch: [43]  [ 290/1251]  eta: 0:08:32  lr: 0.000013  loss: 3.6766 (3.4435)  time: 0.4587  data: 0.0004  max mem: 19734
Epoch: [43]  [ 300/1251]  eta: 0:08:24  lr: 0.000013  loss: 3.7045 (3.4542)  time: 0.4585  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4103, ratio_loss=0.0051, pruning_loss=0.1197, mse_loss=0.4353
Epoch: [43]  [ 310/1251]  eta: 0:08:17  lr: 0.000013  loss: 3.5832 (3.4387)  time: 0.4592  data: 0.0004  max mem: 19734
Epoch: [43]  [ 320/1251]  eta: 0:08:10  lr: 0.000013  loss: 3.0807 (3.4377)  time: 0.4633  data: 0.0005  max mem: 19734
Epoch: [43]  [ 330/1251]  eta: 0:08:03  lr: 0.000013  loss: 3.0807 (3.4246)  time: 0.4615  data: 0.0005  max mem: 19734
Epoch: [43]  [ 340/1251]  eta: 0:07:55  lr: 0.000013  loss: 3.2058 (3.4235)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [43]  [ 350/1251]  eta: 0:07:49  lr: 0.000013  loss: 3.5719 (3.4257)  time: 0.4671  data: 0.0004  max mem: 19734
Epoch: [43]  [ 360/1251]  eta: 0:07:43  lr: 0.000013  loss: 3.5592 (3.4215)  time: 0.4786  data: 0.0004  max mem: 19734
Epoch: [43]  [ 370/1251]  eta: 0:07:37  lr: 0.000013  loss: 3.1867 (3.4151)  time: 0.4747  data: 0.0004  max mem: 19734
Epoch: [43]  [ 380/1251]  eta: 0:07:30  lr: 0.000013  loss: 3.4335 (3.4162)  time: 0.4639  data: 0.0006  max mem: 19734
Epoch: [43]  [ 390/1251]  eta: 0:07:24  lr: 0.000013  loss: 3.5447 (3.4159)  time: 0.4635  data: 0.0006  max mem: 19734
Epoch: [43]  [ 400/1251]  eta: 0:07:17  lr: 0.000013  loss: 3.5905 (3.4197)  time: 0.4637  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3478, ratio_loss=0.0048, pruning_loss=0.1209, mse_loss=0.4297
Epoch: [43]  [ 410/1251]  eta: 0:07:11  lr: 0.000013  loss: 3.6889 (3.4246)  time: 0.4537  data: 0.0005  max mem: 19734
Epoch: [43]  [ 420/1251]  eta: 0:07:05  lr: 0.000013  loss: 3.5326 (3.4237)  time: 0.4537  data: 0.0008  max mem: 19734
Epoch: [43]  [ 430/1251]  eta: 0:06:58  lr: 0.000013  loss: 3.5326 (3.4269)  time: 0.4551  data: 0.0007  max mem: 19734
Epoch: [43]  [ 440/1251]  eta: 0:06:52  lr: 0.000013  loss: 3.5695 (3.4270)  time: 0.4574  data: 0.0005  max mem: 19734
Epoch: [43]  [ 450/1251]  eta: 0:06:46  lr: 0.000013  loss: 3.5904 (3.4287)  time: 0.4565  data: 0.0005  max mem: 19734
Epoch: [43]  [ 460/1251]  eta: 0:06:41  lr: 0.000013  loss: 3.5507 (3.4258)  time: 0.4688  data: 0.0006  max mem: 19734
Epoch: [43]  [ 470/1251]  eta: 0:06:35  lr: 0.000013  loss: 3.5753 (3.4320)  time: 0.4704  data: 0.0006  max mem: 19734
Epoch: [43]  [ 480/1251]  eta: 0:06:29  lr: 0.000013  loss: 3.7897 (3.4335)  time: 0.4596  data: 0.0005  max mem: 19734
Epoch: [43]  [ 490/1251]  eta: 0:06:24  lr: 0.000013  loss: 3.4687 (3.4358)  time: 0.4795  data: 0.0005  max mem: 19734
Epoch: [43]  [ 500/1251]  eta: 0:06:19  lr: 0.000013  loss: 3.3786 (3.4314)  time: 0.4883  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4421, ratio_loss=0.0048, pruning_loss=0.1198, mse_loss=0.4209
Epoch: [43]  [ 510/1251]  eta: 0:06:13  lr: 0.000013  loss: 3.4506 (3.4316)  time: 0.4833  data: 0.0005  max mem: 19734
Epoch: [43]  [ 520/1251]  eta: 0:06:08  lr: 0.000013  loss: 3.4910 (3.4330)  time: 0.4853  data: 0.0004  max mem: 19734
Epoch: [43]  [ 530/1251]  eta: 0:06:03  lr: 0.000013  loss: 3.5785 (3.4336)  time: 0.4830  data: 0.0005  max mem: 19734
Epoch: [43]  [ 540/1251]  eta: 0:05:57  lr: 0.000013  loss: 3.5242 (3.4314)  time: 0.4706  data: 0.0006  max mem: 19734
Epoch: [43]  [ 550/1251]  eta: 0:05:51  lr: 0.000013  loss: 3.1352 (3.4295)  time: 0.4562  data: 0.0006  max mem: 19734
Epoch: [43]  [ 560/1251]  eta: 0:05:46  lr: 0.000013  loss: 3.1352 (3.4211)  time: 0.4551  data: 0.0007  max mem: 19734
Epoch: [43]  [ 570/1251]  eta: 0:05:40  lr: 0.000013  loss: 3.2755 (3.4206)  time: 0.4539  data: 0.0007  max mem: 19734
Epoch: [43]  [ 580/1251]  eta: 0:05:35  lr: 0.000013  loss: 3.3833 (3.4146)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [43]  [ 590/1251]  eta: 0:05:29  lr: 0.000013  loss: 3.5104 (3.4179)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [43]  [ 600/1251]  eta: 0:05:24  lr: 0.000013  loss: 3.6109 (3.4224)  time: 0.4527  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3489, ratio_loss=0.0049, pruning_loss=0.1217, mse_loss=0.4197
Epoch: [43]  [ 610/1251]  eta: 0:05:18  lr: 0.000013  loss: 3.5652 (3.4170)  time: 0.4582  data: 0.0005  max mem: 19734
Epoch: [43]  [ 620/1251]  eta: 0:05:13  lr: 0.000013  loss: 2.7446 (3.4120)  time: 0.4609  data: 0.0004  max mem: 19734
Epoch: [43]  [ 630/1251]  eta: 0:05:07  lr: 0.000013  loss: 3.3488 (3.4126)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [43]  [ 640/1251]  eta: 0:05:03  lr: 0.000013  loss: 3.4779 (3.4133)  time: 0.4754  data: 0.0005  max mem: 19734
Epoch: [43]  [ 650/1251]  eta: 0:04:57  lr: 0.000013  loss: 3.3995 (3.4124)  time: 0.4860  data: 0.0004  max mem: 19734
Epoch: [43]  [ 660/1251]  eta: 0:04:52  lr: 0.000013  loss: 3.1992 (3.4060)  time: 0.4862  data: 0.0004  max mem: 19734
Epoch: [43]  [ 670/1251]  eta: 0:04:47  lr: 0.000013  loss: 3.1992 (3.4074)  time: 0.4767  data: 0.0005  max mem: 19734
Epoch: [43]  [ 680/1251]  eta: 0:04:42  lr: 0.000013  loss: 3.7210 (3.4115)  time: 0.4669  data: 0.0005  max mem: 19734
Epoch: [43]  [ 690/1251]  eta: 0:04:37  lr: 0.000013  loss: 3.6743 (3.4120)  time: 0.4696  data: 0.0004  max mem: 19734
Epoch: [43]  [ 700/1251]  eta: 0:04:32  lr: 0.000013  loss: 3.5124 (3.4116)  time: 0.4605  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3308, ratio_loss=0.0047, pruning_loss=0.1208, mse_loss=0.4183
Epoch: [43]  [ 710/1251]  eta: 0:04:26  lr: 0.000013  loss: 3.4956 (3.4137)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [43]  [ 720/1251]  eta: 0:04:21  lr: 0.000013  loss: 3.4956 (3.4120)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [43]  [ 730/1251]  eta: 0:04:16  lr: 0.000013  loss: 3.4505 (3.4121)  time: 0.4598  data: 0.0005  max mem: 19734
Epoch: [43]  [ 740/1251]  eta: 0:04:11  lr: 0.000013  loss: 3.2822 (3.4078)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [43]  [ 750/1251]  eta: 0:04:06  lr: 0.000013  loss: 3.2822 (3.4089)  time: 0.4670  data: 0.0005  max mem: 19734
Epoch: [43]  [ 760/1251]  eta: 0:04:01  lr: 0.000013  loss: 3.5563 (3.4100)  time: 0.4666  data: 0.0005  max mem: 19734
Epoch: [43]  [ 770/1251]  eta: 0:03:56  lr: 0.000013  loss: 3.5236 (3.4121)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [43]  [ 780/1251]  eta: 0:03:51  lr: 0.000013  loss: 3.5236 (3.4158)  time: 0.4778  data: 0.0005  max mem: 19734
Epoch: [43]  [ 790/1251]  eta: 0:03:46  lr: 0.000013  loss: 3.5993 (3.4163)  time: 0.4892  data: 0.0004  max mem: 19734
Epoch: [43]  [ 800/1251]  eta: 0:03:41  lr: 0.000013  loss: 3.5993 (3.4178)  time: 0.4727  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4424, ratio_loss=0.0046, pruning_loss=0.1198, mse_loss=0.4206
Epoch: [43]  [ 810/1251]  eta: 0:03:36  lr: 0.000013  loss: 3.5339 (3.4192)  time: 0.4694  data: 0.0004  max mem: 19734
Epoch: [43]  [ 820/1251]  eta: 0:03:31  lr: 0.000013  loss: 3.4986 (3.4167)  time: 0.4724  data: 0.0005  max mem: 19734
Epoch: [43]  [ 830/1251]  eta: 0:03:26  lr: 0.000013  loss: 3.2798 (3.4166)  time: 0.4653  data: 0.0004  max mem: 19734
Epoch: [43]  [ 840/1251]  eta: 0:03:21  lr: 0.000013  loss: 3.5788 (3.4180)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [43]  [ 850/1251]  eta: 0:03:15  lr: 0.000013  loss: 3.5788 (3.4171)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [43]  [ 860/1251]  eta: 0:03:10  lr: 0.000013  loss: 3.3340 (3.4154)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [43]  [ 870/1251]  eta: 0:03:05  lr: 0.000013  loss: 3.3277 (3.4148)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [43]  [ 880/1251]  eta: 0:03:00  lr: 0.000013  loss: 3.3937 (3.4148)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [43]  [ 890/1251]  eta: 0:02:55  lr: 0.000013  loss: 3.3953 (3.4159)  time: 0.4578  data: 0.0004  max mem: 19734
Epoch: [43]  [ 900/1251]  eta: 0:02:50  lr: 0.000013  loss: 3.2414 (3.4147)  time: 0.4634  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3262, ratio_loss=0.0046, pruning_loss=0.1221, mse_loss=0.4144
Epoch: [43]  [ 910/1251]  eta: 0:02:45  lr: 0.000013  loss: 3.0358 (3.4105)  time: 0.4599  data: 0.0004  max mem: 19734
Epoch: [43]  [ 920/1251]  eta: 0:02:40  lr: 0.000013  loss: 3.0002 (3.4084)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [43]  [ 930/1251]  eta: 0:02:36  lr: 0.000013  loss: 3.4449 (3.4092)  time: 0.4726  data: 0.0004  max mem: 19734
Epoch: [43]  [ 940/1251]  eta: 0:02:31  lr: 0.000013  loss: 3.2255 (3.4058)  time: 0.4793  data: 0.0004  max mem: 19734
Epoch: [43]  [ 950/1251]  eta: 0:02:26  lr: 0.000013  loss: 2.9961 (3.4035)  time: 0.4728  data: 0.0004  max mem: 19734
Epoch: [43]  [ 960/1251]  eta: 0:02:21  lr: 0.000013  loss: 3.4276 (3.4043)  time: 0.4644  data: 0.0004  max mem: 19734
Epoch: [43]  [ 970/1251]  eta: 0:02:16  lr: 0.000013  loss: 3.5030 (3.4021)  time: 0.4624  data: 0.0004  max mem: 19734
Epoch: [43]  [ 980/1251]  eta: 0:02:11  lr: 0.000013  loss: 3.5156 (3.4016)  time: 0.4644  data: 0.0004  max mem: 19734
Epoch: [43]  [ 990/1251]  eta: 0:02:06  lr: 0.000013  loss: 3.5156 (3.4009)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [43]  [1000/1251]  eta: 0:02:01  lr: 0.000013  loss: 3.4166 (3.3999)  time: 0.4560  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2752, ratio_loss=0.0049, pruning_loss=0.1207, mse_loss=0.4155
Epoch: [43]  [1010/1251]  eta: 0:01:56  lr: 0.000013  loss: 3.4802 (3.3992)  time: 0.4566  data: 0.0005  max mem: 19734
Epoch: [43]  [1020/1251]  eta: 0:01:51  lr: 0.000013  loss: 3.4066 (3.3989)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [43]  [1030/1251]  eta: 0:01:46  lr: 0.000013  loss: 3.3325 (3.3993)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [43]  [1040/1251]  eta: 0:01:42  lr: 0.000013  loss: 3.6606 (3.4008)  time: 0.4535  data: 0.0004  max mem: 19734
Epoch: [43]  [1050/1251]  eta: 0:01:37  lr: 0.000013  loss: 3.5493 (3.3989)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [43]  [1060/1251]  eta: 0:01:32  lr: 0.000013  loss: 3.5461 (3.3993)  time: 0.4583  data: 0.0004  max mem: 19734
Epoch: [43]  [1070/1251]  eta: 0:01:27  lr: 0.000013  loss: 3.5518 (3.4002)  time: 0.4637  data: 0.0004  max mem: 19734
Epoch: [43]  [1080/1251]  eta: 0:01:22  lr: 0.000013  loss: 3.6441 (3.4027)  time: 0.4654  data: 0.0004  max mem: 19734
Epoch: [43]  [1090/1251]  eta: 0:01:17  lr: 0.000013  loss: 3.6098 (3.4038)  time: 0.4675  data: 0.0004  max mem: 19734
Epoch: [43]  [1100/1251]  eta: 0:01:12  lr: 0.000013  loss: 3.4845 (3.4026)  time: 0.4743  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4296, ratio_loss=0.0045, pruning_loss=0.1187, mse_loss=0.4144
Epoch: [43]  [1110/1251]  eta: 0:01:08  lr: 0.000013  loss: 3.5579 (3.4050)  time: 0.4714  data: 0.0004  max mem: 19734
Epoch: [43]  [1120/1251]  eta: 0:01:03  lr: 0.000013  loss: 3.5992 (3.4052)  time: 0.4633  data: 0.0004  max mem: 19734
Epoch: [43]  [1130/1251]  eta: 0:00:58  lr: 0.000013  loss: 3.2531 (3.4041)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [43]  [1140/1251]  eta: 0:00:53  lr: 0.000013  loss: 3.2531 (3.4047)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [43]  [1150/1251]  eta: 0:00:48  lr: 0.000013  loss: 3.6560 (3.4066)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [43]  [1160/1251]  eta: 0:00:43  lr: 0.000013  loss: 3.4803 (3.4045)  time: 0.4527  data: 0.0006  max mem: 19734
Epoch: [43]  [1170/1251]  eta: 0:00:38  lr: 0.000013  loss: 3.4308 (3.4047)  time: 0.4519  data: 0.0006  max mem: 19734
Epoch: [43]  [1180/1251]  eta: 0:00:34  lr: 0.000013  loss: 3.3760 (3.4024)  time: 0.4512  data: 0.0004  max mem: 19734
Epoch: [43]  [1190/1251]  eta: 0:00:29  lr: 0.000013  loss: 3.5025 (3.4028)  time: 0.4527  data: 0.0007  max mem: 19734
Epoch: [43]  [1200/1251]  eta: 0:00:24  lr: 0.000013  loss: 3.5025 (3.4010)  time: 0.4569  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3447, ratio_loss=0.0046, pruning_loss=0.1218, mse_loss=0.4389
Epoch: [43]  [1210/1251]  eta: 0:00:19  lr: 0.000013  loss: 3.3755 (3.4019)  time: 0.4543  data: 0.0001  max mem: 19734
Epoch: [43]  [1220/1251]  eta: 0:00:14  lr: 0.000013  loss: 3.3755 (3.4003)  time: 0.4613  data: 0.0001  max mem: 19734
Epoch: [43]  [1230/1251]  eta: 0:00:10  lr: 0.000013  loss: 3.3127 (3.4018)  time: 0.4656  data: 0.0001  max mem: 19734
Epoch: [43]  [1240/1251]  eta: 0:00:05  lr: 0.000013  loss: 3.7304 (3.4043)  time: 0.4647  data: 0.0002  max mem: 19734
Epoch: [43]  [1250/1251]  eta: 0:00:00  lr: 0.000013  loss: 3.6267 (3.4032)  time: 0.4590  data: 0.0001  max mem: 19734
Epoch: [43] Total time: 0:10:00 (0.4802 s / it)
Averaged stats: lr: 0.000013  loss: 3.6267 (3.4108)
Test:  [  0/261]  eta: 2:09:43  loss: 0.7334 (0.7334)  acc1: 81.2500 (81.2500)  acc5: 95.8333 (95.8333)  time: 29.8206  data: 29.6982  max mem: 19734
Test:  [ 10/261]  eta: 0:11:59  loss: 0.7131 (0.7213)  acc1: 83.8542 (83.6648)  acc5: 96.8750 (96.4015)  time: 2.8672  data: 2.7188  max mem: 19734
Test:  [ 20/261]  eta: 0:06:34  loss: 0.8893 (0.8947)  acc1: 79.1667 (78.9435)  acc5: 94.2708 (94.9157)  time: 0.2267  data: 0.0165  max mem: 19734
Test:  [ 30/261]  eta: 0:04:40  loss: 0.7843 (0.8127)  acc1: 83.3333 (81.8380)  acc5: 94.7917 (95.3125)  time: 0.3084  data: 0.0091  max mem: 19734
Test:  [ 40/261]  eta: 0:03:54  loss: 0.5427 (0.7771)  acc1: 89.5833 (82.9014)  acc5: 96.8750 (95.5793)  time: 0.4589  data: 0.2331  max mem: 19734
Test:  [ 50/261]  eta: 0:03:05  loss: 0.8815 (0.8371)  acc1: 76.5625 (81.0049)  acc5: 94.2708 (95.1491)  time: 0.3507  data: 0.2362  max mem: 19734
Test:  [ 60/261]  eta: 0:02:31  loss: 0.9280 (0.8445)  acc1: 76.5625 (80.7804)  acc5: 94.2708 (95.2015)  time: 0.1290  data: 0.0143  max mem: 19734
Test:  [ 70/261]  eta: 0:02:14  loss: 0.9115 (0.8466)  acc1: 79.1667 (80.3330)  acc5: 95.8333 (95.3859)  time: 0.2629  data: 0.1301  max mem: 19734
Test:  [ 80/261]  eta: 0:01:55  loss: 0.8335 (0.8491)  acc1: 80.2083 (80.4205)  acc5: 96.3542 (95.4668)  time: 0.2762  data: 0.1290  max mem: 19734
Test:  [ 90/261]  eta: 0:01:42  loss: 0.8252 (0.8364)  acc1: 83.3333 (80.7807)  acc5: 95.8333 (95.5243)  time: 0.2358  data: 0.0506  max mem: 19734
Test:  [100/261]  eta: 0:01:50  loss: 0.8255 (0.8396)  acc1: 83.3333 (80.6776)  acc5: 95.3125 (95.6168)  time: 0.8706  data: 0.6634  max mem: 19734
Test:  [110/261]  eta: 0:01:36  loss: 0.8594 (0.8623)  acc1: 76.5625 (80.2834)  acc5: 94.7917 (95.3125)  time: 0.8149  data: 0.6292  max mem: 19734
Test:  [120/261]  eta: 0:01:25  loss: 1.1545 (0.9012)  acc1: 71.8750 (79.4335)  acc5: 90.6250 (94.8089)  time: 0.2274  data: 0.0152  max mem: 19734
Test:  [130/261]  eta: 0:01:15  loss: 1.3725 (0.9450)  acc1: 67.7083 (78.4828)  acc5: 88.0208 (94.2510)  time: 0.2152  data: 0.0114  max mem: 19734
Test:  [140/261]  eta: 0:01:09  loss: 1.2925 (0.9706)  acc1: 70.3125 (77.8590)  acc5: 90.1042 (93.9827)  time: 0.3746  data: 0.1943  max mem: 19734
Test:  [150/261]  eta: 0:01:00  loss: 1.2027 (0.9759)  acc1: 71.3542 (77.8215)  acc5: 91.1458 (93.8431)  time: 0.3647  data: 0.1947  max mem: 19734
Test:  [160/261]  eta: 0:00:52  loss: 0.9788 (0.9958)  acc1: 78.6458 (77.5621)  acc5: 91.1458 (93.5624)  time: 0.1472  data: 0.0143  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.2801 (1.0257)  acc1: 65.6250 (76.8031)  acc5: 88.5417 (93.2505)  time: 0.1561  data: 0.0312  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4088 (1.0429)  acc1: 65.1042 (76.4301)  acc5: 88.5417 (93.0882)  time: 0.1717  data: 0.0746  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3656 (1.0565)  acc1: 68.2292 (76.1698)  acc5: 91.1458 (92.9156)  time: 0.1366  data: 0.0689  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3147 (1.0712)  acc1: 71.8750 (75.8655)  acc5: 89.0625 (92.6643)  time: 0.0797  data: 0.0173  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3328 (1.0842)  acc1: 71.3542 (75.5825)  acc5: 87.5000 (92.4738)  time: 0.0848  data: 0.0232  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.3757 (1.1037)  acc1: 67.1875 (75.0990)  acc5: 88.0208 (92.2653)  time: 0.0855  data: 0.0231  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4471 (1.1130)  acc1: 67.1875 (74.8602)  acc5: 89.5833 (92.1830)  time: 0.0624  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3224 (1.1225)  acc1: 67.1875 (74.6067)  acc5: 90.6250 (92.1032)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0810 (1.1160)  acc1: 75.5208 (74.7614)  acc5: 92.7083 (92.1937)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9289 (1.1164)  acc1: 78.6458 (74.7560)  acc5: 94.7917 (92.2560)  time: 0.0598  data: 0.0002  max mem: 19734
Test: Total time: 0:01:33 (0.3580 s / it)
* Acc@1 74.756 Acc@5 92.256 loss 1.116
Accuracy of the network on the 50000 test images: 74.8%
Max accuracy: 74.76%
Epoch: [44]  [   0/1251]  eta: 4:21:41  lr: 0.000013  loss: 3.4011 (3.4011)  time: 12.5514  data: 12.0577  max mem: 19734
Epoch: [44]  [  10/1251]  eta: 0:35:34  lr: 0.000013  loss: 3.4886 (3.3728)  time: 1.7202  data: 1.1227  max mem: 19734
Epoch: [44]  [  20/1251]  eta: 0:22:56  lr: 0.000013  loss: 3.3456 (3.3788)  time: 0.5466  data: 0.0148  max mem: 19734
Epoch: [44]  [  30/1251]  eta: 0:18:25  lr: 0.000013  loss: 3.5462 (3.4818)  time: 0.4567  data: 0.0007  max mem: 19734
Epoch: [44]  [  40/1251]  eta: 0:16:04  lr: 0.000013  loss: 3.5462 (3.4218)  time: 0.4583  data: 0.0007  max mem: 19734
Epoch: [44]  [  50/1251]  eta: 0:14:39  lr: 0.000013  loss: 3.3475 (3.4345)  time: 0.4647  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4142, ratio_loss=0.0046, pruning_loss=0.1194, mse_loss=0.4137
Epoch: [44]  [  60/1251]  eta: 0:13:39  lr: 0.000013  loss: 3.4722 (3.4407)  time: 0.4650  data: 0.0006  max mem: 19734
Epoch: [44]  [  70/1251]  eta: 0:12:55  lr: 0.000013  loss: 3.5527 (3.4316)  time: 0.4632  data: 0.0004  max mem: 19734
Epoch: [44]  [  80/1251]  eta: 0:12:20  lr: 0.000013  loss: 3.6040 (3.4490)  time: 0.4625  data: 0.0004  max mem: 19734
Epoch: [44]  [  90/1251]  eta: 0:11:51  lr: 0.000013  loss: 3.4858 (3.4226)  time: 0.4594  data: 0.0004  max mem: 19734
Epoch: [44]  [ 100/1251]  eta: 0:11:30  lr: 0.000013  loss: 3.4517 (3.4311)  time: 0.4697  data: 0.0004  max mem: 19734
Epoch: [44]  [ 110/1251]  eta: 0:11:14  lr: 0.000013  loss: 3.5577 (3.4145)  time: 0.4909  data: 0.0004  max mem: 19734
Epoch: [44]  [ 120/1251]  eta: 0:11:00  lr: 0.000013  loss: 3.5087 (3.4153)  time: 0.5018  data: 0.0004  max mem: 19734
Epoch: [44]  [ 130/1251]  eta: 0:10:44  lr: 0.000013  loss: 3.6193 (3.4204)  time: 0.4830  data: 0.0004  max mem: 19734
Epoch: [44]  [ 140/1251]  eta: 0:10:31  lr: 0.000013  loss: 3.4844 (3.3967)  time: 0.4777  data: 0.0005  max mem: 19734
Epoch: [44]  [ 150/1251]  eta: 0:10:18  lr: 0.000013  loss: 3.5201 (3.4073)  time: 0.4754  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3653, ratio_loss=0.0046, pruning_loss=0.1195, mse_loss=0.4136
Epoch: [44]  [ 160/1251]  eta: 0:10:06  lr: 0.000013  loss: 3.6838 (3.4255)  time: 0.4624  data: 0.0005  max mem: 19734
Epoch: [44]  [ 170/1251]  eta: 0:09:54  lr: 0.000013  loss: 3.6000 (3.4175)  time: 0.4643  data: 0.0005  max mem: 19734
Epoch: [44]  [ 180/1251]  eta: 0:09:43  lr: 0.000013  loss: 3.2919 (3.4177)  time: 0.4611  data: 0.0005  max mem: 19734
Epoch: [44]  [ 190/1251]  eta: 0:09:34  lr: 0.000013  loss: 3.5015 (3.4227)  time: 0.4686  data: 0.0004  max mem: 19734
Epoch: [44]  [ 200/1251]  eta: 0:09:25  lr: 0.000013  loss: 3.2664 (3.4085)  time: 0.4709  data: 0.0004  max mem: 19734
Epoch: [44]  [ 210/1251]  eta: 0:09:15  lr: 0.000013  loss: 3.3149 (3.4147)  time: 0.4620  data: 0.0004  max mem: 19734
Epoch: [44]  [ 220/1251]  eta: 0:09:07  lr: 0.000013  loss: 3.3907 (3.4031)  time: 0.4615  data: 0.0005  max mem: 19734
Epoch: [44]  [ 230/1251]  eta: 0:08:58  lr: 0.000013  loss: 3.3178 (3.4051)  time: 0.4602  data: 0.0005  max mem: 19734
Epoch: [44]  [ 240/1251]  eta: 0:08:50  lr: 0.000013  loss: 3.5473 (3.4026)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [44]  [ 250/1251]  eta: 0:08:44  lr: 0.000013  loss: 3.5077 (3.3988)  time: 0.4761  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3566, ratio_loss=0.0048, pruning_loss=0.1206, mse_loss=0.4170
Epoch: [44]  [ 260/1251]  eta: 0:08:36  lr: 0.000013  loss: 3.5089 (3.4033)  time: 0.4792  data: 0.0005  max mem: 19734
Epoch: [44]  [ 270/1251]  eta: 0:08:30  lr: 0.000013  loss: 3.5089 (3.4013)  time: 0.4790  data: 0.0005  max mem: 19734
Epoch: [44]  [ 280/1251]  eta: 0:08:22  lr: 0.000013  loss: 3.4384 (3.3995)  time: 0.4744  data: 0.0005  max mem: 19734
Epoch: [44]  [ 290/1251]  eta: 0:08:16  lr: 0.000013  loss: 3.6319 (3.4060)  time: 0.4611  data: 0.0006  max mem: 19734
Epoch: [44]  [ 300/1251]  eta: 0:08:08  lr: 0.000013  loss: 3.6633 (3.4108)  time: 0.4618  data: 0.0006  max mem: 19734
Epoch: [44]  [ 310/1251]  eta: 0:08:02  lr: 0.000013  loss: 3.5261 (3.4128)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [44]  [ 320/1251]  eta: 0:07:55  lr: 0.000013  loss: 3.5322 (3.4141)  time: 0.4530  data: 0.0004  max mem: 19734
Epoch: [44]  [ 330/1251]  eta: 0:07:48  lr: 0.000013  loss: 3.6104 (3.4210)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [44]  [ 340/1251]  eta: 0:07:42  lr: 0.000013  loss: 3.4760 (3.4175)  time: 0.4684  data: 0.0004  max mem: 19734
Epoch: [44]  [ 350/1251]  eta: 0:07:36  lr: 0.000013  loss: 3.4048 (3.4172)  time: 0.4677  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4426, ratio_loss=0.0049, pruning_loss=0.1198, mse_loss=0.4223
Epoch: [44]  [ 360/1251]  eta: 0:07:29  lr: 0.000013  loss: 3.3557 (3.4121)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [44]  [ 370/1251]  eta: 0:07:23  lr: 0.000013  loss: 3.3557 (3.4181)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [44]  [ 380/1251]  eta: 0:07:17  lr: 0.000013  loss: 3.6305 (3.4223)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [44]  [ 390/1251]  eta: 0:07:12  lr: 0.000013  loss: 3.6030 (3.4222)  time: 0.4697  data: 0.0004  max mem: 19734
Epoch: [44]  [ 400/1251]  eta: 0:07:06  lr: 0.000013  loss: 3.4519 (3.4224)  time: 0.4815  data: 0.0004  max mem: 19734
Epoch: [44]  [ 410/1251]  eta: 0:07:00  lr: 0.000013  loss: 3.4669 (3.4224)  time: 0.4717  data: 0.0004  max mem: 19734
Epoch: [44]  [ 420/1251]  eta: 0:06:55  lr: 0.000013  loss: 3.6811 (3.4209)  time: 0.4708  data: 0.0004  max mem: 19734
Epoch: [44]  [ 430/1251]  eta: 0:06:49  lr: 0.000013  loss: 3.5474 (3.4192)  time: 0.4748  data: 0.0005  max mem: 19734
Epoch: [44]  [ 440/1251]  eta: 0:06:44  lr: 0.000013  loss: 3.6873 (3.4253)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [44]  [ 450/1251]  eta: 0:06:38  lr: 0.000013  loss: 3.6309 (3.4246)  time: 0.4562  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4237, ratio_loss=0.0047, pruning_loss=0.1183, mse_loss=0.4280
Epoch: [44]  [ 460/1251]  eta: 0:06:32  lr: 0.000013  loss: 3.3403 (3.4171)  time: 0.4561  data: 0.0005  max mem: 19734
Epoch: [44]  [ 470/1251]  eta: 0:06:26  lr: 0.000013  loss: 3.4810 (3.4215)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [44]  [ 480/1251]  eta: 0:06:21  lr: 0.000013  loss: 3.5362 (3.4236)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [44]  [ 490/1251]  eta: 0:06:16  lr: 0.000013  loss: 3.5516 (3.4258)  time: 0.4658  data: 0.0005  max mem: 19734
Epoch: [44]  [ 500/1251]  eta: 0:06:10  lr: 0.000013  loss: 3.5516 (3.4228)  time: 0.4669  data: 0.0005  max mem: 19734
Epoch: [44]  [ 510/1251]  eta: 0:06:05  lr: 0.000013  loss: 3.5687 (3.4240)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [44]  [ 520/1251]  eta: 0:05:59  lr: 0.000013  loss: 3.6748 (3.4289)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [44]  [ 530/1251]  eta: 0:05:54  lr: 0.000013  loss: 3.6748 (3.4250)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [44]  [ 540/1251]  eta: 0:05:49  lr: 0.000013  loss: 3.3328 (3.4263)  time: 0.4749  data: 0.0004  max mem: 19734
Epoch: [44]  [ 550/1251]  eta: 0:05:44  lr: 0.000013  loss: 3.6047 (3.4271)  time: 0.4826  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4347, ratio_loss=0.0046, pruning_loss=0.1192, mse_loss=0.4090
Epoch: [44]  [ 560/1251]  eta: 0:05:39  lr: 0.000013  loss: 3.5950 (3.4294)  time: 0.4788  data: 0.0005  max mem: 19734
Epoch: [44]  [ 570/1251]  eta: 0:05:33  lr: 0.000013  loss: 3.4710 (3.4290)  time: 0.4706  data: 0.0005  max mem: 19734
Epoch: [44]  [ 580/1251]  eta: 0:05:28  lr: 0.000013  loss: 3.4679 (3.4306)  time: 0.4626  data: 0.0004  max mem: 19734
Epoch: [44]  [ 590/1251]  eta: 0:05:23  lr: 0.000013  loss: 3.5477 (3.4333)  time: 0.4643  data: 0.0004  max mem: 19734
Epoch: [44]  [ 600/1251]  eta: 0:05:18  lr: 0.000013  loss: 3.4655 (3.4307)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [44]  [ 610/1251]  eta: 0:05:12  lr: 0.000013  loss: 3.2600 (3.4258)  time: 0.4523  data: 0.0005  max mem: 19734
Epoch: [44]  [ 620/1251]  eta: 0:05:07  lr: 0.000013  loss: 3.2600 (3.4238)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [44]  [ 630/1251]  eta: 0:05:02  lr: 0.000013  loss: 3.4190 (3.4216)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [44]  [ 640/1251]  eta: 0:04:57  lr: 0.000013  loss: 3.3957 (3.4200)  time: 0.4639  data: 0.0005  max mem: 19734
Epoch: [44]  [ 650/1251]  eta: 0:04:52  lr: 0.000013  loss: 3.4447 (3.4220)  time: 0.4637  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3699, ratio_loss=0.0047, pruning_loss=0.1205, mse_loss=0.4141
Epoch: [44]  [ 660/1251]  eta: 0:04:47  lr: 0.000013  loss: 3.5868 (3.4261)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [44]  [ 670/1251]  eta: 0:04:42  lr: 0.000013  loss: 3.5025 (3.4216)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [44]  [ 680/1251]  eta: 0:04:37  lr: 0.000013  loss: 3.1539 (3.4198)  time: 0.4765  data: 0.0005  max mem: 19734
Epoch: [44]  [ 690/1251]  eta: 0:04:32  lr: 0.000013  loss: 3.4016 (3.4185)  time: 0.4872  data: 0.0005  max mem: 19734
Epoch: [44]  [ 700/1251]  eta: 0:04:27  lr: 0.000013  loss: 3.3210 (3.4164)  time: 0.4794  data: 0.0005  max mem: 19734
Epoch: [44]  [ 710/1251]  eta: 0:04:22  lr: 0.000013  loss: 3.5691 (3.4204)  time: 0.4696  data: 0.0005  max mem: 19734
Epoch: [44]  [ 720/1251]  eta: 0:04:17  lr: 0.000013  loss: 3.5691 (3.4166)  time: 0.4659  data: 0.0004  max mem: 19734
Epoch: [44]  [ 730/1251]  eta: 0:04:12  lr: 0.000013  loss: 3.5135 (3.4152)  time: 0.4651  data: 0.0004  max mem: 19734
Epoch: [44]  [ 740/1251]  eta: 0:04:07  lr: 0.000013  loss: 3.5462 (3.4149)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [44]  [ 750/1251]  eta: 0:04:02  lr: 0.000013  loss: 3.4757 (3.4143)  time: 0.4603  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3216, ratio_loss=0.0050, pruning_loss=0.1208, mse_loss=0.4153
Epoch: [44]  [ 760/1251]  eta: 0:03:57  lr: 0.000013  loss: 3.4073 (3.4116)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [44]  [ 770/1251]  eta: 0:03:52  lr: 0.000013  loss: 3.2412 (3.4116)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [44]  [ 780/1251]  eta: 0:03:47  lr: 0.000013  loss: 3.2282 (3.4080)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [44]  [ 790/1251]  eta: 0:03:42  lr: 0.000013  loss: 3.1583 (3.4059)  time: 0.4650  data: 0.0004  max mem: 19734
Epoch: [44]  [ 800/1251]  eta: 0:03:37  lr: 0.000013  loss: 3.1914 (3.4058)  time: 0.4651  data: 0.0004  max mem: 19734
Epoch: [44]  [ 810/1251]  eta: 0:03:32  lr: 0.000013  loss: 3.4727 (3.4023)  time: 0.4556  data: 0.0008  max mem: 19734
Epoch: [44]  [ 820/1251]  eta: 0:03:27  lr: 0.000013  loss: 3.6114 (3.4071)  time: 0.4616  data: 0.0007  max mem: 19734
Epoch: [44]  [ 830/1251]  eta: 0:03:22  lr: 0.000013  loss: 3.6114 (3.4083)  time: 0.4814  data: 0.0005  max mem: 19734
Epoch: [44]  [ 840/1251]  eta: 0:03:17  lr: 0.000013  loss: 3.4425 (3.4055)  time: 0.4742  data: 0.0005  max mem: 19734
Epoch: [44]  [ 850/1251]  eta: 0:03:13  lr: 0.000013  loss: 3.4394 (3.4064)  time: 0.4689  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3503, ratio_loss=0.0047, pruning_loss=0.1199, mse_loss=0.4055
Epoch: [44]  [ 860/1251]  eta: 0:03:08  lr: 0.000013  loss: 3.4176 (3.4031)  time: 0.4675  data: 0.0004  max mem: 19734
Epoch: [44]  [ 870/1251]  eta: 0:03:03  lr: 0.000013  loss: 3.4421 (3.4041)  time: 0.4620  data: 0.0004  max mem: 19734
Epoch: [44]  [ 880/1251]  eta: 0:02:58  lr: 0.000013  loss: 3.4372 (3.4039)  time: 0.4633  data: 0.0004  max mem: 19734
Epoch: [44]  [ 890/1251]  eta: 0:02:53  lr: 0.000013  loss: 3.5596 (3.4070)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [44]  [ 900/1251]  eta: 0:02:48  lr: 0.000013  loss: 3.5548 (3.4049)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [44]  [ 910/1251]  eta: 0:02:43  lr: 0.000013  loss: 3.4818 (3.4068)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [44]  [ 920/1251]  eta: 0:02:38  lr: 0.000013  loss: 3.6873 (3.4085)  time: 0.4550  data: 0.0005  max mem: 19734
Epoch: [44]  [ 930/1251]  eta: 0:02:33  lr: 0.000013  loss: 3.7375 (3.4085)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [44]  [ 940/1251]  eta: 0:02:29  lr: 0.000013  loss: 3.6003 (3.4099)  time: 0.4661  data: 0.0005  max mem: 19734
Epoch: [44]  [ 950/1251]  eta: 0:02:24  lr: 0.000013  loss: 3.6115 (3.4120)  time: 0.4644  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4257, ratio_loss=0.0050, pruning_loss=0.1197, mse_loss=0.4127
Epoch: [44]  [ 960/1251]  eta: 0:02:19  lr: 0.000013  loss: 3.5447 (3.4117)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [44]  [ 970/1251]  eta: 0:02:14  lr: 0.000013  loss: 3.4319 (3.4133)  time: 0.4742  data: 0.0004  max mem: 19734
Epoch: [44]  [ 980/1251]  eta: 0:02:09  lr: 0.000013  loss: 3.4319 (3.4139)  time: 0.4845  data: 0.0004  max mem: 19734
Epoch: [44]  [ 990/1251]  eta: 0:02:05  lr: 0.000013  loss: 3.3179 (3.4133)  time: 0.4816  data: 0.0004  max mem: 19734
Epoch: [44]  [1000/1251]  eta: 0:02:00  lr: 0.000013  loss: 3.4562 (3.4135)  time: 0.4716  data: 0.0006  max mem: 19734
Epoch: [44]  [1010/1251]  eta: 0:01:55  lr: 0.000013  loss: 3.5686 (3.4140)  time: 0.4640  data: 0.0006  max mem: 19734
Epoch: [44]  [1020/1251]  eta: 0:01:50  lr: 0.000013  loss: 3.6531 (3.4151)  time: 0.4630  data: 0.0004  max mem: 19734
Epoch: [44]  [1030/1251]  eta: 0:01:45  lr: 0.000013  loss: 3.5407 (3.4129)  time: 0.4524  data: 0.0005  max mem: 19734
Epoch: [44]  [1040/1251]  eta: 0:01:40  lr: 0.000013  loss: 3.2775 (3.4122)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [44]  [1050/1251]  eta: 0:01:36  lr: 0.000013  loss: 3.2233 (3.4108)  time: 0.4561  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3774, ratio_loss=0.0048, pruning_loss=0.1210, mse_loss=0.4208
Epoch: [44]  [1060/1251]  eta: 0:01:31  lr: 0.000013  loss: 3.3763 (3.4119)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [44]  [1070/1251]  eta: 0:01:26  lr: 0.000013  loss: 3.4128 (3.4118)  time: 0.4538  data: 0.0006  max mem: 19734
Epoch: [44]  [1080/1251]  eta: 0:01:21  lr: 0.000013  loss: 3.2623 (3.4093)  time: 0.4546  data: 0.0006  max mem: 19734
Epoch: [44]  [1090/1251]  eta: 0:01:16  lr: 0.000013  loss: 3.2637 (3.4082)  time: 0.4652  data: 0.0004  max mem: 19734
Epoch: [44]  [1100/1251]  eta: 0:01:12  lr: 0.000013  loss: 3.5315 (3.4086)  time: 0.4646  data: 0.0004  max mem: 19734
Epoch: [44]  [1110/1251]  eta: 0:01:07  lr: 0.000013  loss: 3.2534 (3.4074)  time: 0.4649  data: 0.0005  max mem: 19734
Epoch: [44]  [1120/1251]  eta: 0:01:02  lr: 0.000013  loss: 3.2263 (3.4064)  time: 0.4913  data: 0.0004  max mem: 19734
Epoch: [44]  [1130/1251]  eta: 0:00:57  lr: 0.000013  loss: 3.3774 (3.4072)  time: 0.4794  data: 0.0004  max mem: 19734
Epoch: [44]  [1140/1251]  eta: 0:00:52  lr: 0.000013  loss: 3.3453 (3.4061)  time: 0.4682  data: 0.0004  max mem: 19734
Epoch: [44]  [1150/1251]  eta: 0:00:48  lr: 0.000013  loss: 3.5353 (3.4080)  time: 0.4793  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3401, ratio_loss=0.0048, pruning_loss=0.1209, mse_loss=0.4156
Epoch: [44]  [1160/1251]  eta: 0:00:43  lr: 0.000013  loss: 3.5855 (3.4063)  time: 0.4638  data: 0.0005  max mem: 19734
Epoch: [44]  [1170/1251]  eta: 0:00:38  lr: 0.000013  loss: 3.5855 (3.4067)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [44]  [1180/1251]  eta: 0:00:33  lr: 0.000013  loss: 3.5980 (3.4054)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [44]  [1190/1251]  eta: 0:00:29  lr: 0.000013  loss: 3.5774 (3.4057)  time: 0.4527  data: 0.0007  max mem: 19734
Epoch: [44]  [1200/1251]  eta: 0:00:24  lr: 0.000013  loss: 3.6645 (3.4076)  time: 0.4515  data: 0.0005  max mem: 19734
Epoch: [44]  [1210/1251]  eta: 0:00:19  lr: 0.000013  loss: 3.6645 (3.4080)  time: 0.4478  data: 0.0001  max mem: 19734
Epoch: [44]  [1220/1251]  eta: 0:00:14  lr: 0.000013  loss: 3.5558 (3.4072)  time: 0.4473  data: 0.0001  max mem: 19734
Epoch: [44]  [1230/1251]  eta: 0:00:09  lr: 0.000013  loss: 3.5954 (3.4097)  time: 0.4487  data: 0.0001  max mem: 19734
Epoch: [44]  [1240/1251]  eta: 0:00:05  lr: 0.000013  loss: 3.6258 (3.4102)  time: 0.4556  data: 0.0001  max mem: 19734
Epoch: [44]  [1250/1251]  eta: 0:00:00  lr: 0.000013  loss: 3.4616 (3.4088)  time: 0.4558  data: 0.0001  max mem: 19734
Epoch: [44] Total time: 0:09:55 (0.4757 s / it)
Averaged stats: lr: 0.000013  loss: 3.4616 (3.4037)
Test:  [  0/261]  eta: 1:15:23  loss: 0.6996 (0.6996)  acc1: 82.2917 (82.2917)  acc5: 95.8333 (95.8333)  time: 17.3311  data: 17.1275  max mem: 19734
Test:  [ 10/261]  eta: 0:08:34  loss: 0.6968 (0.7140)  acc1: 83.3333 (83.9962)  acc5: 96.8750 (96.3068)  time: 2.0488  data: 1.9083  max mem: 19734
Test:  [ 20/261]  eta: 0:05:44  loss: 0.9052 (0.8839)  acc1: 78.6458 (78.8690)  acc5: 93.7500 (94.7917)  time: 0.6350  data: 0.4968  max mem: 19734
Test:  [ 30/261]  eta: 0:04:00  loss: 0.8053 (0.8050)  acc1: 82.8125 (81.6196)  acc5: 93.7500 (95.1781)  time: 0.4899  data: 0.3120  max mem: 19734
Test:  [ 40/261]  eta: 0:03:05  loss: 0.5795 (0.7682)  acc1: 86.9792 (82.8125)  acc5: 96.3542 (95.4776)  time: 0.2191  data: 0.0155  max mem: 19734
Test:  [ 50/261]  eta: 0:02:41  loss: 0.8865 (0.8288)  acc1: 78.6458 (81.1172)  acc5: 95.3125 (95.0674)  time: 0.3360  data: 0.1444  max mem: 19734
Test:  [ 60/261]  eta: 0:02:14  loss: 0.9618 (0.8378)  acc1: 76.0417 (80.7804)  acc5: 94.7917 (95.1332)  time: 0.3169  data: 0.1462  max mem: 19734
Test:  [ 70/261]  eta: 0:01:53  loss: 0.9289 (0.8405)  acc1: 77.6042 (80.4577)  acc5: 95.8333 (95.3639)  time: 0.1612  data: 0.0147  max mem: 19734
Test:  [ 80/261]  eta: 0:01:37  loss: 0.8385 (0.8405)  acc1: 78.6458 (80.5491)  acc5: 96.8750 (95.4475)  time: 0.1352  data: 0.0190  max mem: 19734
Test:  [ 90/261]  eta: 0:01:42  loss: 0.7844 (0.8264)  acc1: 82.2917 (80.9180)  acc5: 95.8333 (95.5357)  time: 0.6193  data: 0.4590  max mem: 19734
Test:  [100/261]  eta: 0:01:30  loss: 0.8046 (0.8303)  acc1: 82.8125 (80.8736)  acc5: 95.8333 (95.6013)  time: 0.6548  data: 0.4558  max mem: 19734
Test:  [110/261]  eta: 0:01:19  loss: 0.8687 (0.8555)  acc1: 77.0833 (80.3256)  acc5: 94.2708 (95.3031)  time: 0.1752  data: 0.0201  max mem: 19734
Test:  [120/261]  eta: 0:01:11  loss: 1.1919 (0.8959)  acc1: 72.3958 (79.4120)  acc5: 90.6250 (94.8089)  time: 0.2529  data: 0.1066  max mem: 19734
Test:  [130/261]  eta: 0:01:02  loss: 1.3861 (0.9419)  acc1: 67.1875 (78.4073)  acc5: 87.5000 (94.2032)  time: 0.2388  data: 0.1070  max mem: 19734
Test:  [140/261]  eta: 0:00:55  loss: 1.3086 (0.9669)  acc1: 68.7500 (77.7556)  acc5: 90.6250 (93.9458)  time: 0.1297  data: 0.0112  max mem: 19734
Test:  [150/261]  eta: 0:00:51  loss: 1.1594 (0.9729)  acc1: 71.8750 (77.7318)  acc5: 91.6667 (93.8017)  time: 0.3683  data: 0.2362  max mem: 19734
Test:  [160/261]  eta: 0:00:45  loss: 0.9731 (0.9934)  acc1: 77.0833 (77.3907)  acc5: 91.6667 (93.5106)  time: 0.4404  data: 0.2778  max mem: 19734
Test:  [170/261]  eta: 0:00:39  loss: 1.2603 (1.0241)  acc1: 65.6250 (76.6478)  acc5: 88.0208 (93.1652)  time: 0.2157  data: 0.0500  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4287 (1.0416)  acc1: 65.6250 (76.2489)  acc5: 87.5000 (92.9961)  time: 0.4830  data: 0.3074  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: 1.3142 (1.0543)  acc1: 67.7083 (76.0199)  acc5: 90.6250 (92.8229)  time: 0.5110  data: 0.3558  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3061 (1.0693)  acc1: 71.3542 (75.7074)  acc5: 89.5833 (92.5917)  time: 0.1551  data: 0.0568  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.3720 (1.0829)  acc1: 68.7500 (75.4024)  acc5: 89.0625 (92.4097)  time: 0.0897  data: 0.0130  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4062 (1.1025)  acc1: 66.6667 (74.9411)  acc5: 89.0625 (92.1946)  time: 0.1159  data: 0.0489  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4047 (1.1107)  acc1: 66.6667 (74.7137)  acc5: 89.0625 (92.1041)  time: 0.1036  data: 0.0367  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.2894 (1.1200)  acc1: 67.1875 (74.4684)  acc5: 90.6250 (92.0254)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0573 (1.1130)  acc1: 75.0000 (74.6410)  acc5: 92.7083 (92.1273)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9346 (1.1125)  acc1: 79.1667 (74.6580)  acc5: 95.3125 (92.2040)  time: 0.0603  data: 0.0002  max mem: 19734
Test: Total time: 0:01:30 (0.3481 s / it)
* Acc@1 74.658 Acc@5 92.204 loss 1.113
Accuracy of the network on the 50000 test images: 74.7%
Max accuracy: 74.76%
Epoch: [45]  [   0/1251]  eta: 5:15:05  lr: 0.000012  loss: 4.0996 (4.0996)  time: 15.1127  data: 11.9873  max mem: 19734
loss info: cls_loss=3.4153, ratio_loss=0.0048, pruning_loss=0.1204, mse_loss=0.4117
Epoch: [45]  [  10/1251]  eta: 0:43:58  lr: 0.000012  loss: 3.3349 (3.5676)  time: 2.1260  data: 1.2225  max mem: 19734
Epoch: [45]  [  20/1251]  eta: 0:27:41  lr: 0.000012  loss: 3.6635 (3.6231)  time: 0.6619  data: 0.0732  max mem: 19734
Epoch: [45]  [  30/1251]  eta: 0:21:44  lr: 0.000012  loss: 3.7990 (3.6404)  time: 0.4862  data: 0.0004  max mem: 19734
Epoch: [45]  [  40/1251]  eta: 0:18:35  lr: 0.000012  loss: 3.6757 (3.6120)  time: 0.4700  data: 0.0003  max mem: 19734
Epoch: [45]  [  50/1251]  eta: 0:16:43  lr: 0.000012  loss: 3.4292 (3.5247)  time: 0.4762  data: 0.0004  max mem: 19734
Epoch: [45]  [  60/1251]  eta: 0:15:22  lr: 0.000012  loss: 3.4292 (3.5279)  time: 0.4761  data: 0.0004  max mem: 19734
Epoch: [45]  [  70/1251]  eta: 0:14:22  lr: 0.000012  loss: 3.4317 (3.5215)  time: 0.4621  data: 0.0005  max mem: 19734
Epoch: [45]  [  80/1251]  eta: 0:13:36  lr: 0.000012  loss: 3.5907 (3.5190)  time: 0.4610  data: 0.0005  max mem: 19734
Epoch: [45]  [  90/1251]  eta: 0:12:59  lr: 0.000012  loss: 3.7149 (3.5510)  time: 0.4618  data: 0.0005  max mem: 19734
Epoch: [45]  [ 100/1251]  eta: 0:12:31  lr: 0.000012  loss: 3.6435 (3.5287)  time: 0.4712  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5056, ratio_loss=0.0049, pruning_loss=0.1170, mse_loss=0.4005
Epoch: [45]  [ 110/1251]  eta: 0:12:05  lr: 0.000012  loss: 3.5184 (3.5377)  time: 0.4713  data: 0.0005  max mem: 19734
Epoch: [45]  [ 120/1251]  eta: 0:11:42  lr: 0.000012  loss: 3.6924 (3.5208)  time: 0.4609  data: 0.0005  max mem: 19734
Epoch: [45]  [ 130/1251]  eta: 0:11:22  lr: 0.000012  loss: 3.5885 (3.5270)  time: 0.4584  data: 0.0004  max mem: 19734
Epoch: [45]  [ 140/1251]  eta: 0:11:05  lr: 0.000012  loss: 3.4416 (3.5091)  time: 0.4681  data: 0.0005  max mem: 19734
Epoch: [45]  [ 150/1251]  eta: 0:10:51  lr: 0.000012  loss: 3.3943 (3.5020)  time: 0.4829  data: 0.0005  max mem: 19734
Epoch: [45]  [ 160/1251]  eta: 0:10:38  lr: 0.000012  loss: 3.4556 (3.4858)  time: 0.4831  data: 0.0005  max mem: 19734
Epoch: [45]  [ 170/1251]  eta: 0:10:26  lr: 0.000012  loss: 3.4883 (3.4762)  time: 0.4851  data: 0.0004  max mem: 19734
Epoch: [45]  [ 180/1251]  eta: 0:10:13  lr: 0.000012  loss: 3.5450 (3.4682)  time: 0.4733  data: 0.0005  max mem: 19734
Epoch: [45]  [ 190/1251]  eta: 0:10:02  lr: 0.000012  loss: 3.6563 (3.4835)  time: 0.4692  data: 0.0004  max mem: 19734
Epoch: [45]  [ 200/1251]  eta: 0:09:50  lr: 0.000012  loss: 3.7443 (3.4725)  time: 0.4701  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3850, ratio_loss=0.0046, pruning_loss=0.1193, mse_loss=0.4243
Epoch: [45]  [ 210/1251]  eta: 0:09:40  lr: 0.000012  loss: 3.4534 (3.4734)  time: 0.4567  data: 0.0005  max mem: 19734
Epoch: [45]  [ 220/1251]  eta: 0:09:29  lr: 0.000012  loss: 3.4666 (3.4725)  time: 0.4573  data: 0.0005  max mem: 19734
Epoch: [45]  [ 230/1251]  eta: 0:09:20  lr: 0.000012  loss: 3.5230 (3.4757)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [45]  [ 240/1251]  eta: 0:09:12  lr: 0.000012  loss: 3.4961 (3.4610)  time: 0.4722  data: 0.0004  max mem: 19734
Epoch: [45]  [ 250/1251]  eta: 0:09:03  lr: 0.000012  loss: 3.4569 (3.4558)  time: 0.4698  data: 0.0004  max mem: 19734
Epoch: [45]  [ 260/1251]  eta: 0:08:54  lr: 0.000012  loss: 3.4953 (3.4601)  time: 0.4561  data: 0.0007  max mem: 19734
Epoch: [45]  [ 270/1251]  eta: 0:08:46  lr: 0.000012  loss: 3.2715 (3.4560)  time: 0.4573  data: 0.0007  max mem: 19734
Epoch: [45]  [ 280/1251]  eta: 0:08:37  lr: 0.000012  loss: 3.0973 (3.4460)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [45]  [ 290/1251]  eta: 0:08:31  lr: 0.000012  loss: 3.3814 (3.4422)  time: 0.4758  data: 0.0004  max mem: 19734
Epoch: [45]  [ 300/1251]  eta: 0:08:25  lr: 0.000012  loss: 3.6479 (3.4438)  time: 0.5003  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3701, ratio_loss=0.0045, pruning_loss=0.1195, mse_loss=0.4171
Epoch: [45]  [ 310/1251]  eta: 0:08:18  lr: 0.000012  loss: 3.6737 (3.4462)  time: 0.4905  data: 0.0005  max mem: 19734
Epoch: [45]  [ 320/1251]  eta: 0:08:11  lr: 0.000012  loss: 3.7645 (3.4504)  time: 0.4724  data: 0.0005  max mem: 19734
Epoch: [45]  [ 330/1251]  eta: 0:08:04  lr: 0.000012  loss: 3.5573 (3.4454)  time: 0.4702  data: 0.0005  max mem: 19734
Epoch: [45]  [ 340/1251]  eta: 0:07:57  lr: 0.000012  loss: 3.4643 (3.4453)  time: 0.4620  data: 0.0004  max mem: 19734
Epoch: [45]  [ 350/1251]  eta: 0:07:49  lr: 0.000012  loss: 3.6068 (3.4448)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [45]  [ 360/1251]  eta: 0:07:43  lr: 0.000012  loss: 3.6329 (3.4462)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [45]  [ 370/1251]  eta: 0:07:36  lr: 0.000012  loss: 3.6014 (3.4461)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [45]  [ 380/1251]  eta: 0:07:29  lr: 0.000012  loss: 3.5760 (3.4490)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [45]  [ 390/1251]  eta: 0:07:23  lr: 0.000012  loss: 3.6108 (3.4519)  time: 0.4688  data: 0.0004  max mem: 19734
Epoch: [45]  [ 400/1251]  eta: 0:07:17  lr: 0.000012  loss: 3.6108 (3.4513)  time: 0.4688  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4473, ratio_loss=0.0048, pruning_loss=0.1188, mse_loss=0.4158
Epoch: [45]  [ 410/1251]  eta: 0:07:11  lr: 0.000012  loss: 3.4596 (3.4487)  time: 0.4559  data: 0.0004  max mem: 19734
Epoch: [45]  [ 420/1251]  eta: 0:07:04  lr: 0.000012  loss: 3.4596 (3.4478)  time: 0.4557  data: 0.0006  max mem: 19734
Epoch: [45]  [ 430/1251]  eta: 0:06:58  lr: 0.000012  loss: 3.6422 (3.4494)  time: 0.4620  data: 0.0007  max mem: 19734
Epoch: [45]  [ 440/1251]  eta: 0:06:53  lr: 0.000012  loss: 3.5853 (3.4463)  time: 0.4817  data: 0.0005  max mem: 19734
Epoch: [45]  [ 450/1251]  eta: 0:06:48  lr: 0.000012  loss: 3.3711 (3.4431)  time: 0.4938  data: 0.0005  max mem: 19734
Epoch: [45]  [ 460/1251]  eta: 0:06:42  lr: 0.000012  loss: 3.2594 (3.4371)  time: 0.4866  data: 0.0004  max mem: 19734
Epoch: [45]  [ 470/1251]  eta: 0:06:36  lr: 0.000012  loss: 3.3312 (3.4374)  time: 0.4694  data: 0.0004  max mem: 19734
Epoch: [45]  [ 480/1251]  eta: 0:06:30  lr: 0.000012  loss: 3.5730 (3.4384)  time: 0.4673  data: 0.0004  max mem: 19734
Epoch: [45]  [ 490/1251]  eta: 0:06:25  lr: 0.000012  loss: 3.5682 (3.4374)  time: 0.4662  data: 0.0004  max mem: 19734
Epoch: [45]  [ 500/1251]  eta: 0:06:19  lr: 0.000012  loss: 3.5848 (3.4387)  time: 0.4562  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3661, ratio_loss=0.0050, pruning_loss=0.1191, mse_loss=0.4175
Epoch: [45]  [ 510/1251]  eta: 0:06:13  lr: 0.000012  loss: 3.6223 (3.4385)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [45]  [ 520/1251]  eta: 0:06:07  lr: 0.000012  loss: 3.5038 (3.4353)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [45]  [ 530/1251]  eta: 0:06:02  lr: 0.000012  loss: 3.5717 (3.4364)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [45]  [ 540/1251]  eta: 0:05:56  lr: 0.000012  loss: 3.6642 (3.4387)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [45]  [ 550/1251]  eta: 0:05:50  lr: 0.000012  loss: 3.6570 (3.4414)  time: 0.4602  data: 0.0004  max mem: 19734
Epoch: [45]  [ 560/1251]  eta: 0:05:45  lr: 0.000012  loss: 3.6260 (3.4411)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [45]  [ 570/1251]  eta: 0:05:39  lr: 0.000012  loss: 3.4180 (3.4378)  time: 0.4515  data: 0.0006  max mem: 19734
Epoch: [45]  [ 580/1251]  eta: 0:05:34  lr: 0.000012  loss: 3.2408 (3.4348)  time: 0.4812  data: 0.0006  max mem: 19734
Epoch: [45]  [ 590/1251]  eta: 0:05:29  lr: 0.000012  loss: 3.5600 (3.4361)  time: 0.4940  data: 0.0004  max mem: 19734
Epoch: [45]  [ 600/1251]  eta: 0:05:24  lr: 0.000012  loss: 3.4554 (3.4316)  time: 0.4836  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3710, ratio_loss=0.0048, pruning_loss=0.1193, mse_loss=0.4029
Epoch: [45]  [ 610/1251]  eta: 0:05:19  lr: 0.000012  loss: 3.2958 (3.4325)  time: 0.4728  data: 0.0004  max mem: 19734
Epoch: [45]  [ 620/1251]  eta: 0:05:13  lr: 0.000012  loss: 3.5013 (3.4334)  time: 0.4624  data: 0.0004  max mem: 19734
Epoch: [45]  [ 630/1251]  eta: 0:05:08  lr: 0.000012  loss: 3.5013 (3.4324)  time: 0.4635  data: 0.0004  max mem: 19734
Epoch: [45]  [ 640/1251]  eta: 0:05:03  lr: 0.000012  loss: 3.3401 (3.4316)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [45]  [ 650/1251]  eta: 0:04:57  lr: 0.000012  loss: 3.5675 (3.4316)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [45]  [ 660/1251]  eta: 0:04:52  lr: 0.000012  loss: 3.4881 (3.4283)  time: 0.4532  data: 0.0005  max mem: 19734
Epoch: [45]  [ 670/1251]  eta: 0:04:47  lr: 0.000012  loss: 3.4572 (3.4284)  time: 0.4514  data: 0.0005  max mem: 19734
Epoch: [45]  [ 680/1251]  eta: 0:04:41  lr: 0.000012  loss: 3.4279 (3.4270)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [45]  [ 690/1251]  eta: 0:04:36  lr: 0.000012  loss: 3.5950 (3.4296)  time: 0.4641  data: 0.0004  max mem: 19734
Epoch: [45]  [ 700/1251]  eta: 0:04:31  lr: 0.000012  loss: 3.5648 (3.4287)  time: 0.4628  data: 0.0003  max mem: 19734
loss info: cls_loss=3.3787, ratio_loss=0.0048, pruning_loss=0.1195, mse_loss=0.4112
Epoch: [45]  [ 710/1251]  eta: 0:04:26  lr: 0.000012  loss: 3.2564 (3.4262)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [45]  [ 720/1251]  eta: 0:04:21  lr: 0.000012  loss: 3.4620 (3.4260)  time: 0.4635  data: 0.0005  max mem: 19734
Epoch: [45]  [ 730/1251]  eta: 0:04:16  lr: 0.000012  loss: 3.5966 (3.4234)  time: 0.4884  data: 0.0006  max mem: 19734
Epoch: [45]  [ 740/1251]  eta: 0:04:11  lr: 0.000012  loss: 3.2753 (3.4199)  time: 0.4886  data: 0.0005  max mem: 19734
Epoch: [45]  [ 750/1251]  eta: 0:04:06  lr: 0.000012  loss: 3.5562 (3.4208)  time: 0.4713  data: 0.0004  max mem: 19734
Epoch: [45]  [ 760/1251]  eta: 0:04:01  lr: 0.000012  loss: 3.5673 (3.4228)  time: 0.4602  data: 0.0004  max mem: 19734
Epoch: [45]  [ 770/1251]  eta: 0:03:56  lr: 0.000012  loss: 3.6062 (3.4266)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [45]  [ 780/1251]  eta: 0:03:50  lr: 0.000012  loss: 3.5666 (3.4230)  time: 0.4621  data: 0.0004  max mem: 19734
Epoch: [45]  [ 790/1251]  eta: 0:03:45  lr: 0.000012  loss: 3.1735 (3.4202)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [45]  [ 800/1251]  eta: 0:03:40  lr: 0.000012  loss: 3.2731 (3.4203)  time: 0.4530  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3329, ratio_loss=0.0047, pruning_loss=0.1196, mse_loss=0.4138
Epoch: [45]  [ 810/1251]  eta: 0:03:35  lr: 0.000012  loss: 3.6855 (3.4215)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [45]  [ 820/1251]  eta: 0:03:30  lr: 0.000012  loss: 3.6215 (3.4208)  time: 0.4523  data: 0.0006  max mem: 19734
Epoch: [45]  [ 830/1251]  eta: 0:03:25  lr: 0.000012  loss: 3.6430 (3.4239)  time: 0.4523  data: 0.0006  max mem: 19734
Epoch: [45]  [ 840/1251]  eta: 0:03:20  lr: 0.000012  loss: 3.4470 (3.4199)  time: 0.4640  data: 0.0004  max mem: 19734
Epoch: [45]  [ 850/1251]  eta: 0:03:15  lr: 0.000012  loss: 3.2909 (3.4189)  time: 0.4661  data: 0.0006  max mem: 19734
Epoch: [45]  [ 860/1251]  eta: 0:03:10  lr: 0.000012  loss: 3.4714 (3.4196)  time: 0.4575  data: 0.0006  max mem: 19734
Epoch: [45]  [ 870/1251]  eta: 0:03:05  lr: 0.000012  loss: 3.3946 (3.4191)  time: 0.4768  data: 0.0004  max mem: 19734
Epoch: [45]  [ 880/1251]  eta: 0:03:00  lr: 0.000012  loss: 3.3946 (3.4175)  time: 0.4798  data: 0.0005  max mem: 19734
Epoch: [45]  [ 890/1251]  eta: 0:02:55  lr: 0.000012  loss: 3.0957 (3.4132)  time: 0.4725  data: 0.0005  max mem: 19734
Epoch: [45]  [ 900/1251]  eta: 0:02:50  lr: 0.000012  loss: 3.1225 (3.4119)  time: 0.4677  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3352, ratio_loss=0.0047, pruning_loss=0.1212, mse_loss=0.4226
Epoch: [45]  [ 910/1251]  eta: 0:02:45  lr: 0.000012  loss: 3.4497 (3.4146)  time: 0.4636  data: 0.0004  max mem: 19734
Epoch: [45]  [ 920/1251]  eta: 0:02:40  lr: 0.000012  loss: 3.4546 (3.4126)  time: 0.4628  data: 0.0004  max mem: 19734
Epoch: [45]  [ 930/1251]  eta: 0:02:35  lr: 0.000012  loss: 3.3235 (3.4109)  time: 0.4507  data: 0.0003  max mem: 19734
Epoch: [45]  [ 940/1251]  eta: 0:02:30  lr: 0.000012  loss: 3.3347 (3.4080)  time: 0.4510  data: 0.0004  max mem: 19734
Epoch: [45]  [ 950/1251]  eta: 0:02:25  lr: 0.000012  loss: 3.5139 (3.4050)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [45]  [ 960/1251]  eta: 0:02:21  lr: 0.000012  loss: 3.2405 (3.4042)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [45]  [ 970/1251]  eta: 0:02:16  lr: 0.000012  loss: 3.5502 (3.4054)  time: 0.4521  data: 0.0006  max mem: 19734
Epoch: [45]  [ 980/1251]  eta: 0:02:11  lr: 0.000012  loss: 3.5923 (3.4054)  time: 0.4527  data: 0.0007  max mem: 19734
Epoch: [45]  [ 990/1251]  eta: 0:02:06  lr: 0.000012  loss: 3.5205 (3.4051)  time: 0.4628  data: 0.0004  max mem: 19734
Epoch: [45]  [1000/1251]  eta: 0:02:01  lr: 0.000012  loss: 3.4053 (3.4032)  time: 0.4617  data: 0.0003  max mem: 19734
loss info: cls_loss=3.2845, ratio_loss=0.0048, pruning_loss=0.1213, mse_loss=0.4179
Epoch: [45]  [1010/1251]  eta: 0:01:56  lr: 0.000012  loss: 3.4053 (3.4041)  time: 0.4625  data: 0.0004  max mem: 19734
Epoch: [45]  [1020/1251]  eta: 0:01:51  lr: 0.000012  loss: 3.6568 (3.4070)  time: 0.4825  data: 0.0004  max mem: 19734
Epoch: [45]  [1030/1251]  eta: 0:01:46  lr: 0.000012  loss: 3.7323 (3.4104)  time: 0.4837  data: 0.0004  max mem: 19734
Epoch: [45]  [1040/1251]  eta: 0:01:41  lr: 0.000012  loss: 3.6743 (3.4087)  time: 0.4755  data: 0.0004  max mem: 19734
Epoch: [45]  [1050/1251]  eta: 0:01:37  lr: 0.000012  loss: 3.5508 (3.4090)  time: 0.4735  data: 0.0004  max mem: 19734
Epoch: [45]  [1060/1251]  eta: 0:01:32  lr: 0.000012  loss: 3.3860 (3.4072)  time: 0.4614  data: 0.0004  max mem: 19734
Epoch: [45]  [1070/1251]  eta: 0:01:27  lr: 0.000012  loss: 3.3075 (3.4060)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [45]  [1080/1251]  eta: 0:01:22  lr: 0.000012  loss: 3.3075 (3.4044)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [45]  [1090/1251]  eta: 0:01:17  lr: 0.000012  loss: 3.5708 (3.4056)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [45]  [1100/1251]  eta: 0:01:12  lr: 0.000012  loss: 3.5624 (3.4042)  time: 0.4538  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3988, ratio_loss=0.0048, pruning_loss=0.1173, mse_loss=0.4109
Epoch: [45]  [1110/1251]  eta: 0:01:07  lr: 0.000012  loss: 3.3708 (3.4025)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [45]  [1120/1251]  eta: 0:01:03  lr: 0.000012  loss: 3.4518 (3.4052)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [45]  [1130/1251]  eta: 0:00:58  lr: 0.000012  loss: 3.4702 (3.4035)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [45]  [1140/1251]  eta: 0:00:53  lr: 0.000012  loss: 3.4057 (3.4036)  time: 0.4629  data: 0.0004  max mem: 19734
Epoch: [45]  [1150/1251]  eta: 0:00:48  lr: 0.000012  loss: 3.4057 (3.4037)  time: 0.4613  data: 0.0008  max mem: 19734
Epoch: [45]  [1160/1251]  eta: 0:00:43  lr: 0.000012  loss: 3.5831 (3.4035)  time: 0.4706  data: 0.0008  max mem: 19734
Epoch: [45]  [1170/1251]  eta: 0:00:38  lr: 0.000012  loss: 3.3145 (3.4000)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [45]  [1180/1251]  eta: 0:00:34  lr: 0.000012  loss: 3.1917 (3.3991)  time: 0.4667  data: 0.0005  max mem: 19734
Epoch: [45]  [1190/1251]  eta: 0:00:29  lr: 0.000012  loss: 3.4693 (3.3984)  time: 0.4661  data: 0.0008  max mem: 19734
Epoch: [45]  [1200/1251]  eta: 0:00:24  lr: 0.000012  loss: 3.4693 (3.3978)  time: 0.4657  data: 0.0006  max mem: 19734
loss info: cls_loss=3.2957, ratio_loss=0.0045, pruning_loss=0.1208, mse_loss=0.4120
Epoch: [45]  [1210/1251]  eta: 0:00:19  lr: 0.000012  loss: 3.2635 (3.3966)  time: 0.4553  data: 0.0001  max mem: 19734
Epoch: [45]  [1220/1251]  eta: 0:00:14  lr: 0.000012  loss: 3.3454 (3.3974)  time: 0.4442  data: 0.0001  max mem: 19734
Epoch: [45]  [1230/1251]  eta: 0:00:10  lr: 0.000012  loss: 3.6216 (3.3960)  time: 0.4442  data: 0.0001  max mem: 19734
Epoch: [45]  [1240/1251]  eta: 0:00:05  lr: 0.000012  loss: 3.5119 (3.3974)  time: 0.4444  data: 0.0001  max mem: 19734
Epoch: [45]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 3.5009 (3.3970)  time: 0.4441  data: 0.0001  max mem: 19734
Epoch: [45] Total time: 0:09:59 (0.4796 s / it)
Averaged stats: lr: 0.000012  loss: 3.5009 (3.4005)
Test:  [  0/261]  eta: 2:17:11  loss: 0.7138 (0.7138)  acc1: 82.8125 (82.8125)  acc5: 94.7917 (94.7917)  time: 31.5367  data: 31.4055  max mem: 19734
Test:  [ 10/261]  eta: 0:15:55  loss: 0.7138 (0.7296)  acc1: 84.3750 (83.3807)  acc5: 96.8750 (96.3068)  time: 3.8052  data: 3.5545  max mem: 19734
Test:  [ 20/261]  eta: 0:08:22  loss: 0.9189 (0.8988)  acc1: 78.1250 (78.6706)  acc5: 94.2708 (94.7421)  time: 0.6128  data: 0.3894  max mem: 19734
Test:  [ 30/261]  eta: 0:05:43  loss: 0.8133 (0.8162)  acc1: 83.3333 (81.5524)  acc5: 94.2708 (95.1949)  time: 0.2150  data: 0.0104  max mem: 19734
Test:  [ 40/261]  eta: 0:04:38  loss: 0.5553 (0.7821)  acc1: 87.5000 (82.6601)  acc5: 96.3542 (95.4268)  time: 0.3977  data: 0.2437  max mem: 19734
Test:  [ 50/261]  eta: 0:03:37  loss: 0.9217 (0.8439)  acc1: 78.1250 (80.8517)  acc5: 94.7917 (95.0776)  time: 0.3201  data: 0.2421  max mem: 19734
Test:  [ 60/261]  eta: 0:02:57  loss: 0.9407 (0.8484)  acc1: 76.0417 (80.6609)  acc5: 94.7917 (95.1588)  time: 0.0998  data: 0.0096  max mem: 19734
Test:  [ 70/261]  eta: 0:02:40  loss: 0.9102 (0.8481)  acc1: 77.6042 (80.3330)  acc5: 96.3542 (95.3639)  time: 0.3514  data: 0.2418  max mem: 19734
Test:  [ 80/261]  eta: 0:02:17  loss: 0.8030 (0.8491)  acc1: 79.1667 (80.4527)  acc5: 96.8750 (95.4475)  time: 0.3939  data: 0.2534  max mem: 19734
Test:  [ 90/261]  eta: 0:01:58  loss: 0.8030 (0.8346)  acc1: 82.8125 (80.8036)  acc5: 96.8750 (95.5643)  time: 0.1850  data: 0.0233  max mem: 19734
Test:  [100/261]  eta: 0:01:52  loss: 0.8132 (0.8387)  acc1: 82.8125 (80.7085)  acc5: 95.3125 (95.6168)  time: 0.4493  data: 0.2668  max mem: 19734
Test:  [110/261]  eta: 0:01:38  loss: 0.8710 (0.8623)  acc1: 76.5625 (80.2318)  acc5: 94.7917 (95.2843)  time: 0.4572  data: 0.2683  max mem: 19734
Test:  [120/261]  eta: 0:01:27  loss: 1.1842 (0.9014)  acc1: 71.8750 (79.3733)  acc5: 89.0625 (94.7529)  time: 0.2316  data: 0.0647  max mem: 19734
Test:  [130/261]  eta: 0:01:21  loss: 1.3587 (0.9475)  acc1: 67.7083 (78.3874)  acc5: 87.5000 (94.1635)  time: 0.4341  data: 0.2592  max mem: 19734
Test:  [140/261]  eta: 0:01:10  loss: 1.3399 (0.9748)  acc1: 68.2292 (77.6965)  acc5: 88.5417 (93.8645)  time: 0.3572  data: 0.2059  max mem: 19734
Test:  [150/261]  eta: 0:01:01  loss: 1.2286 (0.9803)  acc1: 71.8750 (77.7042)  acc5: 90.6250 (93.7293)  time: 0.1219  data: 0.0061  max mem: 19734
Test:  [160/261]  eta: 0:00:52  loss: 0.9688 (0.9993)  acc1: 78.1250 (77.3939)  acc5: 92.1875 (93.4265)  time: 0.1011  data: 0.0054  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: 1.2953 (1.0285)  acc1: 66.6667 (76.6661)  acc5: 88.0208 (93.1104)  time: 0.1168  data: 0.0407  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4233 (1.0451)  acc1: 65.1042 (76.2805)  acc5: 89.0625 (92.9616)  time: 0.1051  data: 0.0378  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3573 (1.0576)  acc1: 67.7083 (76.0635)  acc5: 90.6250 (92.7901)  time: 0.0642  data: 0.0015  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3345 (1.0729)  acc1: 71.8750 (75.7618)  acc5: 89.0625 (92.5580)  time: 0.0641  data: 0.0014  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3945 (1.0869)  acc1: 71.3542 (75.4887)  acc5: 88.5417 (92.3455)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4038 (1.1062)  acc1: 68.2292 (75.0189)  acc5: 86.9792 (92.1286)  time: 0.0615  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.3959 (1.1146)  acc1: 66.1458 (74.7633)  acc5: 88.0208 (92.0590)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.2991 (1.1241)  acc1: 68.7500 (74.5354)  acc5: 91.1458 (91.9714)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0548 (1.1170)  acc1: 74.4792 (74.7199)  acc5: 93.2292 (92.0755)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9127 (1.1174)  acc1: 78.1250 (74.7200)  acc5: 94.7917 (92.1340)  time: 0.0599  data: 0.0001  max mem: 19734
Test: Total time: 0:01:31 (0.3511 s / it)
* Acc@1 74.720 Acc@5 92.134 loss 1.117
Accuracy of the network on the 50000 test images: 74.7%
Max accuracy: 74.76%
Epoch: [46]  [   0/1251]  eta: 4:26:57  lr: 0.000012  loss: 2.6340 (2.6340)  time: 12.8039  data: 11.8967  max mem: 19734
Epoch: [46]  [  10/1251]  eta: 0:38:47  lr: 0.000012  loss: 3.3299 (3.2208)  time: 1.8756  data: 1.3459  max mem: 19734
Epoch: [46]  [  20/1251]  eta: 0:24:38  lr: 0.000012  loss: 3.4201 (3.2430)  time: 0.6211  data: 0.1456  max mem: 19734
Epoch: [46]  [  30/1251]  eta: 0:19:34  lr: 0.000012  loss: 3.5356 (3.3184)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [46]  [  40/1251]  eta: 0:17:01  lr: 0.000012  loss: 3.5356 (3.3848)  time: 0.4681  data: 0.0005  max mem: 19734
Epoch: [46]  [  50/1251]  eta: 0:15:28  lr: 0.000012  loss: 3.6834 (3.4159)  time: 0.4795  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3994, ratio_loss=0.0053, pruning_loss=0.1187, mse_loss=0.4166
Epoch: [46]  [  60/1251]  eta: 0:14:23  lr: 0.000012  loss: 3.7082 (3.4603)  time: 0.4820  data: 0.0005  max mem: 19734
Epoch: [46]  [  70/1251]  eta: 0:13:39  lr: 0.000012  loss: 3.6009 (3.4754)  time: 0.4920  data: 0.0005  max mem: 19734
Epoch: [46]  [  80/1251]  eta: 0:12:58  lr: 0.000012  loss: 3.5651 (3.4649)  time: 0.4806  data: 0.0004  max mem: 19734
Epoch: [46]  [  90/1251]  eta: 0:12:28  lr: 0.000012  loss: 3.6348 (3.4875)  time: 0.4702  data: 0.0004  max mem: 19734
Epoch: [46]  [ 100/1251]  eta: 0:12:00  lr: 0.000012  loss: 3.6348 (3.4756)  time: 0.4701  data: 0.0004  max mem: 19734
Epoch: [46]  [ 110/1251]  eta: 0:11:37  lr: 0.000012  loss: 3.2720 (3.4438)  time: 0.4574  data: 0.0005  max mem: 19734
Epoch: [46]  [ 120/1251]  eta: 0:11:16  lr: 0.000012  loss: 3.3855 (3.4393)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [46]  [ 130/1251]  eta: 0:10:59  lr: 0.000012  loss: 3.4942 (3.4455)  time: 0.4611  data: 0.0005  max mem: 19734
Epoch: [46]  [ 140/1251]  eta: 0:10:44  lr: 0.000012  loss: 3.7222 (3.4628)  time: 0.4715  data: 0.0005  max mem: 19734
Epoch: [46]  [ 150/1251]  eta: 0:10:30  lr: 0.000012  loss: 3.6934 (3.4660)  time: 0.4705  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4621, ratio_loss=0.0049, pruning_loss=0.1172, mse_loss=0.4166
Epoch: [46]  [ 160/1251]  eta: 0:10:17  lr: 0.000012  loss: 3.5143 (3.4533)  time: 0.4617  data: 0.0005  max mem: 19734
Epoch: [46]  [ 170/1251]  eta: 0:10:04  lr: 0.000012  loss: 3.4354 (3.4411)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [46]  [ 180/1251]  eta: 0:09:52  lr: 0.000012  loss: 3.4354 (3.4322)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [46]  [ 190/1251]  eta: 0:09:44  lr: 0.000012  loss: 3.3808 (3.4234)  time: 0.4783  data: 0.0004  max mem: 19734
Epoch: [46]  [ 200/1251]  eta: 0:09:34  lr: 0.000012  loss: 3.5347 (3.4279)  time: 0.4873  data: 0.0005  max mem: 19734
Epoch: [46]  [ 210/1251]  eta: 0:09:26  lr: 0.000012  loss: 3.5939 (3.4211)  time: 0.4767  data: 0.0005  max mem: 19734
Epoch: [46]  [ 220/1251]  eta: 0:09:17  lr: 0.000012  loss: 3.6392 (3.4316)  time: 0.4824  data: 0.0005  max mem: 19734
Epoch: [46]  [ 230/1251]  eta: 0:09:09  lr: 0.000012  loss: 3.5774 (3.4321)  time: 0.4813  data: 0.0004  max mem: 19734
Epoch: [46]  [ 240/1251]  eta: 0:09:00  lr: 0.000012  loss: 3.5213 (3.4303)  time: 0.4685  data: 0.0004  max mem: 19734
Epoch: [46]  [ 250/1251]  eta: 0:08:52  lr: 0.000012  loss: 3.5540 (3.4359)  time: 0.4604  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3669, ratio_loss=0.0049, pruning_loss=0.1183, mse_loss=0.4127
Epoch: [46]  [ 260/1251]  eta: 0:08:44  lr: 0.000012  loss: 3.5544 (3.4373)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [46]  [ 270/1251]  eta: 0:08:36  lr: 0.000012  loss: 3.5544 (3.4267)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [46]  [ 280/1251]  eta: 0:08:29  lr: 0.000012  loss: 3.3743 (3.4221)  time: 0.4629  data: 0.0005  max mem: 19734
Epoch: [46]  [ 290/1251]  eta: 0:08:21  lr: 0.000012  loss: 3.5474 (3.4235)  time: 0.4656  data: 0.0005  max mem: 19734
Epoch: [46]  [ 300/1251]  eta: 0:08:14  lr: 0.000012  loss: 3.5959 (3.4239)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [46]  [ 310/1251]  eta: 0:08:07  lr: 0.000012  loss: 3.3773 (3.4137)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [46]  [ 320/1251]  eta: 0:08:00  lr: 0.000012  loss: 3.1493 (3.4064)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [46]  [ 330/1251]  eta: 0:07:53  lr: 0.000012  loss: 2.9195 (3.3963)  time: 0.4630  data: 0.0004  max mem: 19734
Epoch: [46]  [ 340/1251]  eta: 0:07:47  lr: 0.000012  loss: 3.2714 (3.3931)  time: 0.4734  data: 0.0006  max mem: 19734
Epoch: [46]  [ 350/1251]  eta: 0:07:41  lr: 0.000012  loss: 3.3422 (3.3925)  time: 0.4754  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2523, ratio_loss=0.0049, pruning_loss=0.1211, mse_loss=0.4179
Epoch: [46]  [ 360/1251]  eta: 0:07:36  lr: 0.000012  loss: 3.3517 (3.3890)  time: 0.4847  data: 0.0005  max mem: 19734
Epoch: [46]  [ 370/1251]  eta: 0:07:29  lr: 0.000012  loss: 3.3098 (3.3849)  time: 0.4757  data: 0.0005  max mem: 19734
Epoch: [46]  [ 380/1251]  eta: 0:07:23  lr: 0.000012  loss: 3.3152 (3.3846)  time: 0.4687  data: 0.0006  max mem: 19734
Epoch: [46]  [ 390/1251]  eta: 0:07:17  lr: 0.000012  loss: 3.4074 (3.3802)  time: 0.4680  data: 0.0005  max mem: 19734
Epoch: [46]  [ 400/1251]  eta: 0:07:11  lr: 0.000012  loss: 3.5718 (3.3884)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [46]  [ 410/1251]  eta: 0:07:05  lr: 0.000012  loss: 3.6706 (3.3881)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [46]  [ 420/1251]  eta: 0:06:58  lr: 0.000012  loss: 3.3616 (3.3888)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [46]  [ 430/1251]  eta: 0:06:53  lr: 0.000012  loss: 3.6173 (3.3909)  time: 0.4592  data: 0.0004  max mem: 19734
Epoch: [46]  [ 440/1251]  eta: 0:06:47  lr: 0.000012  loss: 3.3858 (3.3896)  time: 0.4614  data: 0.0004  max mem: 19734
Epoch: [46]  [ 450/1251]  eta: 0:06:41  lr: 0.000012  loss: 3.3858 (3.3879)  time: 0.4550  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3298, ratio_loss=0.0047, pruning_loss=0.1195, mse_loss=0.4318
Epoch: [46]  [ 460/1251]  eta: 0:06:35  lr: 0.000012  loss: 3.1384 (3.3860)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [46]  [ 470/1251]  eta: 0:06:29  lr: 0.000012  loss: 3.5139 (3.3890)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [46]  [ 480/1251]  eta: 0:06:24  lr: 0.000012  loss: 3.4783 (3.3864)  time: 0.4736  data: 0.0004  max mem: 19734
Epoch: [46]  [ 490/1251]  eta: 0:06:19  lr: 0.000012  loss: 3.4729 (3.3898)  time: 0.4807  data: 0.0004  max mem: 19734
Epoch: [46]  [ 500/1251]  eta: 0:06:14  lr: 0.000012  loss: 3.6770 (3.3983)  time: 0.4797  data: 0.0004  max mem: 19734
Epoch: [46]  [ 510/1251]  eta: 0:06:08  lr: 0.000012  loss: 3.6101 (3.4011)  time: 0.4750  data: 0.0004  max mem: 19734
Epoch: [46]  [ 520/1251]  eta: 0:06:03  lr: 0.000012  loss: 3.5331 (3.4051)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [46]  [ 530/1251]  eta: 0:05:57  lr: 0.000012  loss: 3.5561 (3.4088)  time: 0.4599  data: 0.0005  max mem: 19734
Epoch: [46]  [ 540/1251]  eta: 0:05:52  lr: 0.000012  loss: 3.3197 (3.4046)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [46]  [ 550/1251]  eta: 0:05:46  lr: 0.000012  loss: 3.2177 (3.4063)  time: 0.4531  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4935, ratio_loss=0.0048, pruning_loss=0.1159, mse_loss=0.4008
Epoch: [46]  [ 560/1251]  eta: 0:05:41  lr: 0.000012  loss: 3.4949 (3.4104)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [46]  [ 570/1251]  eta: 0:05:35  lr: 0.000012  loss: 3.4949 (3.4116)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [46]  [ 580/1251]  eta: 0:05:30  lr: 0.000012  loss: 3.3863 (3.4093)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [46]  [ 590/1251]  eta: 0:05:25  lr: 0.000012  loss: 3.3863 (3.4078)  time: 0.4586  data: 0.0004  max mem: 19734
Epoch: [46]  [ 600/1251]  eta: 0:05:19  lr: 0.000012  loss: 3.3091 (3.4051)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [46]  [ 610/1251]  eta: 0:05:14  lr: 0.000012  loss: 3.1147 (3.4002)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [46]  [ 620/1251]  eta: 0:05:09  lr: 0.000012  loss: 3.1329 (3.3983)  time: 0.4619  data: 0.0004  max mem: 19734
Epoch: [46]  [ 630/1251]  eta: 0:05:04  lr: 0.000012  loss: 3.1115 (3.3945)  time: 0.4750  data: 0.0004  max mem: 19734
Epoch: [46]  [ 640/1251]  eta: 0:04:59  lr: 0.000012  loss: 3.2795 (3.3968)  time: 0.4765  data: 0.0005  max mem: 19734
Epoch: [46]  [ 650/1251]  eta: 0:04:54  lr: 0.000012  loss: 3.6744 (3.3949)  time: 0.4814  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3141, ratio_loss=0.0047, pruning_loss=0.1188, mse_loss=0.4037
Epoch: [46]  [ 660/1251]  eta: 0:04:49  lr: 0.000012  loss: 3.7577 (3.3994)  time: 0.4816  data: 0.0007  max mem: 19734
Epoch: [46]  [ 670/1251]  eta: 0:04:44  lr: 0.000012  loss: 3.5504 (3.3976)  time: 0.4660  data: 0.0006  max mem: 19734
Epoch: [46]  [ 680/1251]  eta: 0:04:39  lr: 0.000012  loss: 3.2986 (3.3989)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [46]  [ 690/1251]  eta: 0:04:33  lr: 0.000012  loss: 3.6789 (3.4000)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [46]  [ 700/1251]  eta: 0:04:28  lr: 0.000012  loss: 3.3515 (3.3984)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [46]  [ 710/1251]  eta: 0:04:23  lr: 0.000012  loss: 3.3515 (3.3976)  time: 0.4526  data: 0.0004  max mem: 19734
Epoch: [46]  [ 720/1251]  eta: 0:04:18  lr: 0.000012  loss: 3.5790 (3.3985)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [46]  [ 730/1251]  eta: 0:04:13  lr: 0.000012  loss: 3.6579 (3.3993)  time: 0.4622  data: 0.0004  max mem: 19734
Epoch: [46]  [ 740/1251]  eta: 0:04:08  lr: 0.000012  loss: 3.6579 (3.3982)  time: 0.4618  data: 0.0004  max mem: 19734
Epoch: [46]  [ 750/1251]  eta: 0:04:03  lr: 0.000012  loss: 3.2170 (3.3938)  time: 0.4511  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3592, ratio_loss=0.0048, pruning_loss=0.1186, mse_loss=0.4021
Epoch: [46]  [ 760/1251]  eta: 0:03:58  lr: 0.000012  loss: 3.2414 (3.3957)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [46]  [ 770/1251]  eta: 0:03:53  lr: 0.000012  loss: 3.8398 (3.3987)  time: 0.4762  data: 0.0006  max mem: 19734
Epoch: [46]  [ 780/1251]  eta: 0:03:48  lr: 0.000012  loss: 3.7427 (3.4006)  time: 0.4838  data: 0.0005  max mem: 19734
Epoch: [46]  [ 790/1251]  eta: 0:03:43  lr: 0.000012  loss: 3.6000 (3.4024)  time: 0.4724  data: 0.0007  max mem: 19734
Epoch: [46]  [ 800/1251]  eta: 0:03:38  lr: 0.000012  loss: 3.4136 (3.4009)  time: 0.4715  data: 0.0006  max mem: 19734
Epoch: [46]  [ 810/1251]  eta: 0:03:33  lr: 0.000012  loss: 3.1410 (3.3981)  time: 0.4677  data: 0.0004  max mem: 19734
Epoch: [46]  [ 820/1251]  eta: 0:03:28  lr: 0.000012  loss: 3.2650 (3.3984)  time: 0.4582  data: 0.0004  max mem: 19734
Epoch: [46]  [ 830/1251]  eta: 0:03:23  lr: 0.000012  loss: 3.3530 (3.3958)  time: 0.4516  data: 0.0005  max mem: 19734
Epoch: [46]  [ 840/1251]  eta: 0:03:18  lr: 0.000012  loss: 3.4175 (3.3985)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [46]  [ 850/1251]  eta: 0:03:13  lr: 0.000012  loss: 3.4592 (3.3952)  time: 0.4522  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3698, ratio_loss=0.0045, pruning_loss=0.1192, mse_loss=0.4195
Epoch: [46]  [ 860/1251]  eta: 0:03:08  lr: 0.000012  loss: 2.8581 (3.3929)  time: 0.4535  data: 0.0004  max mem: 19734
Epoch: [46]  [ 870/1251]  eta: 0:03:03  lr: 0.000012  loss: 3.4161 (3.3920)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [46]  [ 880/1251]  eta: 0:02:58  lr: 0.000012  loss: 3.4161 (3.3903)  time: 0.4653  data: 0.0004  max mem: 19734
Epoch: [46]  [ 890/1251]  eta: 0:02:53  lr: 0.000012  loss: 3.5972 (3.3960)  time: 0.4650  data: 0.0004  max mem: 19734
Epoch: [46]  [ 900/1251]  eta: 0:02:49  lr: 0.000012  loss: 3.6895 (3.3966)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [46]  [ 910/1251]  eta: 0:02:44  lr: 0.000012  loss: 3.4402 (3.3952)  time: 0.4597  data: 0.0007  max mem: 19734
Epoch: [46]  [ 920/1251]  eta: 0:02:39  lr: 0.000012  loss: 3.5637 (3.3970)  time: 0.4687  data: 0.0006  max mem: 19734
Epoch: [46]  [ 930/1251]  eta: 0:02:34  lr: 0.000012  loss: 3.5993 (3.3984)  time: 0.4691  data: 0.0004  max mem: 19734
Epoch: [46]  [ 940/1251]  eta: 0:02:29  lr: 0.000012  loss: 3.6062 (3.3979)  time: 0.4772  data: 0.0004  max mem: 19734
Epoch: [46]  [ 950/1251]  eta: 0:02:24  lr: 0.000012  loss: 3.7136 (3.4004)  time: 0.4809  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4271, ratio_loss=0.0046, pruning_loss=0.1170, mse_loss=0.3956
Epoch: [46]  [ 960/1251]  eta: 0:02:19  lr: 0.000012  loss: 3.6969 (3.4021)  time: 0.4621  data: 0.0004  max mem: 19734
Epoch: [46]  [ 970/1251]  eta: 0:02:15  lr: 0.000012  loss: 3.4852 (3.4007)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [46]  [ 980/1251]  eta: 0:02:10  lr: 0.000012  loss: 3.2643 (3.3986)  time: 0.4547  data: 0.0005  max mem: 19734
Epoch: [46]  [ 990/1251]  eta: 0:02:05  lr: 0.000012  loss: 3.3654 (3.3969)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [46]  [1000/1251]  eta: 0:02:00  lr: 0.000012  loss: 3.3466 (3.3960)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [46]  [1010/1251]  eta: 0:01:55  lr: 0.000012  loss: 3.3269 (3.3941)  time: 0.4528  data: 0.0005  max mem: 19734
Epoch: [46]  [1020/1251]  eta: 0:01:50  lr: 0.000012  loss: 3.2474 (3.3916)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [46]  [1030/1251]  eta: 0:01:45  lr: 0.000012  loss: 3.1720 (3.3908)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [46]  [1040/1251]  eta: 0:01:41  lr: 0.000012  loss: 3.3839 (3.3909)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [46]  [1050/1251]  eta: 0:01:36  lr: 0.000012  loss: 3.4260 (3.3900)  time: 0.4518  data: 0.0005  max mem: 19734
loss info: cls_loss=3.2473, ratio_loss=0.0047, pruning_loss=0.1211, mse_loss=0.4077
Epoch: [46]  [1060/1251]  eta: 0:01:31  lr: 0.000012  loss: 3.3470 (3.3884)  time: 0.4737  data: 0.0005  max mem: 19734
Epoch: [46]  [1070/1251]  eta: 0:01:26  lr: 0.000012  loss: 3.4914 (3.3883)  time: 0.4789  data: 0.0005  max mem: 19734
Epoch: [46]  [1080/1251]  eta: 0:01:21  lr: 0.000012  loss: 3.3700 (3.3849)  time: 0.4673  data: 0.0004  max mem: 19734
Epoch: [46]  [1090/1251]  eta: 0:01:17  lr: 0.000012  loss: 3.3700 (3.3841)  time: 0.4736  data: 0.0006  max mem: 19734
Epoch: [46]  [1100/1251]  eta: 0:01:12  lr: 0.000012  loss: 3.4274 (3.3832)  time: 0.4766  data: 0.0006  max mem: 19734
Epoch: [46]  [1110/1251]  eta: 0:01:07  lr: 0.000012  loss: 3.4274 (3.3838)  time: 0.4669  data: 0.0004  max mem: 19734
Epoch: [46]  [1120/1251]  eta: 0:01:02  lr: 0.000012  loss: 3.5084 (3.3840)  time: 0.4533  data: 0.0004  max mem: 19734
Epoch: [46]  [1130/1251]  eta: 0:00:57  lr: 0.000012  loss: 3.6594 (3.3868)  time: 0.4503  data: 0.0005  max mem: 19734
Epoch: [46]  [1140/1251]  eta: 0:00:53  lr: 0.000012  loss: 3.6665 (3.3881)  time: 0.4507  data: 0.0004  max mem: 19734
Epoch: [46]  [1150/1251]  eta: 0:00:48  lr: 0.000012  loss: 3.6613 (3.3884)  time: 0.4523  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3541, ratio_loss=0.0046, pruning_loss=0.1186, mse_loss=0.4269
Epoch: [46]  [1160/1251]  eta: 0:00:43  lr: 0.000012  loss: 3.5645 (3.3892)  time: 0.4521  data: 0.0004  max mem: 19734
Epoch: [46]  [1170/1251]  eta: 0:00:38  lr: 0.000012  loss: 3.5212 (3.3897)  time: 0.4523  data: 0.0004  max mem: 19734
Epoch: [46]  [1180/1251]  eta: 0:00:33  lr: 0.000012  loss: 3.5212 (3.3892)  time: 0.4665  data: 0.0004  max mem: 19734
Epoch: [46]  [1190/1251]  eta: 0:00:29  lr: 0.000012  loss: 3.4874 (3.3880)  time: 0.4649  data: 0.0005  max mem: 19734
Epoch: [46]  [1200/1251]  eta: 0:00:24  lr: 0.000012  loss: 3.4874 (3.3882)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [46]  [1210/1251]  eta: 0:00:19  lr: 0.000012  loss: 3.6001 (3.3897)  time: 0.4652  data: 0.0001  max mem: 19734
Epoch: [46]  [1220/1251]  eta: 0:00:14  lr: 0.000012  loss: 3.5111 (3.3893)  time: 0.4662  data: 0.0002  max mem: 19734
Epoch: [46]  [1230/1251]  eta: 0:00:10  lr: 0.000012  loss: 3.3386 (3.3895)  time: 0.4704  data: 0.0002  max mem: 19734
Epoch: [46]  [1240/1251]  eta: 0:00:05  lr: 0.000012  loss: 3.5336 (3.3903)  time: 0.4656  data: 0.0002  max mem: 19734
Epoch: [46]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 3.5943 (3.3918)  time: 0.4495  data: 0.0002  max mem: 19734
Epoch: [46] Total time: 0:09:56 (0.4767 s / it)
Averaged stats: lr: 0.000012  loss: 3.5943 (3.4055)
Test:  [  0/261]  eta: 2:17:50  loss: 0.7092 (0.7092)  acc1: 82.8125 (82.8125)  acc5: 96.3542 (96.3542)  time: 31.6889  data: 31.4894  max mem: 19734
Test:  [ 10/261]  eta: 0:13:14  loss: 0.7023 (0.7183)  acc1: 83.8542 (84.0436)  acc5: 96.8750 (96.5436)  time: 3.1642  data: 2.8712  max mem: 19734
Test:  [ 20/261]  eta: 0:07:09  loss: 0.9240 (0.8958)  acc1: 80.7292 (79.2163)  acc5: 94.2708 (94.9157)  time: 0.2869  data: 0.0176  max mem: 19734
Test:  [ 30/261]  eta: 0:04:54  loss: 0.7924 (0.8172)  acc1: 85.4167 (81.9724)  acc5: 93.7500 (95.3461)  time: 0.2338  data: 0.0209  max mem: 19734
Test:  [ 40/261]  eta: 0:04:06  loss: 0.5801 (0.7808)  acc1: 87.5000 (83.0666)  acc5: 96.3542 (95.5793)  time: 0.4154  data: 0.2421  max mem: 19734
Test:  [ 50/261]  eta: 0:03:19  loss: 0.9161 (0.8409)  acc1: 77.0833 (81.2398)  acc5: 95.3125 (95.1593)  time: 0.4372  data: 0.2426  max mem: 19734
Test:  [ 60/261]  eta: 0:02:45  loss: 0.9810 (0.8511)  acc1: 77.0833 (80.8743)  acc5: 94.7917 (95.2271)  time: 0.2180  data: 0.0196  max mem: 19734
Test:  [ 70/261]  eta: 0:02:20  loss: 0.9274 (0.8526)  acc1: 78.6458 (80.4871)  acc5: 95.8333 (95.3859)  time: 0.1974  data: 0.0195  max mem: 19734
Test:  [ 80/261]  eta: 0:02:00  loss: 0.8301 (0.8540)  acc1: 79.1667 (80.5877)  acc5: 96.8750 (95.5118)  time: 0.2012  data: 0.0539  max mem: 19734
Test:  [ 90/261]  eta: 0:01:49  loss: 0.7995 (0.8403)  acc1: 84.3750 (80.9238)  acc5: 95.8333 (95.5930)  time: 0.3172  data: 0.1768  max mem: 19734
Test:  [100/261]  eta: 0:01:36  loss: 0.8160 (0.8436)  acc1: 82.2917 (80.8787)  acc5: 95.3125 (95.6322)  time: 0.3207  data: 0.1474  max mem: 19734
Test:  [110/261]  eta: 0:01:29  loss: 0.8781 (0.8663)  acc1: 78.6458 (80.4148)  acc5: 94.2708 (95.3078)  time: 0.3619  data: 0.1973  max mem: 19734
Test:  [120/261]  eta: 0:01:20  loss: 1.1948 (0.9067)  acc1: 71.8750 (79.4852)  acc5: 88.5417 (94.7572)  time: 0.4416  data: 0.2939  max mem: 19734
Test:  [130/261]  eta: 0:01:11  loss: 1.3811 (0.9516)  acc1: 68.7500 (78.5464)  acc5: 87.5000 (94.1794)  time: 0.2630  data: 0.1201  max mem: 19734
Test:  [140/261]  eta: 0:01:03  loss: 1.2927 (0.9778)  acc1: 67.7083 (77.8997)  acc5: 89.5833 (93.9015)  time: 0.2120  data: 0.0470  max mem: 19734
Test:  [150/261]  eta: 0:00:57  loss: 1.2341 (0.9824)  acc1: 72.3958 (77.8870)  acc5: 91.1458 (93.7879)  time: 0.3497  data: 0.1965  max mem: 19734
Test:  [160/261]  eta: 0:00:50  loss: 0.9739 (1.0016)  acc1: 78.1250 (77.5524)  acc5: 92.1875 (93.4912)  time: 0.3720  data: 0.2401  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: 1.3093 (1.0314)  acc1: 65.6250 (76.8092)  acc5: 88.0208 (93.1530)  time: 0.2275  data: 0.0891  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4067 (1.0480)  acc1: 65.6250 (76.4071)  acc5: 88.0208 (92.9875)  time: 0.3418  data: 0.2101  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3338 (1.0609)  acc1: 66.6667 (76.1507)  acc5: 90.1042 (92.8065)  time: 0.2971  data: 0.2043  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3338 (1.0757)  acc1: 71.8750 (75.8499)  acc5: 89.5833 (92.5917)  time: 0.0622  data: 0.0005  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3649 (1.0896)  acc1: 69.7917 (75.5406)  acc5: 89.0625 (92.3899)  time: 0.0621  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4144 (1.1091)  acc1: 66.6667 (75.0731)  acc5: 88.5417 (92.1969)  time: 0.1015  data: 0.0400  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4233 (1.1181)  acc1: 66.6667 (74.8286)  acc5: 89.5833 (92.1176)  time: 0.1034  data: 0.0399  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.2991 (1.1279)  acc1: 66.6667 (74.5959)  acc5: 91.1458 (92.0276)  time: 0.0636  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0410 (1.1214)  acc1: 75.5208 (74.7780)  acc5: 92.7083 (92.1356)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9640 (1.1211)  acc1: 78.6458 (74.7720)  acc5: 94.7917 (92.1920)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3604 s / it)
* Acc@1 74.772 Acc@5 92.192 loss 1.121
Accuracy of the network on the 50000 test images: 74.8%
Max accuracy: 74.77%
Epoch: [47]  [   0/1251]  eta: 5:24:36  lr: 0.000012  loss: 3.8017 (3.8017)  time: 15.5686  data: 15.0879  max mem: 19734
loss info: cls_loss=3.4267, ratio_loss=0.0045, pruning_loss=0.1172, mse_loss=0.4314
Epoch: [47]  [  10/1251]  eta: 0:39:22  lr: 0.000012  loss: 3.7209 (3.5545)  time: 1.9037  data: 1.3721  max mem: 19734
Epoch: [47]  [  20/1251]  eta: 0:25:08  lr: 0.000012  loss: 3.1636 (3.3580)  time: 0.5082  data: 0.0007  max mem: 19734
Epoch: [47]  [  30/1251]  eta: 0:19:54  lr: 0.000012  loss: 3.3883 (3.3509)  time: 0.4697  data: 0.0007  max mem: 19734
Epoch: [47]  [  40/1251]  eta: 0:17:10  lr: 0.000012  loss: 3.3883 (3.3135)  time: 0.4575  data: 0.0005  max mem: 19734
Epoch: [47]  [  50/1251]  eta: 0:15:30  lr: 0.000012  loss: 3.4001 (3.3390)  time: 0.4579  data: 0.0005  max mem: 19734
Epoch: [47]  [  60/1251]  eta: 0:14:20  lr: 0.000012  loss: 3.4878 (3.3635)  time: 0.4602  data: 0.0005  max mem: 19734
Epoch: [47]  [  70/1251]  eta: 0:13:30  lr: 0.000012  loss: 3.4628 (3.3702)  time: 0.4609  data: 0.0005  max mem: 19734
Epoch: [47]  [  80/1251]  eta: 0:12:53  lr: 0.000012  loss: 3.4667 (3.3932)  time: 0.4705  data: 0.0005  max mem: 19734
Epoch: [47]  [  90/1251]  eta: 0:12:23  lr: 0.000012  loss: 3.6391 (3.4196)  time: 0.4799  data: 0.0005  max mem: 19734
Epoch: [47]  [ 100/1251]  eta: 0:11:59  lr: 0.000012  loss: 3.4099 (3.3927)  time: 0.4808  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3554, ratio_loss=0.0048, pruning_loss=0.1191, mse_loss=0.4120
Epoch: [47]  [ 110/1251]  eta: 0:11:38  lr: 0.000012  loss: 3.2765 (3.3830)  time: 0.4819  data: 0.0004  max mem: 19734
Epoch: [47]  [ 120/1251]  eta: 0:11:21  lr: 0.000012  loss: 3.0823 (3.3590)  time: 0.4901  data: 0.0004  max mem: 19734
Epoch: [47]  [ 130/1251]  eta: 0:11:03  lr: 0.000012  loss: 3.3723 (3.3805)  time: 0.4767  data: 0.0005  max mem: 19734
Epoch: [47]  [ 140/1251]  eta: 0:10:46  lr: 0.000012  loss: 3.5002 (3.3798)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [47]  [ 150/1251]  eta: 0:10:31  lr: 0.000012  loss: 3.3627 (3.3724)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [47]  [ 160/1251]  eta: 0:10:17  lr: 0.000012  loss: 3.4735 (3.3945)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [47]  [ 170/1251]  eta: 0:10:06  lr: 0.000012  loss: 3.6810 (3.4084)  time: 0.4634  data: 0.0005  max mem: 19734
Epoch: [47]  [ 180/1251]  eta: 0:09:54  lr: 0.000012  loss: 3.6288 (3.3992)  time: 0.4651  data: 0.0005  max mem: 19734
Epoch: [47]  [ 190/1251]  eta: 0:09:43  lr: 0.000012  loss: 3.4097 (3.4078)  time: 0.4602  data: 0.0005  max mem: 19734
Epoch: [47]  [ 200/1251]  eta: 0:09:33  lr: 0.000012  loss: 3.5616 (3.4061)  time: 0.4593  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3920, ratio_loss=0.0047, pruning_loss=0.1184, mse_loss=0.4142
Epoch: [47]  [ 210/1251]  eta: 0:09:23  lr: 0.000012  loss: 3.4477 (3.3942)  time: 0.4577  data: 0.0006  max mem: 19734
Epoch: [47]  [ 220/1251]  eta: 0:09:14  lr: 0.000012  loss: 3.3321 (3.3960)  time: 0.4594  data: 0.0006  max mem: 19734
Epoch: [47]  [ 230/1251]  eta: 0:09:06  lr: 0.000012  loss: 3.3321 (3.3908)  time: 0.4679  data: 0.0004  max mem: 19734
Epoch: [47]  [ 240/1251]  eta: 0:08:58  lr: 0.000012  loss: 3.5375 (3.3962)  time: 0.4735  data: 0.0004  max mem: 19734
Epoch: [47]  [ 250/1251]  eta: 0:08:50  lr: 0.000012  loss: 3.6636 (3.4054)  time: 0.4712  data: 0.0007  max mem: 19734
Epoch: [47]  [ 260/1251]  eta: 0:08:43  lr: 0.000012  loss: 3.4630 (3.3905)  time: 0.4828  data: 0.0008  max mem: 19734
Epoch: [47]  [ 270/1251]  eta: 0:08:36  lr: 0.000012  loss: 3.3040 (3.3871)  time: 0.4861  data: 0.0005  max mem: 19734
Epoch: [47]  [ 280/1251]  eta: 0:08:28  lr: 0.000012  loss: 3.6021 (3.3991)  time: 0.4659  data: 0.0004  max mem: 19734
Epoch: [47]  [ 290/1251]  eta: 0:08:21  lr: 0.000012  loss: 3.6685 (3.3970)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [47]  [ 300/1251]  eta: 0:08:14  lr: 0.000012  loss: 3.5628 (3.3937)  time: 0.4540  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3435, ratio_loss=0.0046, pruning_loss=0.1192, mse_loss=0.3907
Epoch: [47]  [ 310/1251]  eta: 0:08:07  lr: 0.000012  loss: 3.1874 (3.3879)  time: 0.4681  data: 0.0004  max mem: 19734
Epoch: [47]  [ 320/1251]  eta: 0:08:00  lr: 0.000012  loss: 3.4426 (3.3932)  time: 0.4678  data: 0.0005  max mem: 19734
Epoch: [47]  [ 330/1251]  eta: 0:07:53  lr: 0.000012  loss: 3.5433 (3.3971)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [47]  [ 340/1251]  eta: 0:07:47  lr: 0.000012  loss: 3.5433 (3.3952)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [47]  [ 350/1251]  eta: 0:07:40  lr: 0.000012  loss: 3.3267 (3.3906)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [47]  [ 360/1251]  eta: 0:07:33  lr: 0.000012  loss: 3.5103 (3.3967)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [47]  [ 370/1251]  eta: 0:07:27  lr: 0.000012  loss: 3.3201 (3.3947)  time: 0.4545  data: 0.0007  max mem: 19734
Epoch: [47]  [ 380/1251]  eta: 0:07:22  lr: 0.000012  loss: 3.2936 (3.3974)  time: 0.4755  data: 0.0006  max mem: 19734
Epoch: [47]  [ 390/1251]  eta: 0:07:16  lr: 0.000012  loss: 3.1885 (3.3886)  time: 0.4909  data: 0.0004  max mem: 19734
Epoch: [47]  [ 400/1251]  eta: 0:07:10  lr: 0.000012  loss: 3.2721 (3.3919)  time: 0.4818  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3808, ratio_loss=0.0050, pruning_loss=0.1180, mse_loss=0.4124
Epoch: [47]  [ 410/1251]  eta: 0:07:05  lr: 0.000012  loss: 3.7748 (3.3973)  time: 0.4891  data: 0.0004  max mem: 19734
Epoch: [47]  [ 420/1251]  eta: 0:06:59  lr: 0.000012  loss: 3.7987 (3.4054)  time: 0.4769  data: 0.0004  max mem: 19734
Epoch: [47]  [ 430/1251]  eta: 0:06:53  lr: 0.000012  loss: 3.6921 (3.4107)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [47]  [ 440/1251]  eta: 0:06:47  lr: 0.000012  loss: 3.5914 (3.4091)  time: 0.4531  data: 0.0005  max mem: 19734
Epoch: [47]  [ 450/1251]  eta: 0:06:41  lr: 0.000012  loss: 3.2150 (3.4051)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [47]  [ 460/1251]  eta: 0:06:36  lr: 0.000012  loss: 3.3286 (3.4023)  time: 0.4640  data: 0.0004  max mem: 19734
Epoch: [47]  [ 470/1251]  eta: 0:06:30  lr: 0.000012  loss: 3.3980 (3.4028)  time: 0.4645  data: 0.0005  max mem: 19734
Epoch: [47]  [ 480/1251]  eta: 0:06:24  lr: 0.000012  loss: 3.2575 (3.3960)  time: 0.4534  data: 0.0004  max mem: 19734
Epoch: [47]  [ 490/1251]  eta: 0:06:19  lr: 0.000012  loss: 3.0852 (3.3937)  time: 0.4520  data: 0.0004  max mem: 19734
Epoch: [47]  [ 500/1251]  eta: 0:06:13  lr: 0.000012  loss: 3.4089 (3.3916)  time: 0.4524  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3503, ratio_loss=0.0047, pruning_loss=0.1201, mse_loss=0.4132
Epoch: [47]  [ 510/1251]  eta: 0:06:07  lr: 0.000012  loss: 3.4541 (3.3936)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [47]  [ 520/1251]  eta: 0:06:02  lr: 0.000012  loss: 3.4541 (3.3937)  time: 0.4612  data: 0.0004  max mem: 19734
Epoch: [47]  [ 530/1251]  eta: 0:05:57  lr: 0.000012  loss: 3.4094 (3.3936)  time: 0.4806  data: 0.0004  max mem: 19734
Epoch: [47]  [ 540/1251]  eta: 0:05:51  lr: 0.000012  loss: 3.4506 (3.3930)  time: 0.4735  data: 0.0005  max mem: 19734
Epoch: [47]  [ 550/1251]  eta: 0:05:46  lr: 0.000012  loss: 3.5722 (3.3978)  time: 0.4713  data: 0.0005  max mem: 19734
Epoch: [47]  [ 560/1251]  eta: 0:05:41  lr: 0.000012  loss: 3.7316 (3.4024)  time: 0.4883  data: 0.0004  max mem: 19734
Epoch: [47]  [ 570/1251]  eta: 0:05:36  lr: 0.000012  loss: 3.7573 (3.4055)  time: 0.4720  data: 0.0005  max mem: 19734
Epoch: [47]  [ 580/1251]  eta: 0:05:31  lr: 0.000012  loss: 3.6140 (3.4054)  time: 0.4584  data: 0.0007  max mem: 19734
Epoch: [47]  [ 590/1251]  eta: 0:05:25  lr: 0.000012  loss: 3.5951 (3.4079)  time: 0.4588  data: 0.0007  max mem: 19734
Epoch: [47]  [ 600/1251]  eta: 0:05:20  lr: 0.000012  loss: 3.5951 (3.4088)  time: 0.4550  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4849, ratio_loss=0.0045, pruning_loss=0.1174, mse_loss=0.4322
Epoch: [47]  [ 610/1251]  eta: 0:05:15  lr: 0.000012  loss: 3.5488 (3.4090)  time: 0.4639  data: 0.0004  max mem: 19734
Epoch: [47]  [ 620/1251]  eta: 0:05:09  lr: 0.000012  loss: 3.5743 (3.4089)  time: 0.4635  data: 0.0004  max mem: 19734
Epoch: [47]  [ 630/1251]  eta: 0:05:04  lr: 0.000012  loss: 3.5303 (3.4104)  time: 0.4530  data: 0.0005  max mem: 19734
Epoch: [47]  [ 640/1251]  eta: 0:04:59  lr: 0.000012  loss: 3.4893 (3.4102)  time: 0.4531  data: 0.0007  max mem: 19734
Epoch: [47]  [ 650/1251]  eta: 0:04:54  lr: 0.000012  loss: 3.4893 (3.4119)  time: 0.4525  data: 0.0007  max mem: 19734
Epoch: [47]  [ 660/1251]  eta: 0:04:48  lr: 0.000012  loss: 3.3507 (3.4074)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [47]  [ 670/1251]  eta: 0:04:44  lr: 0.000012  loss: 3.2977 (3.4027)  time: 0.4725  data: 0.0004  max mem: 19734
Epoch: [47]  [ 680/1251]  eta: 0:04:39  lr: 0.000012  loss: 3.4188 (3.4004)  time: 0.4828  data: 0.0004  max mem: 19734
Epoch: [47]  [ 690/1251]  eta: 0:04:34  lr: 0.000012  loss: 3.5125 (3.4008)  time: 0.4803  data: 0.0004  max mem: 19734
Epoch: [47]  [ 700/1251]  eta: 0:04:29  lr: 0.000012  loss: 3.5222 (3.4024)  time: 0.4866  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3470, ratio_loss=0.0044, pruning_loss=0.1191, mse_loss=0.4215
Epoch: [47]  [ 710/1251]  eta: 0:04:24  lr: 0.000012  loss: 3.5727 (3.4038)  time: 0.4702  data: 0.0005  max mem: 19734
Epoch: [47]  [ 720/1251]  eta: 0:04:18  lr: 0.000012  loss: 3.5727 (3.4067)  time: 0.4517  data: 0.0005  max mem: 19734
Epoch: [47]  [ 730/1251]  eta: 0:04:13  lr: 0.000012  loss: 3.6626 (3.4088)  time: 0.4500  data: 0.0007  max mem: 19734
Epoch: [47]  [ 740/1251]  eta: 0:04:08  lr: 0.000012  loss: 3.5056 (3.4068)  time: 0.4515  data: 0.0009  max mem: 19734
Epoch: [47]  [ 750/1251]  eta: 0:04:03  lr: 0.000012  loss: 3.2685 (3.4043)  time: 0.4529  data: 0.0006  max mem: 19734
Epoch: [47]  [ 760/1251]  eta: 0:03:58  lr: 0.000012  loss: 3.5526 (3.4094)  time: 0.4648  data: 0.0004  max mem: 19734
Epoch: [47]  [ 770/1251]  eta: 0:03:53  lr: 0.000012  loss: 3.5620 (3.4075)  time: 0.4647  data: 0.0005  max mem: 19734
Epoch: [47]  [ 780/1251]  eta: 0:03:48  lr: 0.000012  loss: 3.3923 (3.4076)  time: 0.4539  data: 0.0004  max mem: 19734
Epoch: [47]  [ 790/1251]  eta: 0:03:43  lr: 0.000012  loss: 3.3679 (3.4063)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [47]  [ 800/1251]  eta: 0:03:38  lr: 0.000012  loss: 3.5382 (3.4089)  time: 0.4579  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4297, ratio_loss=0.0046, pruning_loss=0.1173, mse_loss=0.4130
Epoch: [47]  [ 810/1251]  eta: 0:03:33  lr: 0.000012  loss: 3.5474 (3.4082)  time: 0.4734  data: 0.0005  max mem: 19734
Epoch: [47]  [ 820/1251]  eta: 0:03:28  lr: 0.000012  loss: 3.6204 (3.4071)  time: 0.4778  data: 0.0005  max mem: 19734
Epoch: [47]  [ 830/1251]  eta: 0:03:23  lr: 0.000012  loss: 3.6204 (3.4111)  time: 0.4682  data: 0.0006  max mem: 19734
Epoch: [47]  [ 840/1251]  eta: 0:03:18  lr: 0.000012  loss: 3.5596 (3.4104)  time: 0.4677  data: 0.0010  max mem: 19734
Epoch: [47]  [ 850/1251]  eta: 0:03:14  lr: 0.000012  loss: 3.4830 (3.4097)  time: 0.4716  data: 0.0008  max mem: 19734
Epoch: [47]  [ 860/1251]  eta: 0:03:09  lr: 0.000012  loss: 3.0890 (3.4051)  time: 0.4633  data: 0.0004  max mem: 19734
Epoch: [47]  [ 870/1251]  eta: 0:03:04  lr: 0.000012  loss: 3.0890 (3.4048)  time: 0.4508  data: 0.0004  max mem: 19734
Epoch: [47]  [ 880/1251]  eta: 0:02:59  lr: 0.000012  loss: 3.5849 (3.4045)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [47]  [ 890/1251]  eta: 0:02:54  lr: 0.000012  loss: 3.1487 (3.4028)  time: 0.4557  data: 0.0004  max mem: 19734
Epoch: [47]  [ 900/1251]  eta: 0:02:49  lr: 0.000012  loss: 3.0634 (3.3984)  time: 0.4647  data: 0.0004  max mem: 19734
loss info: cls_loss=3.2876, ratio_loss=0.0045, pruning_loss=0.1213, mse_loss=0.3980
Epoch: [47]  [ 910/1251]  eta: 0:02:44  lr: 0.000012  loss: 3.2126 (3.3987)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [47]  [ 920/1251]  eta: 0:02:39  lr: 0.000012  loss: 3.5534 (3.4001)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [47]  [ 930/1251]  eta: 0:02:34  lr: 0.000012  loss: 3.4180 (3.3994)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [47]  [ 940/1251]  eta: 0:02:29  lr: 0.000012  loss: 2.9971 (3.3956)  time: 0.4508  data: 0.0004  max mem: 19734
Epoch: [47]  [ 950/1251]  eta: 0:02:24  lr: 0.000012  loss: 3.5288 (3.3987)  time: 0.4504  data: 0.0004  max mem: 19734
Epoch: [47]  [ 960/1251]  eta: 0:02:19  lr: 0.000012  loss: 3.5288 (3.3992)  time: 0.4689  data: 0.0004  max mem: 19734
Epoch: [47]  [ 970/1251]  eta: 0:02:15  lr: 0.000012  loss: 3.4172 (3.3977)  time: 0.4820  data: 0.0004  max mem: 19734
Epoch: [47]  [ 980/1251]  eta: 0:02:10  lr: 0.000012  loss: 3.4172 (3.3964)  time: 0.4828  data: 0.0004  max mem: 19734
Epoch: [47]  [ 990/1251]  eta: 0:02:05  lr: 0.000012  loss: 3.2958 (3.3947)  time: 0.4875  data: 0.0004  max mem: 19734
Epoch: [47]  [1000/1251]  eta: 0:02:00  lr: 0.000012  loss: 3.2958 (3.3952)  time: 0.4688  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3314, ratio_loss=0.0048, pruning_loss=0.1200, mse_loss=0.4116
Epoch: [47]  [1010/1251]  eta: 0:01:55  lr: 0.000012  loss: 3.7740 (3.3980)  time: 0.4519  data: 0.0004  max mem: 19734
Epoch: [47]  [1020/1251]  eta: 0:01:50  lr: 0.000012  loss: 3.8032 (3.3981)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [47]  [1030/1251]  eta: 0:01:46  lr: 0.000012  loss: 3.5180 (3.3988)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [47]  [1040/1251]  eta: 0:01:41  lr: 0.000012  loss: 3.5669 (3.3989)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [47]  [1050/1251]  eta: 0:01:36  lr: 0.000012  loss: 3.2414 (3.3958)  time: 0.4601  data: 0.0004  max mem: 19734
Epoch: [47]  [1060/1251]  eta: 0:01:31  lr: 0.000012  loss: 3.3555 (3.3960)  time: 0.4599  data: 0.0004  max mem: 19734
Epoch: [47]  [1070/1251]  eta: 0:01:26  lr: 0.000012  loss: 3.6179 (3.3979)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [47]  [1080/1251]  eta: 0:01:21  lr: 0.000012  loss: 3.6585 (3.3974)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [47]  [1090/1251]  eta: 0:01:17  lr: 0.000012  loss: 3.3976 (3.3970)  time: 0.4570  data: 0.0004  max mem: 19734
Epoch: [47]  [1100/1251]  eta: 0:01:12  lr: 0.000012  loss: 3.6034 (3.3980)  time: 0.4780  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4082, ratio_loss=0.0048, pruning_loss=0.1186, mse_loss=0.4042
Epoch: [47]  [1110/1251]  eta: 0:01:07  lr: 0.000012  loss: 3.6045 (3.3966)  time: 0.4747  data: 0.0004  max mem: 19734
Epoch: [47]  [1120/1251]  eta: 0:01:02  lr: 0.000012  loss: 3.3043 (3.3958)  time: 0.4755  data: 0.0004  max mem: 19734
Epoch: [47]  [1130/1251]  eta: 0:00:57  lr: 0.000012  loss: 3.3030 (3.3947)  time: 0.4828  data: 0.0004  max mem: 19734
Epoch: [47]  [1140/1251]  eta: 0:00:53  lr: 0.000012  loss: 3.2187 (3.3940)  time: 0.4722  data: 0.0004  max mem: 19734
Epoch: [47]  [1150/1251]  eta: 0:00:48  lr: 0.000012  loss: 3.5525 (3.3960)  time: 0.4663  data: 0.0004  max mem: 19734
Epoch: [47]  [1160/1251]  eta: 0:00:43  lr: 0.000012  loss: 3.5527 (3.3959)  time: 0.4553  data: 0.0005  max mem: 19734
Epoch: [47]  [1170/1251]  eta: 0:00:38  lr: 0.000012  loss: 3.5110 (3.3969)  time: 0.4559  data: 0.0005  max mem: 19734
Epoch: [47]  [1180/1251]  eta: 0:00:33  lr: 0.000012  loss: 3.5260 (3.3989)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [47]  [1190/1251]  eta: 0:00:29  lr: 0.000012  loss: 3.4417 (3.3971)  time: 0.4567  data: 0.0007  max mem: 19734
Epoch: [47]  [1200/1251]  eta: 0:00:24  lr: 0.000012  loss: 3.5819 (3.3994)  time: 0.4583  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3993, ratio_loss=0.0049, pruning_loss=0.1185, mse_loss=0.4058
Epoch: [47]  [1210/1251]  eta: 0:00:19  lr: 0.000012  loss: 3.5819 (3.3978)  time: 0.4527  data: 0.0002  max mem: 19734
Epoch: [47]  [1220/1251]  eta: 0:00:14  lr: 0.000012  loss: 3.4996 (3.3987)  time: 0.4451  data: 0.0002  max mem: 19734
Epoch: [47]  [1230/1251]  eta: 0:00:10  lr: 0.000012  loss: 3.4544 (3.3974)  time: 0.4446  data: 0.0002  max mem: 19734
Epoch: [47]  [1240/1251]  eta: 0:00:05  lr: 0.000012  loss: 3.4544 (3.3973)  time: 0.4439  data: 0.0002  max mem: 19734
Epoch: [47]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 3.6076 (3.4002)  time: 0.4535  data: 0.0002  max mem: 19734
Epoch: [47] Total time: 0:09:56 (0.4770 s / it)
Averaged stats: lr: 0.000012  loss: 3.6076 (3.4006)
Test:  [  0/261]  eta: 2:14:17  loss: 0.6652 (0.6652)  acc1: 84.8958 (84.8958)  acc5: 95.8333 (95.8333)  time: 30.8735  data: 30.5716  max mem: 19734
Test:  [ 10/261]  eta: 0:12:43  loss: 0.6652 (0.7219)  acc1: 84.8958 (83.4280)  acc5: 96.3542 (96.3542)  time: 3.0415  data: 2.7898  max mem: 19734
Test:  [ 20/261]  eta: 0:06:49  loss: 0.9261 (0.8919)  acc1: 77.6042 (78.6954)  acc5: 93.7500 (94.7669)  time: 0.2390  data: 0.0171  max mem: 19734
Test:  [ 30/261]  eta: 0:04:39  loss: 0.8230 (0.8119)  acc1: 82.2917 (81.6196)  acc5: 94.2708 (95.2453)  time: 0.2059  data: 0.0187  max mem: 19734
Test:  [ 40/261]  eta: 0:03:55  loss: 0.5601 (0.7744)  acc1: 89.0625 (82.8125)  acc5: 96.8750 (95.5031)  time: 0.3999  data: 0.2147  max mem: 19734
Test:  [ 50/261]  eta: 0:03:06  loss: 0.8892 (0.8379)  acc1: 78.1250 (80.9845)  acc5: 94.7917 (95.0980)  time: 0.3807  data: 0.2132  max mem: 19734
Test:  [ 60/261]  eta: 0:02:33  loss: 0.9783 (0.8487)  acc1: 76.5625 (80.5413)  acc5: 94.2708 (95.1674)  time: 0.1490  data: 0.0122  max mem: 19734
Test:  [ 70/261]  eta: 0:02:17  loss: 0.9416 (0.8510)  acc1: 77.6042 (80.2450)  acc5: 96.3542 (95.3785)  time: 0.3031  data: 0.1714  max mem: 19734
Test:  [ 80/261]  eta: 0:01:59  loss: 0.8267 (0.8507)  acc1: 80.7292 (80.3948)  acc5: 96.8750 (95.4861)  time: 0.3428  data: 0.1861  max mem: 19734
Test:  [ 90/261]  eta: 0:01:44  loss: 0.8153 (0.8367)  acc1: 83.3333 (80.7463)  acc5: 95.8333 (95.5815)  time: 0.2152  data: 0.0435  max mem: 19734
Test:  [100/261]  eta: 0:01:31  loss: 0.8166 (0.8412)  acc1: 83.3333 (80.6209)  acc5: 95.3125 (95.6064)  time: 0.2059  data: 0.0323  max mem: 19734
Test:  [110/261]  eta: 0:01:22  loss: 0.8699 (0.8637)  acc1: 77.0833 (80.2083)  acc5: 94.7917 (95.3219)  time: 0.2592  data: 0.1196  max mem: 19734
Test:  [120/261]  eta: 0:01:14  loss: 1.2054 (0.9047)  acc1: 71.8750 (79.3173)  acc5: 90.1042 (94.7529)  time: 0.3205  data: 0.2073  max mem: 19734
Test:  [130/261]  eta: 0:01:06  loss: 1.4181 (0.9494)  acc1: 68.7500 (78.4232)  acc5: 86.9792 (94.1476)  time: 0.2946  data: 0.1620  max mem: 19734
Test:  [140/261]  eta: 0:01:01  loss: 1.2862 (0.9765)  acc1: 69.7917 (77.7962)  acc5: 88.0208 (93.8460)  time: 0.3659  data: 0.2386  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: 1.2417 (0.9815)  acc1: 72.3958 (77.8042)  acc5: 91.1458 (93.7190)  time: 0.3999  data: 0.2754  max mem: 19734
Test:  [160/261]  eta: 0:00:49  loss: 1.0088 (1.0010)  acc1: 78.6458 (77.4909)  acc5: 91.1458 (93.4039)  time: 0.3415  data: 0.2212  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: 1.3098 (1.0312)  acc1: 66.1458 (76.7483)  acc5: 88.0208 (93.0799)  time: 0.2361  data: 0.1429  max mem: 19734
Test:  [180/261]  eta: 0:00:35  loss: 1.4454 (1.0477)  acc1: 65.6250 (76.3438)  acc5: 88.5417 (92.9500)  time: 0.1127  data: 0.0366  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: 1.3752 (1.0600)  acc1: 68.2292 (76.0880)  acc5: 91.1458 (92.7765)  time: 0.1062  data: 0.0250  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.3301 (1.0749)  acc1: 71.8750 (75.7670)  acc5: 89.0625 (92.5529)  time: 0.1015  data: 0.0259  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.3437 (1.0889)  acc1: 70.8333 (75.4641)  acc5: 88.5417 (92.3628)  time: 0.0810  data: 0.0147  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.4083 (1.1087)  acc1: 66.6667 (75.0047)  acc5: 88.5417 (92.1781)  time: 0.0632  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4111 (1.1180)  acc1: 66.1458 (74.7159)  acc5: 89.5833 (92.0951)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.2796 (1.1268)  acc1: 67.1875 (74.5029)  acc5: 90.6250 (92.0168)  time: 0.0748  data: 0.0132  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0345 (1.1196)  acc1: 75.5208 (74.6887)  acc5: 93.2292 (92.1294)  time: 0.0759  data: 0.0132  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9870 (1.1199)  acc1: 76.0417 (74.6820)  acc5: 95.3125 (92.1920)  time: 0.0610  data: 0.0001  max mem: 19734
Test: Total time: 0:01:26 (0.3322 s / it)
* Acc@1 74.682 Acc@5 92.192 loss 1.120
Accuracy of the network on the 50000 test images: 74.7%
Max accuracy: 74.77%
Epoch: [48]  [   0/1251]  eta: 5:23:52  lr: 0.000011  loss: 3.7492 (3.7492)  time: 15.5338  data: 10.2951  max mem: 19734
Epoch: [48]  [  10/1251]  eta: 0:45:58  lr: 0.000011  loss: 3.7635 (3.7408)  time: 2.2228  data: 0.9835  max mem: 19734
Epoch: [48]  [  20/1251]  eta: 0:28:24  lr: 0.000011  loss: 3.4532 (3.4862)  time: 0.6772  data: 0.0264  max mem: 19734
Epoch: [48]  [  30/1251]  eta: 0:22:13  lr: 0.000011  loss: 3.1219 (3.4222)  time: 0.4705  data: 0.0004  max mem: 19734
Epoch: [48]  [  40/1251]  eta: 0:18:55  lr: 0.000011  loss: 3.3094 (3.4311)  time: 0.4683  data: 0.0004  max mem: 19734
Epoch: [48]  [  50/1251]  eta: 0:16:52  lr: 0.000011  loss: 3.6581 (3.4398)  time: 0.4578  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3992, ratio_loss=0.0047, pruning_loss=0.1170, mse_loss=0.4110
Epoch: [48]  [  60/1251]  eta: 0:15:32  lr: 0.000011  loss: 3.6119 (3.4373)  time: 0.4660  data: 0.0004  max mem: 19734
Epoch: [48]  [  70/1251]  eta: 0:14:31  lr: 0.000011  loss: 3.6119 (3.4483)  time: 0.4685  data: 0.0004  max mem: 19734
Epoch: [48]  [  80/1251]  eta: 0:13:44  lr: 0.000011  loss: 3.6500 (3.4491)  time: 0.4633  data: 0.0005  max mem: 19734
Epoch: [48]  [  90/1251]  eta: 0:13:06  lr: 0.000011  loss: 3.6523 (3.4650)  time: 0.4619  data: 0.0005  max mem: 19734
Epoch: [48]  [ 100/1251]  eta: 0:12:34  lr: 0.000011  loss: 3.4364 (3.4590)  time: 0.4578  data: 0.0005  max mem: 19734
Epoch: [48]  [ 110/1251]  eta: 0:12:07  lr: 0.000011  loss: 3.3835 (3.4658)  time: 0.4579  data: 0.0006  max mem: 19734
Epoch: [48]  [ 120/1251]  eta: 0:11:44  lr: 0.000011  loss: 3.4493 (3.4605)  time: 0.4589  data: 0.0006  max mem: 19734
Epoch: [48]  [ 130/1251]  eta: 0:11:27  lr: 0.000011  loss: 3.5508 (3.4493)  time: 0.4791  data: 0.0005  max mem: 19734
Epoch: [48]  [ 140/1251]  eta: 0:11:11  lr: 0.000011  loss: 3.5999 (3.4565)  time: 0.4897  data: 0.0007  max mem: 19734
Epoch: [48]  [ 150/1251]  eta: 0:10:54  lr: 0.000011  loss: 3.2468 (3.4314)  time: 0.4701  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4019, ratio_loss=0.0047, pruning_loss=0.1188, mse_loss=0.4201
Epoch: [48]  [ 160/1251]  eta: 0:10:40  lr: 0.000011  loss: 3.3582 (3.4424)  time: 0.4709  data: 0.0005  max mem: 19734
Epoch: [48]  [ 170/1251]  eta: 0:10:28  lr: 0.000011  loss: 3.5826 (3.4523)  time: 0.4792  data: 0.0005  max mem: 19734
Epoch: [48]  [ 180/1251]  eta: 0:10:14  lr: 0.000011  loss: 3.5845 (3.4399)  time: 0.4639  data: 0.0005  max mem: 19734
Epoch: [48]  [ 190/1251]  eta: 0:10:02  lr: 0.000011  loss: 3.4935 (3.4435)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [48]  [ 200/1251]  eta: 0:09:51  lr: 0.000011  loss: 3.5575 (3.4462)  time: 0.4586  data: 0.0005  max mem: 19734
Epoch: [48]  [ 210/1251]  eta: 0:09:40  lr: 0.000011  loss: 3.5491 (3.4399)  time: 0.4629  data: 0.0004  max mem: 19734
Epoch: [48]  [ 220/1251]  eta: 0:09:30  lr: 0.000011  loss: 3.3993 (3.4357)  time: 0.4632  data: 0.0004  max mem: 19734
Epoch: [48]  [ 230/1251]  eta: 0:09:20  lr: 0.000011  loss: 3.4124 (3.4322)  time: 0.4609  data: 0.0004  max mem: 19734
Epoch: [48]  [ 240/1251]  eta: 0:09:11  lr: 0.000011  loss: 3.3005 (3.4210)  time: 0.4604  data: 0.0004  max mem: 19734
Epoch: [48]  [ 250/1251]  eta: 0:09:02  lr: 0.000011  loss: 3.4699 (3.4286)  time: 0.4584  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4047, ratio_loss=0.0047, pruning_loss=0.1186, mse_loss=0.4178
Epoch: [48]  [ 260/1251]  eta: 0:08:54  lr: 0.000011  loss: 3.4220 (3.4210)  time: 0.4595  data: 0.0004  max mem: 19734
Epoch: [48]  [ 270/1251]  eta: 0:08:46  lr: 0.000011  loss: 3.1681 (3.4149)  time: 0.4667  data: 0.0007  max mem: 19734
Epoch: [48]  [ 280/1251]  eta: 0:08:39  lr: 0.000011  loss: 3.3541 (3.4115)  time: 0.4757  data: 0.0006  max mem: 19734
Epoch: [48]  [ 290/1251]  eta: 0:08:31  lr: 0.000011  loss: 3.3811 (3.4052)  time: 0.4758  data: 0.0005  max mem: 19734
Epoch: [48]  [ 300/1251]  eta: 0:08:24  lr: 0.000011  loss: 3.6170 (3.4154)  time: 0.4767  data: 0.0005  max mem: 19734
Epoch: [48]  [ 310/1251]  eta: 0:08:16  lr: 0.000011  loss: 3.6525 (3.4144)  time: 0.4659  data: 0.0005  max mem: 19734
Epoch: [48]  [ 320/1251]  eta: 0:08:10  lr: 0.000011  loss: 3.3640 (3.4148)  time: 0.4608  data: 0.0005  max mem: 19734
Epoch: [48]  [ 330/1251]  eta: 0:08:02  lr: 0.000011  loss: 3.2881 (3.4054)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [48]  [ 340/1251]  eta: 0:07:55  lr: 0.000011  loss: 3.3829 (3.4093)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [48]  [ 350/1251]  eta: 0:07:49  lr: 0.000011  loss: 3.5973 (3.4102)  time: 0.4656  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3352, ratio_loss=0.0047, pruning_loss=0.1189, mse_loss=0.4224
Epoch: [48]  [ 360/1251]  eta: 0:07:42  lr: 0.000011  loss: 3.6007 (3.4109)  time: 0.4664  data: 0.0004  max mem: 19734
Epoch: [48]  [ 370/1251]  eta: 0:07:35  lr: 0.000011  loss: 3.5797 (3.4097)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [48]  [ 380/1251]  eta: 0:07:29  lr: 0.000011  loss: 3.4014 (3.4051)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [48]  [ 390/1251]  eta: 0:07:22  lr: 0.000011  loss: 3.6341 (3.4135)  time: 0.4576  data: 0.0004  max mem: 19734
Epoch: [48]  [ 400/1251]  eta: 0:07:16  lr: 0.000011  loss: 3.6747 (3.4127)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [48]  [ 410/1251]  eta: 0:07:10  lr: 0.000011  loss: 3.5203 (3.4119)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [48]  [ 420/1251]  eta: 0:07:04  lr: 0.000011  loss: 3.6378 (3.4147)  time: 0.4786  data: 0.0004  max mem: 19734
Epoch: [48]  [ 430/1251]  eta: 0:06:59  lr: 0.000011  loss: 3.5678 (3.4102)  time: 0.4907  data: 0.0005  max mem: 19734
Epoch: [48]  [ 440/1251]  eta: 0:06:53  lr: 0.000011  loss: 3.3441 (3.4101)  time: 0.4681  data: 0.0004  max mem: 19734
Epoch: [48]  [ 450/1251]  eta: 0:06:47  lr: 0.000011  loss: 3.3300 (3.4071)  time: 0.4811  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3746, ratio_loss=0.0052, pruning_loss=0.1186, mse_loss=0.3990
Epoch: [48]  [ 460/1251]  eta: 0:06:42  lr: 0.000011  loss: 3.2101 (3.4035)  time: 0.4970  data: 0.0005  max mem: 19734
Epoch: [48]  [ 470/1251]  eta: 0:06:36  lr: 0.000011  loss: 3.2844 (3.4070)  time: 0.4707  data: 0.0004  max mem: 19734
Epoch: [48]  [ 480/1251]  eta: 0:06:30  lr: 0.000011  loss: 3.5640 (3.4100)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [48]  [ 490/1251]  eta: 0:06:24  lr: 0.000011  loss: 3.7414 (3.4141)  time: 0.4553  data: 0.0005  max mem: 19734
Epoch: [48]  [ 500/1251]  eta: 0:06:19  lr: 0.000011  loss: 3.7525 (3.4196)  time: 0.4678  data: 0.0005  max mem: 19734
Epoch: [48]  [ 510/1251]  eta: 0:06:13  lr: 0.000011  loss: 3.6448 (3.4211)  time: 0.4687  data: 0.0005  max mem: 19734
Epoch: [48]  [ 520/1251]  eta: 0:06:07  lr: 0.000011  loss: 3.2893 (3.4161)  time: 0.4564  data: 0.0004  max mem: 19734
Epoch: [48]  [ 530/1251]  eta: 0:06:02  lr: 0.000011  loss: 3.1675 (3.4124)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [48]  [ 540/1251]  eta: 0:05:56  lr: 0.000011  loss: 3.5231 (3.4180)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [48]  [ 550/1251]  eta: 0:05:50  lr: 0.000011  loss: 3.7376 (3.4184)  time: 0.4582  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4486, ratio_loss=0.0047, pruning_loss=0.1166, mse_loss=0.4008
Epoch: [48]  [ 560/1251]  eta: 0:05:45  lr: 0.000011  loss: 3.4315 (3.4179)  time: 0.4747  data: 0.0005  max mem: 19734
Epoch: [48]  [ 570/1251]  eta: 0:05:40  lr: 0.000011  loss: 3.5036 (3.4206)  time: 0.4735  data: 0.0005  max mem: 19734
Epoch: [48]  [ 580/1251]  eta: 0:05:34  lr: 0.000011  loss: 3.3817 (3.4167)  time: 0.4640  data: 0.0005  max mem: 19734
Epoch: [48]  [ 590/1251]  eta: 0:05:29  lr: 0.000011  loss: 3.3202 (3.4204)  time: 0.4847  data: 0.0005  max mem: 19734
Epoch: [48]  [ 600/1251]  eta: 0:05:24  lr: 0.000011  loss: 3.6295 (3.4219)  time: 0.4787  data: 0.0005  max mem: 19734
Epoch: [48]  [ 610/1251]  eta: 0:05:19  lr: 0.000011  loss: 3.4716 (3.4219)  time: 0.4713  data: 0.0006  max mem: 19734
Epoch: [48]  [ 620/1251]  eta: 0:05:13  lr: 0.000011  loss: 3.3913 (3.4206)  time: 0.4710  data: 0.0004  max mem: 19734
Epoch: [48]  [ 630/1251]  eta: 0:05:08  lr: 0.000011  loss: 3.5644 (3.4213)  time: 0.4585  data: 0.0005  max mem: 19734
Epoch: [48]  [ 640/1251]  eta: 0:05:03  lr: 0.000011  loss: 3.5789 (3.4236)  time: 0.4580  data: 0.0005  max mem: 19734
Epoch: [48]  [ 650/1251]  eta: 0:04:58  lr: 0.000011  loss: 3.6271 (3.4235)  time: 0.4669  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4175, ratio_loss=0.0049, pruning_loss=0.1179, mse_loss=0.3910
Epoch: [48]  [ 660/1251]  eta: 0:04:52  lr: 0.000011  loss: 3.5432 (3.4209)  time: 0.4658  data: 0.0005  max mem: 19734
Epoch: [48]  [ 670/1251]  eta: 0:04:47  lr: 0.000011  loss: 3.3722 (3.4207)  time: 0.4553  data: 0.0005  max mem: 19734
Epoch: [48]  [ 680/1251]  eta: 0:04:42  lr: 0.000011  loss: 3.4766 (3.4190)  time: 0.4557  data: 0.0006  max mem: 19734
Epoch: [48]  [ 690/1251]  eta: 0:04:36  lr: 0.000011  loss: 3.6825 (3.4232)  time: 0.4581  data: 0.0006  max mem: 19734
Epoch: [48]  [ 700/1251]  eta: 0:04:31  lr: 0.000011  loss: 3.7829 (3.4259)  time: 0.4590  data: 0.0005  max mem: 19734
Epoch: [48]  [ 710/1251]  eta: 0:04:26  lr: 0.000011  loss: 3.7320 (3.4287)  time: 0.4773  data: 0.0005  max mem: 19734
Epoch: [48]  [ 720/1251]  eta: 0:04:21  lr: 0.000011  loss: 3.3145 (3.4233)  time: 0.4859  data: 0.0005  max mem: 19734
Epoch: [48]  [ 730/1251]  eta: 0:04:16  lr: 0.000011  loss: 3.2933 (3.4234)  time: 0.4632  data: 0.0004  max mem: 19734
Epoch: [48]  [ 740/1251]  eta: 0:04:11  lr: 0.000011  loss: 3.4675 (3.4234)  time: 0.4689  data: 0.0004  max mem: 19734
Epoch: [48]  [ 750/1251]  eta: 0:04:06  lr: 0.000011  loss: 3.6181 (3.4250)  time: 0.4839  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4284, ratio_loss=0.0049, pruning_loss=0.1185, mse_loss=0.4048
Epoch: [48]  [ 760/1251]  eta: 0:04:01  lr: 0.000011  loss: 3.7319 (3.4262)  time: 0.4678  data: 0.0006  max mem: 19734
Epoch: [48]  [ 770/1251]  eta: 0:03:56  lr: 0.000011  loss: 3.5419 (3.4261)  time: 0.4550  data: 0.0006  max mem: 19734
Epoch: [48]  [ 780/1251]  eta: 0:03:51  lr: 0.000011  loss: 3.5483 (3.4280)  time: 0.4564  data: 0.0006  max mem: 19734
Epoch: [48]  [ 790/1251]  eta: 0:03:46  lr: 0.000011  loss: 3.5992 (3.4310)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [48]  [ 800/1251]  eta: 0:03:41  lr: 0.000011  loss: 3.5879 (3.4326)  time: 0.4657  data: 0.0005  max mem: 19734
Epoch: [48]  [ 810/1251]  eta: 0:03:35  lr: 0.000011  loss: 3.4495 (3.4282)  time: 0.4658  data: 0.0005  max mem: 19734
Epoch: [48]  [ 820/1251]  eta: 0:03:30  lr: 0.000011  loss: 3.1541 (3.4262)  time: 0.4562  data: 0.0005  max mem: 19734
Epoch: [48]  [ 830/1251]  eta: 0:03:25  lr: 0.000011  loss: 3.5221 (3.4280)  time: 0.4567  data: 0.0005  max mem: 19734
Epoch: [48]  [ 840/1251]  eta: 0:03:20  lr: 0.000011  loss: 3.5221 (3.4274)  time: 0.4585  data: 0.0005  max mem: 19734
Epoch: [48]  [ 850/1251]  eta: 0:03:15  lr: 0.000011  loss: 3.4522 (3.4249)  time: 0.4711  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4000, ratio_loss=0.0049, pruning_loss=0.1179, mse_loss=0.3980
Epoch: [48]  [ 860/1251]  eta: 0:03:10  lr: 0.000011  loss: 3.6117 (3.4268)  time: 0.4817  data: 0.0004  max mem: 19734
Epoch: [48]  [ 870/1251]  eta: 0:03:06  lr: 0.000011  loss: 3.6117 (3.4267)  time: 0.4755  data: 0.0004  max mem: 19734
Epoch: [48]  [ 880/1251]  eta: 0:03:01  lr: 0.000011  loss: 3.4627 (3.4276)  time: 0.4806  data: 0.0005  max mem: 19734
Epoch: [48]  [ 890/1251]  eta: 0:02:56  lr: 0.000011  loss: 3.3407 (3.4248)  time: 0.4727  data: 0.0004  max mem: 19734
Epoch: [48]  [ 900/1251]  eta: 0:02:51  lr: 0.000011  loss: 3.0848 (3.4251)  time: 0.4613  data: 0.0004  max mem: 19734
Epoch: [48]  [ 910/1251]  eta: 0:02:46  lr: 0.000011  loss: 3.4531 (3.4250)  time: 0.4606  data: 0.0004  max mem: 19734
Epoch: [48]  [ 920/1251]  eta: 0:02:41  lr: 0.000011  loss: 3.4612 (3.4247)  time: 0.4518  data: 0.0005  max mem: 19734
Epoch: [48]  [ 930/1251]  eta: 0:02:36  lr: 0.000011  loss: 3.6213 (3.4272)  time: 0.4539  data: 0.0005  max mem: 19734
Epoch: [48]  [ 940/1251]  eta: 0:02:31  lr: 0.000011  loss: 3.6033 (3.4286)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [48]  [ 950/1251]  eta: 0:02:26  lr: 0.000011  loss: 3.4398 (3.4282)  time: 0.4660  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4288, ratio_loss=0.0047, pruning_loss=0.1184, mse_loss=0.4043
Epoch: [48]  [ 960/1251]  eta: 0:02:21  lr: 0.000011  loss: 3.2790 (3.4256)  time: 0.4656  data: 0.0004  max mem: 19734
Epoch: [48]  [ 970/1251]  eta: 0:02:16  lr: 0.000011  loss: 3.2790 (3.4243)  time: 0.4583  data: 0.0004  max mem: 19734
Epoch: [48]  [ 980/1251]  eta: 0:02:11  lr: 0.000011  loss: 3.4122 (3.4238)  time: 0.4614  data: 0.0005  max mem: 19734
Epoch: [48]  [ 990/1251]  eta: 0:02:06  lr: 0.000011  loss: 3.5326 (3.4273)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [48]  [1000/1251]  eta: 0:02:01  lr: 0.000011  loss: 3.5905 (3.4263)  time: 0.4784  data: 0.0005  max mem: 19734
Epoch: [48]  [1010/1251]  eta: 0:01:56  lr: 0.000011  loss: 3.3844 (3.4268)  time: 0.4864  data: 0.0005  max mem: 19734
Epoch: [48]  [1020/1251]  eta: 0:01:51  lr: 0.000011  loss: 3.4169 (3.4258)  time: 0.4621  data: 0.0005  max mem: 19734
Epoch: [48]  [1030/1251]  eta: 0:01:47  lr: 0.000011  loss: 3.3178 (3.4243)  time: 0.4780  data: 0.0005  max mem: 19734
Epoch: [48]  [1040/1251]  eta: 0:01:42  lr: 0.000011  loss: 3.1696 (3.4236)  time: 0.4833  data: 0.0004  max mem: 19734
Epoch: [48]  [1050/1251]  eta: 0:01:37  lr: 0.000011  loss: 3.3207 (3.4216)  time: 0.4596  data: 0.0008  max mem: 19734
loss info: cls_loss=3.3391, ratio_loss=0.0048, pruning_loss=0.1191, mse_loss=0.4040
Epoch: [48]  [1060/1251]  eta: 0:01:32  lr: 0.000011  loss: 3.3968 (3.4220)  time: 0.4540  data: 0.0008  max mem: 19734
Epoch: [48]  [1070/1251]  eta: 0:01:27  lr: 0.000011  loss: 3.3968 (3.4224)  time: 0.4558  data: 0.0007  max mem: 19734
Epoch: [48]  [1080/1251]  eta: 0:01:22  lr: 0.000011  loss: 3.4034 (3.4218)  time: 0.4546  data: 0.0007  max mem: 19734
Epoch: [48]  [1090/1251]  eta: 0:01:17  lr: 0.000011  loss: 3.4063 (3.4202)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [48]  [1100/1251]  eta: 0:01:12  lr: 0.000011  loss: 3.4612 (3.4212)  time: 0.4654  data: 0.0005  max mem: 19734
Epoch: [48]  [1110/1251]  eta: 0:01:08  lr: 0.000011  loss: 3.6329 (3.4212)  time: 0.4649  data: 0.0005  max mem: 19734
Epoch: [48]  [1120/1251]  eta: 0:01:03  lr: 0.000011  loss: 3.2570 (3.4170)  time: 0.4524  data: 0.0005  max mem: 19734
Epoch: [48]  [1130/1251]  eta: 0:00:58  lr: 0.000011  loss: 3.2570 (3.4171)  time: 0.4534  data: 0.0007  max mem: 19734
Epoch: [48]  [1140/1251]  eta: 0:00:53  lr: 0.000011  loss: 3.3551 (3.4154)  time: 0.4737  data: 0.0006  max mem: 19734
Epoch: [48]  [1150/1251]  eta: 0:00:48  lr: 0.000011  loss: 3.2588 (3.4148)  time: 0.4835  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3102, ratio_loss=0.0046, pruning_loss=0.1201, mse_loss=0.4246
Epoch: [48]  [1160/1251]  eta: 0:00:43  lr: 0.000011  loss: 3.2047 (3.4135)  time: 0.4724  data: 0.0005  max mem: 19734
Epoch: [48]  [1170/1251]  eta: 0:00:39  lr: 0.000011  loss: 3.4243 (3.4153)  time: 0.4835  data: 0.0005  max mem: 19734
Epoch: [48]  [1180/1251]  eta: 0:00:34  lr: 0.000011  loss: 3.5943 (3.4157)  time: 0.4752  data: 0.0005  max mem: 19734
Epoch: [48]  [1190/1251]  eta: 0:00:29  lr: 0.000011  loss: 3.5211 (3.4167)  time: 0.4641  data: 0.0007  max mem: 19734
Epoch: [48]  [1200/1251]  eta: 0:00:24  lr: 0.000011  loss: 3.5211 (3.4170)  time: 0.4640  data: 0.0005  max mem: 19734
Epoch: [48]  [1210/1251]  eta: 0:00:19  lr: 0.000011  loss: 3.4181 (3.4168)  time: 0.4516  data: 0.0002  max mem: 19734
Epoch: [48]  [1220/1251]  eta: 0:00:14  lr: 0.000011  loss: 3.4487 (3.4176)  time: 0.4537  data: 0.0002  max mem: 19734
Epoch: [48]  [1230/1251]  eta: 0:00:10  lr: 0.000011  loss: 3.6358 (3.4198)  time: 0.4529  data: 0.0002  max mem: 19734
Epoch: [48]  [1240/1251]  eta: 0:00:05  lr: 0.000011  loss: 3.2276 (3.4161)  time: 0.4472  data: 0.0002  max mem: 19734
Epoch: [48]  [1250/1251]  eta: 0:00:00  lr: 0.000011  loss: 3.1580 (3.4164)  time: 0.4555  data: 0.0002  max mem: 19734
Epoch: [48] Total time: 0:10:02 (0.4815 s / it)
Averaged stats: lr: 0.000011  loss: 3.1580 (3.4052)
Test:  [  0/261]  eta: 2:10:11  loss: 0.6641 (0.6641)  acc1: 82.8125 (82.8125)  acc5: 95.8333 (95.8333)  time: 29.9309  data: 29.8431  max mem: 19734
Test:  [ 10/261]  eta: 0:13:03  loss: 0.6641 (0.7227)  acc1: 82.8125 (83.5227)  acc5: 97.3958 (96.1648)  time: 3.1235  data: 2.8907  max mem: 19734
Test:  [ 20/261]  eta: 0:07:05  loss: 0.9621 (0.8910)  acc1: 78.1250 (79.0427)  acc5: 93.7500 (94.6429)  time: 0.3570  data: 0.1075  max mem: 19734
Test:  [ 30/261]  eta: 0:04:50  loss: 0.7848 (0.8095)  acc1: 84.8958 (81.9220)  acc5: 93.7500 (95.0605)  time: 0.2343  data: 0.0188  max mem: 19734
Test:  [ 40/261]  eta: 0:04:05  loss: 0.5760 (0.7752)  acc1: 87.5000 (82.9522)  acc5: 96.3542 (95.3506)  time: 0.4230  data: 0.2692  max mem: 19734
Test:  [ 50/261]  eta: 0:03:17  loss: 0.8988 (0.8345)  acc1: 79.1667 (81.3215)  acc5: 95.8333 (95.0878)  time: 0.4306  data: 0.2691  max mem: 19734
Test:  [ 60/261]  eta: 0:02:43  loss: 0.9557 (0.8422)  acc1: 76.5625 (80.8487)  acc5: 94.7917 (95.1417)  time: 0.2052  data: 0.0173  max mem: 19734
Test:  [ 70/261]  eta: 0:02:34  loss: 0.8936 (0.8448)  acc1: 78.6458 (80.4871)  acc5: 96.3542 (95.3418)  time: 0.4959  data: 0.3472  max mem: 19734
Test:  [ 80/261]  eta: 0:02:12  loss: 0.8367 (0.8457)  acc1: 79.6875 (80.5363)  acc5: 96.8750 (95.4475)  time: 0.4768  data: 0.3433  max mem: 19734
Test:  [ 90/261]  eta: 0:01:55  loss: 0.7947 (0.8314)  acc1: 82.2917 (80.9352)  acc5: 96.3542 (95.5414)  time: 0.1962  data: 0.0101  max mem: 19734
Test:  [100/261]  eta: 0:01:49  loss: 0.8154 (0.8359)  acc1: 83.3333 (80.8014)  acc5: 95.3125 (95.6013)  time: 0.4682  data: 0.2198  max mem: 19734
Test:  [110/261]  eta: 0:01:36  loss: 0.8568 (0.8600)  acc1: 75.5208 (80.2506)  acc5: 94.7917 (95.2843)  time: 0.4548  data: 0.2188  max mem: 19734
Test:  [120/261]  eta: 0:01:24  loss: 1.2053 (0.9010)  acc1: 71.3542 (79.3604)  acc5: 89.0625 (94.7185)  time: 0.1874  data: 0.0142  max mem: 19734
Test:  [130/261]  eta: 0:01:14  loss: 1.4086 (0.9479)  acc1: 69.2708 (78.4311)  acc5: 87.5000 (94.1039)  time: 0.2139  data: 0.0221  max mem: 19734
Test:  [140/261]  eta: 0:01:08  loss: 1.3379 (0.9746)  acc1: 69.7917 (77.7556)  acc5: 89.0625 (93.8313)  time: 0.3783  data: 0.1998  max mem: 19734
Test:  [150/261]  eta: 0:00:59  loss: 1.2089 (0.9790)  acc1: 72.3958 (77.8077)  acc5: 91.6667 (93.7155)  time: 0.2866  data: 0.1907  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: 0.9678 (0.9982)  acc1: 77.6042 (77.5265)  acc5: 92.7083 (93.4103)  time: 0.1237  data: 0.0085  max mem: 19734
Test:  [170/261]  eta: 0:00:47  loss: 1.2932 (1.0287)  acc1: 65.1042 (76.7788)  acc5: 87.5000 (93.0708)  time: 0.4022  data: 0.2846  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: 1.4254 (1.0454)  acc1: 64.5833 (76.3467)  acc5: 88.5417 (92.9155)  time: 0.3514  data: 0.2811  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: 1.3453 (1.0576)  acc1: 67.1875 (76.1207)  acc5: 90.6250 (92.7601)  time: 0.0736  data: 0.0029  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: 1.3453 (1.0732)  acc1: 71.8750 (75.8344)  acc5: 89.0625 (92.5036)  time: 0.0725  data: 0.0016  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: 1.3695 (1.0864)  acc1: 71.3542 (75.5431)  acc5: 86.9792 (92.3282)  time: 0.0648  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: 1.4338 (1.1064)  acc1: 67.1875 (75.0259)  acc5: 88.0208 (92.1239)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4143 (1.1154)  acc1: 65.6250 (74.7858)  acc5: 89.5833 (92.0477)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: 1.3082 (1.1239)  acc1: 68.7500 (74.5807)  acc5: 91.6667 (91.9908)  time: 0.0614  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0566 (1.1168)  acc1: 76.5625 (74.7448)  acc5: 93.7500 (92.1024)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9719 (1.1161)  acc1: 76.5625 (74.7500)  acc5: 95.3125 (92.1680)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3629 s / it)
* Acc@1 74.750 Acc@5 92.168 loss 1.116
Accuracy of the network on the 50000 test images: 74.8%
Max accuracy: 74.77%
loss info: cls_loss=3.4214, ratio_loss=0.0045, pruning_loss=0.1179, mse_loss=0.4028
Epoch: [49]  [   0/1251]  eta: 6:15:09  lr: 0.000011  loss: 3.6385 (3.6385)  time: 17.9934  data: 9.0092  max mem: 19734
Epoch: [49]  [  10/1251]  eta: 0:44:07  lr: 0.000011  loss: 3.5618 (3.4548)  time: 2.1332  data: 0.8196  max mem: 19734
Epoch: [49]  [  20/1251]  eta: 0:27:28  lr: 0.000011  loss: 3.4225 (3.3927)  time: 0.5067  data: 0.0006  max mem: 19734
Epoch: [49]  [  30/1251]  eta: 0:21:54  lr: 0.000011  loss: 3.4196 (3.3913)  time: 0.4951  data: 0.0005  max mem: 19734
Epoch: [49]  [  40/1251]  eta: 0:18:45  lr: 0.000011  loss: 3.3653 (3.3984)  time: 0.4992  data: 0.0005  max mem: 19734
Epoch: [49]  [  50/1251]  eta: 0:16:45  lr: 0.000011  loss: 3.5230 (3.4216)  time: 0.4666  data: 0.0006  max mem: 19734
Epoch: [49]  [  60/1251]  eta: 0:15:29  lr: 0.000011  loss: 3.4727 (3.3652)  time: 0.4753  data: 0.0006  max mem: 19734
Epoch: [49]  [  70/1251]  eta: 0:14:33  lr: 0.000011  loss: 3.3919 (3.3999)  time: 0.4894  data: 0.0004  max mem: 19734
Epoch: [49]  [  80/1251]  eta: 0:13:45  lr: 0.000011  loss: 3.5289 (3.3830)  time: 0.4734  data: 0.0005  max mem: 19734
Epoch: [49]  [  90/1251]  eta: 0:13:07  lr: 0.000011  loss: 3.3666 (3.3796)  time: 0.4613  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3314, ratio_loss=0.0048, pruning_loss=0.1199, mse_loss=0.4210
Epoch: [49]  [ 100/1251]  eta: 0:12:35  lr: 0.000011  loss: 3.3121 (3.3578)  time: 0.4599  data: 0.0005  max mem: 19734
Epoch: [49]  [ 110/1251]  eta: 0:12:10  lr: 0.000011  loss: 3.3121 (3.3607)  time: 0.4687  data: 0.0008  max mem: 19734
Epoch: [49]  [ 120/1251]  eta: 0:11:47  lr: 0.000011  loss: 3.4401 (3.3627)  time: 0.4701  data: 0.0007  max mem: 19734
Epoch: [49]  [ 130/1251]  eta: 0:11:26  lr: 0.000011  loss: 3.3155 (3.3582)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [49]  [ 140/1251]  eta: 0:11:08  lr: 0.000011  loss: 3.2295 (3.3563)  time: 0.4597  data: 0.0005  max mem: 19734
Epoch: [49]  [ 150/1251]  eta: 0:10:52  lr: 0.000011  loss: 3.4427 (3.3583)  time: 0.4590  data: 0.0006  max mem: 19734
Epoch: [49]  [ 160/1251]  eta: 0:10:37  lr: 0.000011  loss: 3.4613 (3.3634)  time: 0.4586  data: 0.0006  max mem: 19734
Epoch: [49]  [ 170/1251]  eta: 0:10:24  lr: 0.000011  loss: 3.2860 (3.3451)  time: 0.4663  data: 0.0005  max mem: 19734
Epoch: [49]  [ 180/1251]  eta: 0:10:13  lr: 0.000011  loss: 3.2813 (3.3511)  time: 0.4818  data: 0.0005  max mem: 19734
Epoch: [49]  [ 190/1251]  eta: 0:10:02  lr: 0.000011  loss: 3.5521 (3.3533)  time: 0.4851  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3181, ratio_loss=0.0050, pruning_loss=0.1204, mse_loss=0.4121
Epoch: [49]  [ 200/1251]  eta: 0:09:53  lr: 0.000011  loss: 3.3168 (3.3498)  time: 0.4871  data: 0.0005  max mem: 19734
Epoch: [49]  [ 210/1251]  eta: 0:09:43  lr: 0.000011  loss: 3.2428 (3.3339)  time: 0.4846  data: 0.0005  max mem: 19734
Epoch: [49]  [ 220/1251]  eta: 0:09:32  lr: 0.000011  loss: 3.5035 (3.3401)  time: 0.4672  data: 0.0005  max mem: 19734
Epoch: [49]  [ 230/1251]  eta: 0:09:22  lr: 0.000011  loss: 3.5492 (3.3481)  time: 0.4578  data: 0.0005  max mem: 19734
Epoch: [49]  [ 240/1251]  eta: 0:09:13  lr: 0.000011  loss: 3.5222 (3.3555)  time: 0.4554  data: 0.0005  max mem: 19734
Epoch: [49]  [ 250/1251]  eta: 0:09:04  lr: 0.000011  loss: 3.4464 (3.3516)  time: 0.4591  data: 0.0005  max mem: 19734
Epoch: [49]  [ 260/1251]  eta: 0:08:55  lr: 0.000011  loss: 3.4874 (3.3526)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [49]  [ 270/1251]  eta: 0:08:47  lr: 0.000011  loss: 3.4874 (3.3497)  time: 0.4534  data: 0.0005  max mem: 19734
Epoch: [49]  [ 280/1251]  eta: 0:08:38  lr: 0.000011  loss: 3.4738 (3.3518)  time: 0.4545  data: 0.0004  max mem: 19734
Epoch: [49]  [ 290/1251]  eta: 0:08:30  lr: 0.000011  loss: 3.5595 (3.3562)  time: 0.4564  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3364, ratio_loss=0.0050, pruning_loss=0.1215, mse_loss=0.4315
Epoch: [49]  [ 300/1251]  eta: 0:08:23  lr: 0.000011  loss: 3.5385 (3.3533)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [49]  [ 310/1251]  eta: 0:08:15  lr: 0.000011  loss: 3.4395 (3.3528)  time: 0.4570  data: 0.0005  max mem: 19734
Epoch: [49]  [ 320/1251]  eta: 0:08:10  lr: 0.000011  loss: 3.2979 (3.3487)  time: 0.4931  data: 0.0005  max mem: 19734
Epoch: [49]  [ 330/1251]  eta: 0:08:03  lr: 0.000011  loss: 3.3275 (3.3575)  time: 0.5011  data: 0.0006  max mem: 19734
Epoch: [49]  [ 340/1251]  eta: 0:07:56  lr: 0.000011  loss: 3.5294 (3.3580)  time: 0.4684  data: 0.0005  max mem: 19734
Epoch: [49]  [ 350/1251]  eta: 0:07:50  lr: 0.000011  loss: 3.4528 (3.3546)  time: 0.4778  data: 0.0005  max mem: 19734
Epoch: [49]  [ 360/1251]  eta: 0:07:44  lr: 0.000011  loss: 3.4537 (3.3572)  time: 0.4894  data: 0.0004  max mem: 19734
Epoch: [49]  [ 370/1251]  eta: 0:07:38  lr: 0.000011  loss: 3.4537 (3.3592)  time: 0.4744  data: 0.0004  max mem: 19734
Epoch: [49]  [ 380/1251]  eta: 0:07:31  lr: 0.000011  loss: 3.6382 (3.3627)  time: 0.4623  data: 0.0004  max mem: 19734
Epoch: [49]  [ 390/1251]  eta: 0:07:25  lr: 0.000011  loss: 3.2663 (3.3571)  time: 0.4590  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3742, ratio_loss=0.0050, pruning_loss=0.1187, mse_loss=0.4022
Epoch: [49]  [ 400/1251]  eta: 0:07:19  lr: 0.000011  loss: 3.4277 (3.3644)  time: 0.4721  data: 0.0004  max mem: 19734
Epoch: [49]  [ 410/1251]  eta: 0:07:12  lr: 0.000011  loss: 3.5383 (3.3651)  time: 0.4747  data: 0.0004  max mem: 19734
Epoch: [49]  [ 420/1251]  eta: 0:07:06  lr: 0.000011  loss: 3.5220 (3.3657)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [49]  [ 430/1251]  eta: 0:07:00  lr: 0.000011  loss: 3.4246 (3.3632)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [49]  [ 440/1251]  eta: 0:06:54  lr: 0.000011  loss: 3.4459 (3.3684)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [49]  [ 450/1251]  eta: 0:06:48  lr: 0.000011  loss: 3.5890 (3.3696)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [49]  [ 460/1251]  eta: 0:06:42  lr: 0.000011  loss: 3.4443 (3.3690)  time: 0.4780  data: 0.0004  max mem: 19734
Epoch: [49]  [ 470/1251]  eta: 0:06:37  lr: 0.000011  loss: 3.2207 (3.3647)  time: 0.4879  data: 0.0004  max mem: 19734
Epoch: [49]  [ 480/1251]  eta: 0:06:31  lr: 0.000011  loss: 3.1828 (3.3639)  time: 0.4773  data: 0.0004  max mem: 19734
Epoch: [49]  [ 490/1251]  eta: 0:06:26  lr: 0.000011  loss: 3.5479 (3.3684)  time: 0.4787  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3436, ratio_loss=0.0048, pruning_loss=0.1200, mse_loss=0.4137
Epoch: [49]  [ 500/1251]  eta: 0:06:20  lr: 0.000011  loss: 3.5479 (3.3649)  time: 0.4890  data: 0.0005  max mem: 19734
Epoch: [49]  [ 510/1251]  eta: 0:06:15  lr: 0.000011  loss: 3.3870 (3.3656)  time: 0.4780  data: 0.0005  max mem: 19734
Epoch: [49]  [ 520/1251]  eta: 0:06:09  lr: 0.000011  loss: 3.6303 (3.3703)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [49]  [ 530/1251]  eta: 0:06:03  lr: 0.000011  loss: 3.5579 (3.3669)  time: 0.4581  data: 0.0006  max mem: 19734
Epoch: [49]  [ 540/1251]  eta: 0:05:58  lr: 0.000011  loss: 3.5001 (3.3664)  time: 0.4555  data: 0.0005  max mem: 19734
Epoch: [49]  [ 550/1251]  eta: 0:05:52  lr: 0.000011  loss: 3.4131 (3.3625)  time: 0.4641  data: 0.0004  max mem: 19734
Epoch: [49]  [ 560/1251]  eta: 0:05:47  lr: 0.000011  loss: 3.2150 (3.3604)  time: 0.4690  data: 0.0004  max mem: 19734
Epoch: [49]  [ 570/1251]  eta: 0:05:41  lr: 0.000011  loss: 3.5144 (3.3643)  time: 0.4585  data: 0.0004  max mem: 19734
Epoch: [49]  [ 580/1251]  eta: 0:05:36  lr: 0.000011  loss: 3.4415 (3.3636)  time: 0.4574  data: 0.0004  max mem: 19734
Epoch: [49]  [ 590/1251]  eta: 0:05:30  lr: 0.000011  loss: 3.4702 (3.3666)  time: 0.4598  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3613, ratio_loss=0.0050, pruning_loss=0.1198, mse_loss=0.4038
Epoch: [49]  [ 600/1251]  eta: 0:05:25  lr: 0.000011  loss: 3.5489 (3.3682)  time: 0.4592  data: 0.0004  max mem: 19734
Epoch: [49]  [ 610/1251]  eta: 0:05:20  lr: 0.000011  loss: 3.5395 (3.3705)  time: 0.4887  data: 0.0007  max mem: 19734
Epoch: [49]  [ 620/1251]  eta: 0:05:15  lr: 0.000011  loss: 3.6382 (3.3749)  time: 0.4979  data: 0.0007  max mem: 19734
Epoch: [49]  [ 630/1251]  eta: 0:05:09  lr: 0.000011  loss: 3.6132 (3.3773)  time: 0.4683  data: 0.0004  max mem: 19734
Epoch: [49]  [ 640/1251]  eta: 0:05:04  lr: 0.000011  loss: 3.6080 (3.3807)  time: 0.4806  data: 0.0005  max mem: 19734
Epoch: [49]  [ 650/1251]  eta: 0:04:59  lr: 0.000011  loss: 3.5921 (3.3829)  time: 0.4884  data: 0.0005  max mem: 19734
Epoch: [49]  [ 660/1251]  eta: 0:04:54  lr: 0.000011  loss: 3.5028 (3.3800)  time: 0.4663  data: 0.0004  max mem: 19734
Epoch: [49]  [ 670/1251]  eta: 0:04:48  lr: 0.000011  loss: 3.4517 (3.3807)  time: 0.4556  data: 0.0004  max mem: 19734
Epoch: [49]  [ 680/1251]  eta: 0:04:43  lr: 0.000011  loss: 3.4458 (3.3797)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [49]  [ 690/1251]  eta: 0:04:38  lr: 0.000011  loss: 3.4440 (3.3788)  time: 0.4686  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4362, ratio_loss=0.0049, pruning_loss=0.1183, mse_loss=0.4093
Epoch: [49]  [ 700/1251]  eta: 0:04:33  lr: 0.000011  loss: 3.5941 (3.3812)  time: 0.4672  data: 0.0005  max mem: 19734
Epoch: [49]  [ 710/1251]  eta: 0:04:27  lr: 0.000011  loss: 3.6117 (3.3806)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [49]  [ 720/1251]  eta: 0:04:22  lr: 0.000011  loss: 3.5290 (3.3824)  time: 0.4558  data: 0.0004  max mem: 19734
Epoch: [49]  [ 730/1251]  eta: 0:04:17  lr: 0.000011  loss: 3.4812 (3.3833)  time: 0.4546  data: 0.0005  max mem: 19734
Epoch: [49]  [ 740/1251]  eta: 0:04:12  lr: 0.000011  loss: 3.3930 (3.3828)  time: 0.4527  data: 0.0005  max mem: 19734
Epoch: [49]  [ 750/1251]  eta: 0:04:07  lr: 0.000011  loss: 3.5608 (3.3841)  time: 0.4757  data: 0.0004  max mem: 19734
Epoch: [49]  [ 760/1251]  eta: 0:04:02  lr: 0.000011  loss: 3.6185 (3.3852)  time: 0.4991  data: 0.0006  max mem: 19734
Epoch: [49]  [ 770/1251]  eta: 0:03:57  lr: 0.000011  loss: 3.5262 (3.3841)  time: 0.4779  data: 0.0006  max mem: 19734
Epoch: [49]  [ 780/1251]  eta: 0:03:52  lr: 0.000011  loss: 3.3094 (3.3815)  time: 0.4680  data: 0.0004  max mem: 19734
Epoch: [49]  [ 790/1251]  eta: 0:03:47  lr: 0.000011  loss: 3.4120 (3.3825)  time: 0.4863  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3525, ratio_loss=0.0049, pruning_loss=0.1192, mse_loss=0.4265
Epoch: [49]  [ 800/1251]  eta: 0:03:42  lr: 0.000011  loss: 3.5120 (3.3806)  time: 0.4739  data: 0.0005  max mem: 19734
Epoch: [49]  [ 810/1251]  eta: 0:03:36  lr: 0.000011  loss: 3.1331 (3.3782)  time: 0.4548  data: 0.0005  max mem: 19734
Epoch: [49]  [ 820/1251]  eta: 0:03:31  lr: 0.000011  loss: 3.1331 (3.3785)  time: 0.4541  data: 0.0005  max mem: 19734
Epoch: [49]  [ 830/1251]  eta: 0:03:26  lr: 0.000011  loss: 3.3971 (3.3768)  time: 0.4538  data: 0.0005  max mem: 19734
Epoch: [49]  [ 840/1251]  eta: 0:03:21  lr: 0.000011  loss: 3.4772 (3.3773)  time: 0.4621  data: 0.0006  max mem: 19734
Epoch: [49]  [ 850/1251]  eta: 0:03:16  lr: 0.000011  loss: 3.5329 (3.3773)  time: 0.4629  data: 0.0006  max mem: 19734
Epoch: [49]  [ 860/1251]  eta: 0:03:11  lr: 0.000011  loss: 3.4657 (3.3765)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [49]  [ 870/1251]  eta: 0:03:06  lr: 0.000011  loss: 3.4921 (3.3765)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [49]  [ 880/1251]  eta: 0:03:01  lr: 0.000011  loss: 3.5430 (3.3798)  time: 0.4588  data: 0.0005  max mem: 19734
Epoch: [49]  [ 890/1251]  eta: 0:02:56  lr: 0.000011  loss: 3.6318 (3.3804)  time: 0.4624  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3454, ratio_loss=0.0049, pruning_loss=0.1193, mse_loss=0.4101
Epoch: [49]  [ 900/1251]  eta: 0:02:51  lr: 0.000011  loss: 3.3110 (3.3793)  time: 0.4919  data: 0.0006  max mem: 19734
Epoch: [49]  [ 910/1251]  eta: 0:02:46  lr: 0.000011  loss: 3.5043 (3.3792)  time: 0.5006  data: 0.0004  max mem: 19734
Epoch: [49]  [ 920/1251]  eta: 0:02:41  lr: 0.000011  loss: 3.5043 (3.3780)  time: 0.4795  data: 0.0004  max mem: 19734
Epoch: [49]  [ 930/1251]  eta: 0:02:36  lr: 0.000011  loss: 3.4160 (3.3785)  time: 0.4768  data: 0.0004  max mem: 19734
Epoch: [49]  [ 940/1251]  eta: 0:02:32  lr: 0.000011  loss: 3.3885 (3.3767)  time: 0.4783  data: 0.0004  max mem: 19734
Epoch: [49]  [ 950/1251]  eta: 0:02:27  lr: 0.000011  loss: 3.1626 (3.3778)  time: 0.4731  data: 0.0003  max mem: 19734
Epoch: [49]  [ 960/1251]  eta: 0:02:22  lr: 0.000011  loss: 3.3301 (3.3756)  time: 0.4651  data: 0.0005  max mem: 19734
Epoch: [49]  [ 970/1251]  eta: 0:02:17  lr: 0.000011  loss: 3.4522 (3.3753)  time: 0.4644  data: 0.0006  max mem: 19734
Epoch: [49]  [ 980/1251]  eta: 0:02:12  lr: 0.000011  loss: 3.3947 (3.3743)  time: 0.4622  data: 0.0004  max mem: 19734
Epoch: [49]  [ 990/1251]  eta: 0:02:07  lr: 0.000011  loss: 3.3983 (3.3761)  time: 0.4693  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3379, ratio_loss=0.0050, pruning_loss=0.1205, mse_loss=0.4119
Epoch: [49]  [1000/1251]  eta: 0:02:02  lr: 0.000011  loss: 3.7898 (3.3775)  time: 0.4673  data: 0.0004  max mem: 19734
Epoch: [49]  [1010/1251]  eta: 0:01:57  lr: 0.000011  loss: 3.4108 (3.3770)  time: 0.4585  data: 0.0005  max mem: 19734
Epoch: [49]  [1020/1251]  eta: 0:01:52  lr: 0.000011  loss: 3.4108 (3.3769)  time: 0.4572  data: 0.0005  max mem: 19734
Epoch: [49]  [1030/1251]  eta: 0:01:47  lr: 0.000011  loss: 3.4763 (3.3773)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [49]  [1040/1251]  eta: 0:01:42  lr: 0.000011  loss: 3.5003 (3.3784)  time: 0.4635  data: 0.0005  max mem: 19734
Epoch: [49]  [1050/1251]  eta: 0:01:37  lr: 0.000011  loss: 3.5003 (3.3776)  time: 0.4816  data: 0.0004  max mem: 19734
Epoch: [49]  [1060/1251]  eta: 0:01:32  lr: 0.000011  loss: 3.5026 (3.3778)  time: 0.4845  data: 0.0004  max mem: 19734
Epoch: [49]  [1070/1251]  eta: 0:01:28  lr: 0.000011  loss: 3.6024 (3.3799)  time: 0.4915  data: 0.0004  max mem: 19734
Epoch: [49]  [1080/1251]  eta: 0:01:23  lr: 0.000011  loss: 3.3326 (3.3754)  time: 0.4950  data: 0.0004  max mem: 19734
Epoch: [49]  [1090/1251]  eta: 0:01:18  lr: 0.000011  loss: 3.2074 (3.3765)  time: 0.4722  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3505, ratio_loss=0.0049, pruning_loss=0.1192, mse_loss=0.4092
Epoch: [49]  [1100/1251]  eta: 0:01:13  lr: 0.000011  loss: 3.3858 (3.3772)  time: 0.4589  data: 0.0004  max mem: 19734
Epoch: [49]  [1110/1251]  eta: 0:01:08  lr: 0.000011  loss: 3.3488 (3.3771)  time: 0.4596  data: 0.0004  max mem: 19734
Epoch: [49]  [1120/1251]  eta: 0:01:03  lr: 0.000011  loss: 3.2461 (3.3755)  time: 0.4579  data: 0.0004  max mem: 19734
Epoch: [49]  [1130/1251]  eta: 0:00:58  lr: 0.000011  loss: 3.3217 (3.3741)  time: 0.4678  data: 0.0005  max mem: 19734
Epoch: [49]  [1140/1251]  eta: 0:00:53  lr: 0.000011  loss: 3.4161 (3.3749)  time: 0.4669  data: 0.0004  max mem: 19734
Epoch: [49]  [1150/1251]  eta: 0:00:48  lr: 0.000011  loss: 3.5451 (3.3766)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [49]  [1160/1251]  eta: 0:00:44  lr: 0.000011  loss: 3.7003 (3.3783)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [49]  [1170/1251]  eta: 0:00:39  lr: 0.000011  loss: 3.4723 (3.3779)  time: 0.4572  data: 0.0003  max mem: 19734
Epoch: [49]  [1180/1251]  eta: 0:00:34  lr: 0.000011  loss: 3.3998 (3.3763)  time: 0.4700  data: 0.0003  max mem: 19734
Epoch: [49]  [1190/1251]  eta: 0:00:29  lr: 0.000011  loss: 3.2332 (3.3765)  time: 0.4894  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3380, ratio_loss=0.0049, pruning_loss=0.1197, mse_loss=0.4240
Epoch: [49]  [1200/1251]  eta: 0:00:24  lr: 0.000011  loss: 3.4111 (3.3759)  time: 0.4772  data: 0.0006  max mem: 19734
Epoch: [49]  [1210/1251]  eta: 0:00:19  lr: 0.000011  loss: 3.5583 (3.3773)  time: 0.4508  data: 0.0001  max mem: 19734
Epoch: [49]  [1220/1251]  eta: 0:00:15  lr: 0.000011  loss: 3.2669 (3.3726)  time: 0.4576  data: 0.0001  max mem: 19734
Epoch: [49]  [1230/1251]  eta: 0:00:10  lr: 0.000011  loss: 3.0741 (3.3728)  time: 0.4694  data: 0.0001  max mem: 19734
Epoch: [49]  [1240/1251]  eta: 0:00:05  lr: 0.000011  loss: 3.6326 (3.3752)  time: 0.4586  data: 0.0001  max mem: 19734
Epoch: [49]  [1250/1251]  eta: 0:00:00  lr: 0.000011  loss: 3.7282 (3.3770)  time: 0.4477  data: 0.0001  max mem: 19734
Epoch: [49] Total time: 0:10:05 (0.4838 s / it)
Averaged stats: lr: 0.000011  loss: 3.7282 (3.4009)
Test:  [  0/261]  eta: 1:15:30  loss: 0.7003 (0.7003)  acc1: 82.8125 (82.8125)  acc5: 96.3542 (96.3542)  time: 17.3601  data: 17.1205  max mem: 19734
Test:  [ 10/261]  eta: 0:10:01  loss: 0.6877 (0.7148)  acc1: 85.4167 (84.1383)  acc5: 96.8750 (96.3542)  time: 2.3947  data: 2.1471  max mem: 19734
Test:  [ 20/261]  eta: 0:05:37  loss: 0.8993 (0.8891)  acc1: 80.2083 (79.1171)  acc5: 93.7500 (94.9653)  time: 0.6039  data: 0.3332  max mem: 19734
Test:  [ 30/261]  eta: 0:04:03  loss: 0.8015 (0.8062)  acc1: 83.3333 (81.9220)  acc5: 94.2708 (95.4469)  time: 0.3184  data: 0.0102  max mem: 19734
Test:  [ 40/261]  eta: 0:03:31  loss: 0.5622 (0.7729)  acc1: 87.5000 (82.9903)  acc5: 96.8750 (95.6936)  time: 0.4919  data: 0.2246  max mem: 19734
Test:  [ 50/261]  eta: 0:02:48  loss: 0.8959 (0.8364)  acc1: 78.6458 (81.1479)  acc5: 95.8333 (95.3125)  time: 0.3956  data: 0.2283  max mem: 19734
Test:  [ 60/261]  eta: 0:02:20  loss: 0.9648 (0.8444)  acc1: 75.5208 (80.7804)  acc5: 94.2708 (95.3637)  time: 0.1623  data: 0.0104  max mem: 19734
Test:  [ 70/261]  eta: 0:02:02  loss: 0.9110 (0.8467)  acc1: 77.0833 (80.3477)  acc5: 95.8333 (95.5032)  time: 0.2495  data: 0.0827  max mem: 19734
Test:  [ 80/261]  eta: 0:01:53  loss: 0.8377 (0.8483)  acc1: 79.6875 (80.3948)  acc5: 96.8750 (95.5954)  time: 0.4179  data: 0.2585  max mem: 19734
Test:  [ 90/261]  eta: 0:01:38  loss: 0.8230 (0.8349)  acc1: 82.2917 (80.7406)  acc5: 95.8333 (95.6845)  time: 0.3500  data: 0.1871  max mem: 19734
Test:  [100/261]  eta: 0:01:32  loss: 0.8230 (0.8387)  acc1: 82.8125 (80.6312)  acc5: 95.3125 (95.7405)  time: 0.3705  data: 0.2104  max mem: 19734
Test:  [110/261]  eta: 0:01:21  loss: 0.8677 (0.8624)  acc1: 74.4792 (80.1333)  acc5: 95.3125 (95.4392)  time: 0.3816  data: 0.2139  max mem: 19734
Test:  [120/261]  eta: 0:01:14  loss: 1.2167 (0.9027)  acc1: 70.8333 (79.2528)  acc5: 89.0625 (94.8993)  time: 0.2655  data: 0.0600  max mem: 19734
Test:  [130/261]  eta: 0:01:06  loss: 1.3824 (0.9479)  acc1: 66.6667 (78.2920)  acc5: 87.5000 (94.2589)  time: 0.3183  data: 0.0975  max mem: 19734
Test:  [140/261]  eta: 0:00:59  loss: 1.3380 (0.9755)  acc1: 67.7083 (77.6337)  acc5: 88.0208 (93.9679)  time: 0.2851  data: 0.1027  max mem: 19734
Test:  [150/261]  eta: 0:00:52  loss: 1.1961 (0.9805)  acc1: 72.3958 (77.6490)  acc5: 91.6667 (93.8293)  time: 0.2595  data: 0.1206  max mem: 19734
Test:  [160/261]  eta: 0:00:45  loss: 1.0025 (1.0002)  acc1: 77.6042 (77.3098)  acc5: 91.6667 (93.5333)  time: 0.1847  data: 0.0772  max mem: 19734
Test:  [170/261]  eta: 0:00:39  loss: 1.2760 (1.0288)  acc1: 65.1042 (76.6051)  acc5: 87.5000 (93.1987)  time: 0.1383  data: 0.0577  max mem: 19734
Test:  [180/261]  eta: 0:00:33  loss: 1.4037 (1.0457)  acc1: 64.5833 (76.2287)  acc5: 89.5833 (93.0450)  time: 0.1096  data: 0.0463  max mem: 19734
Test:  [190/261]  eta: 0:00:28  loss: 1.3865 (1.0588)  acc1: 67.7083 (75.9544)  acc5: 90.6250 (92.8638)  time: 0.0621  data: 0.0004  max mem: 19734
Test:  [200/261]  eta: 0:00:23  loss: 1.3909 (1.0750)  acc1: 71.3542 (75.6115)  acc5: 89.0625 (92.6202)  time: 0.0629  data: 0.0012  max mem: 19734
Test:  [210/261]  eta: 0:00:18  loss: 1.4025 (1.0885)  acc1: 70.3125 (75.3357)  acc5: 88.0208 (92.4492)  time: 0.0626  data: 0.0010  max mem: 19734
Test:  [220/261]  eta: 0:00:14  loss: 1.4356 (1.1074)  acc1: 67.1875 (74.8751)  acc5: 88.0208 (92.2252)  time: 0.0622  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:10  loss: 1.4188 (1.1169)  acc1: 65.6250 (74.6280)  acc5: 89.5833 (92.1447)  time: 0.0627  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:06  loss: 1.3547 (1.1260)  acc1: 67.1875 (74.3798)  acc5: 91.1458 (92.0773)  time: 0.0621  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0306 (1.1187)  acc1: 75.0000 (74.5746)  acc5: 92.1875 (92.1771)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9139 (1.1180)  acc1: 78.6458 (74.6080)  acc5: 95.3125 (92.2480)  time: 0.0599  data: 0.0002  max mem: 19734
Test: Total time: 0:01:20 (0.3080 s / it)
* Acc@1 74.608 Acc@5 92.248 loss 1.118
Accuracy of the network on the 50000 test images: 74.6%
Max accuracy: 74.77%
Epoch: [50]  [   0/1251]  eta: 6:06:26  lr: 0.000011  loss: 3.0647 (3.0647)  time: 17.5749  data: 11.1609  max mem: 19734
Epoch: [50]  [  10/1251]  eta: 0:44:24  lr: 0.000011  loss: 3.0647 (3.2266)  time: 2.1470  data: 1.0749  max mem: 19734
Epoch: [50]  [  20/1251]  eta: 0:27:35  lr: 0.000011  loss: 3.2887 (3.3086)  time: 0.5336  data: 0.0333  max mem: 19734
Epoch: [50]  [  30/1251]  eta: 0:21:34  lr: 0.000011  loss: 3.4110 (3.3360)  time: 0.4623  data: 0.0004  max mem: 19734
Epoch: [50]  [  40/1251]  eta: 0:18:25  lr: 0.000011  loss: 3.2903 (3.3062)  time: 0.4599  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3288, ratio_loss=0.0048, pruning_loss=0.1200, mse_loss=0.4153
Epoch: [50]  [  50/1251]  eta: 0:16:30  lr: 0.000011  loss: 3.2903 (3.3156)  time: 0.4604  data: 0.0004  max mem: 19734
Epoch: [50]  [  60/1251]  eta: 0:15:13  lr: 0.000011  loss: 3.5017 (3.3687)  time: 0.4663  data: 0.0004  max mem: 19734
Epoch: [50]  [  70/1251]  eta: 0:14:17  lr: 0.000011  loss: 3.5730 (3.3407)  time: 0.4755  data: 0.0004  max mem: 19734
Epoch: [50]  [  80/1251]  eta: 0:13:39  lr: 0.000011  loss: 3.2855 (3.3352)  time: 0.4954  data: 0.0004  max mem: 19734
Epoch: [50]  [  90/1251]  eta: 0:13:03  lr: 0.000011  loss: 3.4941 (3.3584)  time: 0.4927  data: 0.0004  max mem: 19734
Epoch: [50]  [ 100/1251]  eta: 0:12:34  lr: 0.000011  loss: 3.4941 (3.3395)  time: 0.4765  data: 0.0005  max mem: 19734
Epoch: [50]  [ 110/1251]  eta: 0:12:10  lr: 0.000011  loss: 3.3890 (3.3521)  time: 0.4797  data: 0.0005  max mem: 19734
Epoch: [50]  [ 120/1251]  eta: 0:11:47  lr: 0.000011  loss: 3.7255 (3.3773)  time: 0.4716  data: 0.0005  max mem: 19734
Epoch: [50]  [ 130/1251]  eta: 0:11:26  lr: 0.000011  loss: 3.5716 (3.3655)  time: 0.4607  data: 0.0005  max mem: 19734
Epoch: [50]  [ 140/1251]  eta: 0:11:08  lr: 0.000011  loss: 3.4489 (3.3803)  time: 0.4601  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3948, ratio_loss=0.0048, pruning_loss=0.1174, mse_loss=0.4062
Epoch: [50]  [ 150/1251]  eta: 0:10:52  lr: 0.000011  loss: 3.6117 (3.3808)  time: 0.4648  data: 0.0004  max mem: 19734
Epoch: [50]  [ 160/1251]  eta: 0:10:37  lr: 0.000011  loss: 3.4992 (3.3829)  time: 0.4625  data: 0.0004  max mem: 19734
Epoch: [50]  [ 170/1251]  eta: 0:10:23  lr: 0.000011  loss: 3.4992 (3.3932)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [50]  [ 180/1251]  eta: 0:10:10  lr: 0.000011  loss: 3.4127 (3.3902)  time: 0.4569  data: 0.0005  max mem: 19734
Epoch: [50]  [ 190/1251]  eta: 0:09:58  lr: 0.000011  loss: 3.4127 (3.3916)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [50]  [ 200/1251]  eta: 0:09:47  lr: 0.000011  loss: 3.4450 (3.3787)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [50]  [ 210/1251]  eta: 0:09:36  lr: 0.000011  loss: 3.2615 (3.3704)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [50]  [ 220/1251]  eta: 0:09:29  lr: 0.000011  loss: 3.3377 (3.3665)  time: 0.4832  data: 0.0004  max mem: 19734
Epoch: [50]  [ 230/1251]  eta: 0:09:19  lr: 0.000011  loss: 3.3446 (3.3715)  time: 0.4919  data: 0.0004  max mem: 19734
Epoch: [50]  [ 240/1251]  eta: 0:09:10  lr: 0.000011  loss: 3.5931 (3.3827)  time: 0.4623  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3616, ratio_loss=0.0049, pruning_loss=0.1188, mse_loss=0.3971
Epoch: [50]  [ 250/1251]  eta: 0:09:03  lr: 0.000011  loss: 3.5234 (3.3822)  time: 0.4779  data: 0.0005  max mem: 19734
Epoch: [50]  [ 260/1251]  eta: 0:08:55  lr: 0.000011  loss: 3.4588 (3.3843)  time: 0.4914  data: 0.0005  max mem: 19734
Epoch: [50]  [ 270/1251]  eta: 0:08:47  lr: 0.000011  loss: 3.6098 (3.3863)  time: 0.4690  data: 0.0005  max mem: 19734
Epoch: [50]  [ 280/1251]  eta: 0:08:39  lr: 0.000011  loss: 3.5334 (3.3836)  time: 0.4584  data: 0.0005  max mem: 19734
Epoch: [50]  [ 290/1251]  eta: 0:08:31  lr: 0.000011  loss: 3.4043 (3.3797)  time: 0.4605  data: 0.0005  max mem: 19734
Epoch: [50]  [ 300/1251]  eta: 0:08:24  lr: 0.000011  loss: 3.1778 (3.3676)  time: 0.4662  data: 0.0005  max mem: 19734
Epoch: [50]  [ 310/1251]  eta: 0:08:16  lr: 0.000011  loss: 3.3923 (3.3778)  time: 0.4646  data: 0.0005  max mem: 19734
Epoch: [50]  [ 320/1251]  eta: 0:08:09  lr: 0.000011  loss: 3.6619 (3.3843)  time: 0.4592  data: 0.0005  max mem: 19734
Epoch: [50]  [ 330/1251]  eta: 0:08:02  lr: 0.000011  loss: 3.6869 (3.3923)  time: 0.4631  data: 0.0005  max mem: 19734
Epoch: [50]  [ 340/1251]  eta: 0:07:55  lr: 0.000011  loss: 3.5767 (3.3901)  time: 0.4616  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4345, ratio_loss=0.0049, pruning_loss=0.1177, mse_loss=0.4015
Epoch: [50]  [ 350/1251]  eta: 0:07:48  lr: 0.000011  loss: 3.5767 (3.4014)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [50]  [ 360/1251]  eta: 0:07:42  lr: 0.000011  loss: 3.6258 (3.4035)  time: 0.4783  data: 0.0005  max mem: 19734
Epoch: [50]  [ 370/1251]  eta: 0:07:36  lr: 0.000011  loss: 3.5186 (3.3984)  time: 0.4959  data: 0.0005  max mem: 19734
Epoch: [50]  [ 380/1251]  eta: 0:07:30  lr: 0.000011  loss: 3.5646 (3.4020)  time: 0.4757  data: 0.0004  max mem: 19734
Epoch: [50]  [ 390/1251]  eta: 0:07:24  lr: 0.000011  loss: 3.5646 (3.4011)  time: 0.4760  data: 0.0004  max mem: 19734
Epoch: [50]  [ 400/1251]  eta: 0:07:18  lr: 0.000011  loss: 3.4600 (3.4047)  time: 0.4839  data: 0.0005  max mem: 19734
Epoch: [50]  [ 410/1251]  eta: 0:07:12  lr: 0.000011  loss: 3.3823 (3.4017)  time: 0.4662  data: 0.0005  max mem: 19734
Epoch: [50]  [ 420/1251]  eta: 0:07:06  lr: 0.000011  loss: 3.4985 (3.4087)  time: 0.4556  data: 0.0005  max mem: 19734
Epoch: [50]  [ 430/1251]  eta: 0:06:59  lr: 0.000011  loss: 3.6563 (3.4140)  time: 0.4560  data: 0.0005  max mem: 19734
Epoch: [50]  [ 440/1251]  eta: 0:06:53  lr: 0.000011  loss: 3.5603 (3.4092)  time: 0.4571  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4032, ratio_loss=0.0046, pruning_loss=0.1172, mse_loss=0.4146
Epoch: [50]  [ 450/1251]  eta: 0:06:47  lr: 0.000011  loss: 3.4565 (3.4073)  time: 0.4642  data: 0.0005  max mem: 19734
Epoch: [50]  [ 460/1251]  eta: 0:06:41  lr: 0.000011  loss: 3.4565 (3.4090)  time: 0.4644  data: 0.0005  max mem: 19734
Epoch: [50]  [ 470/1251]  eta: 0:06:35  lr: 0.000011  loss: 3.5416 (3.4091)  time: 0.4563  data: 0.0006  max mem: 19734
Epoch: [50]  [ 480/1251]  eta: 0:06:30  lr: 0.000011  loss: 3.4586 (3.4077)  time: 0.4581  data: 0.0006  max mem: 19734
Epoch: [50]  [ 490/1251]  eta: 0:06:24  lr: 0.000011  loss: 3.3420 (3.4048)  time: 0.4595  data: 0.0005  max mem: 19734
Epoch: [50]  [ 500/1251]  eta: 0:06:18  lr: 0.000011  loss: 3.3680 (3.4051)  time: 0.4581  data: 0.0005  max mem: 19734
Epoch: [50]  [ 510/1251]  eta: 0:06:13  lr: 0.000011  loss: 3.5309 (3.4050)  time: 0.4926  data: 0.0005  max mem: 19734
Epoch: [50]  [ 520/1251]  eta: 0:06:08  lr: 0.000011  loss: 3.4260 (3.4018)  time: 0.5025  data: 0.0005  max mem: 19734
Epoch: [50]  [ 530/1251]  eta: 0:06:02  lr: 0.000011  loss: 3.4864 (3.4054)  time: 0.4678  data: 0.0004  max mem: 19734
Epoch: [50]  [ 540/1251]  eta: 0:05:57  lr: 0.000011  loss: 3.5410 (3.4071)  time: 0.4820  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4097, ratio_loss=0.0043, pruning_loss=0.1177, mse_loss=0.4237
Epoch: [50]  [ 550/1251]  eta: 0:05:52  lr: 0.000011  loss: 3.6496 (3.4134)  time: 0.4957  data: 0.0004  max mem: 19734
Epoch: [50]  [ 560/1251]  eta: 0:05:46  lr: 0.000011  loss: 3.6496 (3.4112)  time: 0.4724  data: 0.0005  max mem: 19734
Epoch: [50]  [ 570/1251]  eta: 0:05:41  lr: 0.000011  loss: 3.3469 (3.4119)  time: 0.4592  data: 0.0005  max mem: 19734
Epoch: [50]  [ 580/1251]  eta: 0:05:35  lr: 0.000011  loss: 3.4835 (3.4127)  time: 0.4578  data: 0.0005  max mem: 19734
Epoch: [50]  [ 590/1251]  eta: 0:05:30  lr: 0.000011  loss: 3.2697 (3.4113)  time: 0.4568  data: 0.0005  max mem: 19734
Epoch: [50]  [ 600/1251]  eta: 0:05:25  lr: 0.000011  loss: 3.2118 (3.4101)  time: 0.4649  data: 0.0005  max mem: 19734
Epoch: [50]  [ 610/1251]  eta: 0:05:19  lr: 0.000011  loss: 3.4199 (3.4061)  time: 0.4646  data: 0.0005  max mem: 19734
Epoch: [50]  [ 620/1251]  eta: 0:05:14  lr: 0.000011  loss: 3.4655 (3.4065)  time: 0.4564  data: 0.0005  max mem: 19734
Epoch: [50]  [ 630/1251]  eta: 0:05:08  lr: 0.000011  loss: 3.5915 (3.4064)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [50]  [ 640/1251]  eta: 0:05:03  lr: 0.000011  loss: 3.4402 (3.4075)  time: 0.4546  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3486, ratio_loss=0.0045, pruning_loss=0.1196, mse_loss=0.4070
Epoch: [50]  [ 650/1251]  eta: 0:04:58  lr: 0.000011  loss: 3.3279 (3.4054)  time: 0.4707  data: 0.0004  max mem: 19734
Epoch: [50]  [ 660/1251]  eta: 0:04:53  lr: 0.000011  loss: 3.4141 (3.4076)  time: 0.4896  data: 0.0004  max mem: 19734
Epoch: [50]  [ 670/1251]  eta: 0:04:48  lr: 0.000011  loss: 3.4683 (3.4061)  time: 0.4749  data: 0.0004  max mem: 19734
Epoch: [50]  [ 680/1251]  eta: 0:04:43  lr: 0.000011  loss: 3.4683 (3.4064)  time: 0.4723  data: 0.0004  max mem: 19734
Epoch: [50]  [ 690/1251]  eta: 0:04:37  lr: 0.000011  loss: 3.4863 (3.4070)  time: 0.4815  data: 0.0005  max mem: 19734
Epoch: [50]  [ 700/1251]  eta: 0:04:32  lr: 0.000011  loss: 3.4026 (3.4046)  time: 0.4651  data: 0.0005  max mem: 19734
Epoch: [50]  [ 710/1251]  eta: 0:04:27  lr: 0.000011  loss: 3.3875 (3.4050)  time: 0.4544  data: 0.0006  max mem: 19734
Epoch: [50]  [ 720/1251]  eta: 0:04:22  lr: 0.000011  loss: 3.5199 (3.4087)  time: 0.4535  data: 0.0005  max mem: 19734
Epoch: [50]  [ 730/1251]  eta: 0:04:16  lr: 0.000011  loss: 3.4998 (3.4050)  time: 0.4553  data: 0.0005  max mem: 19734
Epoch: [50]  [ 740/1251]  eta: 0:04:11  lr: 0.000011  loss: 3.2537 (3.4037)  time: 0.4685  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3618, ratio_loss=0.0049, pruning_loss=0.1181, mse_loss=0.3943
Epoch: [50]  [ 750/1251]  eta: 0:04:06  lr: 0.000011  loss: 3.2878 (3.4024)  time: 0.4672  data: 0.0005  max mem: 19734
Epoch: [50]  [ 760/1251]  eta: 0:04:01  lr: 0.000011  loss: 3.5896 (3.4061)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [50]  [ 770/1251]  eta: 0:03:56  lr: 0.000011  loss: 3.6270 (3.4068)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [50]  [ 780/1251]  eta: 0:03:51  lr: 0.000011  loss: 3.6270 (3.4076)  time: 0.4593  data: 0.0005  max mem: 19734
Epoch: [50]  [ 790/1251]  eta: 0:03:46  lr: 0.000011  loss: 3.7192 (3.4121)  time: 0.4567  data: 0.0007  max mem: 19734
Epoch: [50]  [ 800/1251]  eta: 0:03:41  lr: 0.000011  loss: 3.4505 (3.4104)  time: 0.4833  data: 0.0007  max mem: 19734
Epoch: [50]  [ 810/1251]  eta: 0:03:36  lr: 0.000011  loss: 3.3931 (3.4086)  time: 0.4939  data: 0.0005  max mem: 19734
Epoch: [50]  [ 820/1251]  eta: 0:03:31  lr: 0.000011  loss: 3.5512 (3.4111)  time: 0.4656  data: 0.0005  max mem: 19734
Epoch: [50]  [ 830/1251]  eta: 0:03:26  lr: 0.000011  loss: 3.5980 (3.4121)  time: 0.4687  data: 0.0005  max mem: 19734
Epoch: [50]  [ 840/1251]  eta: 0:03:21  lr: 0.000011  loss: 3.5065 (3.4121)  time: 0.4752  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4570, ratio_loss=0.0047, pruning_loss=0.1169, mse_loss=0.4030
Epoch: [50]  [ 850/1251]  eta: 0:03:16  lr: 0.000011  loss: 3.5908 (3.4125)  time: 0.4600  data: 0.0006  max mem: 19734
Epoch: [50]  [ 860/1251]  eta: 0:03:11  lr: 0.000011  loss: 3.4005 (3.4105)  time: 0.4514  data: 0.0004  max mem: 19734
Epoch: [50]  [ 870/1251]  eta: 0:03:06  lr: 0.000011  loss: 3.3454 (3.4106)  time: 0.4531  data: 0.0004  max mem: 19734
Epoch: [50]  [ 880/1251]  eta: 0:03:01  lr: 0.000011  loss: 3.5540 (3.4124)  time: 0.4535  data: 0.0006  max mem: 19734
Epoch: [50]  [ 890/1251]  eta: 0:02:56  lr: 0.000011  loss: 3.5839 (3.4128)  time: 0.4667  data: 0.0006  max mem: 19734
Epoch: [50]  [ 900/1251]  eta: 0:02:51  lr: 0.000011  loss: 3.5703 (3.4139)  time: 0.4670  data: 0.0006  max mem: 19734
Epoch: [50]  [ 910/1251]  eta: 0:02:46  lr: 0.000011  loss: 3.3844 (3.4127)  time: 0.4533  data: 0.0006  max mem: 19734
Epoch: [50]  [ 920/1251]  eta: 0:02:41  lr: 0.000011  loss: 3.3003 (3.4116)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [50]  [ 930/1251]  eta: 0:02:36  lr: 0.000011  loss: 3.3931 (3.4115)  time: 0.4599  data: 0.0004  max mem: 19734
Epoch: [50]  [ 940/1251]  eta: 0:02:31  lr: 0.000011  loss: 3.5780 (3.4104)  time: 0.4732  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3607, ratio_loss=0.0043, pruning_loss=0.1185, mse_loss=0.4265
Epoch: [50]  [ 950/1251]  eta: 0:02:26  lr: 0.000011  loss: 3.3249 (3.4099)  time: 0.4902  data: 0.0004  max mem: 19734
Epoch: [50]  [ 960/1251]  eta: 0:02:21  lr: 0.000011  loss: 3.4558 (3.4114)  time: 0.4777  data: 0.0005  max mem: 19734
Epoch: [50]  [ 970/1251]  eta: 0:02:16  lr: 0.000011  loss: 3.4558 (3.4095)  time: 0.4744  data: 0.0005  max mem: 19734
Epoch: [50]  [ 980/1251]  eta: 0:02:11  lr: 0.000011  loss: 3.3445 (3.4089)  time: 0.4804  data: 0.0004  max mem: 19734
Epoch: [50]  [ 990/1251]  eta: 0:02:06  lr: 0.000011  loss: 3.5070 (3.4108)  time: 0.4634  data: 0.0004  max mem: 19734
Epoch: [50]  [1000/1251]  eta: 0:02:01  lr: 0.000011  loss: 3.6153 (3.4094)  time: 0.4543  data: 0.0004  max mem: 19734
Epoch: [50]  [1010/1251]  eta: 0:01:57  lr: 0.000011  loss: 3.2819 (3.4092)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [50]  [1020/1251]  eta: 0:01:52  lr: 0.000011  loss: 3.5340 (3.4102)  time: 0.4555  data: 0.0004  max mem: 19734
Epoch: [50]  [1030/1251]  eta: 0:01:47  lr: 0.000011  loss: 3.4366 (3.4088)  time: 0.4549  data: 0.0004  max mem: 19734
Epoch: [50]  [1040/1251]  eta: 0:01:42  lr: 0.000011  loss: 3.3865 (3.4087)  time: 0.4675  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3604, ratio_loss=0.0048, pruning_loss=0.1185, mse_loss=0.4230
Epoch: [50]  [1050/1251]  eta: 0:01:37  lr: 0.000011  loss: 3.4113 (3.4070)  time: 0.4679  data: 0.0004  max mem: 19734
Epoch: [50]  [1060/1251]  eta: 0:01:32  lr: 0.000011  loss: 3.5050 (3.4052)  time: 0.4551  data: 0.0005  max mem: 19734
Epoch: [50]  [1070/1251]  eta: 0:01:27  lr: 0.000011  loss: 3.5050 (3.4055)  time: 0.4551  data: 0.0009  max mem: 19734
Epoch: [50]  [1080/1251]  eta: 0:01:22  lr: 0.000011  loss: 3.4944 (3.4043)  time: 0.4540  data: 0.0009  max mem: 19734
Epoch: [50]  [1090/1251]  eta: 0:01:17  lr: 0.000011  loss: 3.5985 (3.4055)  time: 0.4838  data: 0.0004  max mem: 19734
Epoch: [50]  [1100/1251]  eta: 0:01:13  lr: 0.000011  loss: 3.4502 (3.4033)  time: 0.4831  data: 0.0004  max mem: 19734
Epoch: [50]  [1110/1251]  eta: 0:01:08  lr: 0.000011  loss: 3.1535 (3.4033)  time: 0.4512  data: 0.0004  max mem: 19734
Epoch: [50]  [1120/1251]  eta: 0:01:03  lr: 0.000011  loss: 3.6371 (3.4063)  time: 0.4688  data: 0.0005  max mem: 19734
Epoch: [50]  [1130/1251]  eta: 0:00:58  lr: 0.000011  loss: 3.5742 (3.4062)  time: 0.4799  data: 0.0005  max mem: 19734
Epoch: [50]  [1140/1251]  eta: 0:00:53  lr: 0.000011  loss: 3.3857 (3.4062)  time: 0.4634  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3723, ratio_loss=0.0046, pruning_loss=0.1184, mse_loss=0.4208
Epoch: [50]  [1150/1251]  eta: 0:00:48  lr: 0.000011  loss: 3.4566 (3.4064)  time: 0.4528  data: 0.0004  max mem: 19734
Epoch: [50]  [1160/1251]  eta: 0:00:43  lr: 0.000011  loss: 3.4310 (3.4053)  time: 0.4529  data: 0.0005  max mem: 19734
Epoch: [50]  [1170/1251]  eta: 0:00:39  lr: 0.000011  loss: 3.6056 (3.4060)  time: 0.4527  data: 0.0004  max mem: 19734
Epoch: [50]  [1180/1251]  eta: 0:00:34  lr: 0.000011  loss: 3.6056 (3.4054)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [50]  [1190/1251]  eta: 0:00:29  lr: 0.000011  loss: 3.5393 (3.4064)  time: 0.4583  data: 0.0008  max mem: 19734
Epoch: [50]  [1200/1251]  eta: 0:00:24  lr: 0.000011  loss: 3.4921 (3.4059)  time: 0.4546  data: 0.0007  max mem: 19734
Epoch: [50]  [1210/1251]  eta: 0:00:19  lr: 0.000011  loss: 3.6399 (3.4075)  time: 0.4448  data: 0.0002  max mem: 19734
Epoch: [50]  [1220/1251]  eta: 0:00:14  lr: 0.000011  loss: 3.7685 (3.4085)  time: 0.4442  data: 0.0001  max mem: 19734
Epoch: [50]  [1230/1251]  eta: 0:00:10  lr: 0.000011  loss: 3.5777 (3.4078)  time: 0.4515  data: 0.0001  max mem: 19734
Epoch: [50]  [1240/1251]  eta: 0:00:05  lr: 0.000011  loss: 3.6797 (3.4097)  time: 0.4664  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4197, ratio_loss=0.0046, pruning_loss=0.1172, mse_loss=0.4214
Epoch: [50]  [1250/1251]  eta: 0:00:00  lr: 0.000011  loss: 3.6213 (3.4093)  time: 0.4592  data: 0.0002  max mem: 19734
Epoch: [50] Total time: 0:10:02 (0.4812 s / it)
Averaged stats: lr: 0.000011  loss: 3.6213 (3.4045)
Test:  [  0/261]  eta: 2:11:29  loss: 0.6779 (0.6779)  acc1: 84.3750 (84.3750)  acc5: 95.8333 (95.8333)  time: 30.2285  data: 30.0080  max mem: 19734
Test:  [ 10/261]  eta: 0:12:21  loss: 0.6779 (0.7088)  acc1: 84.3750 (84.3277)  acc5: 97.3958 (96.2121)  time: 2.9545  data: 2.7443  max mem: 19734
Test:  [ 20/261]  eta: 0:06:37  loss: 0.9012 (0.8796)  acc1: 81.2500 (79.4891)  acc5: 93.2292 (94.7421)  time: 0.2188  data: 0.0124  max mem: 19734
Test:  [ 30/261]  eta: 0:04:34  loss: 0.7762 (0.8006)  acc1: 84.3750 (82.1573)  acc5: 93.7500 (95.1781)  time: 0.2181  data: 0.0137  max mem: 19734
Test:  [ 40/261]  eta: 0:03:54  loss: 0.5743 (0.7714)  acc1: 89.5833 (83.1047)  acc5: 96.8750 (95.4395)  time: 0.4433  data: 0.2595  max mem: 19734
Test:  [ 50/261]  eta: 0:03:06  loss: 0.8949 (0.8328)  acc1: 77.6042 (81.3623)  acc5: 94.7917 (95.0674)  time: 0.4109  data: 0.2587  max mem: 19734
Test:  [ 60/261]  eta: 0:02:36  loss: 0.9660 (0.8423)  acc1: 76.5625 (81.0024)  acc5: 94.2708 (95.0905)  time: 0.2056  data: 0.0199  max mem: 19734
Test:  [ 70/261]  eta: 0:02:19  loss: 0.9426 (0.8445)  acc1: 78.6458 (80.5678)  acc5: 95.8333 (95.2905)  time: 0.3370  data: 0.1345  max mem: 19734
Test:  [ 80/261]  eta: 0:02:01  loss: 0.8337 (0.8449)  acc1: 80.2083 (80.6327)  acc5: 96.8750 (95.4347)  time: 0.3488  data: 0.1314  max mem: 19734
Test:  [ 90/261]  eta: 0:01:51  loss: 0.8096 (0.8322)  acc1: 82.8125 (80.9009)  acc5: 95.8333 (95.5243)  time: 0.3807  data: 0.0975  max mem: 19734
Test:  [100/261]  eta: 0:01:46  loss: 0.8096 (0.8365)  acc1: 82.8125 (80.8220)  acc5: 95.8333 (95.6013)  time: 0.6045  data: 0.3911  max mem: 19734
Test:  [110/261]  eta: 0:01:34  loss: 0.8789 (0.8589)  acc1: 77.0833 (80.3585)  acc5: 94.7917 (95.3078)  time: 0.4937  data: 0.3102  max mem: 19734
Test:  [120/261]  eta: 0:01:23  loss: 1.1806 (0.8985)  acc1: 71.3542 (79.4422)  acc5: 89.5833 (94.7745)  time: 0.2420  data: 0.0161  max mem: 19734
Test:  [130/261]  eta: 0:01:13  loss: 1.4018 (0.9419)  acc1: 66.6667 (78.5226)  acc5: 86.9792 (94.1953)  time: 0.2027  data: 0.0307  max mem: 19734
Test:  [140/261]  eta: 0:01:05  loss: 1.3235 (0.9695)  acc1: 68.7500 (77.8590)  acc5: 89.0625 (93.8941)  time: 0.2553  data: 0.1118  max mem: 19734
Test:  [150/261]  eta: 0:00:57  loss: 1.2398 (0.9745)  acc1: 72.3958 (77.8594)  acc5: 91.1458 (93.7776)  time: 0.2716  data: 0.1435  max mem: 19734
Test:  [160/261]  eta: 0:00:49  loss: 0.9839 (0.9941)  acc1: 78.6458 (77.5621)  acc5: 92.7083 (93.4427)  time: 0.1427  data: 0.0573  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: 1.2783 (1.0241)  acc1: 64.0625 (76.7970)  acc5: 86.9792 (93.0830)  time: 0.0816  data: 0.0165  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.3639 (1.0397)  acc1: 65.6250 (76.4244)  acc5: 89.0625 (92.9385)  time: 0.1189  data: 0.0484  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: 1.3384 (1.0539)  acc1: 68.7500 (76.1644)  acc5: 90.1042 (92.7738)  time: 0.1381  data: 0.0693  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.4198 (1.0696)  acc1: 71.3542 (75.8577)  acc5: 89.0625 (92.5347)  time: 0.1000  data: 0.0366  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.3821 (1.0830)  acc1: 69.2708 (75.5677)  acc5: 89.0625 (92.3356)  time: 0.0751  data: 0.0134  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.3903 (1.1017)  acc1: 66.6667 (75.1061)  acc5: 89.0625 (92.1545)  time: 0.0740  data: 0.0126  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.3950 (1.1108)  acc1: 66.1458 (74.8467)  acc5: 90.1042 (92.0906)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3132 (1.1198)  acc1: 67.1875 (74.5980)  acc5: 90.6250 (92.0319)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0296 (1.1126)  acc1: 76.0417 (74.8132)  acc5: 93.2292 (92.1398)  time: 0.0615  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9340 (1.1120)  acc1: 78.6458 (74.8160)  acc5: 94.7917 (92.2120)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:27 (0.3370 s / it)
* Acc@1 74.816 Acc@5 92.212 loss 1.112
Accuracy of the network on the 50000 test images: 74.8%
Max accuracy: 74.82%
Epoch: [51]  [   0/1251]  eta: 3:33:36  lr: 0.000011  loss: 2.3051 (2.3051)  time: 10.2452  data: 9.7916  max mem: 19734
Epoch: [51]  [  10/1251]  eta: 0:29:38  lr: 0.000011  loss: 3.7036 (3.3923)  time: 1.4328  data: 0.8906  max mem: 19734
Epoch: [51]  [  20/1251]  eta: 0:20:11  lr: 0.000011  loss: 3.6738 (3.3752)  time: 0.5214  data: 0.0005  max mem: 19734
Epoch: [51]  [  30/1251]  eta: 0:16:43  lr: 0.000011  loss: 3.6196 (3.3971)  time: 0.4854  data: 0.0004  max mem: 19734
Epoch: [51]  [  40/1251]  eta: 0:14:48  lr: 0.000011  loss: 3.3206 (3.3746)  time: 0.4702  data: 0.0004  max mem: 19734
Epoch: [51]  [  50/1251]  eta: 0:13:36  lr: 0.000011  loss: 3.3206 (3.4064)  time: 0.4592  data: 0.0004  max mem: 19734
Epoch: [51]  [  60/1251]  eta: 0:12:47  lr: 0.000011  loss: 3.2903 (3.3678)  time: 0.4604  data: 0.0004  max mem: 19734
Epoch: [51]  [  70/1251]  eta: 0:12:10  lr: 0.000011  loss: 3.6173 (3.4139)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [51]  [  80/1251]  eta: 0:11:42  lr: 0.000011  loss: 3.6595 (3.4301)  time: 0.4656  data: 0.0005  max mem: 19734
Epoch: [51]  [  90/1251]  eta: 0:11:19  lr: 0.000011  loss: 3.3869 (3.4307)  time: 0.4660  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4242, ratio_loss=0.0047, pruning_loss=0.1165, mse_loss=0.4101
Epoch: [51]  [ 100/1251]  eta: 0:10:59  lr: 0.000011  loss: 3.4805 (3.4455)  time: 0.4630  data: 0.0004  max mem: 19734
Epoch: [51]  [ 110/1251]  eta: 0:10:42  lr: 0.000011  loss: 3.5506 (3.4538)  time: 0.4652  data: 0.0004  max mem: 19734
Epoch: [51]  [ 120/1251]  eta: 0:10:33  lr: 0.000011  loss: 3.6513 (3.4524)  time: 0.4966  data: 0.0004  max mem: 19734
Epoch: [51]  [ 130/1251]  eta: 0:10:19  lr: 0.000011  loss: 3.6205 (3.4588)  time: 0.4927  data: 0.0004  max mem: 19734
Epoch: [51]  [ 140/1251]  eta: 0:10:06  lr: 0.000011  loss: 3.4190 (3.4369)  time: 0.4581  data: 0.0004  max mem: 19734
Epoch: [51]  [ 150/1251]  eta: 0:09:56  lr: 0.000011  loss: 3.1209 (3.4241)  time: 0.4745  data: 0.0004  max mem: 19734
Epoch: [51]  [ 160/1251]  eta: 0:09:47  lr: 0.000011  loss: 3.5496 (3.4335)  time: 0.4881  data: 0.0005  max mem: 19734
Epoch: [51]  [ 170/1251]  eta: 0:09:37  lr: 0.000011  loss: 3.5596 (3.4309)  time: 0.4720  data: 0.0005  max mem: 19734
Epoch: [51]  [ 180/1251]  eta: 0:09:28  lr: 0.000011  loss: 3.6745 (3.4364)  time: 0.4674  data: 0.0004  max mem: 19734
Epoch: [51]  [ 190/1251]  eta: 0:09:19  lr: 0.000011  loss: 3.4386 (3.4294)  time: 0.4666  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3971, ratio_loss=0.0046, pruning_loss=0.1181, mse_loss=0.4147
Epoch: [51]  [ 200/1251]  eta: 0:09:10  lr: 0.000011  loss: 3.3320 (3.4322)  time: 0.4601  data: 0.0005  max mem: 19734
Epoch: [51]  [ 210/1251]  eta: 0:09:01  lr: 0.000011  loss: 3.6216 (3.4378)  time: 0.4598  data: 0.0004  max mem: 19734
Epoch: [51]  [ 220/1251]  eta: 0:08:53  lr: 0.000011  loss: 3.5980 (3.4427)  time: 0.4569  data: 0.0004  max mem: 19734
Epoch: [51]  [ 230/1251]  eta: 0:08:45  lr: 0.000011  loss: 3.4816 (3.4324)  time: 0.4567  data: 0.0004  max mem: 19734
Epoch: [51]  [ 240/1251]  eta: 0:08:38  lr: 0.000011  loss: 3.2993 (3.4226)  time: 0.4540  data: 0.0004  max mem: 19734
Epoch: [51]  [ 250/1251]  eta: 0:08:30  lr: 0.000011  loss: 3.1715 (3.4147)  time: 0.4522  data: 0.0004  max mem: 19734
Epoch: [51]  [ 260/1251]  eta: 0:08:23  lr: 0.000011  loss: 3.4074 (3.4193)  time: 0.4602  data: 0.0004  max mem: 19734
Epoch: [51]  [ 270/1251]  eta: 0:08:18  lr: 0.000011  loss: 3.6107 (3.4234)  time: 0.4809  data: 0.0005  max mem: 19734
Epoch: [51]  [ 280/1251]  eta: 0:08:11  lr: 0.000011  loss: 3.5199 (3.4202)  time: 0.4746  data: 0.0005  max mem: 19734
Epoch: [51]  [ 290/1251]  eta: 0:08:05  lr: 0.000011  loss: 3.3216 (3.4151)  time: 0.4662  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3629, ratio_loss=0.0047, pruning_loss=0.1173, mse_loss=0.4097
Epoch: [51]  [ 300/1251]  eta: 0:08:00  lr: 0.000011  loss: 3.4778 (3.4193)  time: 0.4888  data: 0.0004  max mem: 19734
Epoch: [51]  [ 310/1251]  eta: 0:07:53  lr: 0.000011  loss: 3.5223 (3.4169)  time: 0.4789  data: 0.0004  max mem: 19734
Epoch: [51]  [ 320/1251]  eta: 0:07:47  lr: 0.000011  loss: 3.5102 (3.4127)  time: 0.4593  data: 0.0004  max mem: 19734
Epoch: [51]  [ 330/1251]  eta: 0:07:41  lr: 0.000011  loss: 3.2033 (3.4063)  time: 0.4693  data: 0.0004  max mem: 19734
Epoch: [51]  [ 340/1251]  eta: 0:07:35  lr: 0.000011  loss: 3.0748 (3.4046)  time: 0.4686  data: 0.0004  max mem: 19734
Epoch: [51]  [ 350/1251]  eta: 0:07:29  lr: 0.000011  loss: 3.2162 (3.4048)  time: 0.4601  data: 0.0005  max mem: 19734
Epoch: [51]  [ 360/1251]  eta: 0:07:23  lr: 0.000011  loss: 3.1853 (3.3946)  time: 0.4591  data: 0.0004  max mem: 19734
Epoch: [51]  [ 370/1251]  eta: 0:07:17  lr: 0.000011  loss: 3.2082 (3.3984)  time: 0.4552  data: 0.0004  max mem: 19734
Epoch: [51]  [ 380/1251]  eta: 0:07:11  lr: 0.000011  loss: 3.7030 (3.4068)  time: 0.4536  data: 0.0004  max mem: 19734
Epoch: [51]  [ 390/1251]  eta: 0:07:05  lr: 0.000011  loss: 3.6161 (3.4008)  time: 0.4547  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3205, ratio_loss=0.0045, pruning_loss=0.1197, mse_loss=0.4123
Epoch: [51]  [ 400/1251]  eta: 0:06:59  lr: 0.000011  loss: 3.3162 (3.3954)  time: 0.4554  data: 0.0004  max mem: 19734
Epoch: [51]  [ 410/1251]  eta: 0:06:55  lr: 0.000011  loss: 3.2884 (3.3922)  time: 0.4892  data: 0.0004  max mem: 19734
Epoch: [51]  [ 420/1251]  eta: 0:06:49  lr: 0.000011  loss: 3.2648 (3.3936)  time: 0.4882  data: 0.0004  max mem: 19734
Epoch: [51]  [ 430/1251]  eta: 0:06:44  lr: 0.000011  loss: 3.4889 (3.3959)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [51]  [ 440/1251]  eta: 0:06:39  lr: 0.000011  loss: 3.5558 (3.3962)  time: 0.4725  data: 0.0004  max mem: 19734
Epoch: [51]  [ 450/1251]  eta: 0:06:34  lr: 0.000011  loss: 3.5558 (3.3978)  time: 0.4842  data: 0.0005  max mem: 19734
Epoch: [51]  [ 460/1251]  eta: 0:06:28  lr: 0.000011  loss: 3.5140 (3.3915)  time: 0.4638  data: 0.0004  max mem: 19734
Epoch: [51]  [ 470/1251]  eta: 0:06:23  lr: 0.000011  loss: 3.2176 (3.3896)  time: 0.4573  data: 0.0004  max mem: 19734
Epoch: [51]  [ 480/1251]  eta: 0:06:17  lr: 0.000011  loss: 3.1944 (3.3896)  time: 0.4586  data: 0.0004  max mem: 19734
Epoch: [51]  [ 490/1251]  eta: 0:06:12  lr: 0.000011  loss: 3.3808 (3.3904)  time: 0.4536  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3554, ratio_loss=0.0044, pruning_loss=0.1176, mse_loss=0.4153
Epoch: [51]  [ 500/1251]  eta: 0:06:06  lr: 0.000011  loss: 3.6834 (3.3961)  time: 0.4529  data: 0.0004  max mem: 19734
Epoch: [51]  [ 510/1251]  eta: 0:06:01  lr: 0.000011  loss: 3.6834 (3.3961)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [51]  [ 520/1251]  eta: 0:05:55  lr: 0.000011  loss: 3.4671 (3.3940)  time: 0.4544  data: 0.0004  max mem: 19734
Epoch: [51]  [ 530/1251]  eta: 0:05:50  lr: 0.000011  loss: 3.5061 (3.3952)  time: 0.4547  data: 0.0004  max mem: 19734
Epoch: [51]  [ 540/1251]  eta: 0:05:45  lr: 0.000011  loss: 3.4472 (3.3955)  time: 0.4563  data: 0.0005  max mem: 19734
Epoch: [51]  [ 550/1251]  eta: 0:05:40  lr: 0.000011  loss: 3.3443 (3.3939)  time: 0.4786  data: 0.0005  max mem: 19734
Epoch: [51]  [ 560/1251]  eta: 0:05:35  lr: 0.000011  loss: 3.4983 (3.3932)  time: 0.4882  data: 0.0004  max mem: 19734
Epoch: [51]  [ 570/1251]  eta: 0:05:30  lr: 0.000011  loss: 3.5089 (3.3920)  time: 0.4675  data: 0.0004  max mem: 19734
Epoch: [51]  [ 580/1251]  eta: 0:05:25  lr: 0.000011  loss: 3.5174 (3.3919)  time: 0.4663  data: 0.0004  max mem: 19734
Epoch: [51]  [ 590/1251]  eta: 0:05:20  lr: 0.000011  loss: 3.4880 (3.3936)  time: 0.4858  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3625, ratio_loss=0.0046, pruning_loss=0.1180, mse_loss=0.4042
Epoch: [51]  [ 600/1251]  eta: 0:05:15  lr: 0.000011  loss: 3.4052 (3.3923)  time: 0.4765  data: 0.0004  max mem: 19734
Epoch: [51]  [ 610/1251]  eta: 0:05:10  lr: 0.000011  loss: 3.4357 (3.3951)  time: 0.4562  data: 0.0004  max mem: 19734
Epoch: [51]  [ 620/1251]  eta: 0:05:05  lr: 0.000011  loss: 3.5800 (3.3960)  time: 0.4659  data: 0.0004  max mem: 19734
Epoch: [51]  [ 630/1251]  eta: 0:05:00  lr: 0.000011  loss: 3.3234 (3.3907)  time: 0.4639  data: 0.0004  max mem: 19734
Epoch: [51]  [ 640/1251]  eta: 0:04:55  lr: 0.000011  loss: 3.2338 (3.3898)  time: 0.4551  data: 0.0004  max mem: 19734
Epoch: [51]  [ 650/1251]  eta: 0:04:50  lr: 0.000011  loss: 3.3811 (3.3916)  time: 0.4565  data: 0.0004  max mem: 19734
Epoch: [51]  [ 660/1251]  eta: 0:04:45  lr: 0.000011  loss: 3.4555 (3.3927)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [51]  [ 670/1251]  eta: 0:04:40  lr: 0.000011  loss: 3.4555 (3.3937)  time: 0.4550  data: 0.0004  max mem: 19734
Epoch: [51]  [ 680/1251]  eta: 0:04:35  lr: 0.000011  loss: 3.5760 (3.3937)  time: 0.4563  data: 0.0004  max mem: 19734
Epoch: [51]  [ 690/1251]  eta: 0:04:30  lr: 0.000011  loss: 3.5236 (3.3929)  time: 0.4559  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3740, ratio_loss=0.0047, pruning_loss=0.1181, mse_loss=0.4105
Epoch: [51]  [ 700/1251]  eta: 0:04:25  lr: 0.000011  loss: 3.3896 (3.3925)  time: 0.4835  data: 0.0004  max mem: 19734
Epoch: [51]  [ 710/1251]  eta: 0:04:20  lr: 0.000011  loss: 3.3461 (3.3903)  time: 0.4829  data: 0.0004  max mem: 19734
Epoch: [51]  [ 720/1251]  eta: 0:04:15  lr: 0.000011  loss: 3.2854 (3.3908)  time: 0.4537  data: 0.0004  max mem: 19734
Epoch: [51]  [ 730/1251]  eta: 0:04:10  lr: 0.000011  loss: 3.2854 (3.3874)  time: 0.4693  data: 0.0004  max mem: 19734
Epoch: [51]  [ 740/1251]  eta: 0:04:05  lr: 0.000011  loss: 3.4245 (3.3854)  time: 0.4802  data: 0.0004  max mem: 19734
Epoch: [51]  [ 750/1251]  eta: 0:04:00  lr: 0.000011  loss: 3.3519 (3.3841)  time: 0.4627  data: 0.0004  max mem: 19734
Epoch: [51]  [ 760/1251]  eta: 0:03:55  lr: 0.000011  loss: 3.4384 (3.3843)  time: 0.4516  data: 0.0003  max mem: 19734
Epoch: [51]  [ 770/1251]  eta: 0:03:50  lr: 0.000011  loss: 3.4384 (3.3819)  time: 0.4639  data: 0.0004  max mem: 19734
Epoch: [51]  [ 780/1251]  eta: 0:03:45  lr: 0.000011  loss: 3.4482 (3.3859)  time: 0.4645  data: 0.0004  max mem: 19734
Epoch: [51]  [ 790/1251]  eta: 0:03:41  lr: 0.000011  loss: 3.3160 (3.3834)  time: 0.4527  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3011, ratio_loss=0.0044, pruning_loss=0.1186, mse_loss=0.4224
Epoch: [51]  [ 800/1251]  eta: 0:03:36  lr: 0.000011  loss: 3.2464 (3.3854)  time: 0.4532  data: 0.0004  max mem: 19734
Epoch: [51]  [ 810/1251]  eta: 0:03:31  lr: 0.000011  loss: 3.6187 (3.3898)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [51]  [ 820/1251]  eta: 0:03:26  lr: 0.000011  loss: 3.5870 (3.3894)  time: 0.4549  data: 0.0005  max mem: 19734
Epoch: [51]  [ 830/1251]  eta: 0:03:21  lr: 0.000011  loss: 3.5616 (3.3896)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [51]  [ 840/1251]  eta: 0:03:16  lr: 0.000011  loss: 3.4569 (3.3884)  time: 0.4741  data: 0.0004  max mem: 19734
Epoch: [51]  [ 850/1251]  eta: 0:03:11  lr: 0.000011  loss: 3.2732 (3.3869)  time: 0.4842  data: 0.0004  max mem: 19734
Epoch: [51]  [ 860/1251]  eta: 0:03:06  lr: 0.000011  loss: 3.1595 (3.3851)  time: 0.4659  data: 0.0004  max mem: 19734
Epoch: [51]  [ 870/1251]  eta: 0:03:02  lr: 0.000011  loss: 3.1595 (3.3814)  time: 0.4734  data: 0.0004  max mem: 19734
Epoch: [51]  [ 880/1251]  eta: 0:02:57  lr: 0.000011  loss: 3.3689 (3.3819)  time: 0.4801  data: 0.0003  max mem: 19734
Epoch: [51]  [ 890/1251]  eta: 0:02:52  lr: 0.000011  loss: 3.5783 (3.3839)  time: 0.4628  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3418, ratio_loss=0.0046, pruning_loss=0.1188, mse_loss=0.4129
Epoch: [51]  [ 900/1251]  eta: 0:02:47  lr: 0.000011  loss: 3.4986 (3.3820)  time: 0.4561  data: 0.0004  max mem: 19734
Epoch: [51]  [ 910/1251]  eta: 0:02:42  lr: 0.000011  loss: 3.4018 (3.3830)  time: 0.4542  data: 0.0004  max mem: 19734
Epoch: [51]  [ 920/1251]  eta: 0:02:38  lr: 0.000011  loss: 3.2587 (3.3786)  time: 0.4651  data: 0.0004  max mem: 19734
Epoch: [51]  [ 930/1251]  eta: 0:02:33  lr: 0.000011  loss: 3.1585 (3.3799)  time: 0.4668  data: 0.0004  max mem: 19734
Epoch: [51]  [ 940/1251]  eta: 0:02:28  lr: 0.000011  loss: 3.5677 (3.3818)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [51]  [ 950/1251]  eta: 0:02:23  lr: 0.000011  loss: 3.6001 (3.3835)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [51]  [ 960/1251]  eta: 0:02:18  lr: 0.000011  loss: 3.5199 (3.3824)  time: 0.4524  data: 0.0004  max mem: 19734
Epoch: [51]  [ 970/1251]  eta: 0:02:13  lr: 0.000011  loss: 3.3227 (3.3813)  time: 0.4517  data: 0.0004  max mem: 19734
Epoch: [51]  [ 980/1251]  eta: 0:02:08  lr: 0.000011  loss: 3.1222 (3.3802)  time: 0.4516  data: 0.0004  max mem: 19734
Epoch: [51]  [ 990/1251]  eta: 0:02:04  lr: 0.000011  loss: 3.4722 (3.3821)  time: 0.4838  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3366, ratio_loss=0.0046, pruning_loss=0.1199, mse_loss=0.3961
Epoch: [51]  [1000/1251]  eta: 0:01:59  lr: 0.000011  loss: 3.4278 (3.3786)  time: 0.4838  data: 0.0004  max mem: 19734
Epoch: [51]  [1010/1251]  eta: 0:01:54  lr: 0.000011  loss: 3.2767 (3.3775)  time: 0.4518  data: 0.0004  max mem: 19734
Epoch: [51]  [1020/1251]  eta: 0:01:49  lr: 0.000011  loss: 3.5219 (3.3781)  time: 0.4649  data: 0.0004  max mem: 19734
Epoch: [51]  [1030/1251]  eta: 0:01:45  lr: 0.000011  loss: 3.5219 (3.3755)  time: 0.4766  data: 0.0004  max mem: 19734
Epoch: [51]  [1040/1251]  eta: 0:01:40  lr: 0.000011  loss: 3.1514 (3.3740)  time: 0.4631  data: 0.0004  max mem: 19734
Epoch: [51]  [1050/1251]  eta: 0:01:35  lr: 0.000011  loss: 3.2434 (3.3743)  time: 0.4525  data: 0.0004  max mem: 19734
Epoch: [51]  [1060/1251]  eta: 0:01:30  lr: 0.000011  loss: 3.3157 (3.3757)  time: 0.4538  data: 0.0004  max mem: 19734
Epoch: [51]  [1070/1251]  eta: 0:01:26  lr: 0.000011  loss: 3.5268 (3.3769)  time: 0.4650  data: 0.0004  max mem: 19734
Epoch: [51]  [1080/1251]  eta: 0:01:21  lr: 0.000011  loss: 3.5970 (3.3773)  time: 0.4653  data: 0.0004  max mem: 19734
Epoch: [51]  [1090/1251]  eta: 0:01:16  lr: 0.000011  loss: 3.5970 (3.3784)  time: 0.4536  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3535, ratio_loss=0.0047, pruning_loss=0.1174, mse_loss=0.4010
Epoch: [51]  [1100/1251]  eta: 0:01:11  lr: 0.000011  loss: 3.5458 (3.3801)  time: 0.4528  data: 0.0006  max mem: 19734
Epoch: [51]  [1110/1251]  eta: 0:01:06  lr: 0.000011  loss: 3.5458 (3.3800)  time: 0.4529  data: 0.0006  max mem: 19734
Epoch: [51]  [1120/1251]  eta: 0:01:02  lr: 0.000011  loss: 3.5494 (3.3799)  time: 0.4541  data: 0.0004  max mem: 19734
Epoch: [51]  [1130/1251]  eta: 0:00:57  lr: 0.000011  loss: 3.6621 (3.3828)  time: 0.4713  data: 0.0004  max mem: 19734
Epoch: [51]  [1140/1251]  eta: 0:00:52  lr: 0.000011  loss: 3.6066 (3.3805)  time: 0.4802  data: 0.0005  max mem: 19734
Epoch: [51]  [1150/1251]  eta: 0:00:47  lr: 0.000011  loss: 3.4283 (3.3814)  time: 0.4646  data: 0.0006  max mem: 19734
Epoch: [51]  [1160/1251]  eta: 0:00:43  lr: 0.000011  loss: 3.4671 (3.3802)  time: 0.4642  data: 0.0004  max mem: 19734
Epoch: [51]  [1170/1251]  eta: 0:00:38  lr: 0.000011  loss: 3.2244 (3.3794)  time: 0.4778  data: 0.0004  max mem: 19734
Epoch: [51]  [1180/1251]  eta: 0:00:33  lr: 0.000011  loss: 3.7119 (3.3827)  time: 0.4679  data: 0.0004  max mem: 19734
Epoch: [51]  [1190/1251]  eta: 0:00:28  lr: 0.000011  loss: 3.6014 (3.3824)  time: 0.4502  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3841, ratio_loss=0.0043, pruning_loss=0.1177, mse_loss=0.4221
Epoch: [51]  [1200/1251]  eta: 0:00:24  lr: 0.000011  loss: 3.4046 (3.3819)  time: 0.4464  data: 0.0005  max mem: 19734
Epoch: [51]  [1210/1251]  eta: 0:00:19  lr: 0.000011  loss: 3.4788 (3.3812)  time: 0.4446  data: 0.0002  max mem: 19734
Epoch: [51]  [1220/1251]  eta: 0:00:14  lr: 0.000011  loss: 3.4842 (3.3816)  time: 0.4525  data: 0.0002  max mem: 19734
Epoch: [51]  [1230/1251]  eta: 0:00:09  lr: 0.000011  loss: 3.5867 (3.3818)  time: 0.4525  data: 0.0002  max mem: 19734
Epoch: [51]  [1240/1251]  eta: 0:00:05  lr: 0.000011  loss: 3.5867 (3.3809)  time: 0.4459  data: 0.0002  max mem: 19734
Epoch: [51]  [1250/1251]  eta: 0:00:00  lr: 0.000011  loss: 3.5716 (3.3816)  time: 0.4459  data: 0.0002  max mem: 19734
Epoch: [51] Total time: 0:09:52 (0.4733 s / it)
Averaged stats: lr: 0.000011  loss: 3.5716 (3.3890)
Test:  [  0/261]  eta: 1:34:47  loss: 0.6856 (0.6856)  acc1: 82.8125 (82.8125)  acc5: 96.3542 (96.3542)  time: 21.7927  data: 21.6865  max mem: 19734
Test:  [ 10/261]  eta: 0:10:38  loss: 0.6856 (0.7135)  acc1: 83.3333 (84.2803)  acc5: 97.3958 (96.3068)  time: 2.5450  data: 2.4531  max mem: 19734
Test:  [ 20/261]  eta: 0:05:37  loss: 0.8817 (0.8842)  acc1: 80.7292 (79.3899)  acc5: 93.7500 (94.7669)  time: 0.3795  data: 0.2711  max mem: 19734
Test:  [ 30/261]  eta: 0:03:47  loss: 0.8229 (0.8023)  acc1: 81.2500 (82.1573)  acc5: 93.7500 (95.0941)  time: 0.1291  data: 0.0121  max mem: 19734
Test:  [ 40/261]  eta: 0:03:24  loss: 0.5625 (0.7709)  acc1: 88.5417 (83.1301)  acc5: 96.8750 (95.3760)  time: 0.4245  data: 0.3159  max mem: 19734
Test:  [ 50/261]  eta: 0:02:44  loss: 0.8938 (0.8327)  acc1: 77.6042 (81.2909)  acc5: 94.7917 (95.0572)  time: 0.4576  data: 0.3300  max mem: 19734
Test:  [ 60/261]  eta: 0:02:16  loss: 0.9476 (0.8410)  acc1: 76.0417 (80.9085)  acc5: 94.7917 (95.1588)  time: 0.1798  data: 0.0253  max mem: 19734
Test:  [ 70/261]  eta: 0:02:17  loss: 0.9391 (0.8436)  acc1: 78.1250 (80.5238)  acc5: 96.3542 (95.3492)  time: 0.5638  data: 0.3889  max mem: 19734
Test:  [ 80/261]  eta: 0:01:58  loss: 0.8239 (0.8435)  acc1: 80.2083 (80.6520)  acc5: 96.8750 (95.4540)  time: 0.5789  data: 0.4151  max mem: 19734
Test:  [ 90/261]  eta: 0:01:42  loss: 0.7857 (0.8287)  acc1: 84.3750 (81.0096)  acc5: 96.3542 (95.5758)  time: 0.1841  data: 0.0396  max mem: 19734
Test:  [100/261]  eta: 0:01:39  loss: 0.7945 (0.8332)  acc1: 83.3333 (80.9612)  acc5: 95.8333 (95.6271)  time: 0.4541  data: 0.3068  max mem: 19734
Test:  [110/261]  eta: 0:01:25  loss: 0.8510 (0.8569)  acc1: 78.1250 (80.4852)  acc5: 94.7917 (95.3313)  time: 0.4247  data: 0.3042  max mem: 19734
Test:  [120/261]  eta: 0:01:15  loss: 1.1976 (0.8970)  acc1: 73.4375 (79.5498)  acc5: 90.1042 (94.8046)  time: 0.1337  data: 0.0101  max mem: 19734
Test:  [130/261]  eta: 0:01:09  loss: 1.3349 (0.9420)  acc1: 67.7083 (78.6061)  acc5: 88.5417 (94.2032)  time: 0.3207  data: 0.1562  max mem: 19734
Test:  [140/261]  eta: 0:01:01  loss: 1.3028 (0.9681)  acc1: 68.7500 (77.9588)  acc5: 89.0625 (93.9384)  time: 0.3152  data: 0.1556  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: 1.2228 (0.9736)  acc1: 71.8750 (77.9387)  acc5: 91.1458 (93.7845)  time: 0.2174  data: 0.0740  max mem: 19734
Test:  [160/261]  eta: 0:00:47  loss: 0.9650 (0.9924)  acc1: 78.6458 (77.6203)  acc5: 91.6667 (93.4686)  time: 0.2529  data: 0.0797  max mem: 19734
Test:  [170/261]  eta: 0:00:44  loss: 1.1946 (1.0220)  acc1: 66.1458 (76.8823)  acc5: 86.4583 (93.1347)  time: 0.4948  data: 0.3233  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: 1.4466 (1.0389)  acc1: 66.6667 (76.4906)  acc5: 89.5833 (92.9846)  time: 0.4583  data: 0.3222  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: 1.3495 (1.0520)  acc1: 67.1875 (76.2407)  acc5: 90.6250 (92.8147)  time: 0.1410  data: 0.0176  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.3335 (1.0672)  acc1: 71.8750 (75.9043)  acc5: 89.5833 (92.6099)  time: 0.1650  data: 0.0703  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: 1.3738 (1.0808)  acc1: 68.7500 (75.6294)  acc5: 88.5417 (92.4146)  time: 0.1294  data: 0.0613  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.3838 (1.1006)  acc1: 67.7083 (75.1202)  acc5: 88.0208 (92.1946)  time: 0.0617  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: 1.4562 (1.1097)  acc1: 65.6250 (74.8760)  acc5: 89.5833 (92.1199)  time: 0.0616  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.2793 (1.1186)  acc1: 68.7500 (74.6629)  acc5: 91.1458 (92.0514)  time: 0.0618  data: 0.0003  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: 1.0186 (1.1116)  acc1: 76.5625 (74.8506)  acc5: 92.7083 (92.1585)  time: 0.0618  data: 0.0003  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9243 (1.1113)  acc1: 78.1250 (74.8740)  acc5: 95.3125 (92.2220)  time: 0.0597  data: 0.0002  max mem: 19734
Test: Total time: 0:01:32 (0.3545 s / it)
* Acc@1 74.874 Acc@5 92.222 loss 1.111
Accuracy of the network on the 50000 test images: 74.9%
Max accuracy: 74.87%
Epoch: [52]  [   0/1251]  eta: 1:54:05  lr: 0.000010  loss: 2.8142 (2.8142)  time: 5.4719  data: 4.9828  max mem: 19734
Epoch: [52]  [  10/1251]  eta: 0:22:03  lr: 0.000010  loss: 3.2141 (3.4046)  time: 1.0665  data: 0.5111  max mem: 19734
Epoch: [52]  [  20/1251]  eta: 0:16:36  lr: 0.000010  loss: 3.3396 (3.4010)  time: 0.5760  data: 0.0322  max mem: 19734
Epoch: [52]  [  30/1251]  eta: 0:14:09  lr: 0.000010  loss: 3.4143 (3.3935)  time: 0.4919  data: 0.0006  max mem: 19734
Epoch: [52]  [  40/1251]  eta: 0:12:53  lr: 0.000010  loss: 3.5538 (3.4373)  time: 0.4597  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3508, ratio_loss=0.0044, pruning_loss=0.1191, mse_loss=0.4190
Epoch: [52]  [  50/1251]  eta: 0:12:19  lr: 0.000010  loss: 3.5083 (3.3870)  time: 0.4913  data: 0.0005  max mem: 19734
Epoch: [52]  [  60/1251]  eta: 0:11:48  lr: 0.000010  loss: 3.5550 (3.4291)  time: 0.5048  data: 0.0004  max mem: 19734
Epoch: [52]  [  70/1251]  eta: 0:11:20  lr: 0.000010  loss: 3.5468 (3.4182)  time: 0.4761  data: 0.0004  max mem: 19734
Epoch: [52]  [  80/1251]  eta: 0:10:58  lr: 0.000010  loss: 3.1394 (3.3935)  time: 0.4630  data: 0.0004  max mem: 19734
Epoch: [52]  [  90/1251]  eta: 0:10:39  lr: 0.000010  loss: 3.2938 (3.3911)  time: 0.4620  data: 0.0004  max mem: 19734
Epoch: [52]  [ 100/1251]  eta: 0:10:24  lr: 0.000010  loss: 3.2938 (3.3781)  time: 0.4609  data: 0.0004  max mem: 19734
Epoch: [52]  [ 110/1251]  eta: 0:10:10  lr: 0.000010  loss: 3.2791 (3.3822)  time: 0.4614  data: 0.0006  max mem: 19734
Epoch: [52]  [ 120/1251]  eta: 0:09:57  lr: 0.000010  loss: 3.5445 (3.3975)  time: 0.4597  data: 0.0006  max mem: 19734
Epoch: [52]  [ 130/1251]  eta: 0:09:46  lr: 0.000010  loss: 3.6818 (3.4244)  time: 0.4573  data: 0.0004  max mem: 19734
Epoch: [52]  [ 140/1251]  eta: 0:09:36  lr: 0.000010  loss: 3.6827 (3.4445)  time: 0.4619  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4526, ratio_loss=0.0046, pruning_loss=0.1171, mse_loss=0.4062
Epoch: [52]  [ 150/1251]  eta: 0:09:27  lr: 0.000010  loss: 3.6918 (3.4529)  time: 0.4625  data: 0.0004  max mem: 19734
Epoch: [52]  [ 160/1251]  eta: 0:09:21  lr: 0.000010  loss: 3.6767 (3.4637)  time: 0.4822  data: 0.0004  max mem: 19734
Epoch: [52]  [ 170/1251]  eta: 0:09:14  lr: 0.000010  loss: 3.5251 (3.4530)  time: 0.4968  data: 0.0004  max mem: 19734
Epoch: [52]  [ 180/1251]  eta: 0:09:05  lr: 0.000010  loss: 3.5251 (3.4621)  time: 0.4715  data: 0.0004  max mem: 19734
Epoch: [52]  [ 190/1251]  eta: 0:08:58  lr: 0.000010  loss: 3.6098 (3.4585)  time: 0.4615  data: 0.0005  max mem: 19734
Epoch: [52]  [ 200/1251]  eta: 0:08:53  lr: 0.000010  loss: 3.4472 (3.4553)  time: 0.4858  data: 0.0004  max mem: 19734
Epoch: [52]  [ 210/1251]  eta: 0:08:45  lr: 0.000010  loss: 3.4472 (3.4551)  time: 0.4794  data: 0.0004  max mem: 19734
Epoch: [52]  [ 220/1251]  eta: 0:08:38  lr: 0.000010  loss: 3.5204 (3.4609)  time: 0.4557  data: 0.0005  max mem: 19734
Epoch: [52]  [ 230/1251]  eta: 0:08:31  lr: 0.000010  loss: 3.5458 (3.4606)  time: 0.4576  data: 0.0005  max mem: 19734
Epoch: [52]  [ 240/1251]  eta: 0:08:24  lr: 0.000010  loss: 3.5344 (3.4530)  time: 0.4568  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4183, ratio_loss=0.0047, pruning_loss=0.1181, mse_loss=0.4061
Epoch: [52]  [ 250/1251]  eta: 0:08:17  lr: 0.000010  loss: 3.3178 (3.4341)  time: 0.4546  data: 0.0004  max mem: 19734
Epoch: [52]  [ 260/1251]  eta: 0:08:11  lr: 0.000010  loss: 3.1125 (3.4314)  time: 0.4558  data: 0.0005  max mem: 19734
Epoch: [52]  [ 270/1251]  eta: 0:08:04  lr: 0.000010  loss: 3.4700 (3.4325)  time: 0.4544  data: 0.0005  max mem: 19734
Epoch: [52]  [ 280/1251]  eta: 0:07:58  lr: 0.000010  loss: 3.3615 (3.4296)  time: 0.4536  data: 0.0005  max mem: 19734
Epoch: [52]  [ 290/1251]  eta: 0:07:52  lr: 0.000010  loss: 3.2855 (3.4175)  time: 0.4548  data: 0.0004  max mem: 19734
Epoch: [52]  [ 300/1251]  eta: 0:07:46  lr: 0.000010  loss: 3.5220 (3.4178)  time: 0.4571  data: 0.0004  max mem: 19734
Epoch: [52]  [ 310/1251]  eta: 0:07:41  lr: 0.000010  loss: 3.4576 (3.4139)  time: 0.4787  data: 0.0005  max mem: 19734
Epoch: [52]  [ 320/1251]  eta: 0:07:35  lr: 0.000010  loss: 3.4576 (3.4092)  time: 0.4771  data: 0.0005  max mem: 19734
Epoch: [52]  [ 330/1251]  eta: 0:07:29  lr: 0.000010  loss: 3.5731 (3.4112)  time: 0.4580  data: 0.0004  max mem: 19734
Epoch: [52]  [ 340/1251]  eta: 0:07:25  lr: 0.000010  loss: 3.4649 (3.4114)  time: 0.4752  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3098, ratio_loss=0.0046, pruning_loss=0.1194, mse_loss=0.4074
Epoch: [52]  [ 350/1251]  eta: 0:07:19  lr: 0.000010  loss: 3.4649 (3.4155)  time: 0.4841  data: 0.0005  max mem: 19734
Epoch: [52]  [ 360/1251]  eta: 0:07:14  lr: 0.000010  loss: 3.4067 (3.4153)  time: 0.4670  data: 0.0004  max mem: 19734
Epoch: [52]  [ 370/1251]  eta: 0:07:08  lr: 0.000010  loss: 3.5946 (3.4199)  time: 0.4597  data: 0.0004  max mem: 19734
Epoch: [52]  [ 380/1251]  eta: 0:07:03  lr: 0.000010  loss: 3.6926 (3.4259)  time: 0.4594  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 390/1251]  eta: 0:06:57  lr: 0.000010  loss: 3.6906 (3.4241)  time: 0.4611  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 400/1251]  eta: 0:06:52  lr: 0.000010  loss: 0.0000 (3.3387)  time: 0.4580  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 410/1251]  eta: 0:06:46  lr: 0.000010  loss: 0.0000 (3.2575)  time: 0.4475  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 420/1251]  eta: 0:06:41  lr: 0.000010  loss: 0.0000 (3.1801)  time: 0.4444  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 430/1251]  eta: 0:06:35  lr: 0.000010  loss: 0.0000 (3.1063)  time: 0.4443  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 440/1251]  eta: 0:06:30  lr: 0.000010  loss: 0.0000 (3.0359)  time: 0.4450  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 450/1251]  eta: 0:06:25  lr: 0.000010  loss: 0.0000 (2.9686)  time: 0.4669  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 460/1251]  eta: 0:06:20  lr: 0.000010  loss: 0.0000 (2.9042)  time: 0.4716  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 470/1251]  eta: 0:06:14  lr: 0.000010  loss: 0.0000 (2.8425)  time: 0.4498  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 480/1251]  eta: 0:06:09  lr: 0.000010  loss: 0.0000 (2.7834)  time: 0.4641  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 490/1251]  eta: 0:06:05  lr: 0.000010  loss: 0.0000 (2.7267)  time: 0.4816  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 500/1251]  eta: 0:05:59  lr: 0.000010  loss: 0.0000 (2.6723)  time: 0.4630  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 510/1251]  eta: 0:05:54  lr: 0.000010  loss: 0.0000 (2.6200)  time: 0.4454  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 520/1251]  eta: 0:05:49  lr: 0.000010  loss: 0.0000 (2.5697)  time: 0.4450  data: 0.0012  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 530/1251]  eta: 0:05:44  lr: 0.000010  loss: 0.0000 (2.5213)  time: 0.4485  data: 0.0012  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 540/1251]  eta: 0:05:38  lr: 0.000010  loss: 0.0000 (2.4747)  time: 0.4476  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 550/1251]  eta: 0:05:33  lr: 0.000010  loss: 0.0000 (2.4298)  time: 0.4431  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 560/1251]  eta: 0:05:28  lr: 0.000010  loss: 0.0000 (2.3865)  time: 0.4434  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 570/1251]  eta: 0:05:23  lr: 0.000010  loss: 0.0000 (2.3447)  time: 0.4439  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 580/1251]  eta: 0:05:18  lr: 0.000010  loss: 0.0000 (2.3044)  time: 0.4445  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 590/1251]  eta: 0:05:13  lr: 0.000010  loss: 0.0000 (2.2654)  time: 0.4463  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 600/1251]  eta: 0:05:08  lr: 0.000010  loss: 0.0000 (2.2277)  time: 0.4759  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 610/1251]  eta: 0:05:03  lr: 0.000010  loss: 0.0000 (2.1912)  time: 0.4739  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 620/1251]  eta: 0:04:58  lr: 0.000010  loss: 0.0000 (2.1559)  time: 0.4456  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 630/1251]  eta: 0:04:54  lr: 0.000010  loss: 0.0000 (2.1218)  time: 0.4704  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 640/1251]  eta: 0:04:49  lr: 0.000010  loss: 0.0000 (2.0887)  time: 0.4785  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 650/1251]  eta: 0:04:44  lr: 0.000010  loss: 0.0000 (2.0566)  time: 0.4553  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 660/1251]  eta: 0:04:39  lr: 0.000010  loss: 0.0000 (2.0255)  time: 0.4485  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 670/1251]  eta: 0:04:34  lr: 0.000010  loss: 0.0000 (1.9953)  time: 0.4478  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 680/1251]  eta: 0:04:29  lr: 0.000010  loss: 0.0000 (1.9660)  time: 0.4452  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 690/1251]  eta: 0:04:24  lr: 0.000010  loss: 0.0000 (1.9375)  time: 0.4439  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 700/1251]  eta: 0:04:19  lr: 0.000010  loss: 0.0000 (1.9099)  time: 0.4417  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 710/1251]  eta: 0:04:14  lr: 0.000010  loss: 0.0000 (1.8830)  time: 0.4401  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 720/1251]  eta: 0:04:09  lr: 0.000010  loss: 0.0000 (1.8569)  time: 0.4418  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 730/1251]  eta: 0:04:04  lr: 0.000010  loss: 0.0000 (1.8315)  time: 0.4460  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 740/1251]  eta: 0:04:00  lr: 0.000010  loss: 0.0000 (1.8068)  time: 0.4663  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 750/1251]  eta: 0:03:55  lr: 0.000010  loss: 0.0000 (1.7827)  time: 0.4737  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 760/1251]  eta: 0:03:50  lr: 0.000010  loss: 0.0000 (1.7593)  time: 0.4538  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 770/1251]  eta: 0:03:45  lr: 0.000010  loss: 0.0000 (1.7365)  time: 0.4480  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 780/1251]  eta: 0:03:41  lr: 0.000010  loss: 0.0000 (1.7143)  time: 0.4672  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 790/1251]  eta: 0:03:36  lr: 0.000010  loss: 0.0000 (1.6926)  time: 0.4654  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 800/1251]  eta: 0:03:31  lr: 0.000010  loss: 0.0000 (1.6715)  time: 0.4444  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 810/1251]  eta: 0:03:26  lr: 0.000010  loss: 0.0000 (1.6508)  time: 0.4425  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 820/1251]  eta: 0:03:21  lr: 0.000010  loss: 0.0000 (1.6307)  time: 0.4421  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 830/1251]  eta: 0:03:17  lr: 0.000010  loss: 0.0000 (1.6111)  time: 0.4397  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 840/1251]  eta: 0:03:12  lr: 0.000010  loss: 0.0000 (1.5920)  time: 0.4418  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 850/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (1.5732)  time: 0.4438  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 860/1251]  eta: 0:03:02  lr: 0.000010  loss: 0.0000 (1.5550)  time: 0.4468  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 870/1251]  eta: 0:02:58  lr: 0.000010  loss: 0.0000 (1.5371)  time: 0.4497  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 880/1251]  eta: 0:02:53  lr: 0.000010  loss: 0.0000 (1.5197)  time: 0.4460  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 890/1251]  eta: 0:02:48  lr: 0.000010  loss: 0.0000 (1.5026)  time: 0.4629  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 900/1251]  eta: 0:02:43  lr: 0.000010  loss: 0.0000 (1.4859)  time: 0.4634  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 910/1251]  eta: 0:02:39  lr: 0.000010  loss: 0.0000 (1.4696)  time: 0.4479  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 920/1251]  eta: 0:02:34  lr: 0.000010  loss: 0.0000 (1.4537)  time: 0.4655  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 930/1251]  eta: 0:02:29  lr: 0.000010  loss: 0.0000 (1.4381)  time: 0.4810  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 940/1251]  eta: 0:02:25  lr: 0.000010  loss: 0.0000 (1.4228)  time: 0.4644  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 950/1251]  eta: 0:02:20  lr: 0.000010  loss: 0.0000 (1.4078)  time: 0.4446  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 960/1251]  eta: 0:02:15  lr: 0.000010  loss: 0.0000 (1.3932)  time: 0.4426  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 970/1251]  eta: 0:02:10  lr: 0.000010  loss: 0.0000 (1.3788)  time: 0.4423  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 980/1251]  eta: 0:02:06  lr: 0.000010  loss: 0.0000 (1.3648)  time: 0.4424  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [ 990/1251]  eta: 0:02:01  lr: 0.000010  loss: 0.0000 (1.3510)  time: 0.4465  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1000/1251]  eta: 0:01:56  lr: 0.000010  loss: 0.0000 (1.3375)  time: 0.4486  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1010/1251]  eta: 0:01:52  lr: 0.000010  loss: 0.0000 (1.3243)  time: 0.4435  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1020/1251]  eta: 0:01:47  lr: 0.000010  loss: 0.0000 (1.3113)  time: 0.4431  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1030/1251]  eta: 0:01:42  lr: 0.000010  loss: 0.0000 (1.2986)  time: 0.4724  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1040/1251]  eta: 0:01:38  lr: 0.000010  loss: 0.0000 (1.2861)  time: 0.4724  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1050/1251]  eta: 0:01:33  lr: 0.000010  loss: 0.0000 (1.2739)  time: 0.4476  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1060/1251]  eta: 0:01:28  lr: 0.000010  loss: 0.0000 (1.2619)  time: 0.4693  data: 0.0018  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1070/1251]  eta: 0:01:24  lr: 0.000010  loss: 0.0000 (1.2501)  time: 0.4779  data: 0.0012  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1080/1251]  eta: 0:01:19  lr: 0.000010  loss: 0.0000 (1.2385)  time: 0.4646  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1090/1251]  eta: 0:01:14  lr: 0.000010  loss: 0.0000 (1.2272)  time: 0.4513  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1100/1251]  eta: 0:01:10  lr: 0.000010  loss: 0.0000 (1.2160)  time: 0.4419  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1110/1251]  eta: 0:01:05  lr: 0.000010  loss: 0.0000 (1.2051)  time: 0.4427  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1120/1251]  eta: 0:01:00  lr: 0.000010  loss: 0.0000 (1.1943)  time: 0.4447  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1130/1251]  eta: 0:00:56  lr: 0.000010  loss: 0.0000 (1.1838)  time: 0.4457  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1140/1251]  eta: 0:00:51  lr: 0.000010  loss: 0.0000 (1.1734)  time: 0.4443  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1150/1251]  eta: 0:00:46  lr: 0.000010  loss: 0.0000 (1.1632)  time: 0.4430  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1160/1251]  eta: 0:00:42  lr: 0.000010  loss: 0.0000 (1.1532)  time: 0.4431  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1170/1251]  eta: 0:00:37  lr: 0.000010  loss: 0.0000 (1.1433)  time: 0.4543  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1180/1251]  eta: 0:00:32  lr: 0.000010  loss: 0.0000 (1.1336)  time: 0.4745  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1190/1251]  eta: 0:00:28  lr: 0.000010  loss: 0.0000 (1.1241)  time: 0.4689  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1200/1251]  eta: 0:00:23  lr: 0.000010  loss: 0.0000 (1.1148)  time: 0.4435  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1210/1251]  eta: 0:00:19  lr: 0.000010  loss: 0.0000 (1.1056)  time: 0.4473  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1220/1251]  eta: 0:00:14  lr: 0.000010  loss: 0.0000 (1.0965)  time: 0.4555  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1230/1251]  eta: 0:00:09  lr: 0.000010  loss: 0.0000 (1.0876)  time: 0.4490  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1240/1251]  eta: 0:00:05  lr: 0.000010  loss: 0.0000 (1.0788)  time: 0.4401  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [52]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (1.0702)  time: 0.4348  data: 0.0002  max mem: 19734
Epoch: [52] Total time: 0:09:39 (0.4635 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (1.0578)
Test:  [  0/261]  eta: 2:04:17  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 28.5743  data: 28.4870  max mem: 19734
Test:  [ 10/261]  eta: 0:11:50  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.8300  data: 2.6061  max mem: 19734
Test:  [ 20/261]  eta: 0:06:32  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2799  data: 0.0152  max mem: 19734
Test:  [ 30/261]  eta: 0:04:37  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3029  data: 0.0153  max mem: 19734
Test:  [ 40/261]  eta: 0:04:11  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6223  data: 0.3548  max mem: 19734
Test:  [ 50/261]  eta: 0:03:25  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6302  data: 0.3541  max mem: 19734
Test:  [ 60/261]  eta: 0:02:51  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2726  data: 0.0129  max mem: 19734
Test:  [ 70/261]  eta: 0:02:27  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.2481  data: 0.0103  max mem: 19734
Test:  [ 80/261]  eta: 0:02:13  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3882  data: 0.1178  max mem: 19734
Test:  [ 90/261]  eta: 0:02:02  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.5287  data: 0.2917  max mem: 19734
Test:  [100/261]  eta: 0:01:50  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4818  data: 0.3226  max mem: 19734
Test:  [110/261]  eta: 0:01:38  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3373  data: 0.2110  max mem: 19734
Test:  [120/261]  eta: 0:01:28  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3277  data: 0.2194  max mem: 19734
Test:  [130/261]  eta: 0:01:17  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2536  data: 0.1563  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.1130  data: 0.0089  max mem: 19734
Test:  [150/261]  eta: 0:01:00  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.2389  data: 0.1290  max mem: 19734
Test:  [160/261]  eta: 0:00:52  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3034  data: 0.2039  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1556  data: 0.0811  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1539  data: 0.0925  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1499  data: 0.0911  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0604  data: 0.0015  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0595  data: 0.0006  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0581  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0582  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0572  data: 0.0002  max mem: 19734
Test: Total time: 0:01:32 (0.3539 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [53]  [   0/1251]  eta: 4:10:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 12.0148  data: 7.3688  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [  10/1251]  eta: 0:45:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.2232  data: 1.2240  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [  20/1251]  eta: 0:28:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.8465  data: 0.3050  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [  30/1251]  eta: 0:21:55  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4478  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [  40/1251]  eta: 0:18:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4482  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [  50/1251]  eta: 0:16:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4477  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [  60/1251]  eta: 0:15:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4692  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [  70/1251]  eta: 0:14:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4842  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [  80/1251]  eta: 0:13:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4601  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [  90/1251]  eta: 0:13:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4596  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 100/1251]  eta: 0:12:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 110/1251]  eta: 0:12:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4687  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 120/1251]  eta: 0:11:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4580  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 130/1251]  eta: 0:11:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4488  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 140/1251]  eta: 0:11:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4559  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 150/1251]  eta: 0:10:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4550  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 160/1251]  eta: 0:10:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4479  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 170/1251]  eta: 0:10:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4456  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 180/1251]  eta: 0:10:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 190/1251]  eta: 0:09:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4446  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 200/1251]  eta: 0:09:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4456  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 210/1251]  eta: 0:09:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4807  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 220/1251]  eta: 0:09:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4795  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 230/1251]  eta: 0:09:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4715  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 240/1251]  eta: 0:09:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4709  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 250/1251]  eta: 0:08:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4652  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 260/1251]  eta: 0:08:48  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4668  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 270/1251]  eta: 0:08:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4505  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 280/1251]  eta: 0:08:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4507  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 290/1251]  eta: 0:08:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4475  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 300/1251]  eta: 0:08:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4481  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 310/1251]  eta: 0:08:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 320/1251]  eta: 0:08:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4459  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 330/1251]  eta: 0:07:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4473  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 340/1251]  eta: 0:07:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4471  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 350/1251]  eta: 0:07:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4770  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 360/1251]  eta: 0:07:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4848  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 370/1251]  eta: 0:07:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4523  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 380/1251]  eta: 0:07:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4646  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 390/1251]  eta: 0:07:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4773  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 400/1251]  eta: 0:07:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4648  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 410/1251]  eta: 0:07:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4521  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 420/1251]  eta: 0:06:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4446  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 430/1251]  eta: 0:06:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4451  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 440/1251]  eta: 0:06:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4449  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 450/1251]  eta: 0:06:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4458  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 460/1251]  eta: 0:06:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4457  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 470/1251]  eta: 0:06:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4482  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 480/1251]  eta: 0:06:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4473  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 490/1251]  eta: 0:06:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4496  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 500/1251]  eta: 0:06:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4721  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 510/1251]  eta: 0:06:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4673  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 520/1251]  eta: 0:06:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4446  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 530/1251]  eta: 0:05:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4687  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 540/1251]  eta: 0:05:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4871  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 550/1251]  eta: 0:05:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4638  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 560/1251]  eta: 0:05:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4544  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 570/1251]  eta: 0:05:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4539  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 580/1251]  eta: 0:05:29  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 590/1251]  eta: 0:05:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 600/1251]  eta: 0:05:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4503  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 610/1251]  eta: 0:05:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4504  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 620/1251]  eta: 0:05:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4439  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 630/1251]  eta: 0:05:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 640/1251]  eta: 0:04:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4598  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 650/1251]  eta: 0:04:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4835  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 660/1251]  eta: 0:04:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4673  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 670/1251]  eta: 0:04:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4650  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 680/1251]  eta: 0:04:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4717  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 690/1251]  eta: 0:04:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4661  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 700/1251]  eta: 0:04:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4583  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 710/1251]  eta: 0:04:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4459  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 720/1251]  eta: 0:04:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4455  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 730/1251]  eta: 0:04:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4452  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 740/1251]  eta: 0:04:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 750/1251]  eta: 0:04:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4423  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 760/1251]  eta: 0:03:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4420  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 770/1251]  eta: 0:03:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4437  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 780/1251]  eta: 0:03:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4539  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 790/1251]  eta: 0:03:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4846  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 800/1251]  eta: 0:03:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4769  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 810/1251]  eta: 0:03:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4554  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 820/1251]  eta: 0:03:27  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4705  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 830/1251]  eta: 0:03:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4673  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 840/1251]  eta: 0:03:17  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4497  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 850/1251]  eta: 0:03:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4413  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 860/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 870/1251]  eta: 0:03:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4466  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 880/1251]  eta: 0:02:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4402  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 890/1251]  eta: 0:02:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4382  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 900/1251]  eta: 0:02:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4393  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 910/1251]  eta: 0:02:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4381  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 920/1251]  eta: 0:02:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4385  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 930/1251]  eta: 0:02:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4597  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 940/1251]  eta: 0:02:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4714  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 950/1251]  eta: 0:02:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4532  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 960/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4527  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 970/1251]  eta: 0:02:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4669  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 980/1251]  eta: 0:02:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4645  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [ 990/1251]  eta: 0:02:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4521  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1000/1251]  eta: 0:01:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1010/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4444  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1020/1251]  eta: 0:01:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4410  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1030/1251]  eta: 0:01:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4399  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1040/1251]  eta: 0:01:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4387  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1050/1251]  eta: 0:01:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4381  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1060/1251]  eta: 0:01:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1070/1251]  eta: 0:01:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4580  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1080/1251]  eta: 0:01:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4799  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1090/1251]  eta: 0:01:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4701  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1100/1251]  eta: 0:01:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4524  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1110/1251]  eta: 0:01:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4620  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1120/1251]  eta: 0:01:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1130/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4598  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1140/1251]  eta: 0:00:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1150/1251]  eta: 0:00:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4408  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1160/1251]  eta: 0:00:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4420  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1170/1251]  eta: 0:00:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4404  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1180/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4381  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1190/1251]  eta: 0:00:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4417  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1200/1251]  eta: 0:00:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4410  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1210/1251]  eta: 0:00:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4352  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1220/1251]  eta: 0:00:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4591  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1230/1251]  eta: 0:00:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4724  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1240/1251]  eta: 0:00:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4474  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [53]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4406  data: 0.0001  max mem: 19734
Epoch: [53] Total time: 0:09:49 (0.4709 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:53:23  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 26.0675  data: 25.9278  max mem: 19734
Test:  [ 10/261]  eta: 0:10:23  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.4856  data: 2.3664  max mem: 19734
Test:  [ 20/261]  eta: 0:05:33  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.1480  data: 0.0121  max mem: 19734
Test:  [ 30/261]  eta: 0:03:49  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1710  data: 0.0181  max mem: 19734
Test:  [ 40/261]  eta: 0:03:15  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3639  data: 0.2134  max mem: 19734
Test:  [ 50/261]  eta: 0:02:37  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.3663  data: 0.2090  max mem: 19734
Test:  [ 60/261]  eta: 0:02:11  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1828  data: 0.0187  max mem: 19734
Test:  [ 70/261]  eta: 0:01:58  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3066  data: 0.1552  max mem: 19734
Test:  [ 80/261]  eta: 0:01:46  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3915  data: 0.2476  max mem: 19734
Test:  [ 90/261]  eta: 0:01:34  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3043  data: 0.1132  max mem: 19734
Test:  [100/261]  eta: 0:01:33  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.5339  data: 0.3040  max mem: 19734
Test:  [110/261]  eta: 0:01:28  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.7424  data: 0.5515  max mem: 19734
Test:  [120/261]  eta: 0:01:17  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4122  data: 0.2639  max mem: 19734
Test:  [130/261]  eta: 0:01:08  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.1535  data: 0.0179  max mem: 19734
Test:  [140/261]  eta: 0:01:00  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.1909  data: 0.0451  max mem: 19734
Test:  [150/261]  eta: 0:00:53  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.2033  data: 0.0394  max mem: 19734
Test:  [160/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1795  data: 0.0385  max mem: 19734
Test:  [170/261]  eta: 0:00:40  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1585  data: 0.0536  max mem: 19734
Test:  [180/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1349  data: 0.0566  max mem: 19734
Test:  [190/261]  eta: 0:00:28  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.0974  data: 0.0371  max mem: 19734
Test:  [200/261]  eta: 0:00:23  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0593  data: 0.0004  max mem: 19734
Test:  [210/261]  eta: 0:00:18  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0597  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:14  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0589  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0568  data: 0.0002  max mem: 19734
Test: Total time: 0:01:21 (0.3131 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [54]  [   0/1251]  eta: 6:14:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 17.9584  data: 17.5224  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [  10/1251]  eta: 0:43:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 2.1049  data: 1.5934  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [  20/1251]  eta: 0:27:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4921  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [  30/1251]  eta: 0:21:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4617  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [  40/1251]  eta: 0:18:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4535  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [  50/1251]  eta: 0:16:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4495  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [  60/1251]  eta: 0:14:55  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4494  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [  70/1251]  eta: 0:13:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4491  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [  80/1251]  eta: 0:13:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4492  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [  90/1251]  eta: 0:12:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4530  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 100/1251]  eta: 0:12:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4541  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 110/1251]  eta: 0:11:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4685  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 120/1251]  eta: 0:11:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4778  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 130/1251]  eta: 0:11:10  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4809  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 140/1251]  eta: 0:10:53  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4711  data: 0.0013  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 150/1251]  eta: 0:10:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4710  data: 0.0012  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 160/1251]  eta: 0:10:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4677  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 170/1251]  eta: 0:10:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4442  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 180/1251]  eta: 0:09:58  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 190/1251]  eta: 0:09:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 200/1251]  eta: 0:09:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4452  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 210/1251]  eta: 0:09:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 220/1251]  eta: 0:09:15  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4497  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 230/1251]  eta: 0:09:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4471  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 240/1251]  eta: 0:08:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4440  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 250/1251]  eta: 0:08:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4667  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 260/1251]  eta: 0:08:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4900  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 270/1251]  eta: 0:08:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4680  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 280/1251]  eta: 0:08:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4743  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 290/1251]  eta: 0:08:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4867  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 300/1251]  eta: 0:08:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4673  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 310/1251]  eta: 0:08:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4549  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 320/1251]  eta: 0:08:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4465  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 330/1251]  eta: 0:07:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4455  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 340/1251]  eta: 0:07:45  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 350/1251]  eta: 0:07:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4516  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 360/1251]  eta: 0:07:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4511  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 370/1251]  eta: 0:07:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4475  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 380/1251]  eta: 0:07:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4597  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 390/1251]  eta: 0:07:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4616  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 400/1251]  eta: 0:07:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4757  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 410/1251]  eta: 0:07:03  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4858  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 420/1251]  eta: 0:06:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4685  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 430/1251]  eta: 0:06:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4558  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 440/1251]  eta: 0:06:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4646  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 450/1251]  eta: 0:06:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4616  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 460/1251]  eta: 0:06:34  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 470/1251]  eta: 0:06:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4475  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 480/1251]  eta: 0:06:22  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4522  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 490/1251]  eta: 0:06:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4499  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 500/1251]  eta: 0:06:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4459  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 510/1251]  eta: 0:06:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4472  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 520/1251]  eta: 0:05:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4488  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 530/1251]  eta: 0:05:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4457  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 540/1251]  eta: 0:05:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4688  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 550/1251]  eta: 0:05:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4883  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 560/1251]  eta: 0:05:39  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4639  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 570/1251]  eta: 0:05:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4541  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 580/1251]  eta: 0:05:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4657  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 590/1251]  eta: 0:05:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 600/1251]  eta: 0:05:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4581  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 610/1251]  eta: 0:05:12  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4539  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 620/1251]  eta: 0:05:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4517  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 630/1251]  eta: 0:05:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4433  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 640/1251]  eta: 0:04:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4431  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 650/1251]  eta: 0:04:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4425  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 660/1251]  eta: 0:04:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4427  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 670/1251]  eta: 0:04:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4457  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 680/1251]  eta: 0:04:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4573  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 690/1251]  eta: 0:04:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4752  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 700/1251]  eta: 0:04:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4728  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 710/1251]  eta: 0:04:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4601  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 720/1251]  eta: 0:04:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4617  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 730/1251]  eta: 0:04:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4688  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 740/1251]  eta: 0:04:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4593  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 750/1251]  eta: 0:04:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4435  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 760/1251]  eta: 0:03:56  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4413  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 770/1251]  eta: 0:03:51  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 780/1251]  eta: 0:03:46  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4423  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 790/1251]  eta: 0:03:41  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 800/1251]  eta: 0:03:36  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4494  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 810/1251]  eta: 0:03:31  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4558  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 820/1251]  eta: 0:03:26  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4499  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 830/1251]  eta: 0:03:21  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4489  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 840/1251]  eta: 0:03:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 850/1251]  eta: 0:03:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4656  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 860/1251]  eta: 0:03:07  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4660  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 870/1251]  eta: 0:03:02  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4794  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 880/1251]  eta: 0:02:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4683  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 890/1251]  eta: 0:02:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4552  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 900/1251]  eta: 0:02:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4468  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 910/1251]  eta: 0:02:42  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4472  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 920/1251]  eta: 0:02:37  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4432  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 930/1251]  eta: 0:02:32  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4457  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 940/1251]  eta: 0:02:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4501  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 950/1251]  eta: 0:02:23  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4469  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 960/1251]  eta: 0:02:18  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4436  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 970/1251]  eta: 0:02:13  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4495  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 980/1251]  eta: 0:02:08  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4653  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [ 990/1251]  eta: 0:02:04  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4679  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1000/1251]  eta: 0:01:59  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4555  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1010/1251]  eta: 0:01:54  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4663  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1020/1251]  eta: 0:01:49  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4703  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1030/1251]  eta: 0:01:44  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4507  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1040/1251]  eta: 0:01:40  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4539  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1050/1251]  eta: 0:01:35  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4531  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1060/1251]  eta: 0:01:30  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4419  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1070/1251]  eta: 0:01:25  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4466  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1080/1251]  eta: 0:01:20  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4480  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1090/1251]  eta: 0:01:16  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1100/1251]  eta: 0:01:11  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4515  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1110/1251]  eta: 0:01:06  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4490  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1120/1251]  eta: 0:01:01  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4626  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1130/1251]  eta: 0:00:57  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4792  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1140/1251]  eta: 0:00:52  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4612  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1150/1251]  eta: 0:00:47  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4515  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1160/1251]  eta: 0:00:43  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4815  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1170/1251]  eta: 0:00:38  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4731  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1180/1251]  eta: 0:00:33  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4427  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1190/1251]  eta: 0:00:28  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4458  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1200/1251]  eta: 0:00:24  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4436  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1210/1251]  eta: 0:00:19  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4368  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1220/1251]  eta: 0:00:14  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4326  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1230/1251]  eta: 0:00:09  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4321  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1240/1251]  eta: 0:00:05  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4321  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [54]  [1250/1251]  eta: 0:00:00  lr: 0.000010  loss: 0.0000 (0.0000)  time: 0.4319  data: 0.0001  max mem: 19734
Epoch: [54] Total time: 0:09:48 (0.4708 s / it)
Averaged stats: lr: 0.000010  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:54:23  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 26.2973  data: 26.1759  max mem: 19734
Test:  [ 10/261]  eta: 0:10:41  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.5547  data: 2.4437  max mem: 19734
Test:  [ 20/261]  eta: 0:05:33  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.1383  data: 0.0397  max mem: 19734
Test:  [ 30/261]  eta: 0:03:44  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1017  data: 0.0119  max mem: 19734
Test:  [ 40/261]  eta: 0:03:22  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4218  data: 0.3328  max mem: 19734
Test:  [ 50/261]  eta: 0:02:40  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4277  data: 0.3323  max mem: 19734
Test:  [ 60/261]  eta: 0:02:12  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1311  data: 0.0160  max mem: 19734
Test:  [ 70/261]  eta: 0:02:23  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.7383  data: 0.6030  max mem: 19734
Test:  [ 80/261]  eta: 0:02:03  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.7531  data: 0.6032  max mem: 19734
Test:  [ 90/261]  eta: 0:01:47  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.1890  data: 0.0203  max mem: 19734
Test:  [100/261]  eta: 0:01:34  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.1953  data: 0.0165  max mem: 19734
Test:  [110/261]  eta: 0:01:26  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3048  data: 0.0914  max mem: 19734
Test:  [120/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2957  data: 0.0922  max mem: 19734
Test:  [130/261]  eta: 0:01:06  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.1383  data: 0.0089  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.7095  data: 0.5765  max mem: 19734
Test:  [150/261]  eta: 0:00:59  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.7563  data: 0.5798  max mem: 19734
Test:  [160/261]  eta: 0:00:52  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1991  data: 0.0122  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3451  data: 0.1768  max mem: 19734
Test:  [180/261]  eta: 0:00:40  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.3115  data: 0.1762  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1043  data: 0.0090  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0759  data: 0.0067  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0816  data: 0.0194  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0753  data: 0.0161  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0585  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0584  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0584  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0574  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3640 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [55]  [   0/1251]  eta: 6:07:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 17.6143  data: 16.8793  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [  10/1251]  eta: 0:45:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.2144  data: 1.5364  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [  20/1251]  eta: 0:28:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.5744  data: 0.0013  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [  30/1251]  eta: 0:22:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4862  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [  40/1251]  eta: 0:18:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4723  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [  50/1251]  eta: 0:17:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4687  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [  60/1251]  eta: 0:15:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4726  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [  70/1251]  eta: 0:14:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4550  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [  80/1251]  eta: 0:13:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4524  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [  90/1251]  eta: 0:13:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4527  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 100/1251]  eta: 0:12:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4542  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 110/1251]  eta: 0:12:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4499  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 120/1251]  eta: 0:11:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 130/1251]  eta: 0:11:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4433  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 140/1251]  eta: 0:11:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 150/1251]  eta: 0:10:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4643  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 160/1251]  eta: 0:10:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4862  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 170/1251]  eta: 0:10:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4740  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 180/1251]  eta: 0:10:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4603  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 190/1251]  eta: 0:09:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4713  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 200/1251]  eta: 0:09:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4683  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 210/1251]  eta: 0:09:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4528  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 220/1251]  eta: 0:09:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4499  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 230/1251]  eta: 0:09:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4481  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 240/1251]  eta: 0:09:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4438  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 250/1251]  eta: 0:08:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4424  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 260/1251]  eta: 0:08:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4423  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 270/1251]  eta: 0:08:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4427  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 280/1251]  eta: 0:08:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4434  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 290/1251]  eta: 0:08:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4474  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 300/1251]  eta: 0:08:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4695  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 310/1251]  eta: 0:08:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4894  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 320/1251]  eta: 0:08:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4794  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 330/1251]  eta: 0:07:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4553  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 340/1251]  eta: 0:07:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4653  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 350/1251]  eta: 0:07:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4704  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 360/1251]  eta: 0:07:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4480  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 370/1251]  eta: 0:07:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 380/1251]  eta: 0:07:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4450  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 390/1251]  eta: 0:07:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4434  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 400/1251]  eta: 0:07:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4429  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 410/1251]  eta: 0:07:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4451  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 420/1251]  eta: 0:06:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4482  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 430/1251]  eta: 0:06:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4468  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 440/1251]  eta: 0:06:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4602  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 450/1251]  eta: 0:06:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4756  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 460/1251]  eta: 0:06:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4583  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 470/1251]  eta: 0:06:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4555  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 480/1251]  eta: 0:06:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4692  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 490/1251]  eta: 0:06:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4657  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 500/1251]  eta: 0:06:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4513  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 510/1251]  eta: 0:06:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4394  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 520/1251]  eta: 0:06:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4415  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 530/1251]  eta: 0:05:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4415  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 540/1251]  eta: 0:05:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4404  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 550/1251]  eta: 0:05:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4451  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 560/1251]  eta: 0:05:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4459  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 570/1251]  eta: 0:05:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4417  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 580/1251]  eta: 0:05:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 590/1251]  eta: 0:05:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4632  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 600/1251]  eta: 0:05:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4733  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 610/1251]  eta: 0:05:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4661  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 620/1251]  eta: 0:05:07  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4580  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 630/1251]  eta: 0:05:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4677  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 640/1251]  eta: 0:04:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4651  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 650/1251]  eta: 0:04:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4411  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 660/1251]  eta: 0:04:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4426  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 670/1251]  eta: 0:04:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4433  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 680/1251]  eta: 0:04:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4477  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 690/1251]  eta: 0:04:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4485  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 700/1251]  eta: 0:04:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4421  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 710/1251]  eta: 0:04:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4406  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 720/1251]  eta: 0:04:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4425  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 730/1251]  eta: 0:04:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4570  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 740/1251]  eta: 0:04:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 750/1251]  eta: 0:04:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4608  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 760/1251]  eta: 0:03:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4586  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 770/1251]  eta: 0:03:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4644  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 780/1251]  eta: 0:03:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4586  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 790/1251]  eta: 0:03:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4477  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 800/1251]  eta: 0:03:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4404  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 810/1251]  eta: 0:03:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4444  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 820/1251]  eta: 0:03:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 830/1251]  eta: 0:03:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4426  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 840/1251]  eta: 0:03:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4402  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 850/1251]  eta: 0:03:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 860/1251]  eta: 0:03:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4546  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 870/1251]  eta: 0:03:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4553  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 880/1251]  eta: 0:02:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4709  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 890/1251]  eta: 0:02:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4799  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 900/1251]  eta: 0:02:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4636  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 910/1251]  eta: 0:02:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 920/1251]  eta: 0:02:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4739  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 930/1251]  eta: 0:02:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4594  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 940/1251]  eta: 0:02:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4444  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 950/1251]  eta: 0:02:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4504  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 960/1251]  eta: 0:02:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4468  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 970/1251]  eta: 0:02:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 980/1251]  eta: 0:02:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4416  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [ 990/1251]  eta: 0:02:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4402  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1000/1251]  eta: 0:01:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4390  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1010/1251]  eta: 0:01:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4423  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1020/1251]  eta: 0:01:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4612  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1030/1251]  eta: 0:01:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4800  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1040/1251]  eta: 0:01:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4620  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1050/1251]  eta: 0:01:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4515  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1060/1251]  eta: 0:01:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4683  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1070/1251]  eta: 0:01:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4682  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1080/1251]  eta: 0:01:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4547  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1090/1251]  eta: 0:01:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4451  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1100/1251]  eta: 0:01:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4433  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1110/1251]  eta: 0:01:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4438  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1120/1251]  eta: 0:01:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4450  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1130/1251]  eta: 0:00:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4455  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1140/1251]  eta: 0:00:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4457  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1150/1251]  eta: 0:00:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4478  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1160/1251]  eta: 0:00:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4561  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1170/1251]  eta: 0:00:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4739  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1180/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4741  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1190/1251]  eta: 0:00:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4574  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1200/1251]  eta: 0:00:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1210/1251]  eta: 0:00:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4443  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1220/1251]  eta: 0:00:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4497  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1230/1251]  eta: 0:00:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4369  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1240/1251]  eta: 0:00:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4312  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [55]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4308  data: 0.0001  max mem: 19734
Epoch: [55] Total time: 0:09:48 (0.4701 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:10:27  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 29.9893  data: 29.7830  max mem: 19734
Test:  [ 10/261]  eta: 0:16:36  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.9711  data: 3.7467  max mem: 19734
Test:  [ 20/261]  eta: 0:09:00  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.8557  data: 0.5841  max mem: 19734
Test:  [ 30/261]  eta: 0:06:12  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.3175  data: 0.0247  max mem: 19734
Test:  [ 40/261]  eta: 0:04:46  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.2983  data: 0.0763  max mem: 19734
Test:  [ 50/261]  eta: 0:03:50  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.2892  data: 0.0736  max mem: 19734
Test:  [ 60/261]  eta: 0:03:11  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2541  data: 0.0170  max mem: 19734
Test:  [ 70/261]  eta: 0:02:42  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.2346  data: 0.0176  max mem: 19734
Test:  [ 80/261]  eta: 0:02:23  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3078  data: 0.1385  max mem: 19734
Test:  [ 90/261]  eta: 0:02:07  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3557  data: 0.1827  max mem: 19734
Test:  [100/261]  eta: 0:01:56  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4255  data: 0.2428  max mem: 19734
Test:  [110/261]  eta: 0:01:42  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3983  data: 0.2773  max mem: 19734
Test:  [120/261]  eta: 0:01:28  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.1693  data: 0.0901  max mem: 19734
Test:  [130/261]  eta: 0:01:17  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.0708  data: 0.0086  max mem: 19734
Test:  [140/261]  eta: 0:01:06  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.0701  data: 0.0075  max mem: 19734
Test:  [150/261]  eta: 0:00:57  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.0599  data: 0.0008  max mem: 19734
Test:  [160/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.0608  data: 0.0010  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.0625  data: 0.0029  max mem: 19734
Test:  [180/261]  eta: 0:00:35  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.0609  data: 0.0023  max mem: 19734
Test:  [190/261]  eta: 0:00:29  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.0590  data: 0.0010  max mem: 19734
Test:  [200/261]  eta: 0:00:24  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0591  data: 0.0010  max mem: 19734
Test:  [210/261]  eta: 0:00:19  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0584  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0597  data: 0.0016  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0597  data: 0.0016  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0565  data: 0.0002  max mem: 19734
Test: Total time: 0:01:24 (0.3254 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [56]  [   0/1251]  eta: 5:37:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 16.1726  data: 14.1049  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [  10/1251]  eta: 0:41:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.0197  data: 1.3391  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [  20/1251]  eta: 0:26:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.5256  data: 0.0315  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [  30/1251]  eta: 0:20:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4524  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [  40/1251]  eta: 0:17:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4539  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [  50/1251]  eta: 0:15:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4586  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [  60/1251]  eta: 0:14:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4918  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [  70/1251]  eta: 0:13:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4967  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [  80/1251]  eta: 0:13:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4736  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [  90/1251]  eta: 0:12:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4595  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 100/1251]  eta: 0:12:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4639  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 110/1251]  eta: 0:11:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4612  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 120/1251]  eta: 0:11:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4457  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 130/1251]  eta: 0:11:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4480  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 140/1251]  eta: 0:10:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4486  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 150/1251]  eta: 0:10:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4501  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 160/1251]  eta: 0:10:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4537  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 170/1251]  eta: 0:10:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4536  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 180/1251]  eta: 0:09:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4490  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 190/1251]  eta: 0:09:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4548  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 200/1251]  eta: 0:09:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4692  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 210/1251]  eta: 0:09:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4675  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 220/1251]  eta: 0:09:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4692  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 230/1251]  eta: 0:09:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4660  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 240/1251]  eta: 0:08:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4576  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 250/1251]  eta: 0:08:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4541  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 260/1251]  eta: 0:08:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4443  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 270/1251]  eta: 0:08:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4427  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 280/1251]  eta: 0:08:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4442  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 290/1251]  eta: 0:08:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4488  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 300/1251]  eta: 0:08:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4486  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 310/1251]  eta: 0:08:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4466  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 320/1251]  eta: 0:07:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 330/1251]  eta: 0:07:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 340/1251]  eta: 0:07:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4576  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 350/1251]  eta: 0:07:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4848  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 360/1251]  eta: 0:07:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4834  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 370/1251]  eta: 0:07:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4644  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 380/1251]  eta: 0:07:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4527  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 390/1251]  eta: 0:07:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4540  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 400/1251]  eta: 0:07:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4564  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 410/1251]  eta: 0:07:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4442  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 420/1251]  eta: 0:06:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 430/1251]  eta: 0:06:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 440/1251]  eta: 0:06:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4426  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 450/1251]  eta: 0:06:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0003  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 460/1251]  eta: 0:06:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0003  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 470/1251]  eta: 0:06:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4419  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 480/1251]  eta: 0:06:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4528  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 490/1251]  eta: 0:06:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4696  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 500/1251]  eta: 0:06:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 510/1251]  eta: 0:06:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 520/1251]  eta: 0:05:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4600  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 530/1251]  eta: 0:05:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4519  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 540/1251]  eta: 0:05:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4535  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 550/1251]  eta: 0:05:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4450  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 560/1251]  eta: 0:05:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4477  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 570/1251]  eta: 0:05:31  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4478  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 580/1251]  eta: 0:05:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4452  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 590/1251]  eta: 0:05:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4466  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 600/1251]  eta: 0:05:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 610/1251]  eta: 0:05:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4416  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 620/1251]  eta: 0:05:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4553  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 630/1251]  eta: 0:04:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4553  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 640/1251]  eta: 0:04:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4703  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 650/1251]  eta: 0:04:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4709  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 660/1251]  eta: 0:04:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4642  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 670/1251]  eta: 0:04:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4777  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 680/1251]  eta: 0:04:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4679  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 690/1251]  eta: 0:04:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4594  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 700/1251]  eta: 0:04:24  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4472  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 710/1251]  eta: 0:04:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 720/1251]  eta: 0:04:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4406  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 730/1251]  eta: 0:04:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4412  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 740/1251]  eta: 0:04:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 750/1251]  eta: 0:03:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4434  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 760/1251]  eta: 0:03:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 770/1251]  eta: 0:03:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4582  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 780/1251]  eta: 0:03:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4758  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 790/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4695  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 800/1251]  eta: 0:03:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4484  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 810/1251]  eta: 0:03:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4587  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 820/1251]  eta: 0:03:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4740  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 830/1251]  eta: 0:03:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4570  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 840/1251]  eta: 0:03:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4429  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 850/1251]  eta: 0:03:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4403  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 860/1251]  eta: 0:03:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 870/1251]  eta: 0:03:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4437  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 880/1251]  eta: 0:02:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4430  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 890/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 900/1251]  eta: 0:02:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4449  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 910/1251]  eta: 0:02:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4529  data: 0.0003  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 920/1251]  eta: 0:02:36  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4648  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 930/1251]  eta: 0:02:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4727  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 940/1251]  eta: 0:02:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4608  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 950/1251]  eta: 0:02:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4528  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 960/1251]  eta: 0:02:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4643  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 970/1251]  eta: 0:02:12  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4710  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 980/1251]  eta: 0:02:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4590  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [ 990/1251]  eta: 0:02:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4403  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1000/1251]  eta: 0:01:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1010/1251]  eta: 0:01:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0003  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1020/1251]  eta: 0:01:48  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4444  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1030/1251]  eta: 0:01:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4450  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1040/1251]  eta: 0:01:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4417  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1050/1251]  eta: 0:01:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4432  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1060/1251]  eta: 0:01:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4586  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1070/1251]  eta: 0:01:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4696  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1080/1251]  eta: 0:01:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4725  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1090/1251]  eta: 0:01:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4734  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1100/1251]  eta: 0:01:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4586  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1110/1251]  eta: 0:01:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4681  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1120/1251]  eta: 0:01:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4682  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1130/1251]  eta: 0:00:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4440  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1140/1251]  eta: 0:00:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4422  data: 0.0003  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1150/1251]  eta: 0:00:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4522  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1160/1251]  eta: 0:00:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4520  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1170/1251]  eta: 0:00:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4434  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1180/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4432  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1190/1251]  eta: 0:00:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1200/1251]  eta: 0:00:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4355  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1210/1251]  eta: 0:00:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4405  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1220/1251]  eta: 0:00:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4638  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1230/1251]  eta: 0:00:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4597  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1240/1251]  eta: 0:00:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4473  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [56]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0001  max mem: 19734
Epoch: [56] Total time: 0:09:46 (0.4688 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:10:13  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 29.9385  data: 29.4377  max mem: 19734
Test:  [ 10/261]  eta: 0:12:30  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.9908  data: 2.6911  max mem: 19734
Test:  [ 20/261]  eta: 0:06:59  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3290  data: 0.0131  max mem: 19734
Test:  [ 30/261]  eta: 0:04:45  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2690  data: 0.0137  max mem: 19734
Test:  [ 40/261]  eta: 0:04:01  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4114  data: 0.2546  max mem: 19734
Test:  [ 50/261]  eta: 0:03:15  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4449  data: 0.2530  max mem: 19734
Test:  [ 60/261]  eta: 0:02:41  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2143  data: 0.0181  max mem: 19734
Test:  [ 70/261]  eta: 0:02:15  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.1670  data: 0.0172  max mem: 19734
Test:  [ 80/261]  eta: 0:02:04  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3266  data: 0.1703  max mem: 19734
Test:  [ 90/261]  eta: 0:01:52  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4625  data: 0.3066  max mem: 19734
Test:  [100/261]  eta: 0:01:49  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.6711  data: 0.5249  max mem: 19734
Test:  [110/261]  eta: 0:01:38  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.6339  data: 0.4501  max mem: 19734
Test:  [120/261]  eta: 0:01:27  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3196  data: 0.1392  max mem: 19734
Test:  [130/261]  eta: 0:01:17  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2362  data: 0.0814  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.1864  data: 0.0360  max mem: 19734
Test:  [150/261]  eta: 0:00:59  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.1734  data: 0.0571  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1407  data: 0.0614  max mem: 19734
Test:  [170/261]  eta: 0:00:44  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1185  data: 0.0519  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.0877  data: 0.0225  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.0781  data: 0.0134  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0756  data: 0.0134  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0614  data: 0.0021  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0597  data: 0.0015  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0583  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0584  data: 0.0003  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0676  data: 0.0096  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0688  data: 0.0122  max mem: 19734
Test: Total time: 0:01:29 (0.3414 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [57]  [   0/1251]  eta: 5:25:57  lr: 0.000009  loss: 0.0000 (0.0000)  time: 15.6337  data: 12.2274  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [  10/1251]  eta: 0:44:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 2.1505  data: 1.2454  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [  20/1251]  eta: 0:27:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.6260  data: 0.0738  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [  30/1251]  eta: 0:21:29  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4537  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [  40/1251]  eta: 0:18:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4607  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [  50/1251]  eta: 0:16:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4556  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [  60/1251]  eta: 0:15:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4502  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [  70/1251]  eta: 0:14:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4514  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [  80/1251]  eta: 0:13:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4492  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [  90/1251]  eta: 0:12:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4586  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 100/1251]  eta: 0:12:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4807  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 110/1251]  eta: 0:11:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4823  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 120/1251]  eta: 0:11:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4816  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 130/1251]  eta: 0:11:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4691  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 140/1251]  eta: 0:11:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4590  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 150/1251]  eta: 0:10:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4598  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 160/1251]  eta: 0:10:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4505  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 170/1251]  eta: 0:10:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4530  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 180/1251]  eta: 0:10:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4504  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 190/1251]  eta: 0:09:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4460  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 200/1251]  eta: 0:09:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4476  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 210/1251]  eta: 0:09:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4482  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 220/1251]  eta: 0:09:18  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 230/1251]  eta: 0:09:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4489  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 240/1251]  eta: 0:09:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4622  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 250/1251]  eta: 0:08:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4900  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 260/1251]  eta: 0:08:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 270/1251]  eta: 0:08:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4622  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 280/1251]  eta: 0:08:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4542  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 290/1251]  eta: 0:08:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4567  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 300/1251]  eta: 0:08:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4602  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 310/1251]  eta: 0:08:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4477  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 320/1251]  eta: 0:08:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4423  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 330/1251]  eta: 0:07:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4412  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 340/1251]  eta: 0:07:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4402  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 350/1251]  eta: 0:07:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4412  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 360/1251]  eta: 0:07:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4456  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 370/1251]  eta: 0:07:26  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4492  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 380/1251]  eta: 0:07:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4533  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 390/1251]  eta: 0:07:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4672  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 400/1251]  eta: 0:07:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4700  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 410/1251]  eta: 0:07:02  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4608  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 420/1251]  eta: 0:06:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4518  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 430/1251]  eta: 0:06:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4588  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 440/1251]  eta: 0:06:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4595  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 450/1251]  eta: 0:06:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4451  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 460/1251]  eta: 0:06:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4436  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 470/1251]  eta: 0:06:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4449  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 480/1251]  eta: 0:06:21  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4583  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 490/1251]  eta: 0:06:16  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4607  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 500/1251]  eta: 0:06:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4503  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 510/1251]  eta: 0:06:04  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4468  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 520/1251]  eta: 0:05:59  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4456  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 530/1251]  eta: 0:05:54  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4636  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 540/1251]  eta: 0:05:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4808  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 550/1251]  eta: 0:05:43  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4630  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 560/1251]  eta: 0:05:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4570  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 570/1251]  eta: 0:05:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4546  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 580/1251]  eta: 0:05:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4526  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 590/1251]  eta: 0:05:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4514  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 600/1251]  eta: 0:05:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4397  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 610/1251]  eta: 0:05:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4411  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 620/1251]  eta: 0:05:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4425  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 630/1251]  eta: 0:05:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4460  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 640/1251]  eta: 0:04:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4469  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 650/1251]  eta: 0:04:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4465  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 660/1251]  eta: 0:04:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4466  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 670/1251]  eta: 0:04:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4545  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 680/1251]  eta: 0:04:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4613  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 690/1251]  eta: 0:04:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4641  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 700/1251]  eta: 0:04:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4661  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 710/1251]  eta: 0:04:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4615  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 720/1251]  eta: 0:04:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4587  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 730/1251]  eta: 0:04:10  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4516  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 740/1251]  eta: 0:04:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4435  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 750/1251]  eta: 0:04:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4420  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 760/1251]  eta: 0:03:55  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4466  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 770/1251]  eta: 0:03:50  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4498  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 780/1251]  eta: 0:03:45  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4473  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 790/1251]  eta: 0:03:40  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4436  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 800/1251]  eta: 0:03:35  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4412  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 810/1251]  eta: 0:03:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4495  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 820/1251]  eta: 0:03:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4489  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 830/1251]  eta: 0:03:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4615  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 840/1251]  eta: 0:03:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4625  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 850/1251]  eta: 0:03:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4606  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 860/1251]  eta: 0:03:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4601  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 870/1251]  eta: 0:03:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4534  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 880/1251]  eta: 0:02:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4544  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 890/1251]  eta: 0:02:51  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4431  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 900/1251]  eta: 0:02:46  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4433  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 910/1251]  eta: 0:02:41  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4423  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 920/1251]  eta: 0:02:37  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4412  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 930/1251]  eta: 0:02:32  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4430  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 940/1251]  eta: 0:02:27  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4436  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 950/1251]  eta: 0:02:22  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4422  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 960/1251]  eta: 0:02:17  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4569  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 970/1251]  eta: 0:02:13  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4685  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 980/1251]  eta: 0:02:08  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4611  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [ 990/1251]  eta: 0:02:03  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4560  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1000/1251]  eta: 0:01:58  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4602  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1010/1251]  eta: 0:01:53  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4637  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1020/1251]  eta: 0:01:49  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4563  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1030/1251]  eta: 0:01:44  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4543  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1040/1251]  eta: 0:01:39  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4509  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1050/1251]  eta: 0:01:34  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4419  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1060/1251]  eta: 0:01:30  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4407  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1070/1251]  eta: 0:01:25  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1080/1251]  eta: 0:01:20  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4416  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1090/1251]  eta: 0:01:15  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1100/1251]  eta: 0:01:11  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4483  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1110/1251]  eta: 0:01:06  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4739  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1120/1251]  eta: 0:01:01  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4882  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1130/1251]  eta: 0:00:56  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4588  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1140/1251]  eta: 0:00:52  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4541  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1150/1251]  eta: 0:00:47  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4853  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1160/1251]  eta: 0:00:42  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4956  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1170/1251]  eta: 0:00:38  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4697  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1180/1251]  eta: 0:00:33  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4486  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1190/1251]  eta: 0:00:28  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4455  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1200/1251]  eta: 0:00:23  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4412  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1210/1251]  eta: 0:00:19  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4369  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1220/1251]  eta: 0:00:14  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4361  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1230/1251]  eta: 0:00:09  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4354  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1240/1251]  eta: 0:00:05  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4345  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [57]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 0.0000 (0.0000)  time: 0.4399  data: 0.0002  max mem: 19734
Epoch: [57] Total time: 0:09:47 (0.4695 s / it)
Averaged stats: lr: 0.000009  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:59:03  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 27.3707  data: 27.2229  max mem: 19734
Test:  [ 10/261]  eta: 0:11:06  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.6541  data: 2.4988  max mem: 19734
Test:  [ 20/261]  eta: 0:05:51  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.1636  data: 0.0220  max mem: 19734
Test:  [ 30/261]  eta: 0:04:02  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1653  data: 0.0214  max mem: 19734
Test:  [ 40/261]  eta: 0:03:24  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3680  data: 0.2285  max mem: 19734
Test:  [ 50/261]  eta: 0:02:57  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5248  data: 0.3427  max mem: 19734
Test:  [ 60/261]  eta: 0:02:26  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.3191  data: 0.1349  max mem: 19734
Test:  [ 70/261]  eta: 0:02:23  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5265  data: 0.3864  max mem: 19734
Test:  [ 80/261]  eta: 0:02:03  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.5526  data: 0.3855  max mem: 19734
Test:  [ 90/261]  eta: 0:01:47  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.1813  data: 0.0146  max mem: 19734
Test:  [100/261]  eta: 0:01:33  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.1731  data: 0.0147  max mem: 19734
Test:  [110/261]  eta: 0:01:28  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.3830  data: 0.2436  max mem: 19734
Test:  [120/261]  eta: 0:01:18  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4346  data: 0.2473  max mem: 19734
Test:  [130/261]  eta: 0:01:10  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2742  data: 0.0205  max mem: 19734
Test:  [140/261]  eta: 0:01:03  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3309  data: 0.1374  max mem: 19734
Test:  [150/261]  eta: 0:00:56  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.2990  data: 0.1773  max mem: 19734
Test:  [160/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2484  data: 0.0968  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2944  data: 0.1342  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2184  data: 0.1243  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1001  data: 0.0396  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1207  data: 0.0618  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1187  data: 0.0599  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0581  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0581  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0565  data: 0.0002  max mem: 19734
Test: Total time: 0:01:29 (0.3435 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [58]  [   0/1251]  eta: 6:21:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 18.3065  data: 17.7709  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [  10/1251]  eta: 0:45:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.1821  data: 1.6160  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [  20/1251]  eta: 0:28:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.5307  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [  30/1251]  eta: 0:21:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4687  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [  40/1251]  eta: 0:18:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4502  data: 0.0006  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [  50/1251]  eta: 0:16:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4645  data: 0.0012  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [  60/1251]  eta: 0:15:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4663  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [  70/1251]  eta: 0:14:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4516  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [  80/1251]  eta: 0:13:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4460  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [  90/1251]  eta: 0:12:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 100/1251]  eta: 0:12:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4469  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 110/1251]  eta: 0:11:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4515  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 120/1251]  eta: 0:11:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4519  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 130/1251]  eta: 0:11:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4587  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 140/1251]  eta: 0:10:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4588  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 150/1251]  eta: 0:10:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4729  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 160/1251]  eta: 0:10:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4812  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 170/1251]  eta: 0:10:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4690  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 180/1251]  eta: 0:10:04  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4609  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 190/1251]  eta: 0:09:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4584  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 200/1251]  eta: 0:09:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4554  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 210/1251]  eta: 0:09:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4433  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 220/1251]  eta: 0:09:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4450  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 230/1251]  eta: 0:09:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4450  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 240/1251]  eta: 0:09:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4489  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 250/1251]  eta: 0:08:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4508  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 260/1251]  eta: 0:08:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4449  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 270/1251]  eta: 0:08:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4444  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 280/1251]  eta: 0:08:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4526  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 290/1251]  eta: 0:08:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4621  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 300/1251]  eta: 0:08:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4954  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 310/1251]  eta: 0:08:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.5109  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 320/1251]  eta: 0:08:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4694  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 330/1251]  eta: 0:07:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 340/1251]  eta: 0:07:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4584  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 350/1251]  eta: 0:07:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4572  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 360/1251]  eta: 0:07:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 370/1251]  eta: 0:07:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4501  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 380/1251]  eta: 0:07:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4495  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 390/1251]  eta: 0:07:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4451  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 400/1251]  eta: 0:07:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 410/1251]  eta: 0:07:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4457  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 420/1251]  eta: 0:06:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4552  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 430/1251]  eta: 0:06:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4556  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 440/1251]  eta: 0:06:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4769  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 450/1251]  eta: 0:06:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4757  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 460/1251]  eta: 0:06:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4612  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 470/1251]  eta: 0:06:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4596  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 480/1251]  eta: 0:06:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4591  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 490/1251]  eta: 0:06:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4602  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 500/1251]  eta: 0:06:12  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4468  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 510/1251]  eta: 0:06:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4465  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 520/1251]  eta: 0:06:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4429  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 530/1251]  eta: 0:05:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4434  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 540/1251]  eta: 0:05:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4424  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 550/1251]  eta: 0:05:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4404  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 560/1251]  eta: 0:05:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4415  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 570/1251]  eta: 0:05:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4548  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 580/1251]  eta: 0:05:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4733  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 590/1251]  eta: 0:05:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4709  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 600/1251]  eta: 0:05:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4587  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 610/1251]  eta: 0:05:12  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4596  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 620/1251]  eta: 0:05:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4529  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 630/1251]  eta: 0:05:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4590  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 640/1251]  eta: 0:04:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4620  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 650/1251]  eta: 0:04:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4466  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 660/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 670/1251]  eta: 0:04:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 680/1251]  eta: 0:04:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4420  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 690/1251]  eta: 0:04:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4394  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 700/1251]  eta: 0:04:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 710/1251]  eta: 0:04:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4546  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 720/1251]  eta: 0:04:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4513  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 730/1251]  eta: 0:04:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 740/1251]  eta: 0:04:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4679  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 750/1251]  eta: 0:04:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4510  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 760/1251]  eta: 0:03:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4593  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 770/1251]  eta: 0:03:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 780/1251]  eta: 0:03:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4594  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 790/1251]  eta: 0:03:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4410  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 800/1251]  eta: 0:03:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4423  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 810/1251]  eta: 0:03:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4420  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 820/1251]  eta: 0:03:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4426  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 830/1251]  eta: 0:03:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4462  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 840/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 850/1251]  eta: 0:03:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4413  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 860/1251]  eta: 0:03:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4496  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 870/1251]  eta: 0:03:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4616  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 880/1251]  eta: 0:02:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4609  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 890/1251]  eta: 0:02:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4547  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 900/1251]  eta: 0:02:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4496  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 910/1251]  eta: 0:02:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4671  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 920/1251]  eta: 0:02:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4724  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 930/1251]  eta: 0:02:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4486  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 940/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4400  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 950/1251]  eta: 0:02:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4404  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 960/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4606  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 970/1251]  eta: 0:02:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4667  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 980/1251]  eta: 0:02:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4469  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [ 990/1251]  eta: 0:02:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1000/1251]  eta: 0:01:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4557  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1010/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4525  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1020/1251]  eta: 0:01:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4575  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1030/1251]  eta: 0:01:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4591  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1040/1251]  eta: 0:01:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4571  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1050/1251]  eta: 0:01:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4546  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1060/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4522  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1070/1251]  eta: 0:01:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4511  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1080/1251]  eta: 0:01:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4392  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1090/1251]  eta: 0:01:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4408  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1100/1251]  eta: 0:01:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4460  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1110/1251]  eta: 0:01:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1120/1251]  eta: 0:01:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4421  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1130/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4413  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1140/1251]  eta: 0:00:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1150/1251]  eta: 0:00:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4571  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1160/1251]  eta: 0:00:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4649  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1170/1251]  eta: 0:00:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4641  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1180/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4632  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1190/1251]  eta: 0:00:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4474  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1200/1251]  eta: 0:00:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4461  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1210/1251]  eta: 0:00:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4513  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1220/1251]  eta: 0:00:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4433  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1230/1251]  eta: 0:00:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4363  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1240/1251]  eta: 0:00:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4362  data: 0.0001  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [58]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4350  data: 0.0001  max mem: 19734
Epoch: [58] Total time: 0:09:47 (0.4696 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:56:59  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 26.8959  data: 26.6813  max mem: 19734
Test:  [ 10/261]  eta: 0:10:47  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.5810  data: 2.4322  max mem: 19734
Test:  [ 20/261]  eta: 0:06:07  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2577  data: 0.1118  max mem: 19734
Test:  [ 30/261]  eta: 0:04:09  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2549  data: 0.1131  max mem: 19734
Test:  [ 40/261]  eta: 0:03:30  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3509  data: 0.2343  max mem: 19734
Test:  [ 50/261]  eta: 0:02:48  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.3621  data: 0.2343  max mem: 19734
Test:  [ 60/261]  eta: 0:02:16  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1235  data: 0.0063  max mem: 19734
Test:  [ 70/261]  eta: 0:02:13  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.4436  data: 0.3445  max mem: 19734
Test:  [ 80/261]  eta: 0:01:56  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.5275  data: 0.3848  max mem: 19734
Test:  [ 90/261]  eta: 0:01:42  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2404  data: 0.1155  max mem: 19734
Test:  [100/261]  eta: 0:01:40  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.5390  data: 0.4217  max mem: 19734
Test:  [110/261]  eta: 0:01:27  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4901  data: 0.3555  max mem: 19734
Test:  [120/261]  eta: 0:01:19  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2747  data: 0.1309  max mem: 19734
Test:  [130/261]  eta: 0:01:10  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3213  data: 0.1521  max mem: 19734
Test:  [140/261]  eta: 0:01:05  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3753  data: 0.1930  max mem: 19734
Test:  [150/261]  eta: 0:00:58  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4705  data: 0.2670  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2880  data: 0.1227  max mem: 19734
Test:  [170/261]  eta: 0:00:45  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3038  data: 0.1852  max mem: 19734
Test:  [180/261]  eta: 0:00:38  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2743  data: 0.1690  max mem: 19734
Test:  [190/261]  eta: 0:00:32  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.0926  data: 0.0084  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1489  data: 0.0697  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1351  data: 0.0648  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0747  data: 0.0164  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0744  data: 0.0163  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0583  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0589  data: 0.0007  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0573  data: 0.0008  max mem: 19734
Test: Total time: 0:01:34 (0.3604 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [59]  [   0/1251]  eta: 5:59:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 17.2566  data: 16.2708  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [  10/1251]  eta: 0:45:04  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.1794  data: 1.4835  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [  20/1251]  eta: 0:27:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.5607  data: 0.0026  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [  30/1251]  eta: 0:21:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4497  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [  40/1251]  eta: 0:18:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4648  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [  50/1251]  eta: 0:16:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4812  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [  60/1251]  eta: 0:15:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4715  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [  70/1251]  eta: 0:14:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4709  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [  80/1251]  eta: 0:13:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4811  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [  90/1251]  eta: 0:13:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4630  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 100/1251]  eta: 0:12:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 110/1251]  eta: 0:12:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4500  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 120/1251]  eta: 0:11:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4741  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 130/1251]  eta: 0:11:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4739  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 140/1251]  eta: 0:11:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4504  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 150/1251]  eta: 0:10:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4449  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 160/1251]  eta: 0:10:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4423  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 170/1251]  eta: 0:10:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4443  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 180/1251]  eta: 0:10:04  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4531  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 190/1251]  eta: 0:09:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4728  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 200/1251]  eta: 0:09:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4652  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 210/1251]  eta: 0:09:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4646  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 220/1251]  eta: 0:09:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4641  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 230/1251]  eta: 0:09:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4607  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 240/1251]  eta: 0:09:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4591  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 250/1251]  eta: 0:08:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4479  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 260/1251]  eta: 0:08:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4490  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 270/1251]  eta: 0:08:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4465  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 280/1251]  eta: 0:08:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4468  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 290/1251]  eta: 0:08:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4480  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 300/1251]  eta: 0:08:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4496  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 310/1251]  eta: 0:08:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 320/1251]  eta: 0:08:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4470  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 330/1251]  eta: 0:07:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4563  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 340/1251]  eta: 0:07:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4654  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 350/1251]  eta: 0:07:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4537  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 360/1251]  eta: 0:07:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4568  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 370/1251]  eta: 0:07:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4707  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 380/1251]  eta: 0:07:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4592  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 390/1251]  eta: 0:07:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4479  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 400/1251]  eta: 0:07:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4435  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 410/1251]  eta: 0:07:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4425  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 420/1251]  eta: 0:06:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4418  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 430/1251]  eta: 0:06:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4409  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 440/1251]  eta: 0:06:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4398  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 450/1251]  eta: 0:06:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 460/1251]  eta: 0:06:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4483  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 470/1251]  eta: 0:06:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4509  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 480/1251]  eta: 0:06:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4684  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 490/1251]  eta: 0:06:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4626  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 500/1251]  eta: 0:06:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4554  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 510/1251]  eta: 0:06:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4643  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 520/1251]  eta: 0:06:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4729  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 530/1251]  eta: 0:05:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4646  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 540/1251]  eta: 0:05:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4455  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 550/1251]  eta: 0:05:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4450  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 560/1251]  eta: 0:05:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4446  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 570/1251]  eta: 0:05:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 580/1251]  eta: 0:05:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4470  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 590/1251]  eta: 0:05:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4477  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 600/1251]  eta: 0:05:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4437  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 610/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4517  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 620/1251]  eta: 0:05:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4520  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 630/1251]  eta: 0:05:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4527  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 640/1251]  eta: 0:04:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4522  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 650/1251]  eta: 0:04:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4522  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 660/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4693  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 670/1251]  eta: 0:04:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4568  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 680/1251]  eta: 0:04:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4379  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 690/1251]  eta: 0:04:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4383  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 700/1251]  eta: 0:04:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4402  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 710/1251]  eta: 0:04:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4458  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 720/1251]  eta: 0:04:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4498  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 730/1251]  eta: 0:04:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 740/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4426  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 750/1251]  eta: 0:03:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4410  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 760/1251]  eta: 0:03:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4478  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 770/1251]  eta: 0:03:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4807  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 780/1251]  eta: 0:03:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4809  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 790/1251]  eta: 0:03:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4775  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 800/1251]  eta: 0:03:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4704  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 810/1251]  eta: 0:03:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4608  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 820/1251]  eta: 0:03:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4618  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 830/1251]  eta: 0:03:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4435  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 840/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4451  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 850/1251]  eta: 0:03:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4484  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 860/1251]  eta: 0:03:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4458  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 870/1251]  eta: 0:03:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 880/1251]  eta: 0:02:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4407  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 890/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4403  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 900/1251]  eta: 0:02:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4499  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 910/1251]  eta: 0:02:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4626  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 920/1251]  eta: 0:02:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4757  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 930/1251]  eta: 0:02:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4645  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 940/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4502  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 950/1251]  eta: 0:02:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4657  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 960/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4590  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 970/1251]  eta: 0:02:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4431  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 980/1251]  eta: 0:02:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4471  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [ 990/1251]  eta: 0:02:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4458  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1000/1251]  eta: 0:01:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4405  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1010/1251]  eta: 0:01:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4398  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1020/1251]  eta: 0:01:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4399  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1030/1251]  eta: 0:01:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4398  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1040/1251]  eta: 0:01:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4403  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1050/1251]  eta: 0:01:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4512  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1060/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1070/1251]  eta: 0:01:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4738  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1080/1251]  eta: 0:01:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4501  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [1090/1251]  eta: 0:01:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4605  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1100/1251]  eta: 0:01:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4594  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1110/1251]  eta: 0:01:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4510  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1120/1251]  eta: 0:01:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1130/1251]  eta: 0:00:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4417  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1140/1251]  eta: 0:00:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4416  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1150/1251]  eta: 0:00:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4420  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1160/1251]  eta: 0:00:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4420  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1170/1251]  eta: 0:00:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4416  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1180/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4440  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [59]  [1190/1251]  eta: 0:00:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4533  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1200/1251]  eta: 0:00:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4473  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1210/1251]  eta: 0:00:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4539  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1220/1251]  eta: 0:00:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4543  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1230/1251]  eta: 0:00:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4438  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1240/1251]  eta: 0:00:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4543  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [59]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4536  data: 0.0002  max mem: 19734
Epoch: [59] Total time: 0:09:46 (0.4692 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:53:31  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 26.0972  data: 25.8319  max mem: 19734
Test:  [ 10/261]  eta: 0:11:50  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.8292  data: 2.6944  max mem: 19734
Test:  [ 20/261]  eta: 0:06:17  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.3415  data: 0.1967  max mem: 19734
Test:  [ 30/261]  eta: 0:04:16  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1657  data: 0.0125  max mem: 19734
Test:  [ 40/261]  eta: 0:03:55  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5351  data: 0.3771  max mem: 19734
Test:  [ 50/261]  eta: 0:03:05  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5140  data: 0.3759  max mem: 19734
Test:  [ 60/261]  eta: 0:02:34  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1587  data: 0.0116  max mem: 19734
Test:  [ 70/261]  eta: 0:02:16  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.2931  data: 0.1196  max mem: 19734
Test:  [ 80/261]  eta: 0:02:01  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3820  data: 0.2344  max mem: 19734
Test:  [ 90/261]  eta: 0:01:45  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2830  data: 0.1423  max mem: 19734
Test:  [100/261]  eta: 0:01:34  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.2489  data: 0.1300  max mem: 19734
Test:  [110/261]  eta: 0:01:34  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.6516  data: 0.5643  max mem: 19734
Test:  [120/261]  eta: 0:01:22  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.5710  data: 0.4599  max mem: 19734
Test:  [130/261]  eta: 0:01:11  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.1327  data: 0.0146  max mem: 19734
Test:  [140/261]  eta: 0:01:08  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4760  data: 0.3867  max mem: 19734
Test:  [150/261]  eta: 0:00:59  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4791  data: 0.3833  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1023  data: 0.0128  max mem: 19734
Test:  [170/261]  eta: 0:00:44  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1149  data: 0.0286  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1124  data: 0.0263  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1025  data: 0.0328  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1280  data: 0.0663  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0970  data: 0.0376  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0584  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0583  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0583  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0582  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0565  data: 0.0002  max mem: 19734
Test: Total time: 0:01:31 (0.3491 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [60]  [   0/1251]  eta: 5:35:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 16.0957  data: 13.2464  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [  10/1251]  eta: 0:43:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.1053  data: 1.2057  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [  20/1251]  eta: 0:27:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.5867  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [  30/1251]  eta: 0:21:17  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4632  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [  40/1251]  eta: 0:18:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4566  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [  50/1251]  eta: 0:16:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4523  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [  60/1251]  eta: 0:14:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4513  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [  70/1251]  eta: 0:14:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4544  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [  80/1251]  eta: 0:13:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4653  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [  90/1251]  eta: 0:12:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4902  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 100/1251]  eta: 0:12:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4785  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 110/1251]  eta: 0:11:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4707  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 120/1251]  eta: 0:11:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4817  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 130/1251]  eta: 0:11:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4624  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 140/1251]  eta: 0:10:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4509  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 150/1251]  eta: 0:10:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4452  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 160/1251]  eta: 0:10:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4425  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 170/1251]  eta: 0:10:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4435  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 180/1251]  eta: 0:09:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4422  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 190/1251]  eta: 0:09:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4429  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 200/1251]  eta: 0:09:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4456  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 210/1251]  eta: 0:09:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4478  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 220/1251]  eta: 0:09:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4602  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 230/1251]  eta: 0:09:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4749  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 240/1251]  eta: 0:09:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4744  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 250/1251]  eta: 0:08:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4677  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 260/1251]  eta: 0:08:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4663  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 270/1251]  eta: 0:08:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4727  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 280/1251]  eta: 0:08:29  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4608  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 290/1251]  eta: 0:08:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4428  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 300/1251]  eta: 0:08:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4430  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 310/1251]  eta: 0:08:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4470  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 320/1251]  eta: 0:07:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 330/1251]  eta: 0:07:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4455  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 340/1251]  eta: 0:07:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4487  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 350/1251]  eta: 0:07:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 360/1251]  eta: 0:07:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4443  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 370/1251]  eta: 0:07:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4541  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 380/1251]  eta: 0:07:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4851  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 390/1251]  eta: 0:07:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4753  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 400/1251]  eta: 0:07:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4620  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 410/1251]  eta: 0:07:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 420/1251]  eta: 0:06:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4529  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 430/1251]  eta: 0:06:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4426  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 440/1251]  eta: 0:06:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4403  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 450/1251]  eta: 0:06:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4394  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 460/1251]  eta: 0:06:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 470/1251]  eta: 0:06:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4480  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 480/1251]  eta: 0:06:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4443  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 490/1251]  eta: 0:06:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4408  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 500/1251]  eta: 0:06:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4419  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 510/1251]  eta: 0:06:04  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4565  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 520/1251]  eta: 0:05:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4719  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 530/1251]  eta: 0:05:53  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4724  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 540/1251]  eta: 0:05:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4663  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 550/1251]  eta: 0:05:43  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4576  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 560/1251]  eta: 0:05:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4596  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 570/1251]  eta: 0:05:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4546  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 580/1251]  eta: 0:05:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4405  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 590/1251]  eta: 0:05:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4419  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 600/1251]  eta: 0:05:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 610/1251]  eta: 0:05:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4579  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 620/1251]  eta: 0:05:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4557  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 630/1251]  eta: 0:05:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4397  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 640/1251]  eta: 0:04:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4396  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 650/1251]  eta: 0:04:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4411  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 660/1251]  eta: 0:04:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4622  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 670/1251]  eta: 0:04:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4938  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 680/1251]  eta: 0:04:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4745  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 690/1251]  eta: 0:04:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4538  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 700/1251]  eta: 0:04:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4738  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 710/1251]  eta: 0:04:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4626  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 720/1251]  eta: 0:04:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 730/1251]  eta: 0:04:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4474  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 740/1251]  eta: 0:04:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4437  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 750/1251]  eta: 0:04:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4402  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 760/1251]  eta: 0:03:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 770/1251]  eta: 0:03:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4401  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 780/1251]  eta: 0:03:45  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4404  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 790/1251]  eta: 0:03:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4435  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 800/1251]  eta: 0:03:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4481  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 810/1251]  eta: 0:03:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4765  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 820/1251]  eta: 0:03:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4794  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 830/1251]  eta: 0:03:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4595  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 840/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4612  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 850/1251]  eta: 0:03:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4733  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 860/1251]  eta: 0:03:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4700  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 870/1251]  eta: 0:03:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4490  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 880/1251]  eta: 0:02:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4450  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 890/1251]  eta: 0:02:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4430  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 900/1251]  eta: 0:02:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4416  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 910/1251]  eta: 0:02:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4425  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 920/1251]  eta: 0:02:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4440  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 930/1251]  eta: 0:02:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4486  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 940/1251]  eta: 0:02:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4458  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 950/1251]  eta: 0:02:22  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4566  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 960/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4897  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 970/1251]  eta: 0:02:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4753  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 980/1251]  eta: 0:02:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4516  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [ 990/1251]  eta: 0:02:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4845  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1000/1251]  eta: 0:01:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4762  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1010/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4415  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1020/1251]  eta: 0:01:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4403  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1030/1251]  eta: 0:01:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4413  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1040/1251]  eta: 0:01:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4417  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1050/1251]  eta: 0:01:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4417  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1060/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1070/1251]  eta: 0:01:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4438  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1080/1251]  eta: 0:01:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4422  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1090/1251]  eta: 0:01:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4535  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1100/1251]  eta: 0:01:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4695  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1110/1251]  eta: 0:01:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4697  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1120/1251]  eta: 0:01:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4658  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1130/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4675  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1140/1251]  eta: 0:00:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4774  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1150/1251]  eta: 0:00:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4660  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1160/1251]  eta: 0:00:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1170/1251]  eta: 0:00:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4449  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1180/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4426  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1190/1251]  eta: 0:00:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4446  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1200/1251]  eta: 0:00:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4410  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1210/1251]  eta: 0:00:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4336  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1220/1251]  eta: 0:00:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4324  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1230/1251]  eta: 0:00:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4329  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1240/1251]  eta: 0:00:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4400  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [60]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4506  data: 0.0002  max mem: 19734
Epoch: [60] Total time: 0:09:47 (0.4698 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:01:21  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 27.8980  data: 27.7117  max mem: 19734
Test:  [ 10/261]  eta: 0:11:14  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.6873  data: 2.5281  max mem: 19734
Test:  [ 20/261]  eta: 0:06:17  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2496  data: 0.0109  max mem: 19734
Test:  [ 30/261]  eta: 0:04:22  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2804  data: 0.0143  max mem: 19734
Test:  [ 40/261]  eta: 0:04:09  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6680  data: 0.4652  max mem: 19734
Test:  [ 50/261]  eta: 0:03:22  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6845  data: 0.4641  max mem: 19734
Test:  [ 60/261]  eta: 0:02:47  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2366  data: 0.0177  max mem: 19734
Test:  [ 70/261]  eta: 0:02:27  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3039  data: 0.1277  max mem: 19734
Test:  [ 80/261]  eta: 0:02:09  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3415  data: 0.1232  max mem: 19734
Test:  [ 90/261]  eta: 0:01:52  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2450  data: 0.0160  max mem: 19734
Test:  [100/261]  eta: 0:01:38  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.1982  data: 0.0134  max mem: 19734
Test:  [110/261]  eta: 0:01:28  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.2568  data: 0.0805  max mem: 19734
Test:  [120/261]  eta: 0:01:18  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2898  data: 0.1402  max mem: 19734
Test:  [130/261]  eta: 0:01:11  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.3508  data: 0.1954  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3900  data: 0.2145  max mem: 19734
Test:  [150/261]  eta: 0:00:59  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4359  data: 0.2978  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.3076  data: 0.2187  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.0800  data: 0.0060  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1059  data: 0.0262  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1020  data: 0.0239  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0649  data: 0.0009  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0591  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0610  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0609  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0583  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0583  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0572  data: 0.0002  max mem: 19734
Test: Total time: 0:01:28 (0.3401 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [61]  [   0/1251]  eta: 5:27:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 15.7132  data: 11.8978  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [  10/1251]  eta: 0:45:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 2.2182  data: 1.1036  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [  20/1251]  eta: 0:28:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.6868  data: 0.0127  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [  30/1251]  eta: 0:22:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4796  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [  40/1251]  eta: 0:18:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4534  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [  50/1251]  eta: 0:16:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4509  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [  60/1251]  eta: 0:15:27  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4488  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [  70/1251]  eta: 0:14:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4499  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [  80/1251]  eta: 0:13:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4538  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [  90/1251]  eta: 0:12:58  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4524  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 100/1251]  eta: 0:12:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4484  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 110/1251]  eta: 0:11:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4492  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 120/1251]  eta: 0:11:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4571  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 130/1251]  eta: 0:11:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4774  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 140/1251]  eta: 0:11:04  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4813  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 150/1251]  eta: 0:10:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4685  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 160/1251]  eta: 0:10:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4819  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 170/1251]  eta: 0:10:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4714  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 180/1251]  eta: 0:10:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 190/1251]  eta: 0:09:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4448  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 200/1251]  eta: 0:09:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 210/1251]  eta: 0:09:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4493  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 220/1251]  eta: 0:09:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4482  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 230/1251]  eta: 0:09:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 240/1251]  eta: 0:09:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 250/1251]  eta: 0:08:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4450  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 260/1251]  eta: 0:08:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4445  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 270/1251]  eta: 0:08:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4664  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 280/1251]  eta: 0:08:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4818  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 290/1251]  eta: 0:08:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4605  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 300/1251]  eta: 0:08:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4729  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 310/1251]  eta: 0:08:10  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4790  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 320/1251]  eta: 0:08:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4482  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 330/1251]  eta: 0:07:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4424  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 340/1251]  eta: 0:07:48  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4468  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 350/1251]  eta: 0:07:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4446  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 360/1251]  eta: 0:07:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4411  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 370/1251]  eta: 0:07:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4407  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 380/1251]  eta: 0:07:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4419  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 390/1251]  eta: 0:07:15  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4437  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 400/1251]  eta: 0:07:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4443  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 410/1251]  eta: 0:07:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4547  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 420/1251]  eta: 0:06:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4694  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 430/1251]  eta: 0:06:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4906  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 440/1251]  eta: 0:06:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4850  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 450/1251]  eta: 0:06:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4870  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 460/1251]  eta: 0:06:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4794  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 470/1251]  eta: 0:06:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4500  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 480/1251]  eta: 0:06:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4473  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 490/1251]  eta: 0:06:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 500/1251]  eta: 0:06:12  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4439  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 510/1251]  eta: 0:06:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4414  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 520/1251]  eta: 0:06:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4396  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 530/1251]  eta: 0:05:55  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4438  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 540/1251]  eta: 0:05:50  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4502  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 550/1251]  eta: 0:05:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4477  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 560/1251]  eta: 0:05:39  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4596  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 570/1251]  eta: 0:05:34  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4736  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 580/1251]  eta: 0:05:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4557  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 590/1251]  eta: 0:05:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4516  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 600/1251]  eta: 0:05:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4699  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 610/1251]  eta: 0:05:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4616  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 620/1251]  eta: 0:05:07  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4439  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 630/1251]  eta: 0:05:02  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4407  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 640/1251]  eta: 0:04:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4407  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 650/1251]  eta: 0:04:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4425  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 660/1251]  eta: 0:04:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4467  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 670/1251]  eta: 0:04:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4540  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 680/1251]  eta: 0:04:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4529  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 690/1251]  eta: 0:04:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4480  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 700/1251]  eta: 0:04:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4584  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 710/1251]  eta: 0:04:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4740  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 720/1251]  eta: 0:04:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4618  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 730/1251]  eta: 0:04:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4469  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 740/1251]  eta: 0:04:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4745  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 750/1251]  eta: 0:04:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4832  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 760/1251]  eta: 0:03:56  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4532  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 770/1251]  eta: 0:03:51  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4391  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 780/1251]  eta: 0:03:46  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4404  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 790/1251]  eta: 0:03:41  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4415  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 800/1251]  eta: 0:03:36  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 810/1251]  eta: 0:03:31  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4474  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 820/1251]  eta: 0:03:26  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4434  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 830/1251]  eta: 0:03:21  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4399  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 840/1251]  eta: 0:03:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4394  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 850/1251]  eta: 0:03:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4595  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 860/1251]  eta: 0:03:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4714  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 870/1251]  eta: 0:03:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4544  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 880/1251]  eta: 0:02:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4621  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 890/1251]  eta: 0:02:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4709  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 900/1251]  eta: 0:02:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4634  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 910/1251]  eta: 0:02:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4543  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 920/1251]  eta: 0:02:37  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4424  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 930/1251]  eta: 0:02:32  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4470  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 940/1251]  eta: 0:02:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4497  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 950/1251]  eta: 0:02:23  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4454  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 960/1251]  eta: 0:02:18  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 970/1251]  eta: 0:02:13  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4463  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 980/1251]  eta: 0:02:08  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4420  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [ 990/1251]  eta: 0:02:03  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4415  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1000/1251]  eta: 0:01:59  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4703  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1010/1251]  eta: 0:01:54  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4743  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1020/1251]  eta: 0:01:49  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4499  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1030/1251]  eta: 0:01:44  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4721  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1040/1251]  eta: 0:01:40  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4749  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1050/1251]  eta: 0:01:35  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4475  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1060/1251]  eta: 0:01:30  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4441  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1070/1251]  eta: 0:01:25  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4460  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1080/1251]  eta: 0:01:20  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4427  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1090/1251]  eta: 0:01:16  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4445  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1100/1251]  eta: 0:01:11  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4453  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1110/1251]  eta: 0:01:06  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4413  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1120/1251]  eta: 0:01:01  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4403  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1130/1251]  eta: 0:00:57  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4427  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1140/1251]  eta: 0:00:52  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4528  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1150/1251]  eta: 0:00:47  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4764  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1160/1251]  eta: 0:00:42  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4682  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1170/1251]  eta: 0:00:38  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4615  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1180/1251]  eta: 0:00:33  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4743  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1190/1251]  eta: 0:00:28  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4620  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1200/1251]  eta: 0:00:24  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4452  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1210/1251]  eta: 0:00:19  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4344  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1220/1251]  eta: 0:00:14  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4348  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1230/1251]  eta: 0:00:09  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4349  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1240/1251]  eta: 0:00:05  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4325  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [61]  [1250/1251]  eta: 0:00:00  lr: 0.000008  loss: 0.0000 (0.0000)  time: 0.4327  data: 0.0001  max mem: 19734
Epoch: [61] Total time: 0:09:48 (0.4703 s / it)
Averaged stats: lr: 0.000008  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:18:49  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 31.9155  data: 31.7809  max mem: 19734
Test:  [ 10/261]  eta: 0:13:04  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.1267  data: 2.9029  max mem: 19734
Test:  [ 20/261]  eta: 0:06:56  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2208  data: 0.0143  max mem: 19734
Test:  [ 30/261]  eta: 0:04:48  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2143  data: 0.0174  max mem: 19734
Test:  [ 40/261]  eta: 0:03:56  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.3738  data: 0.1859  max mem: 19734
Test:  [ 50/261]  eta: 0:03:09  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.3521  data: 0.1888  max mem: 19734
Test:  [ 60/261]  eta: 0:02:35  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1710  data: 0.0220  max mem: 19734
Test:  [ 70/261]  eta: 0:02:25  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.4228  data: 0.2820  max mem: 19734
Test:  [ 80/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4393  data: 0.2802  max mem: 19734
Test:  [ 90/261]  eta: 0:01:47  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.1635  data: 0.0129  max mem: 19734
Test:  [100/261]  eta: 0:01:44  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4800  data: 0.3480  max mem: 19734
Test:  [110/261]  eta: 0:01:31  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4813  data: 0.3482  max mem: 19734
Test:  [120/261]  eta: 0:01:20  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.1564  data: 0.0175  max mem: 19734
Test:  [130/261]  eta: 0:01:12  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2613  data: 0.1184  max mem: 19734
Test:  [140/261]  eta: 0:01:04  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3430  data: 0.2244  max mem: 19734
Test:  [150/261]  eta: 0:00:56  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.2214  data: 0.1259  max mem: 19734
Test:  [160/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1824  data: 0.0864  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1808  data: 0.0995  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.0870  data: 0.0238  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.0617  data: 0.0021  max mem: 19734
Test:  [200/261]  eta: 0:00:24  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0652  data: 0.0046  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0650  data: 0.0044  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0584  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0583  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0583  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0583  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0566  data: 0.0002  max mem: 19734
Test: Total time: 0:01:26 (0.3304 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.87%
Loss is nan, stopping training this iteration.
Epoch: [62]  [   0/1251]  eta: 4:56:17  lr: 0.000007  loss: 0.0000 (0.0000)  time: 14.2103  data: 13.0712  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [  10/1251]  eta: 0:42:37  lr: 0.000007  loss: 0.0000 (0.0000)  time: 2.0608  data: 1.4323  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [  20/1251]  eta: 0:26:35  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.6502  data: 0.1344  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [  30/1251]  eta: 0:21:14  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4850  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [  40/1251]  eta: 0:18:07  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4806  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [  50/1251]  eta: 0:16:16  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4546  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [  60/1251]  eta: 0:15:03  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4728  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [  70/1251]  eta: 0:14:08  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4779  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [  80/1251]  eta: 0:13:23  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4654  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [  90/1251]  eta: 0:12:47  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4566  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 100/1251]  eta: 0:12:16  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4510  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 110/1251]  eta: 0:11:49  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4452  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 120/1251]  eta: 0:11:27  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4487  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 130/1251]  eta: 0:11:07  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4499  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 140/1251]  eta: 0:10:50  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4482  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 150/1251]  eta: 0:10:35  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4543  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 160/1251]  eta: 0:10:21  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4554  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 170/1251]  eta: 0:10:11  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4778  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 180/1251]  eta: 0:09:59  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4853  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 190/1251]  eta: 0:09:47  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4558  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 200/1251]  eta: 0:09:39  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4696  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 210/1251]  eta: 0:09:29  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4830  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 220/1251]  eta: 0:09:19  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4606  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 230/1251]  eta: 0:09:10  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4487  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 240/1251]  eta: 0:09:01  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4562  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 250/1251]  eta: 0:08:52  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4561  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 260/1251]  eta: 0:08:44  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4475  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 270/1251]  eta: 0:08:35  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4470  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 280/1251]  eta: 0:08:28  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4508  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 290/1251]  eta: 0:08:20  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4486  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 300/1251]  eta: 0:08:12  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4415  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 310/1251]  eta: 0:08:05  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4501  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 320/1251]  eta: 0:07:59  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4701  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 330/1251]  eta: 0:07:52  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4625  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 340/1251]  eta: 0:07:46  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4663  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 350/1251]  eta: 0:07:39  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4685  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 360/1251]  eta: 0:07:33  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4539  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 370/1251]  eta: 0:07:26  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4513  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 380/1251]  eta: 0:07:20  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4421  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 390/1251]  eta: 0:07:13  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4430  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 400/1251]  eta: 0:07:07  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4442  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 410/1251]  eta: 0:07:01  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4462  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 420/1251]  eta: 0:06:55  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4462  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 430/1251]  eta: 0:06:49  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4426  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 440/1251]  eta: 0:06:43  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4416  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 450/1251]  eta: 0:06:37  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4407  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 460/1251]  eta: 0:06:32  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4644  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 470/1251]  eta: 0:06:26  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4732  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 480/1251]  eta: 0:06:20  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4550  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 490/1251]  eta: 0:06:15  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4700  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 500/1251]  eta: 0:06:10  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4903  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 510/1251]  eta: 0:06:05  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4664  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 520/1251]  eta: 0:05:59  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4429  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 530/1251]  eta: 0:05:53  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4459  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 540/1251]  eta: 0:05:48  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4482  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 550/1251]  eta: 0:05:43  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4524  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 560/1251]  eta: 0:05:37  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4501  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 570/1251]  eta: 0:05:32  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4458  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 580/1251]  eta: 0:05:26  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4433  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 590/1251]  eta: 0:05:21  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4419  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 600/1251]  eta: 0:05:16  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4573  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 610/1251]  eta: 0:05:11  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4861  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 620/1251]  eta: 0:05:06  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4725  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 630/1251]  eta: 0:05:01  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4464  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 640/1251]  eta: 0:04:56  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4618  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 650/1251]  eta: 0:04:51  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4697  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 660/1251]  eta: 0:04:46  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4533  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 670/1251]  eta: 0:04:40  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4438  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 680/1251]  eta: 0:04:35  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4504  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 690/1251]  eta: 0:04:30  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4491  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [62]  [ 700/1251]  eta: 0:04:25  lr: 0.000007  loss: 0.0000 (0.0000)  time: 0.4438  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
