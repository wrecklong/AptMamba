CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
| distributed init (rank 0): env://
| distributed init (rank 3): env://
| distributed init (rank 1): env://
| distributed init (rank 6): env://
| distributed init (rank 7): env://
| distributed init (rank 5): env://
| distributed init (rank 2): env://
| distributed init (rank 4): env://
Namespace(batch_size=128, epochs=100, bce_loss=False, update_freq=1, unscale_lr=False, model='vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.1, sched='cosine', lr=1e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='/cluster/work/cvl/guosun/shangye/pretrained/vim_t_midclstok_76p1acc.pth', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, cosub=False, finetune='', attn_only=False, data_path='/scratch/tmp.13301953.guosun/datasets/imagenet_full_size/', data_set='IMNET', inat_category='name', output_dir='/cluster/work/cvl/guosun/shangye/output/Vim_new/vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_300einit_100e_batch_size128_p0.9_lr0.00001_min_lr1e-6_decoder_pruning_loss3stage_weight0.1_mse_weight0.02_sort_keep_policy_pretrain_mae', log_dir=None, device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=32, pin_mem=True, distributed=True, world_size=8, dist_url='env://', if_amp=False, if_continue_inf=True, if_nan2num=False, if_random_cls_token_position=False, if_random_token_rank=False, base_rate=0.9, lr_scale=0.01, ratio_weight=2.0, pretrained_mae_path='/cluster/work/cvl/guosun/shangye/output/pretrain_mae_vim/vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_mae_300e/checkpoint.pth', local_rank=0, rank=0, gpu=0, dist_backend='nccl')
Creating model: vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2
keep_rate [0.9, 0.81, 0.7290000000000001]
VisionMambaPrunning(
  (token_merge_module): TPSModule()
  (score_predictor): ModuleList(
    (0-2): 3 x PredictorLG(
      (in_conv_local): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (in_conv_cls): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (out_conv): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=96, out_features=48, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=48, out_features=2, bias=True)
        (5): LogSoftmax(dim=-1)
      )
    )
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=192, out_features=1000, bias=True)
  (drop_path): Identity()
  (layers): ModuleList(
    (0-23): 24 x Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=192, out_features=768, bias=False)
        (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (act): SiLU()
        (x_proj): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj): Linear(in_features=12, out_features=384, bias=True)
        (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
        (out_proj): Linear(in_features=384, out_features=192, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): Identity()
    )
  )
  (decoder): MAE_Decoder(
    (decoder_embed): Linear(in_features=192, out_features=512, bias=True)
    (decoder_blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)
  )
  (norm_f): RMSNorm()
)
Weights of VisionMambaPrunning not initialized from pretrained model: ['score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'decoder.mask_token', 'decoder.decoder_pos_embed', 'decoder.decoder_embed.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_blocks.0.norm1.weight', 'decoder.decoder_blocks.0.norm1.bias', 'decoder.decoder_blocks.0.attn.qkv.weight', 'decoder.decoder_blocks.0.attn.qkv.bias', 'decoder.decoder_blocks.0.attn.proj.weight', 'decoder.decoder_blocks.0.attn.proj.bias', 'decoder.decoder_blocks.0.norm2.weight', 'decoder.decoder_blocks.0.norm2.bias', 'decoder.decoder_blocks.0.mlp.fc1.weight', 'decoder.decoder_blocks.0.mlp.fc1.bias', 'decoder.decoder_blocks.0.mlp.fc2.weight', 'decoder.decoder_blocks.0.mlp.fc2.bias', 'decoder.decoder_blocks.1.norm1.weight', 'decoder.decoder_blocks.1.norm1.bias', 'decoder.decoder_blocks.1.attn.qkv.weight', 'decoder.decoder_blocks.1.attn.qkv.bias', 'decoder.decoder_blocks.1.attn.proj.weight', 'decoder.decoder_blocks.1.attn.proj.bias', 'decoder.decoder_blocks.1.norm2.weight', 'decoder.decoder_blocks.1.norm2.bias', 'decoder.decoder_blocks.1.mlp.fc1.weight', 'decoder.decoder_blocks.1.mlp.fc1.bias', 'decoder.decoder_blocks.1.mlp.fc2.weight', 'decoder.decoder_blocks.1.mlp.fc2.bias', 'decoder.decoder_blocks.2.norm1.weight', 'decoder.decoder_blocks.2.norm1.bias', 'decoder.decoder_blocks.2.attn.qkv.weight', 'decoder.decoder_blocks.2.attn.qkv.bias', 'decoder.decoder_blocks.2.attn.proj.weight', 'decoder.decoder_blocks.2.attn.proj.bias', 'decoder.decoder_blocks.2.norm2.weight', 'decoder.decoder_blocks.2.norm2.bias', 'decoder.decoder_blocks.2.mlp.fc1.weight', 'decoder.decoder_blocks.2.mlp.fc1.bias', 'decoder.decoder_blocks.2.mlp.fc2.weight', 'decoder.decoder_blocks.2.mlp.fc2.bias', 'decoder.decoder_blocks.3.norm1.weight', 'decoder.decoder_blocks.3.norm1.bias', 'decoder.decoder_blocks.3.attn.qkv.weight', 'decoder.decoder_blocks.3.attn.qkv.bias', 'decoder.decoder_blocks.3.attn.proj.weight', 'decoder.decoder_blocks.3.attn.proj.bias', 'decoder.decoder_blocks.3.norm2.weight', 'decoder.decoder_blocks.3.norm2.bias', 'decoder.decoder_blocks.3.mlp.fc1.weight', 'decoder.decoder_blocks.3.mlp.fc1.bias', 'decoder.decoder_blocks.3.mlp.fc2.weight', 'decoder.decoder_blocks.3.mlp.fc2.bias', 'decoder.decoder_blocks.4.norm1.weight', 'decoder.decoder_blocks.4.norm1.bias', 'decoder.decoder_blocks.4.attn.qkv.weight', 'decoder.decoder_blocks.4.attn.qkv.bias', 'decoder.decoder_blocks.4.attn.proj.weight', 'decoder.decoder_blocks.4.attn.proj.bias', 'decoder.decoder_blocks.4.norm2.weight', 'decoder.decoder_blocks.4.norm2.bias', 'decoder.decoder_blocks.4.mlp.fc1.weight', 'decoder.decoder_blocks.4.mlp.fc1.bias', 'decoder.decoder_blocks.4.mlp.fc2.weight', 'decoder.decoder_blocks.4.mlp.fc2.bias', 'decoder.decoder_blocks.5.norm1.weight', 'decoder.decoder_blocks.5.norm1.bias', 'decoder.decoder_blocks.5.attn.qkv.weight', 'decoder.decoder_blocks.5.attn.qkv.bias', 'decoder.decoder_blocks.5.attn.proj.weight', 'decoder.decoder_blocks.5.attn.proj.bias', 'decoder.decoder_blocks.5.norm2.weight', 'decoder.decoder_blocks.5.norm2.bias', 'decoder.decoder_blocks.5.mlp.fc1.weight', 'decoder.decoder_blocks.5.mlp.fc1.bias', 'decoder.decoder_blocks.5.mlp.fc2.weight', 'decoder.decoder_blocks.5.mlp.fc2.bias', 'decoder.decoder_blocks.6.norm1.weight', 'decoder.decoder_blocks.6.norm1.bias', 'decoder.decoder_blocks.6.attn.qkv.weight', 'decoder.decoder_blocks.6.attn.qkv.bias', 'decoder.decoder_blocks.6.attn.proj.weight', 'decoder.decoder_blocks.6.attn.proj.bias', 'decoder.decoder_blocks.6.norm2.weight', 'decoder.decoder_blocks.6.norm2.bias', 'decoder.decoder_blocks.6.mlp.fc1.weight', 'decoder.decoder_blocks.6.mlp.fc1.bias', 'decoder.decoder_blocks.6.mlp.fc2.weight', 'decoder.decoder_blocks.6.mlp.fc2.bias', 'decoder.decoder_blocks.7.norm1.weight', 'decoder.decoder_blocks.7.norm1.bias', 'decoder.decoder_blocks.7.attn.qkv.weight', 'decoder.decoder_blocks.7.attn.qkv.bias', 'decoder.decoder_blocks.7.attn.proj.weight', 'decoder.decoder_blocks.7.attn.proj.bias', 'decoder.decoder_blocks.7.norm2.weight', 'decoder.decoder_blocks.7.norm2.bias', 'decoder.decoder_blocks.7.mlp.fc1.weight', 'decoder.decoder_blocks.7.mlp.fc1.bias', 'decoder.decoder_blocks.7.mlp.fc2.weight', 'decoder.decoder_blocks.7.mlp.fc2.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_pred.bias']
Weights of VisionMambaPrunning not initialized from pretrained model: ['cls_token', 'pos_embed', 'score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'head.weight', 'head.bias', 'layers.0.mixer.A_log', 'layers.0.mixer.D', 'layers.0.mixer.A_b_log', 'layers.0.mixer.D_b', 'layers.0.mixer.in_proj.weight', 'layers.0.mixer.conv1d.weight', 'layers.0.mixer.conv1d.bias', 'layers.0.mixer.x_proj.weight', 'layers.0.mixer.dt_proj.weight', 'layers.0.mixer.dt_proj.bias', 'layers.0.mixer.conv1d_b.weight', 'layers.0.mixer.conv1d_b.bias', 'layers.0.mixer.x_proj_b.weight', 'layers.0.mixer.dt_proj_b.weight', 'layers.0.mixer.dt_proj_b.bias', 'layers.0.mixer.out_proj.weight', 'layers.0.norm.weight', 'layers.1.mixer.A_log', 'layers.1.mixer.D', 'layers.1.mixer.A_b_log', 'layers.1.mixer.D_b', 'layers.1.mixer.in_proj.weight', 'layers.1.mixer.conv1d.weight', 'layers.1.mixer.conv1d.bias', 'layers.1.mixer.x_proj.weight', 'layers.1.mixer.dt_proj.weight', 'layers.1.mixer.dt_proj.bias', 'layers.1.mixer.conv1d_b.weight', 'layers.1.mixer.conv1d_b.bias', 'layers.1.mixer.x_proj_b.weight', 'layers.1.mixer.dt_proj_b.weight', 'layers.1.mixer.dt_proj_b.bias', 'layers.1.mixer.out_proj.weight', 'layers.1.norm.weight', 'layers.2.mixer.A_log', 'layers.2.mixer.D', 'layers.2.mixer.A_b_log', 'layers.2.mixer.D_b', 'layers.2.mixer.in_proj.weight', 'layers.2.mixer.conv1d.weight', 'layers.2.mixer.conv1d.bias', 'layers.2.mixer.x_proj.weight', 'layers.2.mixer.dt_proj.weight', 'layers.2.mixer.dt_proj.bias', 'layers.2.mixer.conv1d_b.weight', 'layers.2.mixer.conv1d_b.bias', 'layers.2.mixer.x_proj_b.weight', 'layers.2.mixer.dt_proj_b.weight', 'layers.2.mixer.dt_proj_b.bias', 'layers.2.mixer.out_proj.weight', 'layers.2.norm.weight', 'layers.3.mixer.A_log', 'layers.3.mixer.D', 'layers.3.mixer.A_b_log', 'layers.3.mixer.D_b', 'layers.3.mixer.in_proj.weight', 'layers.3.mixer.conv1d.weight', 'layers.3.mixer.conv1d.bias', 'layers.3.mixer.x_proj.weight', 'layers.3.mixer.dt_proj.weight', 'layers.3.mixer.dt_proj.bias', 'layers.3.mixer.conv1d_b.weight', 'layers.3.mixer.conv1d_b.bias', 'layers.3.mixer.x_proj_b.weight', 'layers.3.mixer.dt_proj_b.weight', 'layers.3.mixer.dt_proj_b.bias', 'layers.3.mixer.out_proj.weight', 'layers.3.norm.weight', 'layers.4.mixer.A_log', 'layers.4.mixer.D', 'layers.4.mixer.A_b_log', 'layers.4.mixer.D_b', 'layers.4.mixer.in_proj.weight', 'layers.4.mixer.conv1d.weight', 'layers.4.mixer.conv1d.bias', 'layers.4.mixer.x_proj.weight', 'layers.4.mixer.dt_proj.weight', 'layers.4.mixer.dt_proj.bias', 'layers.4.mixer.conv1d_b.weight', 'layers.4.mixer.conv1d_b.bias', 'layers.4.mixer.x_proj_b.weight', 'layers.4.mixer.dt_proj_b.weight', 'layers.4.mixer.dt_proj_b.bias', 'layers.4.mixer.out_proj.weight', 'layers.4.norm.weight', 'layers.5.mixer.A_log', 'layers.5.mixer.D', 'layers.5.mixer.A_b_log', 'layers.5.mixer.D_b', 'layers.5.mixer.in_proj.weight', 'layers.5.mixer.conv1d.weight', 'layers.5.mixer.conv1d.bias', 'layers.5.mixer.x_proj.weight', 'layers.5.mixer.dt_proj.weight', 'layers.5.mixer.dt_proj.bias', 'layers.5.mixer.conv1d_b.weight', 'layers.5.mixer.conv1d_b.bias', 'layers.5.mixer.x_proj_b.weight', 'layers.5.mixer.dt_proj_b.weight', 'layers.5.mixer.dt_proj_b.bias', 'layers.5.mixer.out_proj.weight', 'layers.5.norm.weight', 'layers.6.mixer.A_log', 'layers.6.mixer.D', 'layers.6.mixer.A_b_log', 'layers.6.mixer.D_b', 'layers.6.mixer.in_proj.weight', 'layers.6.mixer.conv1d.weight', 'layers.6.mixer.conv1d.bias', 'layers.6.mixer.x_proj.weight', 'layers.6.mixer.dt_proj.weight', 'layers.6.mixer.dt_proj.bias', 'layers.6.mixer.conv1d_b.weight', 'layers.6.mixer.conv1d_b.bias', 'layers.6.mixer.x_proj_b.weight', 'layers.6.mixer.dt_proj_b.weight', 'layers.6.mixer.dt_proj_b.bias', 'layers.6.mixer.out_proj.weight', 'layers.6.norm.weight', 'layers.7.mixer.A_log', 'layers.7.mixer.D', 'layers.7.mixer.A_b_log', 'layers.7.mixer.D_b', 'layers.7.mixer.in_proj.weight', 'layers.7.mixer.conv1d.weight', 'layers.7.mixer.conv1d.bias', 'layers.7.mixer.x_proj.weight', 'layers.7.mixer.dt_proj.weight', 'layers.7.mixer.dt_proj.bias', 'layers.7.mixer.conv1d_b.weight', 'layers.7.mixer.conv1d_b.bias', 'layers.7.mixer.x_proj_b.weight', 'layers.7.mixer.dt_proj_b.weight', 'layers.7.mixer.dt_proj_b.bias', 'layers.7.mixer.out_proj.weight', 'layers.7.norm.weight', 'layers.8.mixer.A_log', 'layers.8.mixer.D', 'layers.8.mixer.A_b_log', 'layers.8.mixer.D_b', 'layers.8.mixer.in_proj.weight', 'layers.8.mixer.conv1d.weight', 'layers.8.mixer.conv1d.bias', 'layers.8.mixer.x_proj.weight', 'layers.8.mixer.dt_proj.weight', 'layers.8.mixer.dt_proj.bias', 'layers.8.mixer.conv1d_b.weight', 'layers.8.mixer.conv1d_b.bias', 'layers.8.mixer.x_proj_b.weight', 'layers.8.mixer.dt_proj_b.weight', 'layers.8.mixer.dt_proj_b.bias', 'layers.8.mixer.out_proj.weight', 'layers.8.norm.weight', 'layers.9.mixer.A_log', 'layers.9.mixer.D', 'layers.9.mixer.A_b_log', 'layers.9.mixer.D_b', 'layers.9.mixer.in_proj.weight', 'layers.9.mixer.conv1d.weight', 'layers.9.mixer.conv1d.bias', 'layers.9.mixer.x_proj.weight', 'layers.9.mixer.dt_proj.weight', 'layers.9.mixer.dt_proj.bias', 'layers.9.mixer.conv1d_b.weight', 'layers.9.mixer.conv1d_b.bias', 'layers.9.mixer.x_proj_b.weight', 'layers.9.mixer.dt_proj_b.weight', 'layers.9.mixer.dt_proj_b.bias', 'layers.9.mixer.out_proj.weight', 'layers.9.norm.weight', 'layers.10.mixer.A_log', 'layers.10.mixer.D', 'layers.10.mixer.A_b_log', 'layers.10.mixer.D_b', 'layers.10.mixer.in_proj.weight', 'layers.10.mixer.conv1d.weight', 'layers.10.mixer.conv1d.bias', 'layers.10.mixer.x_proj.weight', 'layers.10.mixer.dt_proj.weight', 'layers.10.mixer.dt_proj.bias', 'layers.10.mixer.conv1d_b.weight', 'layers.10.mixer.conv1d_b.bias', 'layers.10.mixer.x_proj_b.weight', 'layers.10.mixer.dt_proj_b.weight', 'layers.10.mixer.dt_proj_b.bias', 'layers.10.mixer.out_proj.weight', 'layers.10.norm.weight', 'layers.11.mixer.A_log', 'layers.11.mixer.D', 'layers.11.mixer.A_b_log', 'layers.11.mixer.D_b', 'layers.11.mixer.in_proj.weight', 'layers.11.mixer.conv1d.weight', 'layers.11.mixer.conv1d.bias', 'layers.11.mixer.x_proj.weight', 'layers.11.mixer.dt_proj.weight', 'layers.11.mixer.dt_proj.bias', 'layers.11.mixer.conv1d_b.weight', 'layers.11.mixer.conv1d_b.bias', 'layers.11.mixer.x_proj_b.weight', 'layers.11.mixer.dt_proj_b.weight', 'layers.11.mixer.dt_proj_b.bias', 'layers.11.mixer.out_proj.weight', 'layers.11.norm.weight', 'layers.12.mixer.A_log', 'layers.12.mixer.D', 'layers.12.mixer.A_b_log', 'layers.12.mixer.D_b', 'layers.12.mixer.in_proj.weight', 'layers.12.mixer.conv1d.weight', 'layers.12.mixer.conv1d.bias', 'layers.12.mixer.x_proj.weight', 'layers.12.mixer.dt_proj.weight', 'layers.12.mixer.dt_proj.bias', 'layers.12.mixer.conv1d_b.weight', 'layers.12.mixer.conv1d_b.bias', 'layers.12.mixer.x_proj_b.weight', 'layers.12.mixer.dt_proj_b.weight', 'layers.12.mixer.dt_proj_b.bias', 'layers.12.mixer.out_proj.weight', 'layers.12.norm.weight', 'layers.13.mixer.A_log', 'layers.13.mixer.D', 'layers.13.mixer.A_b_log', 'layers.13.mixer.D_b', 'layers.13.mixer.in_proj.weight', 'layers.13.mixer.conv1d.weight', 'layers.13.mixer.conv1d.bias', 'layers.13.mixer.x_proj.weight', 'layers.13.mixer.dt_proj.weight', 'layers.13.mixer.dt_proj.bias', 'layers.13.mixer.conv1d_b.weight', 'layers.13.mixer.conv1d_b.bias', 'layers.13.mixer.x_proj_b.weight', 'layers.13.mixer.dt_proj_b.weight', 'layers.13.mixer.dt_proj_b.bias', 'layers.13.mixer.out_proj.weight', 'layers.13.norm.weight', 'layers.14.mixer.A_log', 'layers.14.mixer.D', 'layers.14.mixer.A_b_log', 'layers.14.mixer.D_b', 'layers.14.mixer.in_proj.weight', 'layers.14.mixer.conv1d.weight', 'layers.14.mixer.conv1d.bias', 'layers.14.mixer.x_proj.weight', 'layers.14.mixer.dt_proj.weight', 'layers.14.mixer.dt_proj.bias', 'layers.14.mixer.conv1d_b.weight', 'layers.14.mixer.conv1d_b.bias', 'layers.14.mixer.x_proj_b.weight', 'layers.14.mixer.dt_proj_b.weight', 'layers.14.mixer.dt_proj_b.bias', 'layers.14.mixer.out_proj.weight', 'layers.14.norm.weight', 'layers.15.mixer.A_log', 'layers.15.mixer.D', 'layers.15.mixer.A_b_log', 'layers.15.mixer.D_b', 'layers.15.mixer.in_proj.weight', 'layers.15.mixer.conv1d.weight', 'layers.15.mixer.conv1d.bias', 'layers.15.mixer.x_proj.weight', 'layers.15.mixer.dt_proj.weight', 'layers.15.mixer.dt_proj.bias', 'layers.15.mixer.conv1d_b.weight', 'layers.15.mixer.conv1d_b.bias', 'layers.15.mixer.x_proj_b.weight', 'layers.15.mixer.dt_proj_b.weight', 'layers.15.mixer.dt_proj_b.bias', 'layers.15.mixer.out_proj.weight', 'layers.15.norm.weight', 'layers.16.mixer.A_log', 'layers.16.mixer.D', 'layers.16.mixer.A_b_log', 'layers.16.mixer.D_b', 'layers.16.mixer.in_proj.weight', 'layers.16.mixer.conv1d.weight', 'layers.16.mixer.conv1d.bias', 'layers.16.mixer.x_proj.weight', 'layers.16.mixer.dt_proj.weight', 'layers.16.mixer.dt_proj.bias', 'layers.16.mixer.conv1d_b.weight', 'layers.16.mixer.conv1d_b.bias', 'layers.16.mixer.x_proj_b.weight', 'layers.16.mixer.dt_proj_b.weight', 'layers.16.mixer.dt_proj_b.bias', 'layers.16.mixer.out_proj.weight', 'layers.16.norm.weight', 'layers.17.mixer.A_log', 'layers.17.mixer.D', 'layers.17.mixer.A_b_log', 'layers.17.mixer.D_b', 'layers.17.mixer.in_proj.weight', 'layers.17.mixer.conv1d.weight', 'layers.17.mixer.conv1d.bias', 'layers.17.mixer.x_proj.weight', 'layers.17.mixer.dt_proj.weight', 'layers.17.mixer.dt_proj.bias', 'layers.17.mixer.conv1d_b.weight', 'layers.17.mixer.conv1d_b.bias', 'layers.17.mixer.x_proj_b.weight', 'layers.17.mixer.dt_proj_b.weight', 'layers.17.mixer.dt_proj_b.bias', 'layers.17.mixer.out_proj.weight', 'layers.17.norm.weight', 'layers.18.mixer.A_log', 'layers.18.mixer.D', 'layers.18.mixer.A_b_log', 'layers.18.mixer.D_b', 'layers.18.mixer.in_proj.weight', 'layers.18.mixer.conv1d.weight', 'layers.18.mixer.conv1d.bias', 'layers.18.mixer.x_proj.weight', 'layers.18.mixer.dt_proj.weight', 'layers.18.mixer.dt_proj.bias', 'layers.18.mixer.conv1d_b.weight', 'layers.18.mixer.conv1d_b.bias', 'layers.18.mixer.x_proj_b.weight', 'layers.18.mixer.dt_proj_b.weight', 'layers.18.mixer.dt_proj_b.bias', 'layers.18.mixer.out_proj.weight', 'layers.18.norm.weight', 'layers.19.mixer.A_log', 'layers.19.mixer.D', 'layers.19.mixer.A_b_log', 'layers.19.mixer.D_b', 'layers.19.mixer.in_proj.weight', 'layers.19.mixer.conv1d.weight', 'layers.19.mixer.conv1d.bias', 'layers.19.mixer.x_proj.weight', 'layers.19.mixer.dt_proj.weight', 'layers.19.mixer.dt_proj.bias', 'layers.19.mixer.conv1d_b.weight', 'layers.19.mixer.conv1d_b.bias', 'layers.19.mixer.x_proj_b.weight', 'layers.19.mixer.dt_proj_b.weight', 'layers.19.mixer.dt_proj_b.bias', 'layers.19.mixer.out_proj.weight', 'layers.19.norm.weight', 'layers.20.mixer.A_log', 'layers.20.mixer.D', 'layers.20.mixer.A_b_log', 'layers.20.mixer.D_b', 'layers.20.mixer.in_proj.weight', 'layers.20.mixer.conv1d.weight', 'layers.20.mixer.conv1d.bias', 'layers.20.mixer.x_proj.weight', 'layers.20.mixer.dt_proj.weight', 'layers.20.mixer.dt_proj.bias', 'layers.20.mixer.conv1d_b.weight', 'layers.20.mixer.conv1d_b.bias', 'layers.20.mixer.x_proj_b.weight', 'layers.20.mixer.dt_proj_b.weight', 'layers.20.mixer.dt_proj_b.bias', 'layers.20.mixer.out_proj.weight', 'layers.20.norm.weight', 'layers.21.mixer.A_log', 'layers.21.mixer.D', 'layers.21.mixer.A_b_log', 'layers.21.mixer.D_b', 'layers.21.mixer.in_proj.weight', 'layers.21.mixer.conv1d.weight', 'layers.21.mixer.conv1d.bias', 'layers.21.mixer.x_proj.weight', 'layers.21.mixer.dt_proj.weight', 'layers.21.mixer.dt_proj.bias', 'layers.21.mixer.conv1d_b.weight', 'layers.21.mixer.conv1d_b.bias', 'layers.21.mixer.x_proj_b.weight', 'layers.21.mixer.dt_proj_b.weight', 'layers.21.mixer.dt_proj_b.bias', 'layers.21.mixer.out_proj.weight', 'layers.21.norm.weight', 'layers.22.mixer.A_log', 'layers.22.mixer.D', 'layers.22.mixer.A_b_log', 'layers.22.mixer.D_b', 'layers.22.mixer.in_proj.weight', 'layers.22.mixer.conv1d.weight', 'layers.22.mixer.conv1d.bias', 'layers.22.mixer.x_proj.weight', 'layers.22.mixer.dt_proj.weight', 'layers.22.mixer.dt_proj.bias', 'layers.22.mixer.conv1d_b.weight', 'layers.22.mixer.conv1d_b.bias', 'layers.22.mixer.x_proj_b.weight', 'layers.22.mixer.dt_proj_b.weight', 'layers.22.mixer.dt_proj_b.bias', 'layers.22.mixer.out_proj.weight', 'layers.22.norm.weight', 'layers.23.mixer.A_log', 'layers.23.mixer.D', 'layers.23.mixer.A_b_log', 'layers.23.mixer.D_b', 'layers.23.mixer.in_proj.weight', 'layers.23.mixer.conv1d.weight', 'layers.23.mixer.conv1d.bias', 'layers.23.mixer.x_proj.weight', 'layers.23.mixer.dt_proj.weight', 'layers.23.mixer.dt_proj.bias', 'layers.23.mixer.conv1d_b.weight', 'layers.23.mixer.conv1d_b.bias', 'layers.23.mixer.x_proj_b.weight', 'layers.23.mixer.dt_proj_b.weight', 'layers.23.mixer.dt_proj_b.bias', 'layers.23.mixer.out_proj.weight', 'layers.23.norm.weight', 'norm_f.weight']
number of params: 33044734
ratio_weight= 2.0 reconstruction_weight 0.02 pruning_weight 0.1
Start training for 100 epochs
Epoch: [0]  [   0/1251]  eta: 10:14:26  lr: 0.000001  loss: 7.8899 (7.8899)  time: 29.4700  data: 21.3855  max mem: 19423
Epoch: [0]  [  10/1251]  eta: 1:04:40  lr: 0.000001  loss: 7.8456 (7.8183)  time: 3.1272  data: 1.9445  max mem: 19733
Epoch: [0]  [  20/1251]  eta: 0:38:14  lr: 0.000001  loss: 7.8199 (7.8265)  time: 0.4837  data: 0.0004  max mem: 19733
Epoch: [0]  [  30/1251]  eta: 0:28:49  lr: 0.000001  loss: 7.8028 (7.8052)  time: 0.4756  data: 0.0004  max mem: 19733
Epoch: [0]  [  40/1251]  eta: 0:23:59  lr: 0.000001  loss: 7.7250 (7.7849)  time: 0.4803  data: 0.0004  max mem: 19733
Epoch: [0]  [  50/1251]  eta: 0:20:59  lr: 0.000001  loss: 7.6995 (7.7592)  time: 0.4797  data: 0.0004  max mem: 19733
Epoch: [0]  [  60/1251]  eta: 0:19:03  lr: 0.000001  loss: 7.6602 (7.7419)  time: 0.4904  data: 0.0004  max mem: 19733
Epoch: [0]  [  70/1251]  eta: 0:17:34  lr: 0.000001  loss: 7.6341 (7.7199)  time: 0.4940  data: 0.0004  max mem: 19733
Epoch: [0]  [  80/1251]  eta: 0:16:25  lr: 0.000001  loss: 7.5553 (7.6981)  time: 0.4816  data: 0.0004  max mem: 19733
Epoch: [0]  [  90/1251]  eta: 0:15:31  lr: 0.000001  loss: 7.5426 (7.6829)  time: 0.4802  data: 0.0004  max mem: 19733
loss info: cls_loss=7.0538, ratio_loss=0.8391, pruning_loss=0.2689, mse_loss=1.2839
Epoch: [0]  [ 100/1251]  eta: 0:14:46  lr: 0.000001  loss: 7.5084 (7.6660)  time: 0.4791  data: 0.0004  max mem: 19733
Epoch: [0]  [ 110/1251]  eta: 0:14:08  lr: 0.000001  loss: 7.4865 (7.6470)  time: 0.4762  data: 0.0004  max mem: 19733
Epoch: [0]  [ 120/1251]  eta: 0:13:35  lr: 0.000001  loss: 7.4502 (7.6257)  time: 0.4753  data: 0.0004  max mem: 19733
Epoch: [0]  [ 130/1251]  eta: 0:13:07  lr: 0.000001  loss: 7.3169 (7.6038)  time: 0.4767  data: 0.0004  max mem: 19733
Epoch: [0]  [ 140/1251]  eta: 0:12:42  lr: 0.000001  loss: 7.2769 (7.5774)  time: 0.4765  data: 0.0005  max mem: 19733
Epoch: [0]  [ 150/1251]  eta: 0:12:20  lr: 0.000001  loss: 7.2034 (7.5492)  time: 0.4771  data: 0.0004  max mem: 19733
Epoch: [0]  [ 160/1251]  eta: 0:12:00  lr: 0.000001  loss: 7.1181 (7.5199)  time: 0.4776  data: 0.0004  max mem: 19733
Epoch: [0]  [ 170/1251]  eta: 0:11:42  lr: 0.000001  loss: 7.0893 (7.4910)  time: 0.4774  data: 0.0004  max mem: 19733
Epoch: [0]  [ 180/1251]  eta: 0:11:27  lr: 0.000001  loss: 7.0047 (7.4605)  time: 0.4878  data: 0.0004  max mem: 19733
Epoch: [0]  [ 190/1251]  eta: 0:11:11  lr: 0.000001  loss: 6.9404 (7.4281)  time: 0.4872  data: 0.0004  max mem: 19733
loss info: cls_loss=6.5192, ratio_loss=0.8311, pruning_loss=0.2685, mse_loss=1.3301
Epoch: [0]  [ 200/1251]  eta: 0:10:58  lr: 0.000001  loss: 6.8217 (7.3940)  time: 0.4892  data: 0.0004  max mem: 19733
Epoch: [0]  [ 210/1251]  eta: 0:10:44  lr: 0.000001  loss: 6.8230 (7.3648)  time: 0.4924  data: 0.0004  max mem: 19733
Epoch: [0]  [ 220/1251]  eta: 0:10:32  lr: 0.000001  loss: 6.8516 (7.3358)  time: 0.4801  data: 0.0004  max mem: 19733
Epoch: [0]  [ 230/1251]  eta: 0:10:20  lr: 0.000001  loss: 6.8516 (7.3055)  time: 0.4777  data: 0.0004  max mem: 19733
Epoch: [0]  [ 240/1251]  eta: 0:10:08  lr: 0.000001  loss: 6.6415 (7.2750)  time: 0.4769  data: 0.0004  max mem: 19733
Epoch: [0]  [ 250/1251]  eta: 0:09:57  lr: 0.000001  loss: 6.6415 (7.2411)  time: 0.4766  data: 0.0003  max mem: 19733
Epoch: [0]  [ 260/1251]  eta: 0:09:46  lr: 0.000001  loss: 6.3241 (7.2072)  time: 0.4777  data: 0.0010  max mem: 19733
Epoch: [0]  [ 270/1251]  eta: 0:09:36  lr: 0.000001  loss: 6.3156 (7.1730)  time: 0.4786  data: 0.0010  max mem: 19733
Epoch: [0]  [ 280/1251]  eta: 0:09:27  lr: 0.000001  loss: 6.6214 (7.1525)  time: 0.4786  data: 0.0004  max mem: 19733
Epoch: [0]  [ 290/1251]  eta: 0:09:18  lr: 0.000001  loss: 6.5978 (7.1151)  time: 0.4793  data: 0.0004  max mem: 19733
loss info: cls_loss=5.8567, ratio_loss=0.8245, pruning_loss=0.2764, mse_loss=1.3230
Epoch: [0]  [ 300/1251]  eta: 0:09:08  lr: 0.000001  loss: 5.9844 (7.0811)  time: 0.4787  data: 0.0007  max mem: 19733
Epoch: [0]  [ 310/1251]  eta: 0:09:00  lr: 0.000001  loss: 6.1099 (7.0579)  time: 0.4788  data: 0.0007  max mem: 19733
Epoch: [0]  [ 320/1251]  eta: 0:08:51  lr: 0.000001  loss: 6.7473 (7.0445)  time: 0.4798  data: 0.0003  max mem: 19733
Epoch: [0]  [ 330/1251]  eta: 0:08:43  lr: 0.000001  loss: 6.6752 (7.0207)  time: 0.4857  data: 0.0004  max mem: 19733
Epoch: [0]  [ 340/1251]  eta: 0:08:36  lr: 0.000001  loss: 6.3902 (7.0015)  time: 0.4956  data: 0.0004  max mem: 19733
Epoch: [0]  [ 350/1251]  eta: 0:08:28  lr: 0.000001  loss: 6.3919 (6.9789)  time: 0.4891  data: 0.0004  max mem: 19733
Epoch: [0]  [ 360/1251]  eta: 0:08:20  lr: 0.000001  loss: 6.3919 (6.9542)  time: 0.4789  data: 0.0004  max mem: 19733
Epoch: [0]  [ 370/1251]  eta: 0:08:13  lr: 0.000001  loss: 6.0691 (6.9309)  time: 0.4795  data: 0.0004  max mem: 19733
Epoch: [0]  [ 380/1251]  eta: 0:08:05  lr: 0.000001  loss: 6.0607 (6.9058)  time: 0.4798  data: 0.0004  max mem: 19733
Epoch: [0]  [ 390/1251]  eta: 0:07:58  lr: 0.000001  loss: 5.9029 (6.8788)  time: 0.4793  data: 0.0004  max mem: 19733
loss info: cls_loss=5.6077, ratio_loss=0.8190, pruning_loss=0.2737, mse_loss=1.2793
Epoch: [0]  [ 400/1251]  eta: 0:07:51  lr: 0.000001  loss: 6.2103 (6.8639)  time: 0.4792  data: 0.0005  max mem: 19733
Epoch: [0]  [ 410/1251]  eta: 0:07:44  lr: 0.000001  loss: 6.3528 (6.8513)  time: 0.4777  data: 0.0004  max mem: 19733
Epoch: [0]  [ 420/1251]  eta: 0:07:36  lr: 0.000001  loss: 6.3523 (6.8303)  time: 0.4758  data: 0.0004  max mem: 19733
Epoch: [0]  [ 430/1251]  eta: 0:07:30  lr: 0.000001  loss: 6.3523 (6.8170)  time: 0.4749  data: 0.0004  max mem: 19733
Epoch: [0]  [ 440/1251]  eta: 0:07:23  lr: 0.000001  loss: 6.3083 (6.8025)  time: 0.4756  data: 0.0004  max mem: 19733
Epoch: [0]  [ 450/1251]  eta: 0:07:16  lr: 0.000001  loss: 6.0296 (6.7820)  time: 0.4781  data: 0.0003  max mem: 19733
Epoch: [0]  [ 460/1251]  eta: 0:07:09  lr: 0.000001  loss: 5.7670 (6.7588)  time: 0.4782  data: 0.0004  max mem: 19733
Epoch: [0]  [ 470/1251]  eta: 0:07:03  lr: 0.000001  loss: 5.4839 (6.7307)  time: 0.4781  data: 0.0004  max mem: 19733
Epoch: [0]  [ 480/1251]  eta: 0:06:57  lr: 0.000001  loss: 5.6907 (6.7120)  time: 0.4858  data: 0.0003  max mem: 19733
Epoch: [0]  [ 490/1251]  eta: 0:06:51  lr: 0.000001  loss: 5.9433 (6.7021)  time: 0.4967  data: 0.0003  max mem: 19733
loss info: cls_loss=5.3699, ratio_loss=0.8109, pruning_loss=0.2776, mse_loss=1.2511
Epoch: [0]  [ 500/1251]  eta: 0:06:44  lr: 0.000001  loss: 5.9433 (6.6846)  time: 0.4931  data: 0.0004  max mem: 19733
Epoch: [0]  [ 510/1251]  eta: 0:06:38  lr: 0.000001  loss: 5.9579 (6.6746)  time: 0.4823  data: 0.0003  max mem: 19733
Epoch: [0]  [ 520/1251]  eta: 0:06:32  lr: 0.000001  loss: 6.1698 (6.6601)  time: 0.4818  data: 0.0004  max mem: 19733
Epoch: [0]  [ 530/1251]  eta: 0:06:26  lr: 0.000001  loss: 6.1214 (6.6395)  time: 0.4832  data: 0.0004  max mem: 19733
Epoch: [0]  [ 540/1251]  eta: 0:06:20  lr: 0.000001  loss: 6.1214 (6.6270)  time: 0.4836  data: 0.0004  max mem: 19733
Epoch: [0]  [ 550/1251]  eta: 0:06:14  lr: 0.000001  loss: 5.9717 (6.6119)  time: 0.4826  data: 0.0004  max mem: 19733
Epoch: [0]  [ 560/1251]  eta: 0:06:08  lr: 0.000001  loss: 6.1346 (6.6033)  time: 0.4804  data: 0.0004  max mem: 19733
Epoch: [0]  [ 570/1251]  eta: 0:06:02  lr: 0.000001  loss: 6.2661 (6.5940)  time: 0.4785  data: 0.0004  max mem: 19733
Epoch: [0]  [ 580/1251]  eta: 0:05:56  lr: 0.000001  loss: 6.1640 (6.5816)  time: 0.4773  data: 0.0004  max mem: 19733
Epoch: [0]  [ 590/1251]  eta: 0:05:50  lr: 0.000001  loss: 6.1954 (6.5714)  time: 0.4774  data: 0.0004  max mem: 19733
loss info: cls_loss=5.3492, ratio_loss=0.8061, pruning_loss=0.2781, mse_loss=1.2690
Epoch: [0]  [ 600/1251]  eta: 0:05:44  lr: 0.000001  loss: 6.1268 (6.5569)  time: 0.4779  data: 0.0004  max mem: 19733
Epoch: [0]  [ 610/1251]  eta: 0:05:38  lr: 0.000001  loss: 5.8879 (6.5453)  time: 0.4780  data: 0.0004  max mem: 19733
Epoch: [0]  [ 620/1251]  eta: 0:05:33  lr: 0.000001  loss: 5.9070 (6.5359)  time: 0.4788  data: 0.0004  max mem: 19733
Epoch: [0]  [ 630/1251]  eta: 0:05:27  lr: 0.000001  loss: 6.1098 (6.5248)  time: 0.4973  data: 0.0004  max mem: 19733
Epoch: [0]  [ 640/1251]  eta: 0:05:22  lr: 0.000001  loss: 5.5612 (6.5094)  time: 0.5119  data: 0.0004  max mem: 19733
Epoch: [0]  [ 650/1251]  eta: 0:05:16  lr: 0.000001  loss: 5.6790 (6.4986)  time: 0.4967  data: 0.0004  max mem: 19733
Epoch: [0]  [ 660/1251]  eta: 0:05:10  lr: 0.000001  loss: 5.5679 (6.4799)  time: 0.4818  data: 0.0004  max mem: 19733
Epoch: [0]  [ 670/1251]  eta: 0:05:05  lr: 0.000001  loss: 5.9217 (6.4738)  time: 0.4776  data: 0.0004  max mem: 19733
Epoch: [0]  [ 680/1251]  eta: 0:04:59  lr: 0.000001  loss: 6.0191 (6.4634)  time: 0.4777  data: 0.0004  max mem: 19733
Epoch: [0]  [ 690/1251]  eta: 0:04:53  lr: 0.000001  loss: 5.4522 (6.4483)  time: 0.4798  data: 0.0004  max mem: 19733
loss info: cls_loss=5.1023, ratio_loss=0.8004, pruning_loss=0.2838, mse_loss=1.2382
Epoch: [0]  [ 700/1251]  eta: 0:04:48  lr: 0.000001  loss: 5.4522 (6.4360)  time: 0.4805  data: 0.0004  max mem: 19733
Epoch: [0]  [ 710/1251]  eta: 0:04:42  lr: 0.000001  loss: 6.1045 (6.4239)  time: 0.4804  data: 0.0004  max mem: 19733
Epoch: [0]  [ 720/1251]  eta: 0:04:37  lr: 0.000001  loss: 5.9195 (6.4100)  time: 0.4789  data: 0.0004  max mem: 19733
Epoch: [0]  [ 730/1251]  eta: 0:04:31  lr: 0.000001  loss: 5.6177 (6.4004)  time: 0.4827  data: 0.0004  max mem: 19733
Epoch: [0]  [ 740/1251]  eta: 0:04:26  lr: 0.000001  loss: 5.9454 (6.3968)  time: 0.4850  data: 0.0005  max mem: 19733
Epoch: [0]  [ 750/1251]  eta: 0:04:20  lr: 0.000001  loss: 5.9961 (6.3845)  time: 0.4834  data: 0.0004  max mem: 19733
Epoch: [0]  [ 760/1251]  eta: 0:04:15  lr: 0.000001  loss: 5.9961 (6.3765)  time: 0.4876  data: 0.0004  max mem: 19733
Epoch: [0]  [ 770/1251]  eta: 0:04:09  lr: 0.000001  loss: 6.0172 (6.3675)  time: 0.4866  data: 0.0005  max mem: 19733
Epoch: [0]  [ 780/1251]  eta: 0:04:04  lr: 0.000001  loss: 5.7953 (6.3591)  time: 0.5191  data: 0.0005  max mem: 19733
Epoch: [0]  [ 790/1251]  eta: 0:03:59  lr: 0.000001  loss: 5.5625 (6.3488)  time: 0.5181  data: 0.0004  max mem: 19733
loss info: cls_loss=5.0931, ratio_loss=0.7956, pruning_loss=0.2833, mse_loss=1.2736
Epoch: [0]  [ 800/1251]  eta: 0:03:54  lr: 0.000001  loss: 5.6287 (6.3410)  time: 0.4860  data: 0.0004  max mem: 19733
Epoch: [0]  [ 810/1251]  eta: 0:03:48  lr: 0.000001  loss: 5.2599 (6.3239)  time: 0.4854  data: 0.0004  max mem: 19733
Epoch: [0]  [ 820/1251]  eta: 0:03:43  lr: 0.000001  loss: 5.2866 (6.3179)  time: 0.4815  data: 0.0004  max mem: 19733
Epoch: [0]  [ 830/1251]  eta: 0:03:38  lr: 0.000001  loss: 5.8984 (6.3048)  time: 0.4875  data: 0.0004  max mem: 19733
Epoch: [0]  [ 840/1251]  eta: 0:03:32  lr: 0.000001  loss: 5.1059 (6.2944)  time: 0.4862  data: 0.0004  max mem: 19733
Epoch: [0]  [ 850/1251]  eta: 0:03:27  lr: 0.000001  loss: 5.4599 (6.2843)  time: 0.4793  data: 0.0004  max mem: 19733
Epoch: [0]  [ 860/1251]  eta: 0:03:22  lr: 0.000001  loss: 5.8377 (6.2760)  time: 0.4794  data: 0.0004  max mem: 19733
Epoch: [0]  [ 870/1251]  eta: 0:03:16  lr: 0.000001  loss: 5.5458 (6.2693)  time: 0.4839  data: 0.0004  max mem: 19733
Epoch: [0]  [ 880/1251]  eta: 0:03:11  lr: 0.000001  loss: 5.5168 (6.2573)  time: 0.4820  data: 0.0003  max mem: 19733
Epoch: [0]  [ 890/1251]  eta: 0:03:06  lr: 0.000001  loss: 5.2798 (6.2469)  time: 0.4820  data: 0.0004  max mem: 19733
loss info: cls_loss=4.8524, ratio_loss=0.7891, pruning_loss=0.2894, mse_loss=1.2440
Epoch: [0]  [ 900/1251]  eta: 0:03:00  lr: 0.000001  loss: 5.6513 (6.2401)  time: 0.4840  data: 0.0005  max mem: 19733
Epoch: [0]  [ 910/1251]  eta: 0:02:55  lr: 0.000001  loss: 5.9124 (6.2337)  time: 0.4805  data: 0.0004  max mem: 19733
Epoch: [0]  [ 920/1251]  eta: 0:02:50  lr: 0.000001  loss: 5.9124 (6.2281)  time: 0.5049  data: 0.0004  max mem: 19733
Epoch: [0]  [ 930/1251]  eta: 0:02:45  lr: 0.000001  loss: 5.8950 (6.2252)  time: 0.5161  data: 0.0004  max mem: 19733
Epoch: [0]  [ 940/1251]  eta: 0:02:40  lr: 0.000001  loss: 5.8091 (6.2165)  time: 0.4917  data: 0.0004  max mem: 19733
Epoch: [0]  [ 950/1251]  eta: 0:02:34  lr: 0.000001  loss: 4.9927 (6.2018)  time: 0.4821  data: 0.0004  max mem: 19733
Epoch: [0]  [ 960/1251]  eta: 0:02:29  lr: 0.000001  loss: 5.2914 (6.1960)  time: 0.4837  data: 0.0004  max mem: 19733
Epoch: [0]  [ 970/1251]  eta: 0:02:24  lr: 0.000001  loss: 5.9033 (6.1909)  time: 0.4909  data: 0.0004  max mem: 19733
Epoch: [0]  [ 980/1251]  eta: 0:02:19  lr: 0.000001  loss: 5.7854 (6.1844)  time: 0.4916  data: 0.0004  max mem: 19733
Epoch: [0]  [ 990/1251]  eta: 0:02:13  lr: 0.000001  loss: 5.9054 (6.1782)  time: 0.4838  data: 0.0005  max mem: 19733
loss info: cls_loss=4.9837, ratio_loss=0.7830, pruning_loss=0.2858, mse_loss=1.2730
Epoch: [0]  [1000/1251]  eta: 0:02:08  lr: 0.000001  loss: 5.7458 (6.1712)  time: 0.4803  data: 0.0005  max mem: 19733
Epoch: [0]  [1010/1251]  eta: 0:02:03  lr: 0.000001  loss: 5.7089 (6.1671)  time: 0.4824  data: 0.0006  max mem: 19733
Epoch: [0]  [1020/1251]  eta: 0:01:58  lr: 0.000001  loss: 5.6437 (6.1593)  time: 0.4835  data: 0.0006  max mem: 19733
Epoch: [0]  [1030/1251]  eta: 0:01:53  lr: 0.000001  loss: 5.3824 (6.1488)  time: 0.4799  data: 0.0005  max mem: 19733
Epoch: [0]  [1040/1251]  eta: 0:01:47  lr: 0.000001  loss: 5.3824 (6.1446)  time: 0.4781  data: 0.0004  max mem: 19733
Epoch: [0]  [1050/1251]  eta: 0:01:42  lr: 0.000001  loss: 5.8109 (6.1377)  time: 0.4794  data: 0.0004  max mem: 19733
Epoch: [0]  [1060/1251]  eta: 0:01:37  lr: 0.000001  loss: 5.6930 (6.1337)  time: 0.4796  data: 0.0004  max mem: 19733
Epoch: [0]  [1070/1251]  eta: 0:01:32  lr: 0.000001  loss: 5.6401 (6.1266)  time: 0.4990  data: 0.0004  max mem: 19733
Epoch: [0]  [1080/1251]  eta: 0:01:27  lr: 0.000001  loss: 5.3012 (6.1204)  time: 0.5075  data: 0.0004  max mem: 19733
Epoch: [0]  [1090/1251]  eta: 0:01:22  lr: 0.000001  loss: 5.6359 (6.1142)  time: 0.4864  data: 0.0004  max mem: 19733
loss info: cls_loss=4.8935, ratio_loss=0.7762, pruning_loss=0.2870, mse_loss=1.2490
Epoch: [0]  [1100/1251]  eta: 0:01:17  lr: 0.000001  loss: 5.6359 (6.1077)  time: 0.4759  data: 0.0004  max mem: 19733
Epoch: [0]  [1110/1251]  eta: 0:01:11  lr: 0.000001  loss: 5.7176 (6.1028)  time: 0.4757  data: 0.0004  max mem: 19733
Epoch: [0]  [1120/1251]  eta: 0:01:06  lr: 0.000001  loss: 5.8756 (6.0995)  time: 0.4759  data: 0.0004  max mem: 19733
Epoch: [0]  [1130/1251]  eta: 0:01:01  lr: 0.000001  loss: 5.7544 (6.0955)  time: 0.4756  data: 0.0004  max mem: 19733
Epoch: [0]  [1140/1251]  eta: 0:00:56  lr: 0.000001  loss: 5.5426 (6.0879)  time: 0.4758  data: 0.0003  max mem: 19733
Epoch: [0]  [1150/1251]  eta: 0:00:51  lr: 0.000001  loss: 5.4014 (6.0827)  time: 0.4754  data: 0.0004  max mem: 19733
Epoch: [0]  [1160/1251]  eta: 0:00:46  lr: 0.000001  loss: 5.6889 (6.0769)  time: 0.4761  data: 0.0004  max mem: 19733
Epoch: [0]  [1170/1251]  eta: 0:00:41  lr: 0.000001  loss: 5.6889 (6.0718)  time: 0.4763  data: 0.0004  max mem: 19733
Epoch: [0]  [1180/1251]  eta: 0:00:36  lr: 0.000001  loss: 5.8614 (6.0698)  time: 0.4767  data: 0.0003  max mem: 19733
Epoch: [0]  [1190/1251]  eta: 0:00:30  lr: 0.000001  loss: 5.8614 (6.0665)  time: 0.4768  data: 0.0006  max mem: 19733
loss info: cls_loss=5.0081, ratio_loss=0.7695, pruning_loss=0.2819, mse_loss=1.2305
Epoch: [0]  [1200/1251]  eta: 0:00:25  lr: 0.000001  loss: 5.7213 (6.0630)  time: 0.4714  data: 0.0004  max mem: 19733
Epoch: [0]  [1210/1251]  eta: 0:00:20  lr: 0.000001  loss: 5.6239 (6.0570)  time: 0.4918  data: 0.0001  max mem: 19733
Epoch: [0]  [1220/1251]  eta: 0:00:15  lr: 0.000001  loss: 5.3668 (6.0509)  time: 0.4936  data: 0.0001  max mem: 19733
Epoch: [0]  [1230/1251]  eta: 0:00:10  lr: 0.000001  loss: 5.3668 (6.0457)  time: 0.4737  data: 0.0001  max mem: 19733
Epoch: [0]  [1240/1251]  eta: 0:00:05  lr: 0.000001  loss: 5.3979 (6.0395)  time: 0.4725  data: 0.0001  max mem: 19733
Epoch: [0]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 5.2724 (6.0321)  time: 0.4676  data: 0.0001  max mem: 19733
Epoch: [0] Total time: 0:10:34 (0.5068 s / it)
Averaged stats: lr: 0.000001  loss: 5.2724 (6.0341)
Test:  [  0/261]  eta: 2:20:19  loss: 1.3761 (1.3761)  acc1: 73.4375 (73.4375)  acc5: 91.1458 (91.1458)  time: 32.2592  data: 30.4990  max mem: 19733
Test:  [ 10/261]  eta: 0:13:12  loss: 1.3761 (1.3685)  acc1: 78.1250 (75.4261)  acc5: 93.7500 (91.7140)  time: 3.1556  data: 2.7868  max mem: 19733
Test:  [ 20/261]  eta: 0:07:07  loss: 1.4960 (1.5207)  acc1: 68.2292 (70.3373)  acc5: 90.6250 (89.9306)  time: 0.2515  data: 0.0173  max mem: 19733
Test:  [ 30/261]  eta: 0:04:52  loss: 1.4150 (1.4273)  acc1: 73.4375 (73.0175)  acc5: 90.1042 (90.5074)  time: 0.2256  data: 0.0167  max mem: 19733
Test:  [ 40/261]  eta: 0:03:56  loss: 1.2253 (1.4266)  acc1: 77.0833 (72.9548)  acc5: 93.2292 (90.6377)  time: 0.3313  data: 0.1781  max mem: 19733
Test:  [ 50/261]  eta: 0:03:09  loss: 1.6557 (1.5041)  acc1: 65.1042 (70.1900)  acc5: 87.5000 (89.8386)  time: 0.3270  data: 0.1760  max mem: 19733
Test:  [ 60/261]  eta: 0:02:37  loss: 1.6958 (1.5252)  acc1: 63.0208 (69.2623)  acc5: 88.5417 (89.8053)  time: 0.1907  data: 0.0150  max mem: 19733
Test:  [ 70/261]  eta: 0:02:13  loss: 1.6883 (1.5349)  acc1: 64.0625 (68.5446)  acc5: 90.6250 (90.0088)  time: 0.1966  data: 0.0155  max mem: 19733
Test:  [ 80/261]  eta: 0:02:03  loss: 1.4787 (1.5168)  acc1: 68.7500 (69.1037)  acc5: 91.1458 (90.2842)  time: 0.3862  data: 0.2063  max mem: 19733
Test:  [ 90/261]  eta: 0:01:48  loss: 1.3970 (1.4908)  acc1: 73.4375 (69.5799)  acc5: 92.7083 (90.6593)  time: 0.3920  data: 0.2067  max mem: 19733
Test:  [100/261]  eta: 0:01:34  loss: 1.2691 (1.4799)  acc1: 73.4375 (69.7710)  acc5: 91.1458 (90.6456)  time: 0.1851  data: 0.0105  max mem: 19733
Test:  [110/261]  eta: 0:01:35  loss: 1.3675 (1.4969)  acc1: 71.8750 (69.5195)  acc5: 89.5833 (90.2402)  time: 0.6436  data: 0.4808  max mem: 19733
Test:  [120/261]  eta: 0:01:24  loss: 1.7960 (1.5304)  acc1: 61.4583 (68.7500)  acc5: 84.3750 (89.6135)  time: 0.6590  data: 0.4838  max mem: 19733
Test:  [130/261]  eta: 0:01:13  loss: 1.9574 (1.5714)  acc1: 58.3333 (67.8952)  acc5: 80.7292 (88.8756)  time: 0.1717  data: 0.0134  max mem: 19733
Test:  [140/261]  eta: 0:01:04  loss: 1.8424 (1.5901)  acc1: 58.3333 (67.3057)  acc5: 83.3333 (88.6008)  time: 0.1667  data: 0.0295  max mem: 19733
Test:  [150/261]  eta: 0:00:59  loss: 1.6905 (1.5895)  acc1: 63.0208 (67.4427)  acc5: 86.4583 (88.4727)  time: 0.3329  data: 0.2020  max mem: 19733
Test:  [160/261]  eta: 0:00:51  loss: 1.6325 (1.6118)  acc1: 67.7083 (67.0775)  acc5: 85.9375 (88.0273)  time: 0.2916  data: 0.1828  max mem: 19733
Test:  [170/261]  eta: 0:00:43  loss: 2.0021 (1.6403)  acc1: 57.2917 (66.3895)  acc5: 80.2083 (87.5640)  time: 0.1027  data: 0.0075  max mem: 19733
Test:  [180/261]  eta: 0:00:37  loss: 1.9503 (1.6570)  acc1: 55.7292 (65.9732)  acc5: 81.7708 (87.3043)  time: 0.0957  data: 0.0058  max mem: 19733
Test:  [190/261]  eta: 0:00:31  loss: 1.9503 (1.6723)  acc1: 57.2917 (65.6550)  acc5: 82.8125 (87.0501)  time: 0.0812  data: 0.0028  max mem: 19733
Test:  [200/261]  eta: 0:00:25  loss: 1.9844 (1.6837)  acc1: 59.3750 (65.3814)  acc5: 82.2917 (86.8263)  time: 0.0780  data: 0.0032  max mem: 19733
Test:  [210/261]  eta: 0:00:20  loss: 1.8738 (1.6913)  acc1: 59.3750 (65.2399)  acc5: 82.8125 (86.6854)  time: 0.0990  data: 0.0240  max mem: 19733
Test:  [220/261]  eta: 0:00:16  loss: 1.9555 (1.7157)  acc1: 58.8542 (64.6564)  acc5: 81.7708 (86.3051)  time: 0.0961  data: 0.0211  max mem: 19733
Test:  [230/261]  eta: 0:00:11  loss: 2.0395 (1.7255)  acc1: 51.5625 (64.3962)  acc5: 80.7292 (86.1179)  time: 0.0738  data: 0.0001  max mem: 19733
Test:  [240/261]  eta: 0:00:07  loss: 1.7738 (1.7254)  acc1: 59.3750 (64.3002)  acc5: 83.3333 (86.1428)  time: 0.0736  data: 0.0002  max mem: 19733
Test:  [250/261]  eta: 0:00:03  loss: 1.5503 (1.7122)  acc1: 66.6667 (64.5460)  acc5: 89.0625 (86.3276)  time: 0.0737  data: 0.0002  max mem: 19733
Test:  [260/261]  eta: 0:00:00  loss: 1.3990 (1.7059)  acc1: 67.7083 (64.7120)  acc5: 91.6667 (86.4660)  time: 0.0720  data: 0.0002  max mem: 19733
Test: Total time: 0:01:30 (0.3450 s / it)
* Acc@1 64.712 Acc@5 86.466 loss 1.706
Accuracy of the network on the 50000 test images: 64.7%
Max accuracy: 64.71%
Epoch: [1]  [   0/1251]  eta: 3:16:50  lr: 0.000001  loss: 5.4234 (5.4234)  time: 9.4408  data: 8.6341  max mem: 19734
Epoch: [1]  [  10/1251]  eta: 0:28:58  lr: 0.000001  loss: 5.6771 (5.5362)  time: 1.4007  data: 0.8036  max mem: 19734
Epoch: [1]  [  20/1251]  eta: 0:19:49  lr: 0.000001  loss: 5.6363 (5.5030)  time: 0.5423  data: 0.0105  max mem: 19734
Epoch: [1]  [  30/1251]  eta: 0:16:27  lr: 0.000001  loss: 5.8129 (5.5984)  time: 0.4833  data: 0.0004  max mem: 19734
Epoch: [1]  [  40/1251]  eta: 0:14:43  lr: 0.000001  loss: 5.7755 (5.5622)  time: 0.4808  data: 0.0004  max mem: 19734
loss info: cls_loss=4.8637, ratio_loss=0.7616, pruning_loss=0.2825, mse_loss=1.2260
Epoch: [1]  [  50/1251]  eta: 0:13:37  lr: 0.000001  loss: 5.6381 (5.5467)  time: 0.4825  data: 0.0004  max mem: 19734
Epoch: [1]  [  60/1251]  eta: 0:12:55  lr: 0.000001  loss: 5.4570 (5.4840)  time: 0.4906  data: 0.0004  max mem: 19734
Epoch: [1]  [  70/1251]  eta: 0:12:21  lr: 0.000001  loss: 4.9378 (5.3861)  time: 0.4912  data: 0.0005  max mem: 19734
Epoch: [1]  [  80/1251]  eta: 0:11:53  lr: 0.000001  loss: 5.2175 (5.4108)  time: 0.4814  data: 0.0004  max mem: 19734
Epoch: [1]  [  90/1251]  eta: 0:11:33  lr: 0.000001  loss: 5.8035 (5.4109)  time: 0.4888  data: 0.0004  max mem: 19734
Epoch: [1]  [ 100/1251]  eta: 0:11:15  lr: 0.000001  loss: 5.4545 (5.4241)  time: 0.4975  data: 0.0004  max mem: 19734
Epoch: [1]  [ 110/1251]  eta: 0:10:58  lr: 0.000001  loss: 5.6596 (5.4375)  time: 0.4892  data: 0.0004  max mem: 19734
Epoch: [1]  [ 120/1251]  eta: 0:10:44  lr: 0.000001  loss: 5.6881 (5.4485)  time: 0.4830  data: 0.0004  max mem: 19734
Epoch: [1]  [ 130/1251]  eta: 0:10:31  lr: 0.000001  loss: 5.6756 (5.4544)  time: 0.4819  data: 0.0004  max mem: 19734
Epoch: [1]  [ 140/1251]  eta: 0:10:19  lr: 0.000001  loss: 5.6283 (5.4375)  time: 0.4814  data: 0.0005  max mem: 19734
loss info: cls_loss=4.8228, ratio_loss=0.7528, pruning_loss=0.2816, mse_loss=1.2504
Epoch: [1]  [ 150/1251]  eta: 0:10:07  lr: 0.000001  loss: 5.4717 (5.4394)  time: 0.4816  data: 0.0006  max mem: 19734
Epoch: [1]  [ 160/1251]  eta: 0:09:57  lr: 0.000001  loss: 5.4807 (5.4450)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [1]  [ 170/1251]  eta: 0:09:47  lr: 0.000001  loss: 5.4807 (5.4418)  time: 0.4782  data: 0.0005  max mem: 19734
Epoch: [1]  [ 180/1251]  eta: 0:09:38  lr: 0.000001  loss: 5.4590 (5.4323)  time: 0.4779  data: 0.0006  max mem: 19734
Epoch: [1]  [ 190/1251]  eta: 0:09:29  lr: 0.000001  loss: 5.3305 (5.4206)  time: 0.4775  data: 0.0007  max mem: 19734
Epoch: [1]  [ 200/1251]  eta: 0:09:21  lr: 0.000001  loss: 5.5177 (5.4152)  time: 0.4883  data: 0.0007  max mem: 19734
Epoch: [1]  [ 210/1251]  eta: 0:09:13  lr: 0.000001  loss: 5.6877 (5.4206)  time: 0.4858  data: 0.0006  max mem: 19734
Epoch: [1]  [ 220/1251]  eta: 0:09:05  lr: 0.000001  loss: 5.5527 (5.4103)  time: 0.4737  data: 0.0005  max mem: 19734
Epoch: [1]  [ 230/1251]  eta: 0:08:58  lr: 0.000001  loss: 4.9076 (5.3855)  time: 0.4770  data: 0.0005  max mem: 19734
Epoch: [1]  [ 240/1251]  eta: 0:08:52  lr: 0.000001  loss: 4.8180 (5.3626)  time: 0.4959  data: 0.0005  max mem: 19734
loss info: cls_loss=4.6783, ratio_loss=0.7440, pruning_loss=0.2834, mse_loss=1.2195
Epoch: [1]  [ 250/1251]  eta: 0:08:44  lr: 0.000001  loss: 4.9615 (5.3511)  time: 0.4941  data: 0.0004  max mem: 19734
Epoch: [1]  [ 260/1251]  eta: 0:08:37  lr: 0.000001  loss: 5.3491 (5.3616)  time: 0.4761  data: 0.0005  max mem: 19734
Epoch: [1]  [ 270/1251]  eta: 0:08:31  lr: 0.000001  loss: 5.6143 (5.3659)  time: 0.4784  data: 0.0005  max mem: 19734
Epoch: [1]  [ 280/1251]  eta: 0:08:24  lr: 0.000001  loss: 5.4984 (5.3589)  time: 0.4801  data: 0.0005  max mem: 19734
Epoch: [1]  [ 290/1251]  eta: 0:08:17  lr: 0.000001  loss: 5.5779 (5.3592)  time: 0.4783  data: 0.0006  max mem: 19734
Epoch: [1]  [ 300/1251]  eta: 0:08:11  lr: 0.000001  loss: 5.2527 (5.3430)  time: 0.4781  data: 0.0006  max mem: 19734
Epoch: [1]  [ 310/1251]  eta: 0:08:05  lr: 0.000001  loss: 5.2527 (5.3357)  time: 0.4786  data: 0.0004  max mem: 19734
Epoch: [1]  [ 320/1251]  eta: 0:07:58  lr: 0.000001  loss: 5.0850 (5.3267)  time: 0.4774  data: 0.0004  max mem: 19734
Epoch: [1]  [ 330/1251]  eta: 0:07:52  lr: 0.000001  loss: 5.0850 (5.3193)  time: 0.4772  data: 0.0004  max mem: 19734
Epoch: [1]  [ 340/1251]  eta: 0:07:46  lr: 0.000001  loss: 5.4693 (5.3243)  time: 0.4764  data: 0.0005  max mem: 19734
loss info: cls_loss=4.7348, ratio_loss=0.7349, pruning_loss=0.2830, mse_loss=1.2415
Epoch: [1]  [ 350/1251]  eta: 0:07:40  lr: 0.000001  loss: 5.6042 (5.3320)  time: 0.4803  data: 0.0004  max mem: 19734
Epoch: [1]  [ 360/1251]  eta: 0:07:34  lr: 0.000001  loss: 5.6260 (5.3331)  time: 0.4815  data: 0.0004  max mem: 19734
Epoch: [1]  [ 370/1251]  eta: 0:07:28  lr: 0.000001  loss: 5.3078 (5.3271)  time: 0.4763  data: 0.0004  max mem: 19734
Epoch: [1]  [ 380/1251]  eta: 0:07:23  lr: 0.000001  loss: 5.3888 (5.3291)  time: 0.4792  data: 0.0004  max mem: 19734
Epoch: [1]  [ 390/1251]  eta: 0:07:17  lr: 0.000001  loss: 5.3947 (5.3263)  time: 0.4916  data: 0.0004  max mem: 19734
Epoch: [1]  [ 400/1251]  eta: 0:07:12  lr: 0.000001  loss: 5.0852 (5.3056)  time: 0.4886  data: 0.0004  max mem: 19734
Epoch: [1]  [ 410/1251]  eta: 0:07:06  lr: 0.000001  loss: 4.6207 (5.2941)  time: 0.4772  data: 0.0004  max mem: 19734
Epoch: [1]  [ 420/1251]  eta: 0:07:00  lr: 0.000001  loss: 4.8327 (5.2883)  time: 0.4781  data: 0.0004  max mem: 19734
Epoch: [1]  [ 430/1251]  eta: 0:06:55  lr: 0.000001  loss: 5.5013 (5.2875)  time: 0.4780  data: 0.0004  max mem: 19734
Epoch: [1]  [ 440/1251]  eta: 0:06:49  lr: 0.000001  loss: 5.6356 (5.2896)  time: 0.4774  data: 0.0005  max mem: 19734
loss info: cls_loss=4.6234, ratio_loss=0.7270, pruning_loss=0.2823, mse_loss=1.2121
Epoch: [1]  [ 450/1251]  eta: 0:06:44  lr: 0.000001  loss: 5.6359 (5.2949)  time: 0.4786  data: 0.0006  max mem: 19734
Epoch: [1]  [ 460/1251]  eta: 0:06:38  lr: 0.000001  loss: 5.4291 (5.2893)  time: 0.4781  data: 0.0006  max mem: 19734
Epoch: [1]  [ 470/1251]  eta: 0:06:33  lr: 0.000001  loss: 5.0194 (5.2840)  time: 0.4770  data: 0.0006  max mem: 19734
Epoch: [1]  [ 480/1251]  eta: 0:06:27  lr: 0.000001  loss: 5.6340 (5.2893)  time: 0.4762  data: 0.0005  max mem: 19734
Epoch: [1]  [ 490/1251]  eta: 0:06:22  lr: 0.000001  loss: 5.6340 (5.2842)  time: 0.4774  data: 0.0006  max mem: 19734
Epoch: [1]  [ 500/1251]  eta: 0:06:17  lr: 0.000001  loss: 4.8004 (5.2768)  time: 0.4857  data: 0.0005  max mem: 19734
Epoch: [1]  [ 510/1251]  eta: 0:06:11  lr: 0.000001  loss: 4.9660 (5.2744)  time: 0.4842  data: 0.0005  max mem: 19734
Epoch: [1]  [ 520/1251]  eta: 0:06:06  lr: 0.000001  loss: 5.2143 (5.2728)  time: 0.4775  data: 0.0004  max mem: 19734
Epoch: [1]  [ 530/1251]  eta: 0:06:01  lr: 0.000001  loss: 5.5185 (5.2755)  time: 0.4967  data: 0.0004  max mem: 19734
Epoch: [1]  [ 540/1251]  eta: 0:05:56  lr: 0.000001  loss: 5.3461 (5.2711)  time: 0.4949  data: 0.0004  max mem: 19734
loss info: cls_loss=4.6614, ratio_loss=0.7152, pruning_loss=0.2810, mse_loss=1.2313
Epoch: [1]  [ 550/1251]  eta: 0:05:50  lr: 0.000001  loss: 5.2980 (5.2734)  time: 0.4757  data: 0.0004  max mem: 19734
Epoch: [1]  [ 560/1251]  eta: 0:05:45  lr: 0.000001  loss: 5.3175 (5.2699)  time: 0.4767  data: 0.0003  max mem: 19734
Epoch: [1]  [ 570/1251]  eta: 0:05:40  lr: 0.000001  loss: 5.0491 (5.2672)  time: 0.4761  data: 0.0004  max mem: 19734
Epoch: [1]  [ 580/1251]  eta: 0:05:34  lr: 0.000001  loss: 5.4525 (5.2712)  time: 0.4765  data: 0.0004  max mem: 19734
Epoch: [1]  [ 590/1251]  eta: 0:05:29  lr: 0.000001  loss: 5.4522 (5.2662)  time: 0.4774  data: 0.0004  max mem: 19734
Epoch: [1]  [ 600/1251]  eta: 0:05:24  lr: 0.000001  loss: 4.9886 (5.2597)  time: 0.4765  data: 0.0004  max mem: 19734
Epoch: [1]  [ 610/1251]  eta: 0:05:19  lr: 0.000001  loss: 4.9886 (5.2580)  time: 0.4754  data: 0.0005  max mem: 19734
Epoch: [1]  [ 620/1251]  eta: 0:05:14  lr: 0.000001  loss: 4.9666 (5.2507)  time: 0.4752  data: 0.0005  max mem: 19734
Epoch: [1]  [ 630/1251]  eta: 0:05:08  lr: 0.000001  loss: 5.4098 (5.2560)  time: 0.4764  data: 0.0004  max mem: 19734
Epoch: [1]  [ 640/1251]  eta: 0:05:03  lr: 0.000001  loss: 5.3745 (5.2474)  time: 0.4763  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5582, ratio_loss=0.7029, pruning_loss=0.2804, mse_loss=1.2339
Epoch: [1]  [ 650/1251]  eta: 0:04:58  lr: 0.000001  loss: 4.9729 (5.2451)  time: 0.4837  data: 0.0004  max mem: 19734
Epoch: [1]  [ 660/1251]  eta: 0:04:53  lr: 0.000001  loss: 5.2894 (5.2454)  time: 0.4858  data: 0.0004  max mem: 19734
Epoch: [1]  [ 670/1251]  eta: 0:04:48  lr: 0.000001  loss: 4.9388 (5.2353)  time: 0.4958  data: 0.0005  max mem: 19734
Epoch: [1]  [ 680/1251]  eta: 0:04:43  lr: 0.000001  loss: 4.7168 (5.2297)  time: 0.4947  data: 0.0005  max mem: 19734
Epoch: [1]  [ 690/1251]  eta: 0:04:38  lr: 0.000001  loss: 4.8431 (5.2258)  time: 0.4776  data: 0.0004  max mem: 19734
Epoch: [1]  [ 700/1251]  eta: 0:04:33  lr: 0.000001  loss: 5.0768 (5.2207)  time: 0.4773  data: 0.0005  max mem: 19734
Epoch: [1]  [ 710/1251]  eta: 0:04:28  lr: 0.000001  loss: 5.1353 (5.2171)  time: 0.4779  data: 0.0006  max mem: 19734
Epoch: [1]  [ 720/1251]  eta: 0:04:23  lr: 0.000001  loss: 4.9805 (5.2097)  time: 0.4788  data: 0.0006  max mem: 19734
Epoch: [1]  [ 730/1251]  eta: 0:04:18  lr: 0.000001  loss: 4.6240 (5.2045)  time: 0.4786  data: 0.0005  max mem: 19734
Epoch: [1]  [ 740/1251]  eta: 0:04:13  lr: 0.000001  loss: 5.0101 (5.2016)  time: 0.4794  data: 0.0005  max mem: 19734
loss info: cls_loss=4.3833, ratio_loss=0.6918, pruning_loss=0.2894, mse_loss=1.2455
Epoch: [1]  [ 750/1251]  eta: 0:04:07  lr: 0.000001  loss: 5.1291 (5.1974)  time: 0.4791  data: 0.0004  max mem: 19734
Epoch: [1]  [ 760/1251]  eta: 0:04:02  lr: 0.000001  loss: 5.2402 (5.2018)  time: 0.4779  data: 0.0004  max mem: 19734
Epoch: [1]  [ 770/1251]  eta: 0:03:57  lr: 0.000001  loss: 5.4076 (5.1989)  time: 0.4772  data: 0.0004  max mem: 19734
Epoch: [1]  [ 780/1251]  eta: 0:03:52  lr: 0.000001  loss: 5.4186 (5.2002)  time: 0.4769  data: 0.0006  max mem: 19734
Epoch: [1]  [ 790/1251]  eta: 0:03:47  lr: 0.000001  loss: 5.4876 (5.1989)  time: 0.4764  data: 0.0007  max mem: 19734
Epoch: [1]  [ 800/1251]  eta: 0:03:42  lr: 0.000001  loss: 5.0341 (5.1956)  time: 0.4827  data: 0.0006  max mem: 19734
Epoch: [1]  [ 810/1251]  eta: 0:03:37  lr: 0.000001  loss: 4.8840 (5.1896)  time: 0.4844  data: 0.0004  max mem: 19734
Epoch: [1]  [ 820/1251]  eta: 0:03:32  lr: 0.000001  loss: 4.9880 (5.1912)  time: 0.4978  data: 0.0004  max mem: 19734
Epoch: [1]  [ 830/1251]  eta: 0:03:27  lr: 0.000001  loss: 5.0405 (5.1859)  time: 0.4940  data: 0.0004  max mem: 19734
Epoch: [1]  [ 840/1251]  eta: 0:03:22  lr: 0.000001  loss: 5.0668 (5.1881)  time: 0.4758  data: 0.0005  max mem: 19734
loss info: cls_loss=4.5846, ratio_loss=0.6791, pruning_loss=0.2753, mse_loss=1.2007
Epoch: [1]  [ 850/1251]  eta: 0:03:17  lr: 0.000001  loss: 5.3056 (5.1850)  time: 0.4785  data: 0.0005  max mem: 19734
Epoch: [1]  [ 860/1251]  eta: 0:03:12  lr: 0.000001  loss: 5.1578 (5.1839)  time: 0.4761  data: 0.0005  max mem: 19734
Epoch: [1]  [ 870/1251]  eta: 0:03:07  lr: 0.000001  loss: 5.3894 (5.1818)  time: 0.4774  data: 0.0004  max mem: 19734
Epoch: [1]  [ 880/1251]  eta: 0:03:02  lr: 0.000001  loss: 5.2717 (5.1818)  time: 0.4799  data: 0.0004  max mem: 19734
Epoch: [1]  [ 890/1251]  eta: 0:02:57  lr: 0.000001  loss: 5.4066 (5.1829)  time: 0.4783  data: 0.0005  max mem: 19734
Epoch: [1]  [ 900/1251]  eta: 0:02:52  lr: 0.000001  loss: 5.2097 (5.1801)  time: 0.4761  data: 0.0007  max mem: 19734
Epoch: [1]  [ 910/1251]  eta: 0:02:47  lr: 0.000001  loss: 5.1526 (5.1792)  time: 0.4765  data: 0.0006  max mem: 19734
Epoch: [1]  [ 920/1251]  eta: 0:02:42  lr: 0.000001  loss: 5.2476 (5.1786)  time: 0.4757  data: 0.0005  max mem: 19734
Epoch: [1]  [ 930/1251]  eta: 0:02:37  lr: 0.000001  loss: 5.1204 (5.1769)  time: 0.4751  data: 0.0004  max mem: 19734
Epoch: [1]  [ 940/1251]  eta: 0:02:32  lr: 0.000001  loss: 5.3139 (5.1765)  time: 0.4768  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5621, ratio_loss=0.6642, pruning_loss=0.2765, mse_loss=1.2220
Epoch: [1]  [ 950/1251]  eta: 0:02:28  lr: 0.000001  loss: 5.1197 (5.1721)  time: 0.4827  data: 0.0004  max mem: 19734
Epoch: [1]  [ 960/1251]  eta: 0:02:23  lr: 0.000001  loss: 4.8535 (5.1696)  time: 0.4982  data: 0.0004  max mem: 19734
Epoch: [1]  [ 970/1251]  eta: 0:02:18  lr: 0.000001  loss: 4.7638 (5.1645)  time: 0.4931  data: 0.0005  max mem: 19734
Epoch: [1]  [ 980/1251]  eta: 0:02:13  lr: 0.000001  loss: 4.5652 (5.1591)  time: 0.4775  data: 0.0004  max mem: 19734
Epoch: [1]  [ 990/1251]  eta: 0:02:08  lr: 0.000001  loss: 4.7634 (5.1561)  time: 0.4760  data: 0.0004  max mem: 19734
Epoch: [1]  [1000/1251]  eta: 0:02:03  lr: 0.000001  loss: 4.7634 (5.1525)  time: 0.4769  data: 0.0004  max mem: 19734
Epoch: [1]  [1010/1251]  eta: 0:01:58  lr: 0.000001  loss: 4.7109 (5.1489)  time: 0.4784  data: 0.0004  max mem: 19734
Epoch: [1]  [1020/1251]  eta: 0:01:53  lr: 0.000001  loss: 4.7991 (5.1475)  time: 0.4757  data: 0.0004  max mem: 19734
Epoch: [1]  [1030/1251]  eta: 0:01:48  lr: 0.000001  loss: 5.1510 (5.1472)  time: 0.4761  data: 0.0005  max mem: 19734
Epoch: [1]  [1040/1251]  eta: 0:01:43  lr: 0.000001  loss: 5.1510 (5.1446)  time: 0.4776  data: 0.0005  max mem: 19734
loss info: cls_loss=4.3607, ratio_loss=0.6482, pruning_loss=0.2805, mse_loss=1.2300
Epoch: [1]  [1050/1251]  eta: 0:01:38  lr: 0.000001  loss: 4.7360 (5.1389)  time: 0.4769  data: 0.0004  max mem: 19734
Epoch: [1]  [1060/1251]  eta: 0:01:33  lr: 0.000001  loss: 4.9048 (5.1366)  time: 0.4774  data: 0.0004  max mem: 19734
Epoch: [1]  [1070/1251]  eta: 0:01:28  lr: 0.000001  loss: 5.1414 (5.1368)  time: 0.4769  data: 0.0004  max mem: 19734
Epoch: [1]  [1080/1251]  eta: 0:01:23  lr: 0.000001  loss: 5.3345 (5.1375)  time: 0.4752  data: 0.0004  max mem: 19734
Epoch: [1]  [1090/1251]  eta: 0:01:18  lr: 0.000001  loss: 5.1240 (5.1345)  time: 0.4751  data: 0.0004  max mem: 19734
Epoch: [1]  [1100/1251]  eta: 0:01:14  lr: 0.000001  loss: 4.8823 (5.1323)  time: 0.4920  data: 0.0006  max mem: 19734
Epoch: [1]  [1110/1251]  eta: 0:01:09  lr: 0.000001  loss: 4.8548 (5.1286)  time: 0.5040  data: 0.0006  max mem: 19734
Epoch: [1]  [1120/1251]  eta: 0:01:04  lr: 0.000001  loss: 4.8192 (5.1254)  time: 0.4871  data: 0.0005  max mem: 19734
Epoch: [1]  [1130/1251]  eta: 0:00:59  lr: 0.000001  loss: 4.9774 (5.1239)  time: 0.4762  data: 0.0006  max mem: 19734
Epoch: [1]  [1140/1251]  eta: 0:00:54  lr: 0.000001  loss: 4.8932 (5.1193)  time: 0.4762  data: 0.0006  max mem: 19734
loss info: cls_loss=4.3981, ratio_loss=0.6335, pruning_loss=0.2782, mse_loss=1.2219
Epoch: [1]  [1150/1251]  eta: 0:00:49  lr: 0.000001  loss: 4.5874 (5.1181)  time: 0.4749  data: 0.0005  max mem: 19734
Epoch: [1]  [1160/1251]  eta: 0:00:44  lr: 0.000001  loss: 5.2385 (5.1173)  time: 0.4761  data: 0.0004  max mem: 19734
Epoch: [1]  [1170/1251]  eta: 0:00:39  lr: 0.000001  loss: 4.8031 (5.1123)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [1]  [1180/1251]  eta: 0:00:34  lr: 0.000001  loss: 4.9772 (5.1108)  time: 0.4767  data: 0.0005  max mem: 19734
Epoch: [1]  [1190/1251]  eta: 0:00:29  lr: 0.000001  loss: 5.0897 (5.1096)  time: 0.4746  data: 0.0009  max mem: 19734
Epoch: [1]  [1200/1251]  eta: 0:00:24  lr: 0.000001  loss: 5.0934 (5.1098)  time: 0.4691  data: 0.0007  max mem: 19734
Epoch: [1]  [1210/1251]  eta: 0:00:20  lr: 0.000001  loss: 5.2505 (5.1070)  time: 0.4655  data: 0.0002  max mem: 19734
Epoch: [1]  [1220/1251]  eta: 0:00:15  lr: 0.000001  loss: 5.2339 (5.1041)  time: 0.4654  data: 0.0002  max mem: 19734
Epoch: [1]  [1230/1251]  eta: 0:00:10  lr: 0.000001  loss: 4.5756 (5.1006)  time: 0.4654  data: 0.0002  max mem: 19734
Epoch: [1]  [1240/1251]  eta: 0:00:05  lr: 0.000001  loss: 5.0769 (5.1014)  time: 0.4752  data: 0.0002  max mem: 19734
loss info: cls_loss=4.4117, ratio_loss=0.6167, pruning_loss=0.2749, mse_loss=1.2040
Epoch: [1]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 5.0765 (5.0967)  time: 0.4868  data: 0.0002  max mem: 19734
Epoch: [1] Total time: 0:10:12 (0.4892 s / it)
Averaged stats: lr: 0.000001  loss: 5.0765 (5.1018)
Test:  [  0/261]  eta: 1:21:59  loss: 1.4610 (1.4610)  acc1: 69.7917 (69.7917)  acc5: 90.1042 (90.1042)  time: 18.8472  data: 18.3788  max mem: 19734
Test:  [ 10/261]  eta: 0:07:52  loss: 1.3649 (1.3824)  acc1: 76.5625 (72.9640)  acc5: 91.1458 (90.6250)  time: 1.8838  data: 1.7013  max mem: 19734
Test:  [ 20/261]  eta: 0:04:17  loss: 1.5245 (1.5663)  acc1: 66.6667 (68.0804)  acc5: 89.0625 (88.2441)  time: 0.1790  data: 0.0288  max mem: 19734
Test:  [ 30/261]  eta: 0:02:59  loss: 1.5053 (1.4791)  acc1: 69.2708 (70.3965)  acc5: 88.5417 (88.9449)  time: 0.1709  data: 0.0140  max mem: 19734
Test:  [ 40/261]  eta: 0:02:43  loss: 1.3086 (1.4794)  acc1: 74.4792 (70.2744)  acc5: 90.6250 (88.8847)  time: 0.3976  data: 0.2245  max mem: 19734
Test:  [ 50/261]  eta: 0:02:11  loss: 1.7361 (1.5697)  acc1: 63.0208 (67.5041)  acc5: 85.4167 (88.0923)  time: 0.3789  data: 0.2275  max mem: 19734
Test:  [ 60/261]  eta: 0:01:49  loss: 1.7995 (1.6014)  acc1: 59.8958 (66.5215)  acc5: 85.4167 (87.9269)  time: 0.1403  data: 0.0093  max mem: 19734
Test:  [ 70/261]  eta: 0:01:40  loss: 1.7838 (1.6115)  acc1: 62.5000 (65.9991)  acc5: 88.5417 (88.1675)  time: 0.2906  data: 0.1507  max mem: 19734
Test:  [ 80/261]  eta: 0:01:27  loss: 1.5619 (1.5930)  acc1: 67.1875 (66.5059)  acc5: 89.0625 (88.4002)  time: 0.3078  data: 0.1527  max mem: 19734
Test:  [ 90/261]  eta: 0:01:17  loss: 1.4268 (1.5637)  acc1: 72.3958 (67.1303)  acc5: 89.5833 (88.8164)  time: 0.1797  data: 0.0101  max mem: 19734
Test:  [100/261]  eta: 0:01:14  loss: 1.3791 (1.5549)  acc1: 71.3542 (67.3267)  acc5: 90.6250 (88.8511)  time: 0.3720  data: 0.2022  max mem: 19734
Test:  [110/261]  eta: 0:01:05  loss: 1.4473 (1.5675)  acc1: 69.7917 (67.1547)  acc5: 86.9792 (88.5088)  time: 0.3684  data: 0.2040  max mem: 19734
Test:  [120/261]  eta: 0:00:58  loss: 1.8151 (1.5957)  acc1: 61.4583 (66.5591)  acc5: 81.7708 (87.9606)  time: 0.1755  data: 0.0126  max mem: 19734
Test:  [130/261]  eta: 0:00:55  loss: 1.9606 (1.6331)  acc1: 56.7708 (65.7323)  acc5: 80.7292 (87.3171)  time: 0.3307  data: 0.1821  max mem: 19734
Test:  [140/261]  eta: 0:00:51  loss: 1.9624 (1.6487)  acc1: 54.6875 (65.2667)  acc5: 82.2917 (87.1085)  time: 0.4959  data: 0.3253  max mem: 19734
Test:  [150/261]  eta: 0:00:46  loss: 1.7009 (1.6460)  acc1: 63.0208 (65.3801)  acc5: 85.4167 (87.0171)  time: 0.3789  data: 0.1588  max mem: 19734
Test:  [160/261]  eta: 0:00:42  loss: 1.6900 (1.6659)  acc1: 65.1042 (65.0104)  acc5: 83.8542 (86.6395)  time: 0.3888  data: 0.1867  max mem: 19734
Test:  [170/261]  eta: 0:00:38  loss: 1.9656 (1.6932)  acc1: 56.7708 (64.4280)  acc5: 80.2083 (86.2208)  time: 0.4925  data: 0.2930  max mem: 19734
Test:  [180/261]  eta: 0:00:33  loss: 2.0028 (1.7109)  acc1: 54.6875 (64.0510)  acc5: 80.2083 (86.0037)  time: 0.3368  data: 0.1243  max mem: 19734
Test:  [190/261]  eta: 0:00:28  loss: 2.0028 (1.7242)  acc1: 58.3333 (63.7789)  acc5: 81.7708 (85.7548)  time: 0.1967  data: 0.0190  max mem: 19734
Test:  [200/261]  eta: 0:00:24  loss: 2.0077 (1.7357)  acc1: 58.3333 (63.5235)  acc5: 81.2500 (85.5255)  time: 0.2856  data: 0.1367  max mem: 19734
Test:  [210/261]  eta: 0:00:19  loss: 1.9499 (1.7424)  acc1: 59.3750 (63.4627)  acc5: 81.7708 (85.3747)  time: 0.2405  data: 0.1272  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.9931 (1.7654)  acc1: 58.3333 (62.8936)  acc5: 80.2083 (85.0302)  time: 0.0851  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 2.1141 (1.7739)  acc1: 51.5625 (62.6984)  acc5: 79.1667 (84.8846)  time: 0.0799  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.8421 (1.7739)  acc1: 58.3333 (62.6275)  acc5: 82.8125 (84.8742)  time: 0.0892  data: 0.0154  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.5547 (1.7581)  acc1: 66.1458 (62.9171)  acc5: 87.5000 (85.0992)  time: 0.0891  data: 0.0154  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.3756 (1.7505)  acc1: 66.1458 (63.0880)  acc5: 90.6250 (85.2420)  time: 0.0717  data: 0.0002  max mem: 19734
Test: Total time: 0:01:25 (0.3279 s / it)
* Acc@1 63.088 Acc@5 85.242 loss 1.750
Accuracy of the network on the 50000 test images: 63.1%
Max accuracy: 64.71%
Epoch: [2]  [   0/1251]  eta: 5:54:11  lr: 0.000005  loss: 4.4232 (4.4232)  time: 16.9875  data: 11.4762  max mem: 19734
Epoch: [2]  [  10/1251]  eta: 0:42:24  lr: 0.000005  loss: 4.4232 (4.7105)  time: 2.0505  data: 1.0449  max mem: 19734
Epoch: [2]  [  20/1251]  eta: 0:26:45  lr: 0.000005  loss: 4.8639 (4.8140)  time: 0.5204  data: 0.0012  max mem: 19734
Epoch: [2]  [  30/1251]  eta: 0:21:11  lr: 0.000005  loss: 5.1533 (4.8844)  time: 0.4867  data: 0.0005  max mem: 19734
Epoch: [2]  [  40/1251]  eta: 0:18:16  lr: 0.000005  loss: 5.1501 (4.8563)  time: 0.4868  data: 0.0004  max mem: 19734
Epoch: [2]  [  50/1251]  eta: 0:16:28  lr: 0.000005  loss: 5.1501 (4.8880)  time: 0.4834  data: 0.0004  max mem: 19734
Epoch: [2]  [  60/1251]  eta: 0:15:13  lr: 0.000005  loss: 5.1677 (4.9204)  time: 0.4830  data: 0.0004  max mem: 19734
Epoch: [2]  [  70/1251]  eta: 0:14:18  lr: 0.000005  loss: 5.0959 (4.9072)  time: 0.4817  data: 0.0004  max mem: 19734
Epoch: [2]  [  80/1251]  eta: 0:13:35  lr: 0.000005  loss: 4.7601 (4.8885)  time: 0.4812  data: 0.0010  max mem: 19734
Epoch: [2]  [  90/1251]  eta: 0:13:00  lr: 0.000005  loss: 4.7345 (4.8460)  time: 0.4811  data: 0.0010  max mem: 19734
loss info: cls_loss=4.4012, ratio_loss=0.5595, pruning_loss=0.2670, mse_loss=1.2270
Epoch: [2]  [ 100/1251]  eta: 0:12:34  lr: 0.000005  loss: 4.7385 (4.8324)  time: 0.4908  data: 0.0005  max mem: 19734
Epoch: [2]  [ 110/1251]  eta: 0:12:10  lr: 0.000005  loss: 4.4966 (4.7748)  time: 0.4920  data: 0.0005  max mem: 19734
Epoch: [2]  [ 120/1251]  eta: 0:11:49  lr: 0.000005  loss: 4.5117 (4.7702)  time: 0.4813  data: 0.0005  max mem: 19734
Epoch: [2]  [ 130/1251]  eta: 0:11:31  lr: 0.000005  loss: 4.6454 (4.7509)  time: 0.4885  data: 0.0005  max mem: 19734
Epoch: [2]  [ 140/1251]  eta: 0:11:16  lr: 0.000005  loss: 4.6454 (4.7445)  time: 0.5029  data: 0.0005  max mem: 19734
Epoch: [2]  [ 150/1251]  eta: 0:11:01  lr: 0.000005  loss: 4.6598 (4.7112)  time: 0.4976  data: 0.0005  max mem: 19734
Epoch: [2]  [ 160/1251]  eta: 0:10:47  lr: 0.000005  loss: 4.6541 (4.7057)  time: 0.4852  data: 0.0006  max mem: 19734
Epoch: [2]  [ 170/1251]  eta: 0:10:35  lr: 0.000005  loss: 4.6541 (4.6892)  time: 0.4837  data: 0.0005  max mem: 19734
Epoch: [2]  [ 180/1251]  eta: 0:10:22  lr: 0.000005  loss: 4.7057 (4.6847)  time: 0.4831  data: 0.0004  max mem: 19734
Epoch: [2]  [ 190/1251]  eta: 0:10:11  lr: 0.000005  loss: 4.7615 (4.6743)  time: 0.4808  data: 0.0005  max mem: 19734
loss info: cls_loss=4.1735, ratio_loss=0.4321, pruning_loss=0.2592, mse_loss=1.1968
Epoch: [2]  [ 200/1251]  eta: 0:10:00  lr: 0.000005  loss: 4.5511 (4.6699)  time: 0.4792  data: 0.0007  max mem: 19734
Epoch: [2]  [ 210/1251]  eta: 0:09:50  lr: 0.000005  loss: 4.5342 (4.6638)  time: 0.4802  data: 0.0007  max mem: 19734
Epoch: [2]  [ 220/1251]  eta: 0:09:40  lr: 0.000005  loss: 4.4794 (4.6437)  time: 0.4804  data: 0.0006  max mem: 19734
Epoch: [2]  [ 230/1251]  eta: 0:09:31  lr: 0.000005  loss: 4.4430 (4.6401)  time: 0.4792  data: 0.0005  max mem: 19734
Epoch: [2]  [ 240/1251]  eta: 0:09:22  lr: 0.000005  loss: 4.2252 (4.6093)  time: 0.4776  data: 0.0005  max mem: 19734
Epoch: [2]  [ 250/1251]  eta: 0:09:14  lr: 0.000005  loss: 3.9759 (4.5892)  time: 0.4888  data: 0.0005  max mem: 19734
Epoch: [2]  [ 260/1251]  eta: 0:09:06  lr: 0.000005  loss: 4.2143 (4.5710)  time: 0.4896  data: 0.0005  max mem: 19734
Epoch: [2]  [ 270/1251]  eta: 0:08:58  lr: 0.000005  loss: 4.4734 (4.5672)  time: 0.4807  data: 0.0005  max mem: 19734
Epoch: [2]  [ 280/1251]  eta: 0:08:52  lr: 0.000005  loss: 4.4182 (4.5465)  time: 0.5148  data: 0.0005  max mem: 19734
Epoch: [2]  [ 290/1251]  eta: 0:08:44  lr: 0.000005  loss: 3.7431 (4.5148)  time: 0.5116  data: 0.0005  max mem: 19734
loss info: cls_loss=3.9376, ratio_loss=0.2779, pruning_loss=0.2570, mse_loss=1.1983
Epoch: [2]  [ 300/1251]  eta: 0:08:37  lr: 0.000005  loss: 3.8448 (4.4982)  time: 0.4771  data: 0.0005  max mem: 19734
Epoch: [2]  [ 310/1251]  eta: 0:08:29  lr: 0.000005  loss: 4.1338 (4.4820)  time: 0.4784  data: 0.0005  max mem: 19734
Epoch: [2]  [ 320/1251]  eta: 0:08:22  lr: 0.000005  loss: 4.1111 (4.4697)  time: 0.4775  data: 0.0005  max mem: 19734
Epoch: [2]  [ 330/1251]  eta: 0:08:15  lr: 0.000005  loss: 3.8939 (4.4523)  time: 0.4773  data: 0.0005  max mem: 19734
Epoch: [2]  [ 340/1251]  eta: 0:08:08  lr: 0.000005  loss: 3.6983 (4.4342)  time: 0.4794  data: 0.0004  max mem: 19734
Epoch: [2]  [ 350/1251]  eta: 0:08:01  lr: 0.000005  loss: 3.6983 (4.4145)  time: 0.4785  data: 0.0004  max mem: 19734
Epoch: [2]  [ 360/1251]  eta: 0:07:54  lr: 0.000005  loss: 3.5852 (4.3811)  time: 0.4758  data: 0.0004  max mem: 19734
Epoch: [2]  [ 370/1251]  eta: 0:07:47  lr: 0.000005  loss: 3.3346 (4.3643)  time: 0.4750  data: 0.0004  max mem: 19734
Epoch: [2]  [ 380/1251]  eta: 0:07:41  lr: 0.000005  loss: 3.9675 (4.3512)  time: 0.4760  data: 0.0004  max mem: 19734
Epoch: [2]  [ 390/1251]  eta: 0:07:35  lr: 0.000005  loss: 3.7917 (4.3318)  time: 0.4859  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6515, ratio_loss=0.1434, pruning_loss=0.2538, mse_loss=1.2163
Epoch: [2]  [ 400/1251]  eta: 0:07:28  lr: 0.000005  loss: 3.7936 (4.3196)  time: 0.4844  data: 0.0005  max mem: 19734
Epoch: [2]  [ 410/1251]  eta: 0:07:22  lr: 0.000005  loss: 3.9094 (4.3031)  time: 0.4749  data: 0.0005  max mem: 19734
Epoch: [2]  [ 420/1251]  eta: 0:07:16  lr: 0.000005  loss: 3.9094 (4.2933)  time: 0.4838  data: 0.0004  max mem: 19734
Epoch: [2]  [ 430/1251]  eta: 0:07:11  lr: 0.000005  loss: 4.0012 (4.2859)  time: 0.5039  data: 0.0004  max mem: 19734
Epoch: [2]  [ 440/1251]  eta: 0:07:05  lr: 0.000005  loss: 4.0345 (4.2791)  time: 0.4975  data: 0.0004  max mem: 19734
Epoch: [2]  [ 450/1251]  eta: 0:06:58  lr: 0.000005  loss: 3.8463 (4.2619)  time: 0.4768  data: 0.0005  max mem: 19734
Epoch: [2]  [ 460/1251]  eta: 0:06:52  lr: 0.000005  loss: 3.5956 (4.2478)  time: 0.4759  data: 0.0005  max mem: 19734
Epoch: [2]  [ 470/1251]  eta: 0:06:47  lr: 0.000005  loss: 3.8132 (4.2413)  time: 0.4783  data: 0.0004  max mem: 19734
Epoch: [2]  [ 480/1251]  eta: 0:06:41  lr: 0.000005  loss: 3.8760 (4.2277)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [2]  [ 490/1251]  eta: 0:06:35  lr: 0.000005  loss: 3.9026 (4.2237)  time: 0.4768  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6825, ratio_loss=0.0640, pruning_loss=0.2462, mse_loss=1.1892
Epoch: [2]  [ 500/1251]  eta: 0:06:29  lr: 0.000005  loss: 4.0176 (4.2129)  time: 0.4759  data: 0.0004  max mem: 19734
Epoch: [2]  [ 510/1251]  eta: 0:06:23  lr: 0.000005  loss: 3.8531 (4.2055)  time: 0.4766  data: 0.0004  max mem: 19734
Epoch: [2]  [ 520/1251]  eta: 0:06:17  lr: 0.000005  loss: 3.7753 (4.1949)  time: 0.4766  data: 0.0005  max mem: 19734
Epoch: [2]  [ 530/1251]  eta: 0:06:12  lr: 0.000005  loss: 3.7705 (4.1831)  time: 0.4767  data: 0.0004  max mem: 19734
Epoch: [2]  [ 540/1251]  eta: 0:06:06  lr: 0.000005  loss: 3.8953 (4.1752)  time: 0.4837  data: 0.0004  max mem: 19734
Epoch: [2]  [ 550/1251]  eta: 0:06:00  lr: 0.000005  loss: 3.9242 (4.1689)  time: 0.4841  data: 0.0004  max mem: 19734
Epoch: [2]  [ 560/1251]  eta: 0:05:55  lr: 0.000005  loss: 3.7125 (4.1576)  time: 0.4784  data: 0.0004  max mem: 19734
Epoch: [2]  [ 570/1251]  eta: 0:05:50  lr: 0.000005  loss: 3.6319 (4.1490)  time: 0.5089  data: 0.0005  max mem: 19734
Epoch: [2]  [ 580/1251]  eta: 0:05:44  lr: 0.000005  loss: 3.6319 (4.1371)  time: 0.5078  data: 0.0005  max mem: 19734
Epoch: [2]  [ 590/1251]  eta: 0:05:39  lr: 0.000005  loss: 3.6665 (4.1291)  time: 0.4767  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6118, ratio_loss=0.0298, pruning_loss=0.2463, mse_loss=1.2207
Epoch: [2]  [ 600/1251]  eta: 0:05:33  lr: 0.000005  loss: 3.8486 (4.1217)  time: 0.4775  data: 0.0005  max mem: 19734
Epoch: [2]  [ 610/1251]  eta: 0:05:28  lr: 0.000005  loss: 3.7439 (4.1135)  time: 0.4784  data: 0.0004  max mem: 19734
Epoch: [2]  [ 620/1251]  eta: 0:05:22  lr: 0.000005  loss: 3.7003 (4.1081)  time: 0.4777  data: 0.0004  max mem: 19734
Epoch: [2]  [ 630/1251]  eta: 0:05:17  lr: 0.000005  loss: 3.7271 (4.1021)  time: 0.4781  data: 0.0004  max mem: 19734
Epoch: [2]  [ 640/1251]  eta: 0:05:12  lr: 0.000005  loss: 3.3714 (4.0866)  time: 0.4785  data: 0.0006  max mem: 19734
Epoch: [2]  [ 650/1251]  eta: 0:05:06  lr: 0.000005  loss: 3.3599 (4.0789)  time: 0.4767  data: 0.0006  max mem: 19734
Epoch: [2]  [ 660/1251]  eta: 0:05:01  lr: 0.000005  loss: 3.7027 (4.0711)  time: 0.4770  data: 0.0005  max mem: 19734
Epoch: [2]  [ 670/1251]  eta: 0:04:55  lr: 0.000005  loss: 3.5054 (4.0643)  time: 0.4775  data: 0.0004  max mem: 19734
Epoch: [2]  [ 680/1251]  eta: 0:04:50  lr: 0.000005  loss: 3.6185 (4.0573)  time: 0.4762  data: 0.0004  max mem: 19734
Epoch: [2]  [ 690/1251]  eta: 0:04:45  lr: 0.000005  loss: 3.6118 (4.0485)  time: 0.4829  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5047, ratio_loss=0.0155, pruning_loss=0.2442, mse_loss=1.1876
Epoch: [2]  [ 700/1251]  eta: 0:04:39  lr: 0.000005  loss: 3.7627 (4.0443)  time: 0.4832  data: 0.0008  max mem: 19734
Epoch: [2]  [ 710/1251]  eta: 0:04:34  lr: 0.000005  loss: 3.8170 (4.0406)  time: 0.4979  data: 0.0006  max mem: 19734
Epoch: [2]  [ 720/1251]  eta: 0:04:29  lr: 0.000005  loss: 3.7266 (4.0329)  time: 0.5088  data: 0.0004  max mem: 19734
Epoch: [2]  [ 730/1251]  eta: 0:04:24  lr: 0.000005  loss: 3.7426 (4.0310)  time: 0.4878  data: 0.0005  max mem: 19734
Epoch: [2]  [ 740/1251]  eta: 0:04:19  lr: 0.000005  loss: 3.9760 (4.0297)  time: 0.4768  data: 0.0005  max mem: 19734
Epoch: [2]  [ 750/1251]  eta: 0:04:13  lr: 0.000005  loss: 3.9741 (4.0246)  time: 0.4758  data: 0.0004  max mem: 19734
Epoch: [2]  [ 760/1251]  eta: 0:04:08  lr: 0.000005  loss: 3.6161 (4.0214)  time: 0.4758  data: 0.0005  max mem: 19734
Epoch: [2]  [ 770/1251]  eta: 0:04:03  lr: 0.000005  loss: 3.5738 (4.0134)  time: 0.4756  data: 0.0005  max mem: 19734
Epoch: [2]  [ 780/1251]  eta: 0:03:58  lr: 0.000005  loss: 3.3348 (4.0081)  time: 0.4769  data: 0.0005  max mem: 19734
Epoch: [2]  [ 790/1251]  eta: 0:03:52  lr: 0.000005  loss: 3.5679 (4.0035)  time: 0.4785  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6449, ratio_loss=0.0101, pruning_loss=0.2353, mse_loss=1.1061
Epoch: [2]  [ 800/1251]  eta: 0:03:47  lr: 0.000005  loss: 3.8418 (3.9999)  time: 0.4782  data: 0.0005  max mem: 19734
Epoch: [2]  [ 810/1251]  eta: 0:03:42  lr: 0.000005  loss: 3.8418 (3.9962)  time: 0.4787  data: 0.0005  max mem: 19734
Epoch: [2]  [ 820/1251]  eta: 0:03:37  lr: 0.000005  loss: 3.8640 (3.9918)  time: 0.4793  data: 0.0004  max mem: 19734
Epoch: [2]  [ 830/1251]  eta: 0:03:32  lr: 0.000005  loss: 3.8640 (3.9842)  time: 0.4765  data: 0.0004  max mem: 19734
Epoch: [2]  [ 840/1251]  eta: 0:03:27  lr: 0.000005  loss: 3.4145 (3.9770)  time: 0.4851  data: 0.0005  max mem: 19734
Epoch: [2]  [ 850/1251]  eta: 0:03:21  lr: 0.000005  loss: 3.7184 (3.9741)  time: 0.4869  data: 0.0004  max mem: 19734
Epoch: [2]  [ 860/1251]  eta: 0:03:17  lr: 0.000005  loss: 3.7586 (3.9684)  time: 0.5090  data: 0.0004  max mem: 19734
Epoch: [2]  [ 870/1251]  eta: 0:03:11  lr: 0.000005  loss: 3.5020 (3.9628)  time: 0.5074  data: 0.0004  max mem: 19734
Epoch: [2]  [ 880/1251]  eta: 0:03:06  lr: 0.000005  loss: 3.6954 (3.9603)  time: 0.4764  data: 0.0004  max mem: 19734
Epoch: [2]  [ 890/1251]  eta: 0:03:01  lr: 0.000005  loss: 3.8769 (3.9580)  time: 0.4774  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5208, ratio_loss=0.0079, pruning_loss=0.2437, mse_loss=1.1370
Epoch: [2]  [ 900/1251]  eta: 0:02:56  lr: 0.000005  loss: 3.6468 (3.9527)  time: 0.4764  data: 0.0004  max mem: 19734
Epoch: [2]  [ 910/1251]  eta: 0:02:51  lr: 0.000005  loss: 3.6468 (3.9494)  time: 0.4774  data: 0.0004  max mem: 19734
Epoch: [2]  [ 920/1251]  eta: 0:02:46  lr: 0.000005  loss: 3.9060 (3.9474)  time: 0.4778  data: 0.0004  max mem: 19734
Epoch: [2]  [ 930/1251]  eta: 0:02:41  lr: 0.000005  loss: 3.6970 (3.9415)  time: 0.4765  data: 0.0004  max mem: 19734
Epoch: [2]  [ 940/1251]  eta: 0:02:35  lr: 0.000005  loss: 3.3321 (3.9356)  time: 0.4755  data: 0.0004  max mem: 19734
Epoch: [2]  [ 950/1251]  eta: 0:02:30  lr: 0.000005  loss: 3.3321 (3.9312)  time: 0.4753  data: 0.0004  max mem: 19734
Epoch: [2]  [ 960/1251]  eta: 0:02:25  lr: 0.000005  loss: 3.7433 (3.9278)  time: 0.4759  data: 0.0005  max mem: 19734
Epoch: [2]  [ 970/1251]  eta: 0:02:20  lr: 0.000005  loss: 3.3582 (3.9208)  time: 0.4754  data: 0.0004  max mem: 19734
Epoch: [2]  [ 980/1251]  eta: 0:02:15  lr: 0.000005  loss: 3.3742 (3.9159)  time: 0.4753  data: 0.0004  max mem: 19734
Epoch: [2]  [ 990/1251]  eta: 0:02:10  lr: 0.000005  loss: 3.5042 (3.9135)  time: 0.4857  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4563, ratio_loss=0.0062, pruning_loss=0.2414, mse_loss=1.1635
Epoch: [2]  [1000/1251]  eta: 0:02:05  lr: 0.000005  loss: 3.5912 (3.9087)  time: 0.5061  data: 0.0004  max mem: 19734
Epoch: [2]  [1010/1251]  eta: 0:02:00  lr: 0.000005  loss: 3.6101 (3.9063)  time: 0.4977  data: 0.0005  max mem: 19734
Epoch: [2]  [1020/1251]  eta: 0:01:55  lr: 0.000005  loss: 3.4908 (3.9007)  time: 0.4795  data: 0.0004  max mem: 19734
Epoch: [2]  [1030/1251]  eta: 0:01:50  lr: 0.000005  loss: 3.5740 (3.8985)  time: 0.4791  data: 0.0004  max mem: 19734
Epoch: [2]  [1040/1251]  eta: 0:01:45  lr: 0.000005  loss: 3.5315 (3.8936)  time: 0.4775  data: 0.0004  max mem: 19734
Epoch: [2]  [1050/1251]  eta: 0:01:40  lr: 0.000005  loss: 3.5597 (3.8905)  time: 0.4785  data: 0.0004  max mem: 19734
Epoch: [2]  [1060/1251]  eta: 0:01:35  lr: 0.000005  loss: 3.8393 (3.8899)  time: 0.4792  data: 0.0004  max mem: 19734
Epoch: [2]  [1070/1251]  eta: 0:01:30  lr: 0.000005  loss: 3.7908 (3.8875)  time: 0.4781  data: 0.0004  max mem: 19734
Epoch: [2]  [1080/1251]  eta: 0:01:25  lr: 0.000005  loss: 3.6422 (3.8853)  time: 0.4767  data: 0.0004  max mem: 19734
Epoch: [2]  [1090/1251]  eta: 0:01:20  lr: 0.000005  loss: 3.7145 (3.8834)  time: 0.4756  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5558, ratio_loss=0.0054, pruning_loss=0.2359, mse_loss=1.0962
Epoch: [2]  [1100/1251]  eta: 0:01:15  lr: 0.000005  loss: 3.7145 (3.8789)  time: 0.4766  data: 0.0005  max mem: 19734
Epoch: [2]  [1110/1251]  eta: 0:01:10  lr: 0.000005  loss: 3.7216 (3.8782)  time: 0.4778  data: 0.0004  max mem: 19734
Epoch: [2]  [1120/1251]  eta: 0:01:05  lr: 0.000005  loss: 3.7373 (3.8761)  time: 0.4774  data: 0.0004  max mem: 19734
Epoch: [2]  [1130/1251]  eta: 0:01:00  lr: 0.000005  loss: 3.7113 (3.8757)  time: 0.4844  data: 0.0004  max mem: 19734
Epoch: [2]  [1140/1251]  eta: 0:00:55  lr: 0.000005  loss: 3.7766 (3.8728)  time: 0.4821  data: 0.0006  max mem: 19734
Epoch: [2]  [1150/1251]  eta: 0:00:50  lr: 0.000005  loss: 3.7201 (3.8718)  time: 0.4951  data: 0.0007  max mem: 19734
Epoch: [2]  [1160/1251]  eta: 0:00:45  lr: 0.000005  loss: 3.7416 (3.8692)  time: 0.4980  data: 0.0006  max mem: 19734
Epoch: [2]  [1170/1251]  eta: 0:00:40  lr: 0.000005  loss: 3.9015 (3.8683)  time: 0.4789  data: 0.0004  max mem: 19734
Epoch: [2]  [1180/1251]  eta: 0:00:35  lr: 0.000005  loss: 3.6995 (3.8662)  time: 0.4793  data: 0.0004  max mem: 19734
Epoch: [2]  [1190/1251]  eta: 0:00:30  lr: 0.000005  loss: 3.6110 (3.8638)  time: 0.4759  data: 0.0008  max mem: 19734
loss info: cls_loss=3.5875, ratio_loss=0.0047, pruning_loss=0.2378, mse_loss=1.1670
Epoch: [2]  [1200/1251]  eta: 0:00:25  lr: 0.000005  loss: 3.6110 (3.8609)  time: 0.4702  data: 0.0007  max mem: 19734
Epoch: [2]  [1210/1251]  eta: 0:00:20  lr: 0.000005  loss: 3.6742 (3.8582)  time: 0.4678  data: 0.0002  max mem: 19734
Epoch: [2]  [1220/1251]  eta: 0:00:15  lr: 0.000005  loss: 3.7744 (3.8574)  time: 0.4682  data: 0.0002  max mem: 19734
Epoch: [2]  [1230/1251]  eta: 0:00:10  lr: 0.000005  loss: 3.8027 (3.8562)  time: 0.4688  data: 0.0002  max mem: 19734
Epoch: [2]  [1240/1251]  eta: 0:00:05  lr: 0.000005  loss: 3.7698 (3.8547)  time: 0.4695  data: 0.0002  max mem: 19734
Epoch: [2]  [1250/1251]  eta: 0:00:00  lr: 0.000005  loss: 3.6841 (3.8514)  time: 0.4691  data: 0.0002  max mem: 19734
Epoch: [2] Total time: 0:10:21 (0.4966 s / it)
Averaged stats: lr: 0.000005  loss: 3.6841 (3.8310)
Test:  [  0/261]  eta: 1:58:28  loss: 1.0390 (1.0390)  acc1: 78.1250 (78.1250)  acc5: 93.2292 (93.2292)  time: 27.2354  data: 27.1423  max mem: 19734
Test:  [ 10/261]  eta: 0:12:29  loss: 1.0390 (1.1454)  acc1: 78.1250 (74.9527)  acc5: 93.2292 (91.7614)  time: 2.9874  data: 2.8030  max mem: 19734
Test:  [ 20/261]  eta: 0:06:52  loss: 1.3468 (1.3562)  acc1: 69.7917 (69.2956)  acc5: 88.0208 (89.0625)  time: 0.4349  data: 0.1916  max mem: 19734
Test:  [ 30/261]  eta: 0:04:45  loss: 1.1913 (1.2413)  acc1: 73.4375 (72.7655)  acc5: 90.1042 (89.9362)  time: 0.2706  data: 0.0189  max mem: 19734
Test:  [ 40/261]  eta: 0:04:02  loss: 0.9480 (1.2076)  acc1: 78.6458 (73.5010)  acc5: 92.1875 (90.4599)  time: 0.4495  data: 0.2688  max mem: 19734
Test:  [ 50/261]  eta: 0:03:15  loss: 1.4018 (1.2841)  acc1: 66.1458 (71.1091)  acc5: 88.5417 (90.0020)  time: 0.4514  data: 0.2669  max mem: 19734
Test:  [ 60/261]  eta: 0:02:44  loss: 1.5062 (1.3152)  acc1: 63.5417 (69.8856)  acc5: 88.5417 (89.9078)  time: 0.2469  data: 0.0150  max mem: 19734
Test:  [ 70/261]  eta: 0:02:18  loss: 1.3796 (1.3072)  acc1: 66.1458 (69.7917)  acc5: 90.6250 (90.2729)  time: 0.2081  data: 0.0106  max mem: 19734
Test:  [ 80/261]  eta: 0:01:59  loss: 1.2321 (1.2905)  acc1: 71.3542 (70.2418)  acc5: 92.1875 (90.5414)  time: 0.1731  data: 0.0248  max mem: 19734
Test:  [ 90/261]  eta: 0:01:44  loss: 1.1623 (1.2644)  acc1: 73.9583 (70.8505)  acc5: 91.6667 (90.8139)  time: 0.2189  data: 0.0712  max mem: 19734
Test:  [100/261]  eta: 0:01:34  loss: 1.0668 (1.2582)  acc1: 74.4792 (71.0293)  acc5: 91.6667 (90.9035)  time: 0.3046  data: 0.1508  max mem: 19734
Test:  [110/261]  eta: 0:01:24  loss: 1.1549 (1.2665)  acc1: 72.3958 (70.9272)  acc5: 91.1458 (90.6907)  time: 0.3166  data: 0.1343  max mem: 19734
Test:  [120/261]  eta: 0:01:18  loss: 1.5099 (1.2916)  acc1: 65.6250 (70.4072)  acc5: 87.5000 (90.3280)  time: 0.3761  data: 0.1622  max mem: 19734
Test:  [130/261]  eta: 0:01:15  loss: 1.6271 (1.3230)  acc1: 61.4583 (69.8593)  acc5: 85.4167 (89.8935)  time: 0.6847  data: 0.5071  max mem: 19734
Test:  [140/261]  eta: 0:01:06  loss: 1.6216 (1.3394)  acc1: 59.8958 (69.4038)  acc5: 86.4583 (89.7200)  time: 0.5266  data: 0.3880  max mem: 19734
Test:  [150/261]  eta: 0:00:58  loss: 1.4122 (1.3348)  acc1: 68.7500 (69.6813)  acc5: 87.5000 (89.6834)  time: 0.1829  data: 0.0176  max mem: 19734
Test:  [160/261]  eta: 0:00:51  loss: 1.2383 (1.3477)  acc1: 75.0000 (69.5782)  acc5: 88.5417 (89.4345)  time: 0.2300  data: 0.0921  max mem: 19734
Test:  [170/261]  eta: 0:00:44  loss: 1.5500 (1.3706)  acc1: 60.9375 (69.0241)  acc5: 83.3333 (89.0960)  time: 0.2031  data: 0.1202  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: 1.6456 (1.3824)  acc1: 60.9375 (68.7615)  acc5: 84.3750 (88.9589)  time: 0.1135  data: 0.0384  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: 1.5794 (1.3884)  acc1: 63.0208 (68.6900)  acc5: 88.0208 (88.8825)  time: 0.0757  data: 0.0011  max mem: 19734
Test:  [200/261]  eta: 0:00:26  loss: 1.5232 (1.3957)  acc1: 67.7083 (68.5608)  acc5: 87.5000 (88.7412)  time: 0.0790  data: 0.0020  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.6050 (1.4035)  acc1: 66.6667 (68.4661)  acc5: 84.8958 (88.6330)  time: 0.0785  data: 0.0011  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.6340 (1.4199)  acc1: 64.0625 (68.0453)  acc5: 84.3750 (88.4215)  time: 0.0745  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.6735 (1.4273)  acc1: 62.5000 (67.8842)  acc5: 84.3750 (88.3072)  time: 0.0736  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.4910 (1.4302)  acc1: 64.5833 (67.8013)  acc5: 85.9375 (88.2802)  time: 0.0737  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.2053 (1.4165)  acc1: 70.3125 (68.1233)  acc5: 90.1042 (88.4545)  time: 0.0737  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0882 (1.4123)  acc1: 75.0000 (68.2420)  acc5: 92.7083 (88.5760)  time: 0.0716  data: 0.0002  max mem: 19734
Test: Total time: 0:01:30 (0.3469 s / it)
* Acc@1 68.242 Acc@5 88.576 loss 1.412
Accuracy of the network on the 50000 test images: 68.2%
Max accuracy: 68.24%
Epoch: [3]  [   0/1251]  eta: 3:45:53  lr: 0.000009  loss: 3.3729 (3.3729)  time: 10.8340  data: 10.3730  max mem: 19734
Epoch: [3]  [  10/1251]  eta: 0:32:13  lr: 0.000009  loss: 3.8772 (3.7715)  time: 1.5580  data: 0.9677  max mem: 19734
Epoch: [3]  [  20/1251]  eta: 0:21:29  lr: 0.000009  loss: 3.7747 (3.6248)  time: 0.5579  data: 0.0138  max mem: 19734
Epoch: [3]  [  30/1251]  eta: 0:17:51  lr: 0.000009  loss: 3.6133 (3.5933)  time: 0.5038  data: 0.0004  max mem: 19734
Epoch: [3]  [  40/1251]  eta: 0:16:05  lr: 0.000009  loss: 3.6133 (3.6074)  time: 0.5349  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5790, ratio_loss=0.0047, pruning_loss=0.2312, mse_loss=1.1276
Epoch: [3]  [  50/1251]  eta: 0:14:42  lr: 0.000009  loss: 3.5334 (3.6128)  time: 0.5129  data: 0.0004  max mem: 19734
Epoch: [3]  [  60/1251]  eta: 0:13:44  lr: 0.000009  loss: 3.3437 (3.5639)  time: 0.4782  data: 0.0004  max mem: 19734
Epoch: [3]  [  70/1251]  eta: 0:13:03  lr: 0.000009  loss: 3.6042 (3.5768)  time: 0.4826  data: 0.0004  max mem: 19734
Epoch: [3]  [  80/1251]  eta: 0:12:31  lr: 0.000009  loss: 3.7970 (3.5841)  time: 0.4869  data: 0.0004  max mem: 19734
Epoch: [3]  [  90/1251]  eta: 0:12:05  lr: 0.000009  loss: 3.7297 (3.5559)  time: 0.4870  data: 0.0004  max mem: 19734
Epoch: [3]  [ 100/1251]  eta: 0:11:43  lr: 0.000009  loss: 3.7297 (3.5793)  time: 0.4868  data: 0.0004  max mem: 19734
Epoch: [3]  [ 110/1251]  eta: 0:11:24  lr: 0.000009  loss: 3.7032 (3.5631)  time: 0.4854  data: 0.0004  max mem: 19734
Epoch: [3]  [ 120/1251]  eta: 0:11:07  lr: 0.000009  loss: 3.4983 (3.5580)  time: 0.4826  data: 0.0004  max mem: 19734
Epoch: [3]  [ 130/1251]  eta: 0:10:53  lr: 0.000009  loss: 3.4595 (3.5406)  time: 0.4904  data: 0.0004  max mem: 19734
Epoch: [3]  [ 140/1251]  eta: 0:10:39  lr: 0.000009  loss: 3.3816 (3.5389)  time: 0.4901  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4233, ratio_loss=0.0038, pruning_loss=0.2386, mse_loss=1.1416
Epoch: [3]  [ 150/1251]  eta: 0:10:26  lr: 0.000009  loss: 3.5758 (3.5234)  time: 0.4791  data: 0.0004  max mem: 19734
Epoch: [3]  [ 160/1251]  eta: 0:10:14  lr: 0.000009  loss: 3.4345 (3.5193)  time: 0.4780  data: 0.0004  max mem: 19734
Epoch: [3]  [ 170/1251]  eta: 0:10:04  lr: 0.000009  loss: 3.4345 (3.5194)  time: 0.4800  data: 0.0005  max mem: 19734
Epoch: [3]  [ 180/1251]  eta: 0:09:58  lr: 0.000009  loss: 3.7113 (3.5196)  time: 0.5214  data: 0.0005  max mem: 19734
Epoch: [3]  [ 190/1251]  eta: 0:09:48  lr: 0.000009  loss: 3.2119 (3.5093)  time: 0.5203  data: 0.0005  max mem: 19734
Epoch: [3]  [ 200/1251]  eta: 0:09:39  lr: 0.000009  loss: 3.2635 (3.5043)  time: 0.4814  data: 0.0005  max mem: 19734
Epoch: [3]  [ 210/1251]  eta: 0:09:30  lr: 0.000009  loss: 3.5967 (3.5116)  time: 0.4822  data: 0.0005  max mem: 19734
Epoch: [3]  [ 220/1251]  eta: 0:09:22  lr: 0.000009  loss: 3.7914 (3.5227)  time: 0.4837  data: 0.0012  max mem: 19734
Epoch: [3]  [ 230/1251]  eta: 0:09:13  lr: 0.000009  loss: 3.7101 (3.5262)  time: 0.4836  data: 0.0012  max mem: 19734
Epoch: [3]  [ 240/1251]  eta: 0:09:05  lr: 0.000009  loss: 3.7064 (3.5351)  time: 0.4799  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4791, ratio_loss=0.0036, pruning_loss=0.2323, mse_loss=1.0777
Epoch: [3]  [ 250/1251]  eta: 0:08:57  lr: 0.000009  loss: 3.7064 (3.5288)  time: 0.4801  data: 0.0005  max mem: 19734
Epoch: [3]  [ 260/1251]  eta: 0:08:50  lr: 0.000009  loss: 3.4586 (3.5216)  time: 0.4810  data: 0.0005  max mem: 19734
Epoch: [3]  [ 270/1251]  eta: 0:08:43  lr: 0.000009  loss: 3.5170 (3.5264)  time: 0.4892  data: 0.0004  max mem: 19734
Epoch: [3]  [ 280/1251]  eta: 0:08:36  lr: 0.000009  loss: 3.5663 (3.5231)  time: 0.4919  data: 0.0006  max mem: 19734
Epoch: [3]  [ 290/1251]  eta: 0:08:29  lr: 0.000009  loss: 3.6809 (3.5338)  time: 0.4830  data: 0.0005  max mem: 19734
Epoch: [3]  [ 300/1251]  eta: 0:08:22  lr: 0.000009  loss: 3.7789 (3.5413)  time: 0.4806  data: 0.0004  max mem: 19734
Epoch: [3]  [ 310/1251]  eta: 0:08:16  lr: 0.000009  loss: 3.7634 (3.5460)  time: 0.4811  data: 0.0006  max mem: 19734
Epoch: [3]  [ 320/1251]  eta: 0:08:11  lr: 0.000009  loss: 3.3545 (3.5313)  time: 0.5090  data: 0.0006  max mem: 19734
Epoch: [3]  [ 330/1251]  eta: 0:08:05  lr: 0.000009  loss: 3.0711 (3.5211)  time: 0.5261  data: 0.0005  max mem: 19734
Epoch: [3]  [ 340/1251]  eta: 0:07:58  lr: 0.000009  loss: 3.2995 (3.5227)  time: 0.4977  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4663, ratio_loss=0.0035, pruning_loss=0.2366, mse_loss=1.1091
Epoch: [3]  [ 350/1251]  eta: 0:07:52  lr: 0.000009  loss: 3.5211 (3.5185)  time: 0.4785  data: 0.0005  max mem: 19734
Epoch: [3]  [ 360/1251]  eta: 0:07:46  lr: 0.000009  loss: 3.5843 (3.5195)  time: 0.4784  data: 0.0006  max mem: 19734
Epoch: [3]  [ 370/1251]  eta: 0:07:39  lr: 0.000009  loss: 3.5843 (3.5192)  time: 0.4793  data: 0.0006  max mem: 19734
Epoch: [3]  [ 380/1251]  eta: 0:07:33  lr: 0.000009  loss: 3.3581 (3.5159)  time: 0.4802  data: 0.0005  max mem: 19734
Epoch: [3]  [ 390/1251]  eta: 0:07:27  lr: 0.000009  loss: 3.5317 (3.5187)  time: 0.4804  data: 0.0004  max mem: 19734
Epoch: [3]  [ 400/1251]  eta: 0:07:21  lr: 0.000009  loss: 3.6442 (3.5193)  time: 0.4807  data: 0.0008  max mem: 19734
Epoch: [3]  [ 410/1251]  eta: 0:07:15  lr: 0.000009  loss: 3.6442 (3.5198)  time: 0.4808  data: 0.0008  max mem: 19734
Epoch: [3]  [ 420/1251]  eta: 0:07:10  lr: 0.000009  loss: 3.7555 (3.5259)  time: 0.4915  data: 0.0004  max mem: 19734
Epoch: [3]  [ 430/1251]  eta: 0:07:04  lr: 0.000009  loss: 3.7598 (3.5307)  time: 0.4918  data: 0.0007  max mem: 19734
Epoch: [3]  [ 440/1251]  eta: 0:06:58  lr: 0.000009  loss: 3.4683 (3.5213)  time: 0.4792  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4836, ratio_loss=0.0033, pruning_loss=0.2306, mse_loss=1.0840
Epoch: [3]  [ 450/1251]  eta: 0:06:52  lr: 0.000009  loss: 3.3954 (3.5237)  time: 0.4794  data: 0.0004  max mem: 19734
Epoch: [3]  [ 460/1251]  eta: 0:06:46  lr: 0.000009  loss: 3.7637 (3.5276)  time: 0.4825  data: 0.0004  max mem: 19734
Epoch: [3]  [ 470/1251]  eta: 0:06:42  lr: 0.000009  loss: 3.7637 (3.5286)  time: 0.5230  data: 0.0004  max mem: 19734
Epoch: [3]  [ 480/1251]  eta: 0:06:36  lr: 0.000009  loss: 3.6515 (3.5232)  time: 0.5196  data: 0.0004  max mem: 19734
Epoch: [3]  [ 490/1251]  eta: 0:06:31  lr: 0.000009  loss: 3.6230 (3.5253)  time: 0.4782  data: 0.0004  max mem: 19734
Epoch: [3]  [ 500/1251]  eta: 0:06:25  lr: 0.000009  loss: 3.6230 (3.5259)  time: 0.4790  data: 0.0004  max mem: 19734
Epoch: [3]  [ 510/1251]  eta: 0:06:19  lr: 0.000009  loss: 3.4455 (3.5233)  time: 0.4785  data: 0.0004  max mem: 19734
Epoch: [3]  [ 520/1251]  eta: 0:06:14  lr: 0.000009  loss: 3.5090 (3.5235)  time: 0.4801  data: 0.0004  max mem: 19734
Epoch: [3]  [ 530/1251]  eta: 0:06:08  lr: 0.000009  loss: 3.5952 (3.5212)  time: 0.4849  data: 0.0004  max mem: 19734
Epoch: [3]  [ 540/1251]  eta: 0:06:03  lr: 0.000009  loss: 3.5985 (3.5221)  time: 0.4845  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4633, ratio_loss=0.0033, pruning_loss=0.2335, mse_loss=1.1560
Epoch: [3]  [ 550/1251]  eta: 0:05:57  lr: 0.000009  loss: 3.5985 (3.5172)  time: 0.4809  data: 0.0004  max mem: 19734
Epoch: [3]  [ 560/1251]  eta: 0:05:52  lr: 0.000009  loss: 3.6627 (3.5189)  time: 0.4787  data: 0.0004  max mem: 19734
Epoch: [3]  [ 570/1251]  eta: 0:05:46  lr: 0.000009  loss: 3.7355 (3.5248)  time: 0.4854  data: 0.0005  max mem: 19734
Epoch: [3]  [ 580/1251]  eta: 0:05:41  lr: 0.000009  loss: 3.6691 (3.5204)  time: 0.4868  data: 0.0005  max mem: 19734
Epoch: [3]  [ 590/1251]  eta: 0:05:36  lr: 0.000009  loss: 3.3827 (3.5164)  time: 0.4794  data: 0.0004  max mem: 19734
Epoch: [3]  [ 600/1251]  eta: 0:05:30  lr: 0.000009  loss: 3.7810 (3.5173)  time: 0.4801  data: 0.0004  max mem: 19734
Epoch: [3]  [ 610/1251]  eta: 0:05:25  lr: 0.000009  loss: 3.7436 (3.5158)  time: 0.5025  data: 0.0004  max mem: 19734
Epoch: [3]  [ 620/1251]  eta: 0:05:20  lr: 0.000009  loss: 3.7266 (3.5202)  time: 0.5216  data: 0.0004  max mem: 19734
Epoch: [3]  [ 630/1251]  eta: 0:05:15  lr: 0.000009  loss: 3.7409 (3.5183)  time: 0.5003  data: 0.0004  max mem: 19734
Epoch: [3]  [ 640/1251]  eta: 0:05:10  lr: 0.000009  loss: 3.6819 (3.5206)  time: 0.4805  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4886, ratio_loss=0.0033, pruning_loss=0.2321, mse_loss=1.1439
Epoch: [3]  [ 650/1251]  eta: 0:05:04  lr: 0.000009  loss: 3.8198 (3.5250)  time: 0.4790  data: 0.0004  max mem: 19734
Epoch: [3]  [ 660/1251]  eta: 0:04:59  lr: 0.000009  loss: 3.8140 (3.5246)  time: 0.4778  data: 0.0004  max mem: 19734
Epoch: [3]  [ 670/1251]  eta: 0:04:54  lr: 0.000009  loss: 3.7137 (3.5255)  time: 0.4792  data: 0.0004  max mem: 19734
Epoch: [3]  [ 680/1251]  eta: 0:04:48  lr: 0.000009  loss: 3.7137 (3.5248)  time: 0.4799  data: 0.0005  max mem: 19734
Epoch: [3]  [ 690/1251]  eta: 0:04:43  lr: 0.000009  loss: 3.6473 (3.5250)  time: 0.4795  data: 0.0004  max mem: 19734
Epoch: [3]  [ 700/1251]  eta: 0:04:38  lr: 0.000009  loss: 3.8218 (3.5240)  time: 0.4787  data: 0.0004  max mem: 19734
Epoch: [3]  [ 710/1251]  eta: 0:04:33  lr: 0.000009  loss: 3.7702 (3.5230)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [3]  [ 720/1251]  eta: 0:04:28  lr: 0.000009  loss: 3.7917 (3.5291)  time: 0.4912  data: 0.0004  max mem: 19734
Epoch: [3]  [ 730/1251]  eta: 0:04:22  lr: 0.000009  loss: 3.7701 (3.5294)  time: 0.4925  data: 0.0004  max mem: 19734
Epoch: [3]  [ 740/1251]  eta: 0:04:17  lr: 0.000009  loss: 3.4232 (3.5248)  time: 0.4819  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4868, ratio_loss=0.0033, pruning_loss=0.2288, mse_loss=1.1162
Epoch: [3]  [ 750/1251]  eta: 0:04:12  lr: 0.000009  loss: 3.3111 (3.5246)  time: 0.4884  data: 0.0004  max mem: 19734
Epoch: [3]  [ 760/1251]  eta: 0:04:07  lr: 0.000009  loss: 3.7311 (3.5268)  time: 0.5113  data: 0.0004  max mem: 19734
Epoch: [3]  [ 770/1251]  eta: 0:04:02  lr: 0.000009  loss: 3.7311 (3.5264)  time: 0.5025  data: 0.0004  max mem: 19734
Epoch: [3]  [ 780/1251]  eta: 0:03:57  lr: 0.000009  loss: 3.6591 (3.5264)  time: 0.4785  data: 0.0004  max mem: 19734
Epoch: [3]  [ 790/1251]  eta: 0:03:52  lr: 0.000009  loss: 3.6592 (3.5258)  time: 0.4777  data: 0.0004  max mem: 19734
Epoch: [3]  [ 800/1251]  eta: 0:03:46  lr: 0.000009  loss: 3.5494 (3.5250)  time: 0.4775  data: 0.0004  max mem: 19734
Epoch: [3]  [ 810/1251]  eta: 0:03:41  lr: 0.000009  loss: 3.6146 (3.5271)  time: 0.4795  data: 0.0004  max mem: 19734
Epoch: [3]  [ 820/1251]  eta: 0:03:36  lr: 0.000009  loss: 3.6752 (3.5273)  time: 0.4826  data: 0.0004  max mem: 19734
Epoch: [3]  [ 830/1251]  eta: 0:03:31  lr: 0.000009  loss: 3.6752 (3.5279)  time: 0.4835  data: 0.0005  max mem: 19734
Epoch: [3]  [ 840/1251]  eta: 0:03:26  lr: 0.000009  loss: 3.8039 (3.5303)  time: 0.4808  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5175, ratio_loss=0.0032, pruning_loss=0.2270, mse_loss=1.0975
Epoch: [3]  [ 850/1251]  eta: 0:03:21  lr: 0.000009  loss: 3.6432 (3.5312)  time: 0.4803  data: 0.0004  max mem: 19734
Epoch: [3]  [ 860/1251]  eta: 0:03:16  lr: 0.000009  loss: 3.5333 (3.5289)  time: 0.4787  data: 0.0003  max mem: 19734
Epoch: [3]  [ 870/1251]  eta: 0:03:10  lr: 0.000009  loss: 3.4485 (3.5302)  time: 0.4828  data: 0.0004  max mem: 19734
Epoch: [3]  [ 880/1251]  eta: 0:03:05  lr: 0.000009  loss: 3.6723 (3.5301)  time: 0.4833  data: 0.0007  max mem: 19734
Epoch: [3]  [ 890/1251]  eta: 0:03:00  lr: 0.000009  loss: 3.7298 (3.5317)  time: 0.4777  data: 0.0006  max mem: 19734
Epoch: [3]  [ 900/1251]  eta: 0:02:55  lr: 0.000009  loss: 3.7298 (3.5327)  time: 0.5018  data: 0.0005  max mem: 19734
Epoch: [3]  [ 910/1251]  eta: 0:02:50  lr: 0.000009  loss: 3.6489 (3.5335)  time: 0.5254  data: 0.0005  max mem: 19734
Epoch: [3]  [ 920/1251]  eta: 0:02:45  lr: 0.000009  loss: 3.6489 (3.5346)  time: 0.5030  data: 0.0004  max mem: 19734
Epoch: [3]  [ 930/1251]  eta: 0:02:40  lr: 0.000009  loss: 3.6964 (3.5355)  time: 0.4793  data: 0.0004  max mem: 19734
Epoch: [3]  [ 940/1251]  eta: 0:02:35  lr: 0.000009  loss: 3.7968 (3.5379)  time: 0.4780  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5778, ratio_loss=0.0035, pruning_loss=0.2225, mse_loss=1.0394
Epoch: [3]  [ 950/1251]  eta: 0:02:30  lr: 0.000009  loss: 3.7544 (3.5400)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [3]  [ 960/1251]  eta: 0:02:25  lr: 0.000009  loss: 3.5447 (3.5391)  time: 0.4773  data: 0.0005  max mem: 19734
Epoch: [3]  [ 970/1251]  eta: 0:02:20  lr: 0.000009  loss: 3.7014 (3.5405)  time: 0.4787  data: 0.0005  max mem: 19734
Epoch: [3]  [ 980/1251]  eta: 0:02:15  lr: 0.000009  loss: 3.7171 (3.5386)  time: 0.4799  data: 0.0005  max mem: 19734
Epoch: [3]  [ 990/1251]  eta: 0:02:10  lr: 0.000009  loss: 3.6207 (3.5391)  time: 0.4798  data: 0.0004  max mem: 19734
Epoch: [3]  [1000/1251]  eta: 0:02:05  lr: 0.000009  loss: 3.7045 (3.5396)  time: 0.4784  data: 0.0004  max mem: 19734
Epoch: [3]  [1010/1251]  eta: 0:02:00  lr: 0.000009  loss: 3.7231 (3.5398)  time: 0.4804  data: 0.0004  max mem: 19734
Epoch: [3]  [1020/1251]  eta: 0:01:55  lr: 0.000009  loss: 3.6462 (3.5368)  time: 0.4901  data: 0.0004  max mem: 19734
Epoch: [3]  [1030/1251]  eta: 0:01:50  lr: 0.000009  loss: 3.5130 (3.5365)  time: 0.4885  data: 0.0004  max mem: 19734
Epoch: [3]  [1040/1251]  eta: 0:01:45  lr: 0.000009  loss: 3.6760 (3.5369)  time: 0.4862  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4630, ratio_loss=0.0034, pruning_loss=0.2274, mse_loss=1.1010
Epoch: [3]  [1050/1251]  eta: 0:01:40  lr: 0.000009  loss: 3.7074 (3.5374)  time: 0.5186  data: 0.0005  max mem: 19734
Epoch: [3]  [1060/1251]  eta: 0:01:35  lr: 0.000009  loss: 3.4909 (3.5372)  time: 0.5119  data: 0.0006  max mem: 19734
Epoch: [3]  [1070/1251]  eta: 0:01:30  lr: 0.000009  loss: 3.4040 (3.5350)  time: 0.4792  data: 0.0006  max mem: 19734
Epoch: [3]  [1080/1251]  eta: 0:01:25  lr: 0.000009  loss: 3.6349 (3.5358)  time: 0.4780  data: 0.0006  max mem: 19734
Epoch: [3]  [1090/1251]  eta: 0:01:20  lr: 0.000009  loss: 3.6539 (3.5360)  time: 0.4771  data: 0.0006  max mem: 19734
Epoch: [3]  [1100/1251]  eta: 0:01:15  lr: 0.000009  loss: 3.5640 (3.5341)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [3]  [1110/1251]  eta: 0:01:10  lr: 0.000009  loss: 3.3624 (3.5308)  time: 0.4809  data: 0.0004  max mem: 19734
Epoch: [3]  [1120/1251]  eta: 0:01:05  lr: 0.000009  loss: 3.3624 (3.5291)  time: 0.4805  data: 0.0004  max mem: 19734
Epoch: [3]  [1130/1251]  eta: 0:01:00  lr: 0.000009  loss: 3.4714 (3.5286)  time: 0.4801  data: 0.0003  max mem: 19734
Epoch: [3]  [1140/1251]  eta: 0:00:55  lr: 0.000009  loss: 3.5958 (3.5294)  time: 0.4794  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3954, ratio_loss=0.0032, pruning_loss=0.2290, mse_loss=1.1046
Epoch: [3]  [1150/1251]  eta: 0:00:50  lr: 0.000009  loss: 3.5674 (3.5283)  time: 0.4777  data: 0.0004  max mem: 19734
Epoch: [3]  [1160/1251]  eta: 0:00:45  lr: 0.000009  loss: 3.3947 (3.5280)  time: 0.4845  data: 0.0004  max mem: 19734
Epoch: [3]  [1170/1251]  eta: 0:00:40  lr: 0.000009  loss: 3.6345 (3.5286)  time: 0.4854  data: 0.0004  max mem: 19734
Epoch: [3]  [1180/1251]  eta: 0:00:35  lr: 0.000009  loss: 3.6903 (3.5287)  time: 0.4774  data: 0.0003  max mem: 19734
Epoch: [3]  [1190/1251]  eta: 0:00:30  lr: 0.000009  loss: 3.7279 (3.5299)  time: 0.4852  data: 0.0006  max mem: 19734
Epoch: [3]  [1200/1251]  eta: 0:00:25  lr: 0.000009  loss: 3.6368 (3.5306)  time: 0.4983  data: 0.0005  max mem: 19734
Epoch: [3]  [1210/1251]  eta: 0:00:20  lr: 0.000009  loss: 3.5326 (3.5283)  time: 0.4868  data: 0.0001  max mem: 19734
Epoch: [3]  [1220/1251]  eta: 0:00:15  lr: 0.000009  loss: 3.5563 (3.5295)  time: 0.4702  data: 0.0001  max mem: 19734
Epoch: [3]  [1230/1251]  eta: 0:00:10  lr: 0.000009  loss: 3.8765 (3.5327)  time: 0.4682  data: 0.0001  max mem: 19734
Epoch: [3]  [1240/1251]  eta: 0:00:05  lr: 0.000009  loss: 3.8409 (3.5320)  time: 0.4684  data: 0.0001  max mem: 19734
loss info: cls_loss=3.5512, ratio_loss=0.0033, pruning_loss=0.2240, mse_loss=1.0471
Epoch: [3]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 3.7142 (3.5339)  time: 0.4686  data: 0.0001  max mem: 19734
Epoch: [3] Total time: 0:10:21 (0.4966 s / it)
Averaged stats: lr: 0.000009  loss: 3.7142 (3.5393)
Test:  [  0/261]  eta: 1:47:53  loss: 0.7751 (0.7751)  acc1: 82.8125 (82.8125)  acc5: 96.3542 (96.3542)  time: 24.8011  data: 24.4911  max mem: 19734
Test:  [ 10/261]  eta: 0:11:42  loss: 0.7751 (0.7931)  acc1: 84.8958 (83.0492)  acc5: 96.3542 (95.9754)  time: 2.7991  data: 2.6110  max mem: 19734
Test:  [ 20/261]  eta: 0:06:18  loss: 1.0132 (0.9698)  acc1: 77.6042 (78.2242)  acc5: 93.2292 (94.0228)  time: 0.4090  data: 0.2183  max mem: 19734
Test:  [ 30/261]  eta: 0:04:22  loss: 0.8736 (0.8806)  acc1: 82.8125 (81.2500)  acc5: 93.7500 (94.5565)  time: 0.2243  data: 0.0151  max mem: 19734
Test:  [ 40/261]  eta: 0:03:29  loss: 0.6225 (0.8505)  acc1: 87.5000 (82.0376)  acc5: 95.8333 (94.8806)  time: 0.2955  data: 0.1123  max mem: 19734
Test:  [ 50/261]  eta: 0:02:46  loss: 1.0110 (0.9264)  acc1: 76.0417 (79.8305)  acc5: 92.7083 (94.3423)  time: 0.2500  data: 0.1064  max mem: 19734
Test:  [ 60/261]  eta: 0:02:18  loss: 1.1247 (0.9451)  acc1: 72.9167 (79.0898)  acc5: 92.7083 (94.4501)  time: 0.1646  data: 0.0074  max mem: 19734
Test:  [ 70/261]  eta: 0:02:10  loss: 1.0508 (0.9474)  acc1: 76.0417 (78.5578)  acc5: 95.3125 (94.6596)  time: 0.4031  data: 0.2522  max mem: 19734
Test:  [ 80/261]  eta: 0:01:51  loss: 0.8911 (0.9415)  acc1: 79.6875 (78.7166)  acc5: 96.3542 (94.7531)  time: 0.3908  data: 0.2543  max mem: 19734
Test:  [ 90/261]  eta: 0:01:36  loss: 0.8529 (0.9232)  acc1: 82.2917 (79.1152)  acc5: 95.8333 (94.9348)  time: 0.1542  data: 0.0076  max mem: 19734
Test:  [100/261]  eta: 0:01:35  loss: 0.8472 (0.9234)  acc1: 82.8125 (79.1667)  acc5: 95.8333 (95.0186)  time: 0.4949  data: 0.3698  max mem: 19734
Test:  [110/261]  eta: 0:01:24  loss: 0.8558 (0.9416)  acc1: 75.5208 (78.7444)  acc5: 94.2708 (94.7729)  time: 0.5288  data: 0.3751  max mem: 19734
Test:  [120/261]  eta: 0:01:13  loss: 1.2836 (0.9791)  acc1: 69.7917 (77.9012)  acc5: 89.0625 (94.2192)  time: 0.1697  data: 0.0088  max mem: 19734
Test:  [130/261]  eta: 0:01:10  loss: 1.4838 (1.0223)  acc1: 68.7500 (77.0157)  acc5: 85.9375 (93.6228)  time: 0.4393  data: 0.2843  max mem: 19734
Test:  [140/261]  eta: 0:01:02  loss: 1.4165 (1.0474)  acc1: 66.1458 (76.4000)  acc5: 88.0208 (93.3326)  time: 0.4603  data: 0.2864  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: 1.2136 (1.0492)  acc1: 72.3958 (76.4832)  acc5: 90.1042 (93.2085)  time: 0.1558  data: 0.0105  max mem: 19734
Test:  [160/261]  eta: 0:00:47  loss: 1.0107 (1.0684)  acc1: 77.6042 (76.1711)  acc5: 91.6667 (92.9283)  time: 0.1987  data: 0.0597  max mem: 19734
Test:  [170/261]  eta: 0:00:41  loss: 1.3677 (1.0972)  acc1: 64.5833 (75.4356)  acc5: 87.5000 (92.5682)  time: 0.2153  data: 0.0609  max mem: 19734
Test:  [180/261]  eta: 0:00:35  loss: 1.5081 (1.1137)  acc1: 64.0625 (75.0604)  acc5: 87.5000 (92.4120)  time: 0.1948  data: 0.0348  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: 1.4169 (1.1266)  acc1: 66.6667 (74.8228)  acc5: 90.6250 (92.2420)  time: 0.2438  data: 0.0987  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.4023 (1.1405)  acc1: 69.7917 (74.4869)  acc5: 89.0625 (92.0217)  time: 0.2505  data: 0.1413  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.4023 (1.1510)  acc1: 68.2292 (74.2694)  acc5: 88.5417 (91.8888)  time: 0.1679  data: 0.0869  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: 1.4317 (1.1703)  acc1: 66.6667 (73.7816)  acc5: 88.5417 (91.6690)  time: 0.0905  data: 0.0164  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4734 (1.1807)  acc1: 65.1042 (73.5480)  acc5: 88.5417 (91.5449)  time: 0.0742  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.4256 (1.1891)  acc1: 66.6667 (73.3316)  acc5: 89.0625 (91.4592)  time: 0.0741  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0693 (1.1807)  acc1: 75.0000 (73.5599)  acc5: 91.1458 (91.5940)  time: 0.0740  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9798 (1.1801)  acc1: 77.0833 (73.5760)  acc5: 94.7917 (91.6620)  time: 0.0717  data: 0.0002  max mem: 19734
Test: Total time: 0:01:30 (0.3461 s / it)
* Acc@1 73.576 Acc@5 91.662 loss 1.180
Accuracy of the network on the 50000 test images: 73.6%
Max accuracy: 73.58%
Epoch: [4]  [   0/1251]  eta: 1:36:09  lr: 0.000012  loss: 4.2080 (4.2080)  time: 4.6122  data: 4.1414  max mem: 19734
Epoch: [4]  [  10/1251]  eta: 0:19:30  lr: 0.000012  loss: 3.6642 (3.6373)  time: 0.9432  data: 0.3770  max mem: 19734
Epoch: [4]  [  20/1251]  eta: 0:14:58  lr: 0.000012  loss: 3.6642 (3.5476)  time: 0.5358  data: 0.0005  max mem: 19734
Epoch: [4]  [  30/1251]  eta: 0:13:15  lr: 0.000012  loss: 3.5755 (3.4485)  time: 0.4906  data: 0.0005  max mem: 19734
Epoch: [4]  [  40/1251]  eta: 0:12:18  lr: 0.000012  loss: 3.5755 (3.4962)  time: 0.4831  data: 0.0006  max mem: 19734
Epoch: [4]  [  50/1251]  eta: 0:11:41  lr: 0.000012  loss: 3.6273 (3.4850)  time: 0.4804  data: 0.0006  max mem: 19734
Epoch: [4]  [  60/1251]  eta: 0:11:15  lr: 0.000012  loss: 3.6962 (3.5123)  time: 0.4812  data: 0.0005  max mem: 19734
Epoch: [4]  [  70/1251]  eta: 0:11:00  lr: 0.000012  loss: 3.6660 (3.4803)  time: 0.4949  data: 0.0005  max mem: 19734
Epoch: [4]  [  80/1251]  eta: 0:10:50  lr: 0.000012  loss: 3.5244 (3.4678)  time: 0.5182  data: 0.0004  max mem: 19734
Epoch: [4]  [  90/1251]  eta: 0:10:38  lr: 0.000012  loss: 3.4246 (3.4526)  time: 0.5168  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4053, ratio_loss=0.0035, pruning_loss=0.2271, mse_loss=1.0713
Epoch: [4]  [ 100/1251]  eta: 0:10:24  lr: 0.000012  loss: 3.5370 (3.4715)  time: 0.4923  data: 0.0004  max mem: 19734
Epoch: [4]  [ 110/1251]  eta: 0:10:12  lr: 0.000012  loss: 3.6244 (3.4714)  time: 0.4801  data: 0.0004  max mem: 19734
Epoch: [4]  [ 120/1251]  eta: 0:10:02  lr: 0.000012  loss: 3.3131 (3.4662)  time: 0.4806  data: 0.0004  max mem: 19734
Epoch: [4]  [ 130/1251]  eta: 0:09:52  lr: 0.000012  loss: 3.5680 (3.4806)  time: 0.4802  data: 0.0004  max mem: 19734
Epoch: [4]  [ 140/1251]  eta: 0:09:43  lr: 0.000012  loss: 3.7928 (3.4988)  time: 0.4822  data: 0.0004  max mem: 19734
Epoch: [4]  [ 150/1251]  eta: 0:09:37  lr: 0.000012  loss: 3.6190 (3.4928)  time: 0.4961  data: 0.0005  max mem: 19734
Epoch: [4]  [ 160/1251]  eta: 0:09:29  lr: 0.000012  loss: 3.5994 (3.5104)  time: 0.4974  data: 0.0005  max mem: 19734
Epoch: [4]  [ 170/1251]  eta: 0:09:21  lr: 0.000012  loss: 3.6126 (3.5024)  time: 0.4856  data: 0.0005  max mem: 19734
Epoch: [4]  [ 180/1251]  eta: 0:09:14  lr: 0.000012  loss: 3.6516 (3.5103)  time: 0.4849  data: 0.0005  max mem: 19734
Epoch: [4]  [ 190/1251]  eta: 0:09:07  lr: 0.000012  loss: 3.5662 (3.4993)  time: 0.4829  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5119, ratio_loss=0.0037, pruning_loss=0.2220, mse_loss=1.0664
Epoch: [4]  [ 200/1251]  eta: 0:09:00  lr: 0.000012  loss: 3.5714 (3.5111)  time: 0.4816  data: 0.0005  max mem: 19734
Epoch: [4]  [ 210/1251]  eta: 0:08:53  lr: 0.000012  loss: 3.9076 (3.5206)  time: 0.4836  data: 0.0005  max mem: 19734
Epoch: [4]  [ 220/1251]  eta: 0:08:49  lr: 0.000012  loss: 3.7953 (3.5248)  time: 0.5044  data: 0.0005  max mem: 19734
Epoch: [4]  [ 230/1251]  eta: 0:08:44  lr: 0.000012  loss: 3.6685 (3.5175)  time: 0.5288  data: 0.0005  max mem: 19734
Epoch: [4]  [ 240/1251]  eta: 0:08:38  lr: 0.000012  loss: 3.8099 (3.5345)  time: 0.5087  data: 0.0004  max mem: 19734
Epoch: [4]  [ 250/1251]  eta: 0:08:32  lr: 0.000012  loss: 3.8855 (3.5375)  time: 0.4829  data: 0.0005  max mem: 19734
Epoch: [4]  [ 260/1251]  eta: 0:08:25  lr: 0.000012  loss: 3.7251 (3.5389)  time: 0.4814  data: 0.0005  max mem: 19734
Epoch: [4]  [ 270/1251]  eta: 0:08:19  lr: 0.000012  loss: 3.7251 (3.5404)  time: 0.4825  data: 0.0005  max mem: 19734
Epoch: [4]  [ 280/1251]  eta: 0:08:13  lr: 0.000012  loss: 3.8302 (3.5456)  time: 0.4825  data: 0.0006  max mem: 19734
Epoch: [4]  [ 290/1251]  eta: 0:08:07  lr: 0.000012  loss: 3.7671 (3.5430)  time: 0.4818  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5876, ratio_loss=0.0034, pruning_loss=0.2199, mse_loss=1.0039
Epoch: [4]  [ 300/1251]  eta: 0:08:02  lr: 0.000012  loss: 3.6294 (3.5505)  time: 0.4886  data: 0.0004  max mem: 19734
Epoch: [4]  [ 310/1251]  eta: 0:07:56  lr: 0.000012  loss: 3.6294 (3.5424)  time: 0.4876  data: 0.0004  max mem: 19734
Epoch: [4]  [ 320/1251]  eta: 0:07:50  lr: 0.000012  loss: 3.4088 (3.5377)  time: 0.4802  data: 0.0004  max mem: 19734
Epoch: [4]  [ 330/1251]  eta: 0:07:44  lr: 0.000012  loss: 3.3374 (3.5301)  time: 0.4804  data: 0.0005  max mem: 19734
Epoch: [4]  [ 340/1251]  eta: 0:07:39  lr: 0.000012  loss: 3.6926 (3.5345)  time: 0.4833  data: 0.0005  max mem: 19734
Epoch: [4]  [ 350/1251]  eta: 0:07:33  lr: 0.000012  loss: 3.8514 (3.5382)  time: 0.4830  data: 0.0005  max mem: 19734
Epoch: [4]  [ 360/1251]  eta: 0:07:28  lr: 0.000012  loss: 3.6549 (3.5364)  time: 0.4940  data: 0.0007  max mem: 19734
Epoch: [4]  [ 370/1251]  eta: 0:07:24  lr: 0.000012  loss: 3.4193 (3.5348)  time: 0.5298  data: 0.0006  max mem: 19734
Epoch: [4]  [ 380/1251]  eta: 0:07:19  lr: 0.000012  loss: 3.4193 (3.5313)  time: 0.5168  data: 0.0004  max mem: 19734
Epoch: [4]  [ 390/1251]  eta: 0:07:13  lr: 0.000012  loss: 3.5798 (3.5292)  time: 0.4811  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4472, ratio_loss=0.0035, pruning_loss=0.2261, mse_loss=1.0554
Epoch: [4]  [ 400/1251]  eta: 0:07:08  lr: 0.000012  loss: 3.7286 (3.5330)  time: 0.4810  data: 0.0005  max mem: 19734
Epoch: [4]  [ 410/1251]  eta: 0:07:02  lr: 0.000012  loss: 3.6773 (3.5350)  time: 0.4807  data: 0.0005  max mem: 19734
Epoch: [4]  [ 420/1251]  eta: 0:06:57  lr: 0.000012  loss: 3.5617 (3.5303)  time: 0.4800  data: 0.0005  max mem: 19734
Epoch: [4]  [ 430/1251]  eta: 0:06:51  lr: 0.000012  loss: 3.6322 (3.5308)  time: 0.4807  data: 0.0005  max mem: 19734
Epoch: [4]  [ 440/1251]  eta: 0:06:46  lr: 0.000012  loss: 3.4555 (3.5290)  time: 0.4921  data: 0.0005  max mem: 19734
Epoch: [4]  [ 450/1251]  eta: 0:06:41  lr: 0.000012  loss: 3.4555 (3.5297)  time: 0.4906  data: 0.0005  max mem: 19734
Epoch: [4]  [ 460/1251]  eta: 0:06:36  lr: 0.000012  loss: 3.7633 (3.5332)  time: 0.4811  data: 0.0004  max mem: 19734
Epoch: [4]  [ 470/1251]  eta: 0:06:30  lr: 0.000012  loss: 3.8274 (3.5361)  time: 0.4808  data: 0.0004  max mem: 19734
Epoch: [4]  [ 480/1251]  eta: 0:06:25  lr: 0.000012  loss: 3.4124 (3.5297)  time: 0.4786  data: 0.0004  max mem: 19734
Epoch: [4]  [ 490/1251]  eta: 0:06:20  lr: 0.000012  loss: 3.5174 (3.5330)  time: 0.4800  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4763, ratio_loss=0.0038, pruning_loss=0.2243, mse_loss=1.0685
Epoch: [4]  [ 500/1251]  eta: 0:06:14  lr: 0.000012  loss: 3.6353 (3.5333)  time: 0.4797  data: 0.0004  max mem: 19734
Epoch: [4]  [ 510/1251]  eta: 0:06:10  lr: 0.000012  loss: 3.3375 (3.5293)  time: 0.5116  data: 0.0004  max mem: 19734
Epoch: [4]  [ 520/1251]  eta: 0:06:05  lr: 0.000012  loss: 3.6167 (3.5311)  time: 0.5212  data: 0.0004  max mem: 19734
Epoch: [4]  [ 530/1251]  eta: 0:06:00  lr: 0.000012  loss: 3.4275 (3.5238)  time: 0.4897  data: 0.0004  max mem: 19734
Epoch: [4]  [ 540/1251]  eta: 0:05:54  lr: 0.000012  loss: 3.3964 (3.5225)  time: 0.4799  data: 0.0004  max mem: 19734
Epoch: [4]  [ 550/1251]  eta: 0:05:49  lr: 0.000012  loss: 3.6927 (3.5203)  time: 0.4798  data: 0.0004  max mem: 19734
Epoch: [4]  [ 560/1251]  eta: 0:05:44  lr: 0.000012  loss: 3.6302 (3.5212)  time: 0.4818  data: 0.0004  max mem: 19734
Epoch: [4]  [ 570/1251]  eta: 0:05:39  lr: 0.000012  loss: 3.4482 (3.5180)  time: 0.4818  data: 0.0004  max mem: 19734
Epoch: [4]  [ 580/1251]  eta: 0:05:34  lr: 0.000012  loss: 3.5932 (3.5190)  time: 0.4799  data: 0.0004  max mem: 19734
Epoch: [4]  [ 590/1251]  eta: 0:05:28  lr: 0.000012  loss: 3.7584 (3.5231)  time: 0.4837  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4396, ratio_loss=0.0034, pruning_loss=0.2225, mse_loss=0.9807
Epoch: [4]  [ 600/1251]  eta: 0:05:23  lr: 0.000012  loss: 3.7339 (3.5199)  time: 0.4848  data: 0.0004  max mem: 19734
Epoch: [4]  [ 610/1251]  eta: 0:05:18  lr: 0.000012  loss: 3.5728 (3.5193)  time: 0.4812  data: 0.0003  max mem: 19734
Epoch: [4]  [ 620/1251]  eta: 0:05:13  lr: 0.000012  loss: 3.6673 (3.5196)  time: 0.4824  data: 0.0003  max mem: 19734
Epoch: [4]  [ 630/1251]  eta: 0:05:08  lr: 0.000012  loss: 3.8229 (3.5250)  time: 0.4811  data: 0.0003  max mem: 19734
Epoch: [4]  [ 640/1251]  eta: 0:05:03  lr: 0.000012  loss: 3.8229 (3.5246)  time: 0.4797  data: 0.0004  max mem: 19734
Epoch: [4]  [ 650/1251]  eta: 0:04:58  lr: 0.000012  loss: 3.5970 (3.5239)  time: 0.4988  data: 0.0004  max mem: 19734
Epoch: [4]  [ 660/1251]  eta: 0:04:54  lr: 0.000012  loss: 3.3205 (3.5216)  time: 0.5399  data: 0.0004  max mem: 19734
Epoch: [4]  [ 670/1251]  eta: 0:04:49  lr: 0.000012  loss: 3.3205 (3.5201)  time: 0.5224  data: 0.0004  max mem: 19734
Epoch: [4]  [ 680/1251]  eta: 0:04:43  lr: 0.000012  loss: 3.5213 (3.5219)  time: 0.4809  data: 0.0004  max mem: 19734
Epoch: [4]  [ 690/1251]  eta: 0:04:38  lr: 0.000012  loss: 3.6356 (3.5220)  time: 0.4802  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4727, ratio_loss=0.0035, pruning_loss=0.2210, mse_loss=1.0000
Epoch: [4]  [ 700/1251]  eta: 0:04:33  lr: 0.000012  loss: 3.7735 (3.5239)  time: 0.4812  data: 0.0004  max mem: 19734
Epoch: [4]  [ 710/1251]  eta: 0:04:28  lr: 0.000012  loss: 3.7735 (3.5217)  time: 0.4810  data: 0.0004  max mem: 19734
Epoch: [4]  [ 720/1251]  eta: 0:04:23  lr: 0.000012  loss: 3.2681 (3.5179)  time: 0.4815  data: 0.0004  max mem: 19734
Epoch: [4]  [ 730/1251]  eta: 0:04:18  lr: 0.000012  loss: 3.3124 (3.5167)  time: 0.4810  data: 0.0004  max mem: 19734
Epoch: [4]  [ 740/1251]  eta: 0:04:13  lr: 0.000012  loss: 3.4610 (3.5155)  time: 0.4916  data: 0.0004  max mem: 19734
Epoch: [4]  [ 750/1251]  eta: 0:04:08  lr: 0.000012  loss: 3.5868 (3.5170)  time: 0.4906  data: 0.0004  max mem: 19734
Epoch: [4]  [ 760/1251]  eta: 0:04:03  lr: 0.000012  loss: 3.7891 (3.5192)  time: 0.4790  data: 0.0004  max mem: 19734
Epoch: [4]  [ 770/1251]  eta: 0:03:58  lr: 0.000012  loss: 3.5472 (3.5184)  time: 0.4787  data: 0.0004  max mem: 19734
Epoch: [4]  [ 780/1251]  eta: 0:03:53  lr: 0.000012  loss: 3.5472 (3.5180)  time: 0.4777  data: 0.0004  max mem: 19734
Epoch: [4]  [ 790/1251]  eta: 0:03:48  lr: 0.000012  loss: 3.4748 (3.5148)  time: 0.4779  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4160, ratio_loss=0.0038, pruning_loss=0.2200, mse_loss=1.0153
Epoch: [4]  [ 800/1251]  eta: 0:03:43  lr: 0.000012  loss: 3.4748 (3.5149)  time: 0.4977  data: 0.0004  max mem: 19734
Epoch: [4]  [ 810/1251]  eta: 0:03:38  lr: 0.000012  loss: 3.5781 (3.5146)  time: 0.5231  data: 0.0005  max mem: 19734
Epoch: [4]  [ 820/1251]  eta: 0:03:33  lr: 0.000012  loss: 3.6105 (3.5164)  time: 0.5031  data: 0.0006  max mem: 19734
Epoch: [4]  [ 830/1251]  eta: 0:03:28  lr: 0.000012  loss: 3.6979 (3.5178)  time: 0.4794  data: 0.0004  max mem: 19734
Epoch: [4]  [ 840/1251]  eta: 0:03:23  lr: 0.000012  loss: 3.6589 (3.5179)  time: 0.4820  data: 0.0004  max mem: 19734
Epoch: [4]  [ 850/1251]  eta: 0:03:18  lr: 0.000012  loss: 3.5366 (3.5163)  time: 0.4817  data: 0.0004  max mem: 19734
Epoch: [4]  [ 860/1251]  eta: 0:03:13  lr: 0.000012  loss: 3.7581 (3.5207)  time: 0.4805  data: 0.0005  max mem: 19734
Epoch: [4]  [ 870/1251]  eta: 0:03:08  lr: 0.000012  loss: 3.8653 (3.5239)  time: 0.4799  data: 0.0004  max mem: 19734
Epoch: [4]  [ 880/1251]  eta: 0:03:03  lr: 0.000012  loss: 3.7909 (3.5244)  time: 0.4803  data: 0.0004  max mem: 19734
Epoch: [4]  [ 890/1251]  eta: 0:02:58  lr: 0.000012  loss: 3.5802 (3.5229)  time: 0.4873  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5007, ratio_loss=0.0036, pruning_loss=0.2171, mse_loss=1.0001
Epoch: [4]  [ 900/1251]  eta: 0:02:53  lr: 0.000012  loss: 3.2410 (3.5200)  time: 0.4835  data: 0.0004  max mem: 19734
Epoch: [4]  [ 910/1251]  eta: 0:02:48  lr: 0.000012  loss: 3.5808 (3.5221)  time: 0.4774  data: 0.0004  max mem: 19734
Epoch: [4]  [ 920/1251]  eta: 0:02:43  lr: 0.000012  loss: 3.8077 (3.5230)  time: 0.4798  data: 0.0006  max mem: 19734
Epoch: [4]  [ 930/1251]  eta: 0:02:38  lr: 0.000012  loss: 3.7958 (3.5237)  time: 0.4786  data: 0.0006  max mem: 19734
Epoch: [4]  [ 940/1251]  eta: 0:02:33  lr: 0.000012  loss: 3.7176 (3.5247)  time: 0.4997  data: 0.0004  max mem: 19734
Epoch: [4]  [ 950/1251]  eta: 0:02:28  lr: 0.000012  loss: 3.6769 (3.5239)  time: 0.5318  data: 0.0004  max mem: 19734
Epoch: [4]  [ 960/1251]  eta: 0:02:23  lr: 0.000012  loss: 3.6443 (3.5224)  time: 0.5083  data: 0.0004  max mem: 19734
Epoch: [4]  [ 970/1251]  eta: 0:02:18  lr: 0.000012  loss: 3.7031 (3.5234)  time: 0.4750  data: 0.0004  max mem: 19734
Epoch: [4]  [ 980/1251]  eta: 0:02:13  lr: 0.000012  loss: 3.7031 (3.5241)  time: 0.4758  data: 0.0003  max mem: 19734
Epoch: [4]  [ 990/1251]  eta: 0:02:08  lr: 0.000012  loss: 3.4561 (3.5235)  time: 0.4773  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5286, ratio_loss=0.0034, pruning_loss=0.2169, mse_loss=1.0150
Epoch: [4]  [1000/1251]  eta: 0:02:03  lr: 0.000012  loss: 3.6099 (3.5240)  time: 0.4790  data: 0.0004  max mem: 19734
Epoch: [4]  [1010/1251]  eta: 0:01:58  lr: 0.000012  loss: 3.6911 (3.5260)  time: 0.4794  data: 0.0004  max mem: 19734
Epoch: [4]  [1020/1251]  eta: 0:01:53  lr: 0.000012  loss: 3.7974 (3.5251)  time: 0.4814  data: 0.0005  max mem: 19734
Epoch: [4]  [1030/1251]  eta: 0:01:49  lr: 0.000012  loss: 3.1633 (3.5221)  time: 0.4832  data: 0.0006  max mem: 19734
Epoch: [4]  [1040/1251]  eta: 0:01:44  lr: 0.000012  loss: 3.4007 (3.5216)  time: 0.4895  data: 0.0004  max mem: 19734
Epoch: [4]  [1050/1251]  eta: 0:01:39  lr: 0.000012  loss: 3.4830 (3.5212)  time: 0.4875  data: 0.0003  max mem: 19734
Epoch: [4]  [1060/1251]  eta: 0:01:34  lr: 0.000012  loss: 3.6758 (3.5218)  time: 0.4794  data: 0.0004  max mem: 19734
Epoch: [4]  [1070/1251]  eta: 0:01:29  lr: 0.000012  loss: 3.6804 (3.5232)  time: 0.4783  data: 0.0004  max mem: 19734
Epoch: [4]  [1080/1251]  eta: 0:01:24  lr: 0.000012  loss: 3.6483 (3.5233)  time: 0.4767  data: 0.0004  max mem: 19734
Epoch: [4]  [1090/1251]  eta: 0:01:19  lr: 0.000012  loss: 3.6231 (3.5233)  time: 0.5187  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4732, ratio_loss=0.0036, pruning_loss=0.2168, mse_loss=0.9963
Epoch: [4]  [1100/1251]  eta: 0:01:14  lr: 0.000012  loss: 3.5605 (3.5235)  time: 0.5374  data: 0.0004  max mem: 19734
Epoch: [4]  [1110/1251]  eta: 0:01:09  lr: 0.000012  loss: 3.2577 (3.5204)  time: 0.4958  data: 0.0004  max mem: 19734
Epoch: [4]  [1120/1251]  eta: 0:01:04  lr: 0.000012  loss: 3.3373 (3.5199)  time: 0.4791  data: 0.0004  max mem: 19734
Epoch: [4]  [1130/1251]  eta: 0:00:59  lr: 0.000012  loss: 3.4890 (3.5195)  time: 0.4803  data: 0.0003  max mem: 19734
Epoch: [4]  [1140/1251]  eta: 0:00:54  lr: 0.000012  loss: 3.2665 (3.5161)  time: 0.4798  data: 0.0003  max mem: 19734
Epoch: [4]  [1150/1251]  eta: 0:00:49  lr: 0.000012  loss: 3.3094 (3.5161)  time: 0.4783  data: 0.0004  max mem: 19734
Epoch: [4]  [1160/1251]  eta: 0:00:44  lr: 0.000012  loss: 3.5657 (3.5148)  time: 0.4781  data: 0.0004  max mem: 19734
Epoch: [4]  [1170/1251]  eta: 0:00:39  lr: 0.000012  loss: 3.5759 (3.5166)  time: 0.4786  data: 0.0004  max mem: 19734
Epoch: [4]  [1180/1251]  eta: 0:00:34  lr: 0.000012  loss: 3.6115 (3.5161)  time: 0.4878  data: 0.0004  max mem: 19734
Epoch: [4]  [1190/1251]  eta: 0:00:30  lr: 0.000012  loss: 3.6072 (3.5158)  time: 0.4841  data: 0.0012  max mem: 19734
loss info: cls_loss=3.3999, ratio_loss=0.0036, pruning_loss=0.2204, mse_loss=0.9887
Epoch: [4]  [1200/1251]  eta: 0:00:25  lr: 0.000012  loss: 3.6187 (3.5169)  time: 0.4708  data: 0.0010  max mem: 19734
Epoch: [4]  [1210/1251]  eta: 0:00:20  lr: 0.000012  loss: 3.3026 (3.5128)  time: 0.4707  data: 0.0002  max mem: 19734
Epoch: [4]  [1220/1251]  eta: 0:00:15  lr: 0.000012  loss: 3.3418 (3.5133)  time: 0.4700  data: 0.0002  max mem: 19734
Epoch: [4]  [1230/1251]  eta: 0:00:10  lr: 0.000012  loss: 3.4738 (3.5120)  time: 0.4780  data: 0.0002  max mem: 19734
Epoch: [4]  [1240/1251]  eta: 0:00:05  lr: 0.000012  loss: 3.7735 (3.5133)  time: 0.5048  data: 0.0002  max mem: 19734
Epoch: [4]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 3.7735 (3.5156)  time: 0.4983  data: 0.0002  max mem: 19734
Epoch: [4] Total time: 0:10:16 (0.4928 s / it)
Averaged stats: lr: 0.000012  loss: 3.7735 (3.5131)
Test:  [  0/261]  eta: 1:30:21  loss: 0.7078 (0.7078)  acc1: 81.7708 (81.7708)  acc5: 96.8750 (96.8750)  time: 20.7706  data: 20.6008  max mem: 19734
Test:  [ 10/261]  eta: 0:09:48  loss: 0.7078 (0.7625)  acc1: 84.8958 (83.5701)  acc5: 96.8750 (96.1174)  time: 2.3458  data: 2.2076  max mem: 19734
Test:  [ 20/261]  eta: 0:05:29  loss: 0.9829 (0.9201)  acc1: 79.6875 (78.7450)  acc5: 94.7917 (94.7421)  time: 0.3959  data: 0.1887  max mem: 19734
Test:  [ 30/261]  eta: 0:03:48  loss: 0.8190 (0.8326)  acc1: 83.3333 (81.6028)  acc5: 93.7500 (95.1613)  time: 0.2416  data: 0.0130  max mem: 19734
Test:  [ 40/261]  eta: 0:03:23  loss: 0.5770 (0.8030)  acc1: 86.9792 (82.6347)  acc5: 96.3542 (95.3379)  time: 0.4521  data: 0.3042  max mem: 19734
Test:  [ 50/261]  eta: 0:02:47  loss: 0.9566 (0.8658)  acc1: 77.6042 (80.8007)  acc5: 94.2708 (94.8632)  time: 0.4911  data: 0.3019  max mem: 19734
Test:  [ 60/261]  eta: 0:02:20  loss: 1.0143 (0.8778)  acc1: 76.5625 (80.4133)  acc5: 94.2708 (94.9454)  time: 0.2392  data: 0.0126  max mem: 19734
Test:  [ 70/261]  eta: 0:02:17  loss: 0.9641 (0.8812)  acc1: 77.6042 (79.8416)  acc5: 95.8333 (95.2025)  time: 0.5268  data: 0.3408  max mem: 19734
Test:  [ 80/261]  eta: 0:01:57  loss: 0.8480 (0.8807)  acc1: 78.6458 (80.0347)  acc5: 96.8750 (95.2675)  time: 0.5076  data: 0.3410  max mem: 19734
Test:  [ 90/261]  eta: 0:01:45  loss: 0.8466 (0.8666)  acc1: 83.8542 (80.4487)  acc5: 96.3542 (95.3755)  time: 0.2543  data: 0.0105  max mem: 19734
Test:  [100/261]  eta: 0:01:45  loss: 0.8355 (0.8695)  acc1: 83.8542 (80.4249)  acc5: 95.3125 (95.4466)  time: 0.6761  data: 0.3599  max mem: 19734
Test:  [110/261]  eta: 0:01:31  loss: 0.9065 (0.8953)  acc1: 77.0833 (79.9315)  acc5: 94.7917 (95.0967)  time: 0.5733  data: 0.3603  max mem: 19734
Test:  [120/261]  eta: 0:01:20  loss: 1.2279 (0.9329)  acc1: 71.8750 (79.1796)  acc5: 89.0625 (94.5764)  time: 0.1407  data: 0.0107  max mem: 19734
Test:  [130/261]  eta: 0:01:10  loss: 1.3918 (0.9797)  acc1: 69.2708 (78.1926)  acc5: 86.4583 (93.9886)  time: 0.1537  data: 0.0129  max mem: 19734
Test:  [140/261]  eta: 0:01:02  loss: 1.2693 (1.0047)  acc1: 68.7500 (77.5709)  acc5: 89.5833 (93.7648)  time: 0.1966  data: 0.0103  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: 1.2350 (1.0082)  acc1: 72.9167 (77.6042)  acc5: 91.6667 (93.6500)  time: 0.1876  data: 0.0078  max mem: 19734
Test:  [160/261]  eta: 0:00:48  loss: 1.0255 (1.0291)  acc1: 77.0833 (77.3130)  acc5: 92.1875 (93.3197)  time: 0.1915  data: 0.0697  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: 1.3710 (1.0606)  acc1: 64.5833 (76.4833)  acc5: 86.9792 (92.9946)  time: 0.2421  data: 0.1215  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: 1.4877 (1.0772)  acc1: 64.5833 (76.0877)  acc5: 89.5833 (92.8436)  time: 0.2700  data: 0.1496  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: 1.3997 (1.0911)  acc1: 67.1875 (75.8153)  acc5: 90.6250 (92.6702)  time: 0.1967  data: 0.1055  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: 1.3646 (1.1055)  acc1: 69.2708 (75.4742)  acc5: 89.0625 (92.4466)  time: 0.0862  data: 0.0117  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: 1.3646 (1.1191)  acc1: 69.2708 (75.2296)  acc5: 87.5000 (92.2862)  time: 0.0942  data: 0.0202  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: 1.4077 (1.1391)  acc1: 67.1875 (74.7290)  acc5: 88.5417 (92.1003)  time: 0.1073  data: 0.0335  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: 1.4928 (1.1493)  acc1: 65.6250 (74.5491)  acc5: 89.0625 (91.9688)  time: 0.0872  data: 0.0136  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.3424 (1.1592)  acc1: 68.7500 (74.2760)  acc5: 90.1042 (91.8763)  time: 0.0737  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 0.9927 (1.1505)  acc1: 75.0000 (74.4688)  acc5: 92.7083 (92.0028)  time: 0.0737  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9275 (1.1492)  acc1: 77.0833 (74.4780)  acc5: 95.8333 (92.0780)  time: 0.0716  data: 0.0002  max mem: 19734
Test: Total time: 0:01:29 (0.3417 s / it)
* Acc@1 74.478 Acc@5 92.078 loss 1.149
Accuracy of the network on the 50000 test images: 74.5%
Max accuracy: 74.48%
Epoch: [5]  [   0/1251]  eta: 3:32:10  lr: 0.000016  loss: 3.6416 (3.6416)  time: 10.1765  data: 9.7409  max mem: 19734
Epoch: [5]  [  10/1251]  eta: 0:29:38  lr: 0.000016  loss: 3.7817 (3.7487)  time: 1.4331  data: 0.8859  max mem: 19734
Epoch: [5]  [  20/1251]  eta: 0:20:15  lr: 0.000016  loss: 3.7697 (3.6626)  time: 0.5278  data: 0.0004  max mem: 19734
Epoch: [5]  [  30/1251]  eta: 0:16:45  lr: 0.000016  loss: 3.6647 (3.6085)  time: 0.4880  data: 0.0004  max mem: 19734
Epoch: [5]  [  40/1251]  eta: 0:14:55  lr: 0.000016  loss: 3.6647 (3.6614)  time: 0.4789  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5242, ratio_loss=0.0040, pruning_loss=0.2155, mse_loss=0.9755
Epoch: [5]  [  50/1251]  eta: 0:13:46  lr: 0.000016  loss: 3.7652 (3.6778)  time: 0.4788  data: 0.0004  max mem: 19734
Epoch: [5]  [  60/1251]  eta: 0:12:59  lr: 0.000016  loss: 3.7652 (3.6700)  time: 0.4808  data: 0.0005  max mem: 19734
Epoch: [5]  [  70/1251]  eta: 0:12:25  lr: 0.000016  loss: 3.7969 (3.6678)  time: 0.4849  data: 0.0005  max mem: 19734
Epoch: [5]  [  80/1251]  eta: 0:11:57  lr: 0.000016  loss: 3.7341 (3.6476)  time: 0.4845  data: 0.0005  max mem: 19734
Epoch: [5]  [  90/1251]  eta: 0:11:34  lr: 0.000016  loss: 3.5557 (3.6397)  time: 0.4808  data: 0.0005  max mem: 19734
Epoch: [5]  [ 100/1251]  eta: 0:11:14  lr: 0.000016  loss: 3.5026 (3.5989)  time: 0.4805  data: 0.0004  max mem: 19734
Epoch: [5]  [ 110/1251]  eta: 0:11:00  lr: 0.000016  loss: 3.5596 (3.5982)  time: 0.4925  data: 0.0004  max mem: 19734
Epoch: [5]  [ 120/1251]  eta: 0:10:51  lr: 0.000016  loss: 3.7768 (3.6035)  time: 0.5263  data: 0.0004  max mem: 19734
Epoch: [5]  [ 130/1251]  eta: 0:10:39  lr: 0.000016  loss: 3.7128 (3.5976)  time: 0.5238  data: 0.0005  max mem: 19734
Epoch: [5]  [ 140/1251]  eta: 0:10:26  lr: 0.000016  loss: 3.4627 (3.5890)  time: 0.4891  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5140, ratio_loss=0.0037, pruning_loss=0.2125, mse_loss=0.9356
Epoch: [5]  [ 150/1251]  eta: 0:10:15  lr: 0.000016  loss: 3.6592 (3.5874)  time: 0.4809  data: 0.0005  max mem: 19734
Epoch: [5]  [ 160/1251]  eta: 0:10:04  lr: 0.000016  loss: 3.6642 (3.5845)  time: 0.4813  data: 0.0005  max mem: 19734
Epoch: [5]  [ 170/1251]  eta: 0:09:54  lr: 0.000016  loss: 3.6874 (3.5891)  time: 0.4880  data: 0.0004  max mem: 19734
Epoch: [5]  [ 180/1251]  eta: 0:09:45  lr: 0.000016  loss: 3.4705 (3.5746)  time: 0.4866  data: 0.0005  max mem: 19734
Epoch: [5]  [ 190/1251]  eta: 0:09:35  lr: 0.000016  loss: 3.3602 (3.5665)  time: 0.4766  data: 0.0004  max mem: 19734
Epoch: [5]  [ 200/1251]  eta: 0:09:26  lr: 0.000016  loss: 3.5709 (3.5543)  time: 0.4770  data: 0.0004  max mem: 19734
Epoch: [5]  [ 210/1251]  eta: 0:09:18  lr: 0.000016  loss: 3.5157 (3.5392)  time: 0.4806  data: 0.0004  max mem: 19734
Epoch: [5]  [ 220/1251]  eta: 0:09:10  lr: 0.000016  loss: 3.1937 (3.5284)  time: 0.4820  data: 0.0004  max mem: 19734
Epoch: [5]  [ 230/1251]  eta: 0:09:03  lr: 0.000016  loss: 3.4246 (3.5231)  time: 0.4810  data: 0.0004  max mem: 19734
Epoch: [5]  [ 240/1251]  eta: 0:08:55  lr: 0.000016  loss: 3.5885 (3.5221)  time: 0.4795  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3467, ratio_loss=0.0043, pruning_loss=0.2179, mse_loss=0.9679
Epoch: [5]  [ 250/1251]  eta: 0:08:48  lr: 0.000016  loss: 3.6797 (3.5152)  time: 0.4774  data: 0.0004  max mem: 19734
Epoch: [5]  [ 260/1251]  eta: 0:08:42  lr: 0.000016  loss: 3.7080 (3.5216)  time: 0.5036  data: 0.0004  max mem: 19734
Epoch: [5]  [ 270/1251]  eta: 0:08:37  lr: 0.000016  loss: 3.6188 (3.5185)  time: 0.5332  data: 0.0004  max mem: 19734
Epoch: [5]  [ 280/1251]  eta: 0:08:30  lr: 0.000016  loss: 3.6330 (3.5194)  time: 0.5069  data: 0.0004  max mem: 19734
Epoch: [5]  [ 290/1251]  eta: 0:08:24  lr: 0.000016  loss: 3.6723 (3.5237)  time: 0.4786  data: 0.0004  max mem: 19734
Epoch: [5]  [ 300/1251]  eta: 0:08:17  lr: 0.000016  loss: 3.6340 (3.5228)  time: 0.4779  data: 0.0004  max mem: 19734
Epoch: [5]  [ 310/1251]  eta: 0:08:10  lr: 0.000016  loss: 3.5572 (3.5225)  time: 0.4779  data: 0.0004  max mem: 19734
Epoch: [5]  [ 320/1251]  eta: 0:08:04  lr: 0.000016  loss: 3.5944 (3.5218)  time: 0.4881  data: 0.0004  max mem: 19734
Epoch: [5]  [ 330/1251]  eta: 0:07:58  lr: 0.000016  loss: 3.6253 (3.5187)  time: 0.4873  data: 0.0006  max mem: 19734
Epoch: [5]  [ 340/1251]  eta: 0:07:52  lr: 0.000016  loss: 3.6490 (3.5222)  time: 0.4795  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5211, ratio_loss=0.0038, pruning_loss=0.2102, mse_loss=0.9266
Epoch: [5]  [ 350/1251]  eta: 0:07:46  lr: 0.000016  loss: 3.5538 (3.5146)  time: 0.4817  data: 0.0004  max mem: 19734
Epoch: [5]  [ 360/1251]  eta: 0:07:40  lr: 0.000016  loss: 3.5249 (3.5138)  time: 0.4823  data: 0.0004  max mem: 19734
Epoch: [5]  [ 370/1251]  eta: 0:07:34  lr: 0.000016  loss: 3.6044 (3.5220)  time: 0.4826  data: 0.0004  max mem: 19734
Epoch: [5]  [ 380/1251]  eta: 0:07:28  lr: 0.000016  loss: 3.7046 (3.5185)  time: 0.4823  data: 0.0004  max mem: 19734
Epoch: [5]  [ 390/1251]  eta: 0:07:22  lr: 0.000016  loss: 3.5786 (3.5187)  time: 0.4801  data: 0.0004  max mem: 19734
Epoch: [5]  [ 400/1251]  eta: 0:07:16  lr: 0.000016  loss: 3.4066 (3.5124)  time: 0.4883  data: 0.0004  max mem: 19734
Epoch: [5]  [ 410/1251]  eta: 0:07:12  lr: 0.000016  loss: 3.4066 (3.5113)  time: 0.5177  data: 0.0004  max mem: 19734
Epoch: [5]  [ 420/1251]  eta: 0:07:06  lr: 0.000016  loss: 3.5017 (3.5053)  time: 0.5217  data: 0.0004  max mem: 19734
Epoch: [5]  [ 430/1251]  eta: 0:07:01  lr: 0.000016  loss: 3.2463 (3.4986)  time: 0.4923  data: 0.0004  max mem: 19734
Epoch: [5]  [ 440/1251]  eta: 0:06:55  lr: 0.000016  loss: 3.4175 (3.4957)  time: 0.4790  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3761, ratio_loss=0.0040, pruning_loss=0.2155, mse_loss=0.9824
Epoch: [5]  [ 450/1251]  eta: 0:06:49  lr: 0.000016  loss: 3.7312 (3.5017)  time: 0.4808  data: 0.0004  max mem: 19734
Epoch: [5]  [ 460/1251]  eta: 0:06:44  lr: 0.000016  loss: 3.6641 (3.5044)  time: 0.4810  data: 0.0004  max mem: 19734
Epoch: [5]  [ 470/1251]  eta: 0:06:38  lr: 0.000016  loss: 3.6482 (3.5088)  time: 0.4867  data: 0.0004  max mem: 19734
Epoch: [5]  [ 480/1251]  eta: 0:06:33  lr: 0.000016  loss: 3.7150 (3.5092)  time: 0.4889  data: 0.0004  max mem: 19734
Epoch: [5]  [ 490/1251]  eta: 0:06:27  lr: 0.000016  loss: 3.3893 (3.5032)  time: 0.4804  data: 0.0004  max mem: 19734
Epoch: [5]  [ 500/1251]  eta: 0:06:21  lr: 0.000016  loss: 3.3893 (3.5023)  time: 0.4786  data: 0.0003  max mem: 19734
Epoch: [5]  [ 510/1251]  eta: 0:06:16  lr: 0.000016  loss: 3.4554 (3.4987)  time: 0.4810  data: 0.0004  max mem: 19734
Epoch: [5]  [ 520/1251]  eta: 0:06:11  lr: 0.000016  loss: 3.2819 (3.5003)  time: 0.4808  data: 0.0003  max mem: 19734
Epoch: [5]  [ 530/1251]  eta: 0:06:05  lr: 0.000016  loss: 3.8052 (3.5014)  time: 0.4804  data: 0.0003  max mem: 19734
Epoch: [5]  [ 540/1251]  eta: 0:06:00  lr: 0.000016  loss: 3.7088 (3.5064)  time: 0.4796  data: 0.0003  max mem: 19734
loss info: cls_loss=3.4689, ratio_loss=0.0042, pruning_loss=0.2135, mse_loss=0.9505
Epoch: [5]  [ 550/1251]  eta: 0:05:55  lr: 0.000016  loss: 3.6380 (3.5000)  time: 0.5126  data: 0.0005  max mem: 19734
Epoch: [5]  [ 560/1251]  eta: 0:05:50  lr: 0.000016  loss: 3.5893 (3.5001)  time: 0.5359  data: 0.0005  max mem: 19734
Epoch: [5]  [ 570/1251]  eta: 0:05:45  lr: 0.000016  loss: 3.5587 (3.5007)  time: 0.5009  data: 0.0003  max mem: 19734
Epoch: [5]  [ 580/1251]  eta: 0:05:39  lr: 0.000016  loss: 3.4723 (3.4981)  time: 0.4765  data: 0.0003  max mem: 19734
Epoch: [5]  [ 590/1251]  eta: 0:05:34  lr: 0.000016  loss: 3.5460 (3.5000)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [5]  [ 600/1251]  eta: 0:05:29  lr: 0.000016  loss: 3.3166 (3.4934)  time: 0.4762  data: 0.0003  max mem: 19734
Epoch: [5]  [ 610/1251]  eta: 0:05:23  lr: 0.000016  loss: 3.1512 (3.4927)  time: 0.4771  data: 0.0003  max mem: 19734
Epoch: [5]  [ 620/1251]  eta: 0:05:18  lr: 0.000016  loss: 3.4777 (3.4958)  time: 0.4838  data: 0.0005  max mem: 19734
Epoch: [5]  [ 630/1251]  eta: 0:05:13  lr: 0.000016  loss: 3.7626 (3.4948)  time: 0.4836  data: 0.0005  max mem: 19734
Epoch: [5]  [ 640/1251]  eta: 0:05:07  lr: 0.000016  loss: 3.6589 (3.4945)  time: 0.4804  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4039, ratio_loss=0.0043, pruning_loss=0.2152, mse_loss=0.8929
Epoch: [5]  [ 650/1251]  eta: 0:05:02  lr: 0.000016  loss: 3.5069 (3.4890)  time: 0.4826  data: 0.0005  max mem: 19734
Epoch: [5]  [ 660/1251]  eta: 0:04:57  lr: 0.000016  loss: 3.3147 (3.4898)  time: 0.4802  data: 0.0005  max mem: 19734
Epoch: [5]  [ 670/1251]  eta: 0:04:52  lr: 0.000016  loss: 3.6229 (3.4883)  time: 0.4780  data: 0.0005  max mem: 19734
Epoch: [5]  [ 680/1251]  eta: 0:04:46  lr: 0.000016  loss: 3.5374 (3.4915)  time: 0.4790  data: 0.0005  max mem: 19734
Epoch: [5]  [ 690/1251]  eta: 0:04:42  lr: 0.000016  loss: 3.4201 (3.4869)  time: 0.4987  data: 0.0004  max mem: 19734
Epoch: [5]  [ 700/1251]  eta: 0:04:37  lr: 0.000016  loss: 3.2150 (3.4832)  time: 0.5249  data: 0.0003  max mem: 19734
Epoch: [5]  [ 710/1251]  eta: 0:04:32  lr: 0.000016  loss: 3.5070 (3.4838)  time: 0.5105  data: 0.0003  max mem: 19734
Epoch: [5]  [ 720/1251]  eta: 0:04:26  lr: 0.000016  loss: 3.5535 (3.4825)  time: 0.4841  data: 0.0005  max mem: 19734
Epoch: [5]  [ 730/1251]  eta: 0:04:21  lr: 0.000016  loss: 3.3841 (3.4821)  time: 0.4770  data: 0.0005  max mem: 19734
Epoch: [5]  [ 740/1251]  eta: 0:04:16  lr: 0.000016  loss: 3.4458 (3.4800)  time: 0.4776  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3347, ratio_loss=0.0043, pruning_loss=0.2160, mse_loss=0.9022
Epoch: [5]  [ 750/1251]  eta: 0:04:11  lr: 0.000016  loss: 3.0411 (3.4742)  time: 0.4798  data: 0.0004  max mem: 19734
Epoch: [5]  [ 760/1251]  eta: 0:04:06  lr: 0.000016  loss: 3.3406 (3.4757)  time: 0.4803  data: 0.0004  max mem: 19734
Epoch: [5]  [ 770/1251]  eta: 0:04:01  lr: 0.000016  loss: 3.6928 (3.4752)  time: 0.4846  data: 0.0003  max mem: 19734
Epoch: [5]  [ 780/1251]  eta: 0:03:55  lr: 0.000016  loss: 3.6387 (3.4767)  time: 0.4810  data: 0.0003  max mem: 19734
Epoch: [5]  [ 790/1251]  eta: 0:03:50  lr: 0.000016  loss: 3.5585 (3.4731)  time: 0.4752  data: 0.0003  max mem: 19734
Epoch: [5]  [ 800/1251]  eta: 0:03:45  lr: 0.000016  loss: 3.5346 (3.4720)  time: 0.4776  data: 0.0003  max mem: 19734
Epoch: [5]  [ 810/1251]  eta: 0:03:40  lr: 0.000016  loss: 3.7513 (3.4752)  time: 0.4786  data: 0.0003  max mem: 19734
Epoch: [5]  [ 820/1251]  eta: 0:03:35  lr: 0.000016  loss: 3.5834 (3.4738)  time: 0.4799  data: 0.0004  max mem: 19734
Epoch: [5]  [ 830/1251]  eta: 0:03:30  lr: 0.000016  loss: 3.1491 (3.4708)  time: 0.4795  data: 0.0004  max mem: 19734
Epoch: [5]  [ 840/1251]  eta: 0:03:25  lr: 0.000016  loss: 3.4213 (3.4723)  time: 0.5155  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4016, ratio_loss=0.0046, pruning_loss=0.2167, mse_loss=0.9481
Epoch: [5]  [ 850/1251]  eta: 0:03:20  lr: 0.000016  loss: 3.5541 (3.4732)  time: 0.5351  data: 0.0005  max mem: 19734
Epoch: [5]  [ 860/1251]  eta: 0:03:15  lr: 0.000016  loss: 3.4354 (3.4718)  time: 0.4998  data: 0.0005  max mem: 19734
Epoch: [5]  [ 870/1251]  eta: 0:03:10  lr: 0.000016  loss: 3.4527 (3.4726)  time: 0.4798  data: 0.0004  max mem: 19734
Epoch: [5]  [ 880/1251]  eta: 0:03:05  lr: 0.000016  loss: 3.5964 (3.4735)  time: 0.4767  data: 0.0003  max mem: 19734
Epoch: [5]  [ 890/1251]  eta: 0:03:00  lr: 0.000016  loss: 3.5964 (3.4743)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [5]  [ 900/1251]  eta: 0:02:55  lr: 0.000016  loss: 3.9103 (3.4793)  time: 0.4780  data: 0.0003  max mem: 19734
Epoch: [5]  [ 910/1251]  eta: 0:02:50  lr: 0.000016  loss: 3.8750 (3.4806)  time: 0.4864  data: 0.0004  max mem: 19734
Epoch: [5]  [ 920/1251]  eta: 0:02:45  lr: 0.000016  loss: 3.8252 (3.4838)  time: 0.4839  data: 0.0004  max mem: 19734
Epoch: [5]  [ 930/1251]  eta: 0:02:40  lr: 0.000016  loss: 3.6772 (3.4798)  time: 0.4760  data: 0.0003  max mem: 19734
Epoch: [5]  [ 940/1251]  eta: 0:02:35  lr: 0.000016  loss: 3.4043 (3.4795)  time: 0.4767  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4879, ratio_loss=0.0047, pruning_loss=0.2117, mse_loss=0.8793
Epoch: [5]  [ 950/1251]  eta: 0:02:29  lr: 0.000016  loss: 3.4100 (3.4794)  time: 0.4739  data: 0.0004  max mem: 19734
Epoch: [5]  [ 960/1251]  eta: 0:02:24  lr: 0.000016  loss: 3.4900 (3.4784)  time: 0.4754  data: 0.0003  max mem: 19734
Epoch: [5]  [ 970/1251]  eta: 0:02:19  lr: 0.000016  loss: 3.5165 (3.4796)  time: 0.4787  data: 0.0003  max mem: 19734
Epoch: [5]  [ 980/1251]  eta: 0:02:14  lr: 0.000016  loss: 3.6066 (3.4799)  time: 0.5059  data: 0.0004  max mem: 19734
Epoch: [5]  [ 990/1251]  eta: 0:02:10  lr: 0.000016  loss: 3.6549 (3.4812)  time: 0.5315  data: 0.0004  max mem: 19734
Epoch: [5]  [1000/1251]  eta: 0:02:05  lr: 0.000016  loss: 3.6549 (3.4807)  time: 0.5108  data: 0.0004  max mem: 19734
Epoch: [5]  [1010/1251]  eta: 0:02:00  lr: 0.000016  loss: 3.7654 (3.4838)  time: 0.4821  data: 0.0003  max mem: 19734
Epoch: [5]  [1020/1251]  eta: 0:01:55  lr: 0.000016  loss: 3.7654 (3.4838)  time: 0.4742  data: 0.0003  max mem: 19734
Epoch: [5]  [1030/1251]  eta: 0:01:49  lr: 0.000016  loss: 3.1384 (3.4810)  time: 0.4760  data: 0.0003  max mem: 19734
Epoch: [5]  [1040/1251]  eta: 0:01:44  lr: 0.000016  loss: 3.3686 (3.4810)  time: 0.4764  data: 0.0003  max mem: 19734
loss info: cls_loss=3.4455, ratio_loss=0.0047, pruning_loss=0.2086, mse_loss=0.8648
Epoch: [5]  [1050/1251]  eta: 0:01:39  lr: 0.000016  loss: 3.5089 (3.4822)  time: 0.4744  data: 0.0003  max mem: 19734
Epoch: [5]  [1060/1251]  eta: 0:01:34  lr: 0.000016  loss: 3.5089 (3.4823)  time: 0.4885  data: 0.0004  max mem: 19734
Epoch: [5]  [1070/1251]  eta: 0:01:29  lr: 0.000016  loss: 3.4304 (3.4817)  time: 0.4911  data: 0.0006  max mem: 19734
Epoch: [5]  [1080/1251]  eta: 0:01:24  lr: 0.000016  loss: 3.3365 (3.4798)  time: 0.4781  data: 0.0006  max mem: 19734
Epoch: [5]  [1090/1251]  eta: 0:01:19  lr: 0.000016  loss: 3.5564 (3.4827)  time: 0.4773  data: 0.0004  max mem: 19734
Epoch: [5]  [1100/1251]  eta: 0:01:14  lr: 0.000016  loss: 3.6505 (3.4828)  time: 0.4771  data: 0.0004  max mem: 19734
Epoch: [5]  [1110/1251]  eta: 0:01:09  lr: 0.000016  loss: 3.6737 (3.4829)  time: 0.4758  data: 0.0004  max mem: 19734
Epoch: [5]  [1120/1251]  eta: 0:01:05  lr: 0.000016  loss: 3.6737 (3.4820)  time: 0.4770  data: 0.0004  max mem: 19734
Epoch: [5]  [1130/1251]  eta: 0:01:00  lr: 0.000016  loss: 3.4614 (3.4818)  time: 0.5080  data: 0.0006  max mem: 19734
Epoch: [5]  [1140/1251]  eta: 0:00:55  lr: 0.000016  loss: 3.5938 (3.4823)  time: 0.5350  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4356, ratio_loss=0.0047, pruning_loss=0.2111, mse_loss=0.8456
Epoch: [5]  [1150/1251]  eta: 0:00:50  lr: 0.000016  loss: 3.4242 (3.4803)  time: 0.5061  data: 0.0005  max mem: 19734
Epoch: [5]  [1160/1251]  eta: 0:00:45  lr: 0.000016  loss: 3.4621 (3.4802)  time: 0.4773  data: 0.0006  max mem: 19734
Epoch: [5]  [1170/1251]  eta: 0:00:40  lr: 0.000016  loss: 3.5975 (3.4819)  time: 0.4757  data: 0.0006  max mem: 19734
Epoch: [5]  [1180/1251]  eta: 0:00:35  lr: 0.000016  loss: 3.5224 (3.4804)  time: 0.4770  data: 0.0004  max mem: 19734
Epoch: [5]  [1190/1251]  eta: 0:00:30  lr: 0.000016  loss: 3.3549 (3.4778)  time: 0.4776  data: 0.0007  max mem: 19734
Epoch: [5]  [1200/1251]  eta: 0:00:25  lr: 0.000016  loss: 3.4542 (3.4780)  time: 0.4732  data: 0.0006  max mem: 19734
Epoch: [5]  [1210/1251]  eta: 0:00:20  lr: 0.000016  loss: 3.6804 (3.4790)  time: 0.4770  data: 0.0002  max mem: 19734
Epoch: [5]  [1220/1251]  eta: 0:00:15  lr: 0.000016  loss: 3.8181 (3.4801)  time: 0.4764  data: 0.0002  max mem: 19734
Epoch: [5]  [1230/1251]  eta: 0:00:10  lr: 0.000016  loss: 3.4550 (3.4799)  time: 0.4702  data: 0.0002  max mem: 19734
Epoch: [5]  [1240/1251]  eta: 0:00:05  lr: 0.000016  loss: 3.3712 (3.4786)  time: 0.4721  data: 0.0002  max mem: 19734
loss info: cls_loss=3.4112, ratio_loss=0.0047, pruning_loss=0.2113, mse_loss=0.8630
Epoch: [5]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.5348 (3.4783)  time: 0.4713  data: 0.0002  max mem: 19734
Epoch: [5] Total time: 0:10:20 (0.4956 s / it)
Averaged stats: lr: 0.000016  loss: 3.5348 (3.4843)
Test:  [  0/261]  eta: 1:06:49  loss: 0.7083 (0.7083)  acc1: 83.3333 (83.3333)  acc5: 96.8750 (96.8750)  time: 15.3625  data: 15.2549  max mem: 19734
Test:  [ 10/261]  eta: 0:08:43  loss: 0.7083 (0.7503)  acc1: 85.4167 (84.0436)  acc5: 96.3542 (96.1174)  time: 2.0842  data: 1.8697  max mem: 19734
Test:  [ 20/261]  eta: 0:05:01  loss: 0.9372 (0.9128)  acc1: 80.2083 (79.1419)  acc5: 94.2708 (94.7669)  time: 0.5444  data: 0.2729  max mem: 19734
Test:  [ 30/261]  eta: 0:03:31  loss: 0.7685 (0.8266)  acc1: 82.2917 (81.8212)  acc5: 95.3125 (95.3293)  time: 0.2723  data: 0.0133  max mem: 19734
Test:  [ 40/261]  eta: 0:03:11  loss: 0.5941 (0.7950)  acc1: 86.9792 (82.8252)  acc5: 96.8750 (95.5666)  time: 0.4646  data: 0.2907  max mem: 19734
Test:  [ 50/261]  eta: 0:02:32  loss: 0.9084 (0.8605)  acc1: 77.6042 (80.9436)  acc5: 94.7917 (95.0776)  time: 0.4212  data: 0.2911  max mem: 19734
Test:  [ 60/261]  eta: 0:02:05  loss: 1.0072 (0.8748)  acc1: 76.5625 (80.4901)  acc5: 93.7500 (95.0734)  time: 0.1248  data: 0.0114  max mem: 19734
Test:  [ 70/261]  eta: 0:01:50  loss: 0.9595 (0.8776)  acc1: 77.6042 (80.0690)  acc5: 95.8333 (95.3198)  time: 0.2052  data: 0.0885  max mem: 19734
Test:  [ 80/261]  eta: 0:01:52  loss: 0.8535 (0.8776)  acc1: 78.6458 (80.1698)  acc5: 96.8750 (95.4218)  time: 0.6044  data: 0.4976  max mem: 19734
Test:  [ 90/261]  eta: 0:01:36  loss: 0.8238 (0.8623)  acc1: 82.8125 (80.5861)  acc5: 95.8333 (95.5128)  time: 0.5337  data: 0.4214  max mem: 19734
Test:  [100/261]  eta: 0:01:24  loss: 0.8174 (0.8647)  acc1: 82.8125 (80.6828)  acc5: 95.3125 (95.5394)  time: 0.1524  data: 0.0136  max mem: 19734
Test:  [110/261]  eta: 0:01:17  loss: 0.9065 (0.8932)  acc1: 78.1250 (80.1990)  acc5: 94.2708 (95.1530)  time: 0.2698  data: 0.0896  max mem: 19734
Test:  [120/261]  eta: 0:01:08  loss: 1.2633 (0.9346)  acc1: 70.3125 (79.2657)  acc5: 88.5417 (94.5764)  time: 0.2869  data: 0.0889  max mem: 19734
Test:  [130/261]  eta: 0:01:00  loss: 1.4684 (0.9839)  acc1: 67.7083 (78.2801)  acc5: 87.5000 (93.9289)  time: 0.1931  data: 0.0138  max mem: 19734
Test:  [140/261]  eta: 0:00:53  loss: 1.2880 (1.0117)  acc1: 69.2708 (77.6559)  acc5: 88.5417 (93.6466)  time: 0.1942  data: 0.0169  max mem: 19734
Test:  [150/261]  eta: 0:00:47  loss: 1.2096 (1.0198)  acc1: 72.9167 (77.6111)  acc5: 90.6250 (93.5017)  time: 0.1938  data: 0.0133  max mem: 19734
Test:  [160/261]  eta: 0:00:43  loss: 1.1029 (1.0415)  acc1: 76.5625 (77.2710)  acc5: 91.6667 (93.1806)  time: 0.3390  data: 0.1857  max mem: 19734
Test:  [170/261]  eta: 0:00:38  loss: 1.3255 (1.0732)  acc1: 65.6250 (76.4681)  acc5: 86.4583 (92.8698)  time: 0.3618  data: 0.2177  max mem: 19734
Test:  [180/261]  eta: 0:00:32  loss: 1.5052 (1.0905)  acc1: 64.5833 (76.0532)  acc5: 88.5417 (92.6853)  time: 0.2028  data: 0.0584  max mem: 19734
Test:  [190/261]  eta: 0:00:28  loss: 1.4375 (1.1035)  acc1: 66.1458 (75.7363)  acc5: 90.1042 (92.5447)  time: 0.2976  data: 0.1860  max mem: 19734
Test:  [200/261]  eta: 0:00:23  loss: 1.4375 (1.1185)  acc1: 70.8333 (75.4198)  acc5: 90.1042 (92.3248)  time: 0.2534  data: 0.1658  max mem: 19734
Test:  [210/261]  eta: 0:00:19  loss: 1.4309 (1.1320)  acc1: 69.2708 (75.1679)  acc5: 88.0208 (92.1455)  time: 0.0776  data: 0.0018  max mem: 19734
Test:  [220/261]  eta: 0:00:14  loss: 1.4407 (1.1524)  acc1: 66.1458 (74.6653)  acc5: 86.9792 (91.9353)  time: 0.0793  data: 0.0051  max mem: 19734
Test:  [230/261]  eta: 0:00:10  loss: 1.4226 (1.1619)  acc1: 66.1458 (74.4566)  acc5: 87.5000 (91.8177)  time: 0.0788  data: 0.0051  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: 1.4048 (1.1721)  acc1: 67.1875 (74.1917)  acc5: 90.6250 (91.7207)  time: 0.0739  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: 1.0425 (1.1633)  acc1: 73.9583 (74.3941)  acc5: 93.7500 (91.8513)  time: 0.0739  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9258 (1.1614)  acc1: 78.6458 (74.4240)  acc5: 95.3125 (91.9320)  time: 0.0717  data: 0.0002  max mem: 19734
Test: Total time: 0:01:23 (0.3196 s / it)
* Acc@1 74.424 Acc@5 91.932 loss 1.161
Accuracy of the network on the 50000 test images: 74.4%
Max accuracy: 74.48%
Epoch: [6]  [   0/1251]  eta: 6:27:48  lr: 0.000020  loss: 2.7736 (2.7736)  time: 18.6001  data: 8.8062  max mem: 19734
Epoch: [6]  [  10/1251]  eta: 0:47:18  lr: 0.000020  loss: 3.3468 (3.3444)  time: 2.2871  data: 0.8027  max mem: 19734
Epoch: [6]  [  20/1251]  eta: 0:30:12  lr: 0.000020  loss: 3.4531 (3.3615)  time: 0.6156  data: 0.0018  max mem: 19734
Epoch: [6]  [  30/1251]  eta: 0:23:46  lr: 0.000020  loss: 3.4883 (3.3671)  time: 0.5528  data: 0.0008  max mem: 19734
Epoch: [6]  [  40/1251]  eta: 0:20:13  lr: 0.000020  loss: 3.4883 (3.4161)  time: 0.5083  data: 0.0005  max mem: 19734
Epoch: [6]  [  50/1251]  eta: 0:18:02  lr: 0.000020  loss: 3.6088 (3.3799)  time: 0.4870  data: 0.0005  max mem: 19734
Epoch: [6]  [  60/1251]  eta: 0:16:32  lr: 0.000020  loss: 3.0392 (3.3262)  time: 0.4868  data: 0.0005  max mem: 19734
Epoch: [6]  [  70/1251]  eta: 0:15:30  lr: 0.000020  loss: 3.3776 (3.3523)  time: 0.4989  data: 0.0005  max mem: 19734
Epoch: [6]  [  80/1251]  eta: 0:14:39  lr: 0.000020  loss: 3.5606 (3.3852)  time: 0.5006  data: 0.0007  max mem: 19734
Epoch: [6]  [  90/1251]  eta: 0:13:57  lr: 0.000020  loss: 3.5606 (3.3847)  time: 0.4873  data: 0.0007  max mem: 19734
loss info: cls_loss=3.3517, ratio_loss=0.0052, pruning_loss=0.2112, mse_loss=0.8500
Epoch: [6]  [ 100/1251]  eta: 0:13:23  lr: 0.000020  loss: 3.6705 (3.4004)  time: 0.4850  data: 0.0005  max mem: 19734
Epoch: [6]  [ 110/1251]  eta: 0:12:54  lr: 0.000020  loss: 3.5678 (3.3859)  time: 0.4839  data: 0.0005  max mem: 19734
Epoch: [6]  [ 120/1251]  eta: 0:12:29  lr: 0.000020  loss: 3.5250 (3.3948)  time: 0.4824  data: 0.0004  max mem: 19734
Epoch: [6]  [ 130/1251]  eta: 0:12:08  lr: 0.000020  loss: 3.5966 (3.4064)  time: 0.4860  data: 0.0004  max mem: 19734
Epoch: [6]  [ 140/1251]  eta: 0:11:48  lr: 0.000020  loss: 3.5055 (3.4108)  time: 0.4890  data: 0.0004  max mem: 19734
Epoch: [6]  [ 150/1251]  eta: 0:11:31  lr: 0.000020  loss: 3.5055 (3.4264)  time: 0.4875  data: 0.0004  max mem: 19734
Epoch: [6]  [ 160/1251]  eta: 0:11:19  lr: 0.000020  loss: 3.7764 (3.4391)  time: 0.5181  data: 0.0004  max mem: 19734
Epoch: [6]  [ 170/1251]  eta: 0:11:06  lr: 0.000020  loss: 3.5954 (3.4308)  time: 0.5331  data: 0.0004  max mem: 19734
Epoch: [6]  [ 180/1251]  eta: 0:10:55  lr: 0.000020  loss: 3.4534 (3.4346)  time: 0.5205  data: 0.0004  max mem: 19734
Epoch: [6]  [ 190/1251]  eta: 0:10:41  lr: 0.000020  loss: 3.5687 (3.4435)  time: 0.5025  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4686, ratio_loss=0.0053, pruning_loss=0.2066, mse_loss=0.8414
Epoch: [6]  [ 200/1251]  eta: 0:10:29  lr: 0.000020  loss: 3.7311 (3.4589)  time: 0.4832  data: 0.0005  max mem: 19734
Epoch: [6]  [ 210/1251]  eta: 0:10:17  lr: 0.000020  loss: 3.5239 (3.4561)  time: 0.4846  data: 0.0005  max mem: 19734
Epoch: [6]  [ 220/1251]  eta: 0:10:07  lr: 0.000020  loss: 3.2189 (3.4335)  time: 0.4965  data: 0.0004  max mem: 19734
Epoch: [6]  [ 230/1251]  eta: 0:09:57  lr: 0.000020  loss: 3.3136 (3.4365)  time: 0.4966  data: 0.0004  max mem: 19734
Epoch: [6]  [ 240/1251]  eta: 0:09:47  lr: 0.000020  loss: 3.4423 (3.4380)  time: 0.4835  data: 0.0004  max mem: 19734
Epoch: [6]  [ 250/1251]  eta: 0:09:37  lr: 0.000020  loss: 3.4380 (3.4347)  time: 0.4826  data: 0.0004  max mem: 19734
Epoch: [6]  [ 260/1251]  eta: 0:09:28  lr: 0.000020  loss: 3.4344 (3.4364)  time: 0.4825  data: 0.0005  max mem: 19734
Epoch: [6]  [ 270/1251]  eta: 0:09:19  lr: 0.000020  loss: 3.5890 (3.4387)  time: 0.4828  data: 0.0005  max mem: 19734
Epoch: [6]  [ 280/1251]  eta: 0:09:10  lr: 0.000020  loss: 3.2924 (3.4369)  time: 0.4838  data: 0.0004  max mem: 19734
Epoch: [6]  [ 290/1251]  eta: 0:09:02  lr: 0.000020  loss: 3.4678 (3.4397)  time: 0.4831  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3611, ratio_loss=0.0059, pruning_loss=0.2108, mse_loss=0.8056
Epoch: [6]  [ 300/1251]  eta: 0:08:53  lr: 0.000020  loss: 3.4681 (3.4409)  time: 0.4835  data: 0.0004  max mem: 19734
Epoch: [6]  [ 310/1251]  eta: 0:08:47  lr: 0.000020  loss: 3.6060 (3.4421)  time: 0.5132  data: 0.0004  max mem: 19734
Epoch: [6]  [ 320/1251]  eta: 0:08:41  lr: 0.000020  loss: 3.5418 (3.4365)  time: 0.5410  data: 0.0004  max mem: 19734
Epoch: [6]  [ 330/1251]  eta: 0:08:33  lr: 0.000020  loss: 3.5418 (3.4457)  time: 0.5141  data: 0.0004  max mem: 19734
Epoch: [6]  [ 340/1251]  eta: 0:08:26  lr: 0.000020  loss: 3.7732 (3.4464)  time: 0.4856  data: 0.0004  max mem: 19734
Epoch: [6]  [ 350/1251]  eta: 0:08:18  lr: 0.000020  loss: 3.5845 (3.4412)  time: 0.4852  data: 0.0004  max mem: 19734
Epoch: [6]  [ 360/1251]  eta: 0:08:11  lr: 0.000020  loss: 3.7026 (3.4473)  time: 0.4866  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 370/1251]  eta: 0:08:04  lr: 0.000020  loss: 3.1542 (3.3761)  time: 0.4882  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 380/1251]  eta: 0:07:57  lr: 0.000020  loss: 0.0000 (3.2875)  time: 0.4791  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 390/1251]  eta: 0:07:50  lr: 0.000020  loss: 0.0000 (3.2034)  time: 0.4699  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 400/1251]  eta: 0:07:43  lr: 0.000020  loss: 0.0000 (3.1235)  time: 0.4702  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 410/1251]  eta: 0:07:36  lr: 0.000020  loss: 0.0000 (3.0475)  time: 0.4764  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 420/1251]  eta: 0:07:29  lr: 0.000020  loss: 0.0000 (2.9751)  time: 0.4765  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 430/1251]  eta: 0:07:22  lr: 0.000020  loss: 0.0000 (2.9061)  time: 0.4686  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 440/1251]  eta: 0:07:16  lr: 0.000020  loss: 0.0000 (2.8402)  time: 0.4671  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 450/1251]  eta: 0:07:10  lr: 0.000020  loss: 0.0000 (2.7772)  time: 0.5004  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 460/1251]  eta: 0:07:04  lr: 0.000020  loss: 0.0000 (2.7170)  time: 0.5217  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 470/1251]  eta: 0:06:58  lr: 0.000020  loss: 0.0000 (2.6593)  time: 0.5039  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 480/1251]  eta: 0:06:52  lr: 0.000020  loss: 0.0000 (2.6040)  time: 0.4861  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 490/1251]  eta: 0:06:45  lr: 0.000020  loss: 0.0000 (2.5510)  time: 0.4694  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 500/1251]  eta: 0:06:39  lr: 0.000020  loss: 0.0000 (2.5001)  time: 0.4677  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 510/1251]  eta: 0:06:33  lr: 0.000020  loss: 0.0000 (2.4511)  time: 0.4700  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 520/1251]  eta: 0:06:27  lr: 0.000020  loss: 0.0000 (2.4041)  time: 0.4778  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 530/1251]  eta: 0:06:21  lr: 0.000020  loss: 0.0000 (2.3588)  time: 0.4832  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 540/1251]  eta: 0:06:15  lr: 0.000020  loss: 0.0000 (2.3152)  time: 0.4768  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 550/1251]  eta: 0:06:09  lr: 0.000020  loss: 0.0000 (2.2732)  time: 0.4701  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 560/1251]  eta: 0:06:03  lr: 0.000020  loss: 0.0000 (2.2327)  time: 0.4708  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 570/1251]  eta: 0:05:57  lr: 0.000020  loss: 0.0000 (2.1936)  time: 0.4712  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 580/1251]  eta: 0:05:51  lr: 0.000020  loss: 0.0000 (2.1558)  time: 0.4678  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 590/1251]  eta: 0:05:45  lr: 0.000020  loss: 0.0000 (2.1193)  time: 0.4707  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 600/1251]  eta: 0:05:40  lr: 0.000020  loss: 0.0000 (2.0841)  time: 0.5091  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 610/1251]  eta: 0:05:35  lr: 0.000020  loss: 0.0000 (2.0500)  time: 0.5179  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 620/1251]  eta: 0:05:29  lr: 0.000020  loss: 0.0000 (2.0170)  time: 0.4793  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 630/1251]  eta: 0:05:23  lr: 0.000020  loss: 0.0000 (1.9850)  time: 0.4684  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 640/1251]  eta: 0:05:18  lr: 0.000020  loss: 0.0000 (1.9540)  time: 0.4705  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 650/1251]  eta: 0:05:12  lr: 0.000020  loss: 0.0000 (1.9240)  time: 0.4714  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 660/1251]  eta: 0:05:06  lr: 0.000020  loss: 0.0000 (1.8949)  time: 0.4792  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 670/1251]  eta: 0:05:01  lr: 0.000020  loss: 0.0000 (1.8667)  time: 0.4910  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 680/1251]  eta: 0:04:55  lr: 0.000020  loss: 0.0000 (1.8393)  time: 0.4827  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 690/1251]  eta: 0:04:50  lr: 0.000020  loss: 0.0000 (1.8126)  time: 0.4702  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 700/1251]  eta: 0:04:44  lr: 0.000020  loss: 0.0000 (1.7868)  time: 0.4711  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 710/1251]  eta: 0:04:39  lr: 0.000020  loss: 0.0000 (1.7616)  time: 0.4714  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 720/1251]  eta: 0:04:33  lr: 0.000020  loss: 0.0000 (1.7372)  time: 0.4736  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 730/1251]  eta: 0:04:28  lr: 0.000020  loss: 0.0000 (1.7135)  time: 0.4772  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 740/1251]  eta: 0:04:23  lr: 0.000020  loss: 0.0000 (1.6903)  time: 0.5055  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 750/1251]  eta: 0:04:18  lr: 0.000020  loss: 0.0000 (1.6678)  time: 0.5198  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 760/1251]  eta: 0:04:12  lr: 0.000020  loss: 0.0000 (1.6459)  time: 0.4945  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 770/1251]  eta: 0:04:07  lr: 0.000020  loss: 0.0000 (1.6246)  time: 0.4860  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 780/1251]  eta: 0:04:02  lr: 0.000020  loss: 0.0000 (1.6038)  time: 0.4826  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 790/1251]  eta: 0:03:56  lr: 0.000020  loss: 0.0000 (1.5835)  time: 0.4760  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 800/1251]  eta: 0:03:51  lr: 0.000020  loss: 0.0000 (1.5637)  time: 0.4724  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 810/1251]  eta: 0:03:45  lr: 0.000020  loss: 0.0000 (1.5444)  time: 0.4688  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 820/1251]  eta: 0:03:40  lr: 0.000020  loss: 0.0000 (1.5256)  time: 0.4785  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 830/1251]  eta: 0:03:35  lr: 0.000020  loss: 0.0000 (1.5073)  time: 0.4768  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 840/1251]  eta: 0:03:30  lr: 0.000020  loss: 0.0000 (1.4893)  time: 0.4673  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 850/1251]  eta: 0:03:24  lr: 0.000020  loss: 0.0000 (1.4718)  time: 0.4740  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 860/1251]  eta: 0:03:19  lr: 0.000020  loss: 0.0000 (1.4547)  time: 0.4723  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 870/1251]  eta: 0:03:14  lr: 0.000020  loss: 0.0000 (1.4380)  time: 0.4653  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 880/1251]  eta: 0:03:09  lr: 0.000020  loss: 0.0000 (1.4217)  time: 0.4786  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 890/1251]  eta: 0:03:03  lr: 0.000020  loss: 0.0000 (1.4058)  time: 0.5049  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 900/1251]  eta: 0:02:58  lr: 0.000020  loss: 0.0000 (1.3902)  time: 0.5075  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 910/1251]  eta: 0:02:53  lr: 0.000020  loss: 0.0000 (1.3749)  time: 0.4892  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 920/1251]  eta: 0:02:48  lr: 0.000020  loss: 0.0000 (1.3600)  time: 0.4769  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 930/1251]  eta: 0:02:43  lr: 0.000020  loss: 0.0000 (1.3454)  time: 0.4695  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 940/1251]  eta: 0:02:37  lr: 0.000020  loss: 0.0000 (1.3311)  time: 0.4675  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 950/1251]  eta: 0:02:32  lr: 0.000020  loss: 0.0000 (1.3171)  time: 0.4685  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 960/1251]  eta: 0:02:27  lr: 0.000020  loss: 0.0000 (1.3034)  time: 0.4735  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 970/1251]  eta: 0:02:22  lr: 0.000020  loss: 0.0000 (1.2899)  time: 0.4773  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 980/1251]  eta: 0:02:17  lr: 0.000020  loss: 0.0000 (1.2768)  time: 0.4747  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 990/1251]  eta: 0:02:12  lr: 0.000020  loss: 0.0000 (1.2639)  time: 0.4703  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1000/1251]  eta: 0:02:06  lr: 0.000020  loss: 0.0000 (1.2513)  time: 0.4690  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1010/1251]  eta: 0:02:01  lr: 0.000020  loss: 0.0000 (1.2389)  time: 0.4687  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1020/1251]  eta: 0:01:56  lr: 0.000020  loss: 0.0000 (1.2268)  time: 0.4664  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1030/1251]  eta: 0:01:51  lr: 0.000020  loss: 0.0000 (1.2149)  time: 0.4902  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1040/1251]  eta: 0:01:46  lr: 0.000020  loss: 0.0000 (1.2032)  time: 0.5069  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1050/1251]  eta: 0:01:41  lr: 0.000020  loss: 0.0000 (1.1918)  time: 0.4903  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1060/1251]  eta: 0:01:36  lr: 0.000020  loss: 0.0000 (1.1805)  time: 0.4731  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1070/1251]  eta: 0:01:31  lr: 0.000020  loss: 0.0000 (1.1695)  time: 0.4656  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1080/1251]  eta: 0:01:26  lr: 0.000020  loss: 0.0000 (1.1587)  time: 0.4656  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1090/1251]  eta: 0:01:21  lr: 0.000020  loss: 0.0000 (1.1481)  time: 0.4649  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1100/1251]  eta: 0:01:15  lr: 0.000020  loss: 0.0000 (1.1376)  time: 0.4700  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1110/1251]  eta: 0:01:10  lr: 0.000020  loss: 0.0000 (1.1274)  time: 0.4804  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1120/1251]  eta: 0:01:05  lr: 0.000020  loss: 0.0000 (1.1173)  time: 0.4749  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1130/1251]  eta: 0:01:00  lr: 0.000020  loss: 0.0000 (1.1075)  time: 0.4645  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1140/1251]  eta: 0:00:55  lr: 0.000020  loss: 0.0000 (1.0977)  time: 0.4677  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1150/1251]  eta: 0:00:50  lr: 0.000020  loss: 0.0000 (1.0882)  time: 0.4673  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1160/1251]  eta: 0:00:45  lr: 0.000020  loss: 0.0000 (1.0788)  time: 0.4709  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1170/1251]  eta: 0:00:40  lr: 0.000020  loss: 0.0000 (1.0696)  time: 0.4819  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1180/1251]  eta: 0:00:35  lr: 0.000020  loss: 0.0000 (1.0606)  time: 0.5124  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1190/1251]  eta: 0:00:30  lr: 0.000020  loss: 0.0000 (1.0517)  time: 0.5139  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1200/1251]  eta: 0:00:25  lr: 0.000020  loss: 0.0000 (1.0429)  time: 0.4716  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1210/1251]  eta: 0:00:20  lr: 0.000020  loss: 0.0000 (1.0343)  time: 0.4539  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1220/1251]  eta: 0:00:15  lr: 0.000020  loss: 0.0000 (1.0258)  time: 0.4550  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1230/1251]  eta: 0:00:10  lr: 0.000020  loss: 0.0000 (1.0175)  time: 0.4564  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 0.0000 (1.0093)  time: 0.4553  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 0.0000 (1.0012)  time: 0.4538  data: 0.0002  max mem: 19734
Epoch: [6] Total time: 0:10:25 (0.4999 s / it)
Averaged stats: lr: 0.000020  loss: 0.0000 (1.0084)
Test:  [  0/261]  eta: 2:23:36  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 33.0132  data: 32.9323  max mem: 19734
Test:  [ 10/261]  eta: 0:12:55  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.0882  data: 3.0019  max mem: 19734
Test:  [ 20/261]  eta: 0:07:16  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2502  data: 0.0083  max mem: 19734
Test:  [ 30/261]  eta: 0:04:55  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2855  data: 0.0135  max mem: 19734
Test:  [ 40/261]  eta: 0:04:11  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4343  data: 0.2613  max mem: 19734
Test:  [ 50/261]  eta: 0:03:20  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4387  data: 0.2577  max mem: 19734
Test:  [ 60/261]  eta: 0:02:45  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1705  data: 0.0119  max mem: 19734
Test:  [ 70/261]  eta: 0:02:19  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.1698  data: 0.0113  max mem: 19734
Test:  [ 80/261]  eta: 0:02:08  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3767  data: 0.2266  max mem: 19734
Test:  [ 90/261]  eta: 0:01:52  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.3888  data: 0.2290  max mem: 19734
Test:  [100/261]  eta: 0:01:40  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.2563  data: 0.0770  max mem: 19734
Test:  [110/261]  eta: 0:01:35  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.5208  data: 0.3481  max mem: 19734
Test:  [120/261]  eta: 0:01:24  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.4745  data: 0.2870  max mem: 19734
Test:  [130/261]  eta: 0:01:13  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.1822  data: 0.0123  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3295  data: 0.1826  max mem: 19734
Test:  [150/261]  eta: 0:01:00  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4166  data: 0.2361  max mem: 19734
Test:  [160/261]  eta: 0:00:52  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2603  data: 0.0684  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2226  data: 0.0752  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2117  data: 0.1191  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1274  data: 0.0556  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0725  data: 0.0013  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0713  data: 0.0003  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0705  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0704  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0703  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0703  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0682  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3605 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.48%
Loss is nan, stopping training this iteration.
Epoch: [7]  [   0/1251]  eta: 6:34:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 18.9221  data: 16.3928  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  10/1251]  eta: 0:46:59  lr: 0.000020  loss: 0.0000 (0.0000)  time: 2.2717  data: 1.4924  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  20/1251]  eta: 0:29:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5395  data: 0.0014  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  30/1251]  eta: 0:22:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4740  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  40/1251]  eta: 0:19:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4793  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  50/1251]  eta: 0:17:16  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4779  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  60/1251]  eta: 0:16:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5080  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  70/1251]  eta: 0:15:09  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5325  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  80/1251]  eta: 0:14:22  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5115  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  90/1251]  eta: 0:13:42  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4908  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 100/1251]  eta: 0:13:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4828  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 110/1251]  eta: 0:12:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4785  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 120/1251]  eta: 0:12:16  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4720  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 130/1251]  eta: 0:11:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4822  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 140/1251]  eta: 0:11:36  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4804  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 150/1251]  eta: 0:11:19  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4708  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 160/1251]  eta: 0:11:04  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4800  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 170/1251]  eta: 0:10:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4797  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 180/1251]  eta: 0:10:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4733  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 190/1251]  eta: 0:10:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4736  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 200/1251]  eta: 0:10:12  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4826  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 210/1251]  eta: 0:10:03  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5089  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 220/1251]  eta: 0:09:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5192  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 230/1251]  eta: 0:09:44  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4940  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 240/1251]  eta: 0:09:34  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4735  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 250/1251]  eta: 0:09:24  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4709  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 260/1251]  eta: 0:09:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 270/1251]  eta: 0:09:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4708  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 280/1251]  eta: 0:08:59  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4811  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 290/1251]  eta: 0:08:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4865  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 300/1251]  eta: 0:08:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4769  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 310/1251]  eta: 0:08:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4710  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 320/1251]  eta: 0:08:27  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4678  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 330/1251]  eta: 0:08:19  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4673  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 340/1251]  eta: 0:08:12  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4691  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 350/1251]  eta: 0:08:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5060  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 360/1251]  eta: 0:08:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5298  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 370/1251]  eta: 0:07:53  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4925  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 380/1251]  eta: 0:07:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4686  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 390/1251]  eta: 0:07:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4683  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 400/1251]  eta: 0:07:33  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4693  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 410/1251]  eta: 0:07:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4751  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 420/1251]  eta: 0:07:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4803  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 430/1251]  eta: 0:07:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4747  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 440/1251]  eta: 0:07:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4669  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 450/1251]  eta: 0:07:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4677  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 460/1251]  eta: 0:06:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4687  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 470/1251]  eta: 0:06:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4699  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 480/1251]  eta: 0:06:42  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4737  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 490/1251]  eta: 0:06:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4822  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 500/1251]  eta: 0:06:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5098  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 510/1251]  eta: 0:06:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5158  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 520/1251]  eta: 0:06:21  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5039  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 530/1251]  eta: 0:06:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4934  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 540/1251]  eta: 0:06:09  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4810  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 550/1251]  eta: 0:06:03  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4772  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 560/1251]  eta: 0:05:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4707  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 570/1251]  eta: 0:05:52  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4783  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 580/1251]  eta: 0:05:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4772  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 590/1251]  eta: 0:05:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4699  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 600/1251]  eta: 0:05:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4735  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 610/1251]  eta: 0:05:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 620/1251]  eta: 0:05:24  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4673  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 630/1251]  eta: 0:05:18  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 640/1251]  eta: 0:05:13  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5163  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 650/1251]  eta: 0:05:09  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5517  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 660/1251]  eta: 0:05:03  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5085  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 670/1251]  eta: 0:04:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4738  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 680/1251]  eta: 0:04:52  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4674  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 690/1251]  eta: 0:04:47  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4677  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 700/1251]  eta: 0:04:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4697  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 710/1251]  eta: 0:04:36  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4690  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 720/1251]  eta: 0:04:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4801  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 730/1251]  eta: 0:04:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4810  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 740/1251]  eta: 0:04:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 750/1251]  eta: 0:04:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4672  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 760/1251]  eta: 0:04:09  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4674  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 770/1251]  eta: 0:04:04  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4690  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 780/1251]  eta: 0:03:59  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4889  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 790/1251]  eta: 0:03:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5098  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 800/1251]  eta: 0:03:48  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5107  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 810/1251]  eta: 0:03:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4876  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 820/1251]  eta: 0:03:38  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4661  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 830/1251]  eta: 0:03:33  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4670  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 840/1251]  eta: 0:03:27  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4678  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 850/1251]  eta: 0:03:22  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4730  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 860/1251]  eta: 0:03:17  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4713  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 870/1251]  eta: 0:03:12  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4742  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 880/1251]  eta: 0:03:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4742  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 890/1251]  eta: 0:03:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4656  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 900/1251]  eta: 0:02:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4653  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 910/1251]  eta: 0:02:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 920/1251]  eta: 0:02:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4791  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 930/1251]  eta: 0:02:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5000  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 940/1251]  eta: 0:02:36  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5139  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 950/1251]  eta: 0:02:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4985  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 960/1251]  eta: 0:02:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4760  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 970/1251]  eta: 0:02:21  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4669  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 980/1251]  eta: 0:02:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4688  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 990/1251]  eta: 0:02:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1000/1251]  eta: 0:02:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4655  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1010/1251]  eta: 0:02:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4646  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1020/1251]  eta: 0:01:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4739  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1030/1251]  eta: 0:01:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4754  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1040/1251]  eta: 0:01:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4690  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1050/1251]  eta: 0:01:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4667  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1060/1251]  eta: 0:01:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4641  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1070/1251]  eta: 0:01:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4912  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1080/1251]  eta: 0:01:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5280  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1090/1251]  eta: 0:01:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5127  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1100/1251]  eta: 0:01:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4819  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1110/1251]  eta: 0:01:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1120/1251]  eta: 0:01:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4652  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1130/1251]  eta: 0:01:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4658  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1140/1251]  eta: 0:00:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4658  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1150/1251]  eta: 0:00:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4783  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1160/1251]  eta: 0:00:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4925  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1170/1251]  eta: 0:00:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4834  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1180/1251]  eta: 0:00:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4700  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1190/1251]  eta: 0:00:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4658  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1200/1251]  eta: 0:00:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4601  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1210/1251]  eta: 0:00:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4595  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1220/1251]  eta: 0:00:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4742  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1230/1251]  eta: 0:00:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4905  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4806  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4592  data: 0.0001  max mem: 19734
Epoch: [7] Total time: 0:10:21 (0.4968 s / it)
Averaged stats: lr: 0.000020  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:00:41  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 27.7452  data: 27.4691  max mem: 19734
Test:  [ 10/261]  eta: 0:12:08  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.9020  data: 2.7084  max mem: 19734
Test:  [ 20/261]  eta: 0:06:24  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2881  data: 0.1211  max mem: 19734
Test:  [ 30/261]  eta: 0:04:23  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1696  data: 0.0113  max mem: 19734
Test:  [ 40/261]  eta: 0:03:56  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.5201  data: 0.3594  max mem: 19734
Test:  [ 50/261]  eta: 0:03:11  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5543  data: 0.3575  max mem: 19734
Test:  [ 60/261]  eta: 0:02:40  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2433  data: 0.0120  max mem: 19734
Test:  [ 70/261]  eta: 0:02:38  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.6297  data: 0.4141  max mem: 19734
Test:  [ 80/261]  eta: 0:02:16  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.6123  data: 0.4143  max mem: 19734
Test:  [ 90/261]  eta: 0:01:58  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.1987  data: 0.0149  max mem: 19734
Test:  [100/261]  eta: 0:01:52  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4817  data: 0.3070  max mem: 19734
Test:  [110/261]  eta: 0:01:38  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4550  data: 0.3067  max mem: 19734
Test:  [120/261]  eta: 0:01:25  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.1480  data: 0.0140  max mem: 19734
Test:  [130/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.1727  data: 0.0113  max mem: 19734
Test:  [140/261]  eta: 0:01:07  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.2635  data: 0.0911  max mem: 19734
Test:  [150/261]  eta: 0:01:03  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.5325  data: 0.3616  max mem: 19734
Test:  [160/261]  eta: 0:00:54  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.4370  data: 0.2813  max mem: 19734
Test:  [170/261]  eta: 0:00:47  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1701  data: 0.0448  max mem: 19734
Test:  [180/261]  eta: 0:00:40  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1965  data: 0.0720  max mem: 19734
Test:  [190/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1879  data: 0.0614  max mem: 19734
Test:  [200/261]  eta: 0:00:28  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1863  data: 0.0714  max mem: 19734
Test:  [210/261]  eta: 0:00:23  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1327  data: 0.0439  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0707  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:13  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0706  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0704  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0708  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0686  data: 0.0002  max mem: 19734
Test: Total time: 0:01:39 (0.3823 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.48%
Loss is nan, stopping training this iteration.
Epoch: [8]  [   0/1251]  eta: 6:03:13  lr: 0.000020  loss: 0.0000 (0.0000)  time: 17.4211  data: 12.5029  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  10/1251]  eta: 0:44:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 2.1525  data: 1.1375  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  20/1251]  eta: 0:27:44  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5486  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  30/1251]  eta: 0:21:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4897  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  40/1251]  eta: 0:18:47  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4895  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  50/1251]  eta: 0:16:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4722  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  60/1251]  eta: 0:15:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4752  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  70/1251]  eta: 0:14:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4768  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  80/1251]  eta: 0:13:48  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4797  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [  90/1251]  eta: 0:13:12  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4814  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 100/1251]  eta: 0:12:44  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4905  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 110/1251]  eta: 0:12:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5276  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 120/1251]  eta: 0:12:04  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5224  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 130/1251]  eta: 0:11:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4814  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 140/1251]  eta: 0:11:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4716  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 150/1251]  eta: 0:11:08  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4744  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 160/1251]  eta: 0:10:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4771  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 170/1251]  eta: 0:10:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4853  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 180/1251]  eta: 0:10:27  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4800  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 190/1251]  eta: 0:10:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4673  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 200/1251]  eta: 0:10:03  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4690  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 210/1251]  eta: 0:09:53  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4709  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 220/1251]  eta: 0:09:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4750  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 230/1251]  eta: 0:09:33  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4738  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 240/1251]  eta: 0:09:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4703  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 250/1251]  eta: 0:09:17  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4988  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 260/1251]  eta: 0:09:09  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5135  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 270/1251]  eta: 0:09:02  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5035  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 280/1251]  eta: 0:08:53  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4931  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 290/1251]  eta: 0:08:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4746  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 300/1251]  eta: 0:08:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4679  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 310/1251]  eta: 0:08:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4658  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 320/1251]  eta: 0:08:22  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4721  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 330/1251]  eta: 0:08:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4734  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 340/1251]  eta: 0:08:08  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4723  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 350/1251]  eta: 0:08:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4735  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 360/1251]  eta: 0:07:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4690  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 370/1251]  eta: 0:07:47  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4678  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 380/1251]  eta: 0:07:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4687  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 390/1251]  eta: 0:07:34  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4842  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 400/1251]  eta: 0:07:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5212  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 410/1251]  eta: 0:07:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5186  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 420/1251]  eta: 0:07:17  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4796  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 430/1251]  eta: 0:07:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4653  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 440/1251]  eta: 0:07:04  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4661  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 450/1251]  eta: 0:06:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4681  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 460/1251]  eta: 0:06:52  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 470/1251]  eta: 0:06:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4837  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 480/1251]  eta: 0:06:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4801  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 490/1251]  eta: 0:06:34  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4686  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 500/1251]  eta: 0:06:28  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4708  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 510/1251]  eta: 0:06:22  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4688  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 520/1251]  eta: 0:06:16  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4667  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 530/1251]  eta: 0:06:11  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4734  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 540/1251]  eta: 0:06:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4933  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 550/1251]  eta: 0:06:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5229  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 560/1251]  eta: 0:05:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5037  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 570/1251]  eta: 0:05:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 580/1251]  eta: 0:05:44  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 590/1251]  eta: 0:05:38  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4728  data: 0.0005  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 600/1251]  eta: 0:05:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4725  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 610/1251]  eta: 0:05:27  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4690  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 620/1251]  eta: 0:05:22  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4766  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 630/1251]  eta: 0:05:16  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4765  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 640/1251]  eta: 0:05:11  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4678  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 650/1251]  eta: 0:05:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4700  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 660/1251]  eta: 0:05:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4743  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 670/1251]  eta: 0:04:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4721  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 680/1251]  eta: 0:04:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4860  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 690/1251]  eta: 0:04:44  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5190  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 700/1251]  eta: 0:04:39  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5077  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 710/1251]  eta: 0:04:34  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4759  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 720/1251]  eta: 0:04:28  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4730  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 730/1251]  eta: 0:04:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4701  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 740/1251]  eta: 0:04:18  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4653  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 750/1251]  eta: 0:04:12  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4652  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 760/1251]  eta: 0:04:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4630  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 770/1251]  eta: 0:04:02  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4688  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 780/1251]  eta: 0:03:57  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4773  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 790/1251]  eta: 0:03:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4727  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 800/1251]  eta: 0:03:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4656  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 810/1251]  eta: 0:03:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4641  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 820/1251]  eta: 0:03:36  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4650  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 830/1251]  eta: 0:03:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4921  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 840/1251]  eta: 0:03:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5253  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 850/1251]  eta: 0:03:21  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5030  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 860/1251]  eta: 0:03:16  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4691  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 870/1251]  eta: 0:03:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4673  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 880/1251]  eta: 0:03:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4695  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 890/1251]  eta: 0:03:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4696  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 900/1251]  eta: 0:02:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4703  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 910/1251]  eta: 0:02:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4859  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 920/1251]  eta: 0:02:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4900  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 930/1251]  eta: 0:02:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 940/1251]  eta: 0:02:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4646  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 950/1251]  eta: 0:02:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4651  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 960/1251]  eta: 0:02:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4646  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 970/1251]  eta: 0:02:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4896  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 980/1251]  eta: 0:02:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5162  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [ 990/1251]  eta: 0:02:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5002  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1000/1251]  eta: 0:02:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4758  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1010/1251]  eta: 0:02:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4683  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1020/1251]  eta: 0:01:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4679  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1030/1251]  eta: 0:01:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4708  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1040/1251]  eta: 0:01:44  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4723  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1050/1251]  eta: 0:01:39  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4685  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1060/1251]  eta: 0:01:34  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4659  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1070/1251]  eta: 0:01:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4735  data: 0.0003  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1080/1251]  eta: 0:01:24  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4731  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1090/1251]  eta: 0:01:19  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0004  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1100/1251]  eta: 0:01:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4718  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1110/1251]  eta: 0:01:09  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4694  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1120/1251]  eta: 0:01:04  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4850  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1130/1251]  eta: 0:00:59  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4949  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1140/1251]  eta: 0:00:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4809  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1150/1251]  eta: 0:00:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4704  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1160/1251]  eta: 0:00:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4722  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1170/1251]  eta: 0:00:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4738  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1180/1251]  eta: 0:00:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4675  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1190/1251]  eta: 0:00:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4645  data: 0.0007  max mem: 19734
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1200/1251]  eta: 0:00:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4582  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1210/1251]  eta: 0:00:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4549  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1220/1251]  eta: 0:00:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4616  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1230/1251]  eta: 0:00:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4632  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4562  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [8]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4544  data: 0.0002  max mem: 19734
Epoch: [8] Total time: 0:10:16 (0.4930 s / it)
Averaged stats: lr: 0.000020  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:35:35  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 35.7669  data: 35.6181  max mem: 19734
Test:  [ 10/261]  eta: 0:14:11  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.3935  data: 3.2448  max mem: 19734
Test:  [ 20/261]  eta: 0:07:25  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.1510  data: 0.0104  max mem: 19734
Test:  [ 30/261]  eta: 0:05:00  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1532  data: 0.0128  max mem: 19734
Test:  [ 40/261]  eta: 0:04:37  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.6305  data: 0.4835  max mem: 19734
Test:  [ 50/261]  eta: 0:03:39  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.6280  data: 0.4841  max mem: 19734
Test:  [ 60/261]  eta: 0:02:59  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1581  data: 0.0160  max mem: 19734
Test:  [ 70/261]  eta: 0:02:39  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3111  data: 0.1684  max mem: 19734
Test:  [ 80/261]  eta: 0:02:16  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3183  data: 0.1658  max mem: 19734
Test:  [ 90/261]  eta: 0:01:58  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.1888  data: 0.0131  max mem: 19734
Test:  [100/261]  eta: 0:01:49  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.3843  data: 0.2166  max mem: 19734
Test:  [110/261]  eta: 0:01:39  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.5196  data: 0.3574  max mem: 19734
Test:  [120/261]  eta: 0:01:27  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.3362  data: 0.1568  max mem: 19734
Test:  [130/261]  eta: 0:01:22  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.4798  data: 0.3149  max mem: 19734
Test:  [140/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.6610  data: 0.5027  max mem: 19734
Test:  [150/261]  eta: 0:01:06  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.3499  data: 0.2030  max mem: 19734
Test:  [160/261]  eta: 0:00:57  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1216  data: 0.0117  max mem: 19734
Test:  [170/261]  eta: 0:00:49  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1151  data: 0.0292  max mem: 19734
Test:  [180/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.0990  data: 0.0245  max mem: 19734
Test:  [190/261]  eta: 0:00:34  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.0723  data: 0.0018  max mem: 19734
Test:  [200/261]  eta: 0:00:28  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0745  data: 0.0018  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0737  data: 0.0013  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0701  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0700  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0701  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0701  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0681  data: 0.0002  max mem: 19734
Test: Total time: 0:01:38 (0.3786 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.48%
Loss is nan, stopping training this iteration.
Epoch: [9]  [   0/1251]  eta: 6:43:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 19.3727  data: 12.8859  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  10/1251]  eta: 0:47:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 2.3174  data: 1.1722  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  20/1251]  eta: 0:29:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5474  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  30/1251]  eta: 0:23:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5104  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [  40/1251]  eta: 0:19:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5064  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  50/1251]  eta: 0:17:44  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4758  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  60/1251]  eta: 0:16:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4750  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  70/1251]  eta: 0:15:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4760  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  80/1251]  eta: 0:14:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4886  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [  90/1251]  eta: 0:13:42  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4872  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 100/1251]  eta: 0:13:08  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4756  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 110/1251]  eta: 0:12:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4734  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 120/1251]  eta: 0:12:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4719  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 130/1251]  eta: 0:11:53  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4719  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 140/1251]  eta: 0:11:34  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4763  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 150/1251]  eta: 0:11:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4971  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 160/1251]  eta: 0:11:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5171  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 170/1251]  eta: 0:10:53  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5045  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 180/1251]  eta: 0:10:39  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4803  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 190/1251]  eta: 0:10:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 200/1251]  eta: 0:10:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4718  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 210/1251]  eta: 0:10:03  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4764  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 220/1251]  eta: 0:09:52  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4734  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 230/1251]  eta: 0:09:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4816  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 240/1251]  eta: 0:09:33  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4798  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 250/1251]  eta: 0:09:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4650  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 260/1251]  eta: 0:09:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4674  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 270/1251]  eta: 0:09:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4748  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 280/1251]  eta: 0:08:57  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4757  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 290/1251]  eta: 0:08:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4818  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 300/1251]  eta: 0:08:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5143  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 310/1251]  eta: 0:08:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5259  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 320/1251]  eta: 0:08:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4920  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 330/1251]  eta: 0:08:21  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4714  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 340/1251]  eta: 0:08:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4709  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 350/1251]  eta: 0:08:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 360/1251]  eta: 0:07:59  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4701  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 370/1251]  eta: 0:07:52  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4680  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 380/1251]  eta: 0:07:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4725  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 390/1251]  eta: 0:07:39  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4797  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 400/1251]  eta: 0:07:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4759  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 410/1251]  eta: 0:07:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 420/1251]  eta: 0:07:19  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4668  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 430/1251]  eta: 0:07:13  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4672  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 440/1251]  eta: 0:07:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4947  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 450/1251]  eta: 0:07:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5077  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 460/1251]  eta: 0:06:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4941  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 470/1251]  eta: 0:06:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4820  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 480/1251]  eta: 0:06:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4699  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 490/1251]  eta: 0:06:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4701  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 500/1251]  eta: 0:06:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4691  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 510/1251]  eta: 0:06:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4693  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 520/1251]  eta: 0:06:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4880  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 530/1251]  eta: 0:06:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4848  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 540/1251]  eta: 0:06:08  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4633  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 550/1251]  eta: 0:06:02  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4654  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 560/1251]  eta: 0:05:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4660  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 570/1251]  eta: 0:05:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4665  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 580/1251]  eta: 0:05:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4906  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 590/1251]  eta: 0:05:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5212  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 600/1251]  eta: 0:05:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5056  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 610/1251]  eta: 0:05:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4758  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 620/1251]  eta: 0:05:24  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4683  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 630/1251]  eta: 0:05:18  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4650  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 640/1251]  eta: 0:05:12  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4681  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 650/1251]  eta: 0:05:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4784  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 660/1251]  eta: 0:05:02  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4801  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 670/1251]  eta: 0:04:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4852  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 680/1251]  eta: 0:04:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4798  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 690/1251]  eta: 0:04:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4665  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 700/1251]  eta: 0:04:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4683  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 710/1251]  eta: 0:04:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4707  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 720/1251]  eta: 0:04:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4680  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 730/1251]  eta: 0:04:24  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4882  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 740/1251]  eta: 0:04:19  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5164  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 750/1251]  eta: 0:04:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5040  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 760/1251]  eta: 0:04:09  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4774  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 770/1251]  eta: 0:04:03  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4705  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 780/1251]  eta: 0:03:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4719  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 790/1251]  eta: 0:03:53  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4686  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 800/1251]  eta: 0:03:47  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4682  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 810/1251]  eta: 0:03:42  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4697  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 820/1251]  eta: 0:03:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4804  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 830/1251]  eta: 0:03:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4814  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 840/1251]  eta: 0:03:27  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4725  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 850/1251]  eta: 0:03:21  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4685  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 860/1251]  eta: 0:03:16  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4666  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 870/1251]  eta: 0:03:11  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4788  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 880/1251]  eta: 0:03:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5310  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 890/1251]  eta: 0:03:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5347  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 900/1251]  eta: 0:02:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4839  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 910/1251]  eta: 0:02:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4696  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 920/1251]  eta: 0:02:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4666  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 930/1251]  eta: 0:02:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 940/1251]  eta: 0:02:36  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 950/1251]  eta: 0:02:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4666  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 960/1251]  eta: 0:02:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4698  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 970/1251]  eta: 0:02:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4793  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 980/1251]  eta: 0:02:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4739  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [ 990/1251]  eta: 0:02:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4639  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1000/1251]  eta: 0:02:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4652  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1010/1251]  eta: 0:02:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4657  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1020/1251]  eta: 0:01:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4903  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1030/1251]  eta: 0:01:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5180  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [1040/1251]  eta: 0:01:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5005  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1050/1251]  eta: 0:01:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4721  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1060/1251]  eta: 0:01:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4657  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1070/1251]  eta: 0:01:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4653  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1080/1251]  eta: 0:01:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4662  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1090/1251]  eta: 0:01:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4693  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1100/1251]  eta: 0:01:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4675  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1110/1251]  eta: 0:01:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4653  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1120/1251]  eta: 0:01:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4731  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1130/1251]  eta: 0:01:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4739  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [1140/1251]  eta: 0:00:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4662  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1150/1251]  eta: 0:00:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4698  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1160/1251]  eta: 0:00:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4891  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1170/1251]  eta: 0:00:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5120  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1180/1251]  eta: 0:00:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5023  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1190/1251]  eta: 0:00:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4743  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1200/1251]  eta: 0:00:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4594  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1210/1251]  eta: 0:00:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4562  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1220/1251]  eta: 0:00:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4553  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1230/1251]  eta: 0:00:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4537  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Epoch: [9]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4543  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [9]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4531  data: 0.0002  max mem: 19734
Epoch: [9] Total time: 0:10:19 (0.4953 s / it)
Averaged stats: lr: 0.000020  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:27:26  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 20.1033  data: 19.9578  max mem: 19734
Test:  [ 10/261]  eta: 0:10:30  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.5121  data: 2.3382  max mem: 19734
Test:  [ 20/261]  eta: 0:05:42  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.4865  data: 0.2955  max mem: 19734
Test:  [ 30/261]  eta: 0:03:57  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2105  data: 0.0176  max mem: 19734
Test:  [ 40/261]  eta: 0:03:32  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4758  data: 0.3026  max mem: 19734
Test:  [ 50/261]  eta: 0:02:51  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4823  data: 0.2960  max mem: 19734
Test:  [ 60/261]  eta: 0:02:23  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2073  data: 0.0112  max mem: 19734
Test:  [ 70/261]  eta: 0:02:01  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.1768  data: 0.0114  max mem: 19734
Test:  [ 80/261]  eta: 0:01:54  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.3866  data: 0.2342  max mem: 19734
Test:  [ 90/261]  eta: 0:01:40  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.4233  data: 0.2608  max mem: 19734
Test:  [100/261]  eta: 0:01:35  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4209  data: 0.2582  max mem: 19734
Test:  [110/261]  eta: 0:01:25  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4664  data: 0.2989  max mem: 19734
Test:  [120/261]  eta: 0:01:15  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2499  data: 0.0767  max mem: 19734
Test:  [130/261]  eta: 0:01:06  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2037  data: 0.0103  max mem: 19734
Test:  [140/261]  eta: 0:01:02  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3881  data: 0.1897  max mem: 19734
Test:  [150/261]  eta: 0:00:55  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4069  data: 0.2459  max mem: 19734
Test:  [160/261]  eta: 0:00:47  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1995  data: 0.0673  max mem: 19734
Test:  [170/261]  eta: 0:00:41  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1902  data: 0.0713  max mem: 19734
Test:  [180/261]  eta: 0:00:35  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.1844  data: 0.0901  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.0991  data: 0.0266  max mem: 19734
Test:  [200/261]  eta: 0:00:24  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0724  data: 0.0020  max mem: 19734
Test:  [210/261]  eta: 0:00:19  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0731  data: 0.0006  max mem: 19734
Test:  [220/261]  eta: 0:00:15  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0848  data: 0.0124  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0825  data: 0.0123  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0703  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0704  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0683  data: 0.0002  max mem: 19734
Test: Total time: 0:01:26 (0.3304 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.48%
Loss is nan, stopping training this iteration.
Epoch: [10]  [   0/1251]  eta: 6:33:12  lr: 0.000020  loss: 0.0000 (0.0000)  time: 18.8588  data: 12.5380  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  10/1251]  eta: 0:45:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 2.1949  data: 1.1411  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  20/1251]  eta: 0:28:18  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5056  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  30/1251]  eta: 0:22:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4820  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  40/1251]  eta: 0:18:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4788  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  50/1251]  eta: 0:17:11  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5010  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  60/1251]  eta: 0:15:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5230  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  70/1251]  eta: 0:14:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5112  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  80/1251]  eta: 0:14:11  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4954  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [  90/1251]  eta: 0:13:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4809  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 100/1251]  eta: 0:12:59  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4752  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 110/1251]  eta: 0:12:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4725  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 120/1251]  eta: 0:12:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4711  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 130/1251]  eta: 0:11:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4733  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 140/1251]  eta: 0:11:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4884  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 150/1251]  eta: 0:11:13  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4904  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 160/1251]  eta: 0:10:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4759  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 170/1251]  eta: 0:10:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4730  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 180/1251]  eta: 0:10:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4695  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 190/1251]  eta: 0:10:17  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4671  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 200/1251]  eta: 0:10:09  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5033  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 210/1251]  eta: 0:09:59  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5148  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 220/1251]  eta: 0:09:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4901  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 230/1251]  eta: 0:09:39  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4795  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 240/1251]  eta: 0:09:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4693  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 250/1251]  eta: 0:09:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4703  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 260/1251]  eta: 0:09:11  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4730  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 270/1251]  eta: 0:09:02  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4744  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 280/1251]  eta: 0:08:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4710  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 290/1251]  eta: 0:08:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4762  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 300/1251]  eta: 0:08:38  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4760  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 310/1251]  eta: 0:08:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4681  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 320/1251]  eta: 0:08:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4713  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 330/1251]  eta: 0:08:16  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4753  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 340/1251]  eta: 0:08:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4970  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 350/1251]  eta: 0:08:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5334  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 360/1251]  eta: 0:07:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5145  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 370/1251]  eta: 0:07:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4757  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 380/1251]  eta: 0:07:44  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4693  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 390/1251]  eta: 0:07:38  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4745  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 400/1251]  eta: 0:07:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4755  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 410/1251]  eta: 0:07:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4824  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 420/1251]  eta: 0:07:18  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4819  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 430/1251]  eta: 0:07:12  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4690  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 440/1251]  eta: 0:07:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4761  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 450/1251]  eta: 0:07:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4801  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 460/1251]  eta: 0:06:54  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4749  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 470/1251]  eta: 0:06:47  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4698  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 480/1251]  eta: 0:06:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4663  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 490/1251]  eta: 0:06:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5341  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 500/1251]  eta: 0:06:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5359  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 510/1251]  eta: 0:06:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4813  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 520/1251]  eta: 0:06:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4822  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 530/1251]  eta: 0:06:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 540/1251]  eta: 0:06:08  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4666  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 550/1251]  eta: 0:06:02  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4672  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 560/1251]  eta: 0:05:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4683  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 570/1251]  eta: 0:05:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4721  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 580/1251]  eta: 0:05:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4779  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 590/1251]  eta: 0:05:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4805  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 600/1251]  eta: 0:05:34  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4753  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 610/1251]  eta: 0:05:28  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4694  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 620/1251]  eta: 0:05:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4674  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 630/1251]  eta: 0:05:18  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4913  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 640/1251]  eta: 0:05:13  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5232  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 650/1251]  eta: 0:05:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5068  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 660/1251]  eta: 0:05:02  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4757  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 670/1251]  eta: 0:04:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4692  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 680/1251]  eta: 0:04:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4685  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 690/1251]  eta: 0:04:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4666  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 700/1251]  eta: 0:04:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4719  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 710/1251]  eta: 0:04:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4745  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 720/1251]  eta: 0:04:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4688  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 730/1251]  eta: 0:04:24  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4659  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 740/1251]  eta: 0:04:19  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4740  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 750/1251]  eta: 0:04:13  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4763  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 760/1251]  eta: 0:04:08  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4726  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 770/1251]  eta: 0:04:03  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4753  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 780/1251]  eta: 0:03:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5306  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 790/1251]  eta: 0:03:53  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5353  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 800/1251]  eta: 0:03:48  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4871  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 810/1251]  eta: 0:03:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4772  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 820/1251]  eta: 0:03:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4687  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 830/1251]  eta: 0:03:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4734  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 840/1251]  eta: 0:03:27  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4732  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 850/1251]  eta: 0:03:22  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 860/1251]  eta: 0:03:17  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 870/1251]  eta: 0:03:11  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4672  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 880/1251]  eta: 0:03:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4679  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 890/1251]  eta: 0:03:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4824  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 900/1251]  eta: 0:02:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4848  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 910/1251]  eta: 0:02:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4715  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 920/1251]  eta: 0:02:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4984  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 930/1251]  eta: 0:02:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5172  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 940/1251]  eta: 0:02:36  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4969  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 950/1251]  eta: 0:02:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4820  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 960/1251]  eta: 0:02:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4730  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 970/1251]  eta: 0:02:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4679  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 980/1251]  eta: 0:02:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4662  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [ 990/1251]  eta: 0:02:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4691  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1000/1251]  eta: 0:02:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4703  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1010/1251]  eta: 0:02:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4751  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1020/1251]  eta: 0:01:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4761  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1030/1251]  eta: 0:01:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4691  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1040/1251]  eta: 0:01:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4813  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1050/1251]  eta: 0:01:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4802  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1060/1251]  eta: 0:01:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4768  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1070/1251]  eta: 0:01:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5104  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1080/1251]  eta: 0:01:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5235  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1090/1251]  eta: 0:01:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4915  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1100/1251]  eta: 0:01:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4677  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1110/1251]  eta: 0:01:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4651  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1120/1251]  eta: 0:01:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4656  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1130/1251]  eta: 0:01:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4682  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1140/1251]  eta: 0:00:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4751  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1150/1251]  eta: 0:00:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4744  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1160/1251]  eta: 0:00:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4663  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1170/1251]  eta: 0:00:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4663  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1180/1251]  eta: 0:00:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4662  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1190/1251]  eta: 0:00:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4740  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1200/1251]  eta: 0:00:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4707  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1210/1251]  eta: 0:00:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4748  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1220/1251]  eta: 0:00:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4917  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1230/1251]  eta: 0:00:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4783  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4584  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [10]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4529  data: 0.0001  max mem: 19734
Epoch: [10] Total time: 0:10:20 (0.4962 s / it)
Averaged stats: lr: 0.000020  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:01:07  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 14.0535  data: 13.8171  max mem: 19734
Test:  [ 10/261]  eta: 0:11:05  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 2.6517  data: 2.4412  max mem: 19734
Test:  [ 20/261]  eta: 0:05:58  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.8580  data: 0.6644  max mem: 19734
Test:  [ 30/261]  eta: 0:04:04  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1854  data: 0.0200  max mem: 19734
Test:  [ 40/261]  eta: 0:03:13  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.2323  data: 0.0879  max mem: 19734
Test:  [ 50/261]  eta: 0:02:34  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.2267  data: 0.0864  max mem: 19734
Test:  [ 60/261]  eta: 0:02:10  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1840  data: 0.0115  max mem: 19734
Test:  [ 70/261]  eta: 0:02:10  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.5565  data: 0.3474  max mem: 19734
Test:  [ 80/261]  eta: 0:01:52  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.5477  data: 0.3476  max mem: 19734
Test:  [ 90/261]  eta: 0:01:38  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.1861  data: 0.0100  max mem: 19734
Test:  [100/261]  eta: 0:01:35  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4811  data: 0.3210  max mem: 19734
Test:  [110/261]  eta: 0:01:23  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4541  data: 0.3198  max mem: 19734
Test:  [120/261]  eta: 0:01:13  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.1421  data: 0.0106  max mem: 19734
Test:  [130/261]  eta: 0:01:04  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.1455  data: 0.0145  max mem: 19734
Test:  [140/261]  eta: 0:01:00  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3905  data: 0.2414  max mem: 19734
Test:  [150/261]  eta: 0:00:53  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.4074  data: 0.2403  max mem: 19734
Test:  [160/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.1847  data: 0.0094  max mem: 19734
Test:  [170/261]  eta: 0:00:43  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.4793  data: 0.2970  max mem: 19734
Test:  [180/261]  eta: 0:00:37  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.4686  data: 0.3002  max mem: 19734
Test:  [190/261]  eta: 0:00:31  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1722  data: 0.0136  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.3469  data: 0.2083  max mem: 19734
Test:  [210/261]  eta: 0:00:22  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.3218  data: 0.2211  max mem: 19734
Test:  [220/261]  eta: 0:00:17  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0900  data: 0.0197  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0702  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0703  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0703  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0681  data: 0.0001  max mem: 19734
Test: Total time: 0:01:35 (0.3660 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.48%
Loss is nan, stopping training this iteration.
Epoch: [11]  [   0/1251]  eta: 6:42:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 19.3108  data: 5.2457  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  10/1251]  eta: 0:46:59  lr: 0.000020  loss: 0.0000 (0.0000)  time: 2.2721  data: 0.4980  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  20/1251]  eta: 0:29:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5239  data: 0.0119  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  30/1251]  eta: 0:22:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4786  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  40/1251]  eta: 0:19:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4757  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  50/1251]  eta: 0:17:21  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4823  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  60/1251]  eta: 0:15:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4830  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  70/1251]  eta: 0:14:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4813  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  80/1251]  eta: 0:14:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4826  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [  90/1251]  eta: 0:13:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4869  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 100/1251]  eta: 0:13:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5209  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 110/1251]  eta: 0:12:44  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5403  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 120/1251]  eta: 0:12:19  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5050  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 130/1251]  eta: 0:11:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4810  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 140/1251]  eta: 0:11:39  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4811  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 150/1251]  eta: 0:11:22  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4832  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 160/1251]  eta: 0:11:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4826  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 170/1251]  eta: 0:10:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4716  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 180/1251]  eta: 0:10:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4694  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 190/1251]  eta: 0:10:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4721  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 200/1251]  eta: 0:10:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4835  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 210/1251]  eta: 0:10:02  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4794  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 220/1251]  eta: 0:09:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4671  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 230/1251]  eta: 0:09:41  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4690  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 240/1251]  eta: 0:09:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4781  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 250/1251]  eta: 0:09:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5211  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 260/1251]  eta: 0:09:17  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5250  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 270/1251]  eta: 0:09:08  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4819  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 280/1251]  eta: 0:09:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 290/1251]  eta: 0:08:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 300/1251]  eta: 0:08:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 310/1251]  eta: 0:08:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4743  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 320/1251]  eta: 0:08:28  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4789  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 330/1251]  eta: 0:08:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4747  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 340/1251]  eta: 0:08:13  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4770  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 350/1251]  eta: 0:08:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4758  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 360/1251]  eta: 0:07:59  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4677  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 370/1251]  eta: 0:07:52  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4664  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 380/1251]  eta: 0:07:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4752  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 390/1251]  eta: 0:07:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5071  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 400/1251]  eta: 0:07:34  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5342  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 410/1251]  eta: 0:07:28  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5029  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 420/1251]  eta: 0:07:21  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4668  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 430/1251]  eta: 0:07:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4663  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 440/1251]  eta: 0:07:08  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4708  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 450/1251]  eta: 0:07:02  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4734  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 460/1251]  eta: 0:06:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4714  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 470/1251]  eta: 0:06:49  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4696  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 480/1251]  eta: 0:06:43  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4692  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 490/1251]  eta: 0:06:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4771  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 500/1251]  eta: 0:06:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4816  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 510/1251]  eta: 0:06:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4765  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 520/1251]  eta: 0:06:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4716  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 530/1251]  eta: 0:06:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4848  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 540/1251]  eta: 0:06:09  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5061  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 550/1251]  eta: 0:06:03  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4964  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 560/1251]  eta: 0:05:57  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4746  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 570/1251]  eta: 0:05:52  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4707  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 580/1251]  eta: 0:05:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4703  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 590/1251]  eta: 0:05:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4678  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 600/1251]  eta: 0:05:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4672  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 610/1251]  eta: 0:05:29  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4674  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 620/1251]  eta: 0:05:23  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 630/1251]  eta: 0:05:18  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4736  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 640/1251]  eta: 0:05:12  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4791  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 650/1251]  eta: 0:05:07  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4753  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 660/1251]  eta: 0:05:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4678  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 670/1251]  eta: 0:04:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4777  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 680/1251]  eta: 0:04:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5080  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 690/1251]  eta: 0:04:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5060  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 700/1251]  eta: 0:04:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4779  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 710/1251]  eta: 0:04:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4699  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 720/1251]  eta: 0:04:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4704  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 730/1251]  eta: 0:04:24  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4705  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 740/1251]  eta: 0:04:19  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4705  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 750/1251]  eta: 0:04:14  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4747  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 760/1251]  eta: 0:04:08  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4752  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 770/1251]  eta: 0:04:03  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4699  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 780/1251]  eta: 0:03:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4823  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 790/1251]  eta: 0:03:53  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4872  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 800/1251]  eta: 0:03:47  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4733  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 810/1251]  eta: 0:03:42  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4695  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 820/1251]  eta: 0:03:37  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4930  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 830/1251]  eta: 0:03:32  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5075  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 840/1251]  eta: 0:03:27  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4911  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 850/1251]  eta: 0:03:22  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4745  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 860/1251]  eta: 0:03:16  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4658  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 870/1251]  eta: 0:03:11  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4665  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 880/1251]  eta: 0:03:06  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 890/1251]  eta: 0:03:01  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4731  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 900/1251]  eta: 0:02:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4691  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 910/1251]  eta: 0:02:51  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4670  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 920/1251]  eta: 0:02:46  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4667  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 930/1251]  eta: 0:02:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4759  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 940/1251]  eta: 0:02:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4779  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 950/1251]  eta: 0:02:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4724  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 960/1251]  eta: 0:02:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4737  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 970/1251]  eta: 0:02:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5178  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 980/1251]  eta: 0:02:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5326  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [ 990/1251]  eta: 0:02:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4863  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1000/1251]  eta: 0:02:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4710  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1010/1251]  eta: 0:02:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4715  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1020/1251]  eta: 0:01:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4670  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1030/1251]  eta: 0:01:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4646  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1040/1251]  eta: 0:01:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4668  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1050/1251]  eta: 0:01:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4664  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1060/1251]  eta: 0:01:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4666  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1070/1251]  eta: 0:01:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4737  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1080/1251]  eta: 0:01:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4805  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1090/1251]  eta: 0:01:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4727  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1100/1251]  eta: 0:01:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4637  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1110/1251]  eta: 0:01:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4975  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1120/1251]  eta: 0:01:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5331  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1130/1251]  eta: 0:01:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.5162  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1140/1251]  eta: 0:00:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4858  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1150/1251]  eta: 0:00:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4707  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1160/1251]  eta: 0:00:45  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4672  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1170/1251]  eta: 0:00:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4675  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1180/1251]  eta: 0:00:35  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4667  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1190/1251]  eta: 0:00:30  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4662  data: 0.0009  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1200/1251]  eta: 0:00:25  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4615  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1210/1251]  eta: 0:00:20  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4550  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1220/1251]  eta: 0:00:15  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4540  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1230/1251]  eta: 0:00:10  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4616  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1240/1251]  eta: 0:00:05  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4614  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [11]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.4609  data: 0.0002  max mem: 19734
Epoch: [11] Total time: 0:10:20 (0.4956 s / it)
Averaged stats: lr: 0.000020  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 2:29:07  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 34.2820  data: 34.0745  max mem: 19734
Test:  [ 10/261]  eta: 0:14:04  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 3.3657  data: 3.1115  max mem: 19734
Test:  [ 20/261]  eta: 0:07:33  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.2638  data: 0.0149  max mem: 19734
Test:  [ 30/261]  eta: 0:05:05  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2013  data: 0.0123  max mem: 19734
Test:  [ 40/261]  eta: 0:04:23  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4708  data: 0.3078  max mem: 19734
Test:  [ 50/261]  eta: 0:03:33  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.5224  data: 0.3115  max mem: 19734
Test:  [ 60/261]  eta: 0:02:55  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2105  data: 0.0146  max mem: 19734
Test:  [ 70/261]  eta: 0:02:27  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.1736  data: 0.0501  max mem: 19734
Test:  [ 80/261]  eta: 0:02:04  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.1321  data: 0.0492  max mem: 19734
Test:  [ 90/261]  eta: 0:01:49  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.1687  data: 0.0741  max mem: 19734
Test:  [100/261]  eta: 0:01:46  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.5599  data: 0.4182  max mem: 19734
Test:  [110/261]  eta: 0:01:34  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.5486  data: 0.3658  max mem: 19734
Test:  [120/261]  eta: 0:01:23  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.2514  data: 0.0698  max mem: 19734
Test:  [130/261]  eta: 0:01:17  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.4187  data: 0.2487  max mem: 19734
Test:  [140/261]  eta: 0:01:09  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.4743  data: 0.3153  max mem: 19734
Test:  [150/261]  eta: 0:01:02  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.3812  data: 0.2075  max mem: 19734
Test:  [160/261]  eta: 0:00:54  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2818  data: 0.1431  max mem: 19734
Test:  [170/261]  eta: 0:00:46  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.1285  data: 0.0555  max mem: 19734
Test:  [180/261]  eta: 0:00:39  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.0708  data: 0.0004  max mem: 19734
Test:  [190/261]  eta: 0:00:33  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.0744  data: 0.0035  max mem: 19734
Test:  [200/261]  eta: 0:00:27  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.0749  data: 0.0039  max mem: 19734
Test:  [210/261]  eta: 0:00:21  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.0709  data: 0.0007  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0701  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:12  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0700  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:08  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0700  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:04  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0701  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0680  data: 0.0002  max mem: 19734
Test: Total time: 0:01:34 (0.3618 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.48%
Loss is nan, stopping training this iteration.
Epoch: [12]  [   0/1251]  eta: 6:17:16  lr: 0.000019  loss: 0.0000 (0.0000)  time: 18.0945  data: 11.4621  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  10/1251]  eta: 0:46:48  lr: 0.000019  loss: 0.0000 (0.0000)  time: 2.2632  data: 1.0623  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  20/1251]  eta: 0:29:06  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5849  data: 0.0114  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  30/1251]  eta: 0:22:38  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4802  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  40/1251]  eta: 0:19:16  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4686  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  50/1251]  eta: 0:17:12  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4675  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  60/1251]  eta: 0:15:50  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4757  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  70/1251]  eta: 0:14:49  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4805  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  80/1251]  eta: 0:14:01  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4766  data: 0.0012  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [  90/1251]  eta: 0:13:27  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4898  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 100/1251]  eta: 0:12:55  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4908  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 110/1251]  eta: 0:12:27  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4754  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 120/1251]  eta: 0:12:05  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4778  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 130/1251]  eta: 0:11:44  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4801  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 140/1251]  eta: 0:11:32  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5128  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 150/1251]  eta: 0:11:18  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5312  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 160/1251]  eta: 0:11:03  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5009  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 170/1251]  eta: 0:10:48  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4789  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 180/1251]  eta: 0:10:35  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4749  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 190/1251]  eta: 0:10:22  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4778  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 200/1251]  eta: 0:10:10  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4718  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 210/1251]  eta: 0:09:59  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4680  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 220/1251]  eta: 0:09:48  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4686  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 230/1251]  eta: 0:09:39  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4821  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 240/1251]  eta: 0:09:30  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4898  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 250/1251]  eta: 0:09:21  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4830  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 260/1251]  eta: 0:09:12  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4758  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 270/1251]  eta: 0:09:03  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4695  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 280/1251]  eta: 0:08:55  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4777  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 290/1251]  eta: 0:08:49  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5035  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 300/1251]  eta: 0:08:42  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5153  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 310/1251]  eta: 0:08:35  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5012  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 320/1251]  eta: 0:08:27  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4819  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 330/1251]  eta: 0:08:19  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4692  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 340/1251]  eta: 0:08:12  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4707  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 350/1251]  eta: 0:08:05  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4721  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 360/1251]  eta: 0:07:58  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4684  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 370/1251]  eta: 0:07:51  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4737  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 380/1251]  eta: 0:07:45  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4864  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 390/1251]  eta: 0:07:38  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4827  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 400/1251]  eta: 0:07:31  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 410/1251]  eta: 0:07:25  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4705  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 420/1251]  eta: 0:07:18  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 430/1251]  eta: 0:07:13  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5063  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 440/1251]  eta: 0:07:08  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5330  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 450/1251]  eta: 0:07:02  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5055  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 460/1251]  eta: 0:06:56  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4823  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 470/1251]  eta: 0:06:50  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4728  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 480/1251]  eta: 0:06:43  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4676  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 490/1251]  eta: 0:06:38  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4742  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 500/1251]  eta: 0:06:32  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4771  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 510/1251]  eta: 0:06:26  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4694  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 520/1251]  eta: 0:06:20  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4660  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 530/1251]  eta: 0:06:14  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4886  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 540/1251]  eta: 0:06:08  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4919  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 550/1251]  eta: 0:06:03  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4706  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 560/1251]  eta: 0:05:57  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4708  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 570/1251]  eta: 0:05:51  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4859  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 580/1251]  eta: 0:05:47  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5185  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 590/1251]  eta: 0:05:41  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5087  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 600/1251]  eta: 0:05:35  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4800  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 610/1251]  eta: 0:05:30  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4751  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 620/1251]  eta: 0:05:24  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4748  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 630/1251]  eta: 0:05:19  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4755  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 640/1251]  eta: 0:05:13  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4694  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 650/1251]  eta: 0:05:07  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4668  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 660/1251]  eta: 0:05:02  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4652  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 670/1251]  eta: 0:04:56  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4646  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 680/1251]  eta: 0:04:51  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4783  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 690/1251]  eta: 0:04:46  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4822  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 700/1251]  eta: 0:04:40  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4688  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 710/1251]  eta: 0:04:35  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4778  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 720/1251]  eta: 0:04:30  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4878  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 730/1251]  eta: 0:04:25  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5110  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 740/1251]  eta: 0:04:20  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5135  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 750/1251]  eta: 0:04:14  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4802  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 760/1251]  eta: 0:04:09  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4673  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 770/1251]  eta: 0:04:04  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4659  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 780/1251]  eta: 0:03:58  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4674  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 790/1251]  eta: 0:03:53  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4660  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 800/1251]  eta: 0:03:48  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4671  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 810/1251]  eta: 0:03:42  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4725  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 820/1251]  eta: 0:03:37  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4808  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 830/1251]  eta: 0:03:32  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4738  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 840/1251]  eta: 0:03:27  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4630  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 850/1251]  eta: 0:03:22  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4659  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 860/1251]  eta: 0:03:17  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4842  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 870/1251]  eta: 0:03:12  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5086  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 880/1251]  eta: 0:03:06  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5080  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 890/1251]  eta: 0:03:01  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4934  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 900/1251]  eta: 0:02:56  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4752  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 910/1251]  eta: 0:02:51  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4641  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 920/1251]  eta: 0:02:46  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4647  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 930/1251]  eta: 0:02:41  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4678  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 940/1251]  eta: 0:02:36  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4689  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 950/1251]  eta: 0:02:30  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4669  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 960/1251]  eta: 0:02:25  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4655  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 970/1251]  eta: 0:02:20  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4757  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 980/1251]  eta: 0:02:15  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4783  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [ 990/1251]  eta: 0:02:10  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4745  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1000/1251]  eta: 0:02:05  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4814  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1010/1251]  eta: 0:02:00  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4986  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1020/1251]  eta: 0:01:55  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5087  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1030/1251]  eta: 0:01:50  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4935  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1040/1251]  eta: 0:01:45  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4735  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1050/1251]  eta: 0:01:40  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4670  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1060/1251]  eta: 0:01:35  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4733  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1070/1251]  eta: 0:01:30  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4726  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1080/1251]  eta: 0:01:25  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4667  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1090/1251]  eta: 0:01:20  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4666  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1100/1251]  eta: 0:01:15  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4674  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1110/1251]  eta: 0:01:10  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4669  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1120/1251]  eta: 0:01:05  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4787  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1130/1251]  eta: 0:01:00  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4810  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1140/1251]  eta: 0:00:55  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4705  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1150/1251]  eta: 0:00:50  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4873  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1160/1251]  eta: 0:00:45  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5141  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1170/1251]  eta: 0:00:40  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5113  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1180/1251]  eta: 0:00:35  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4961  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1190/1251]  eta: 0:00:30  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4790  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1200/1251]  eta: 0:00:25  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4597  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1210/1251]  eta: 0:00:20  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4542  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1220/1251]  eta: 0:00:15  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4550  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1230/1251]  eta: 0:00:10  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4548  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1240/1251]  eta: 0:00:05  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4550  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [12]  [1250/1251]  eta: 0:00:00  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4572  data: 0.0002  max mem: 19734
Epoch: [12] Total time: 0:10:20 (0.4959 s / it)
Averaged stats: lr: 0.000019  loss: 0.0000 (0.0000)
Test:  [  0/261]  eta: 1:14:51  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 17.2078  data: 17.0935  max mem: 19734
Test:  [ 10/261]  eta: 0:06:59  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 1.6722  data: 1.5592  max mem: 19734
Test:  [ 20/261]  eta: 0:03:54  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.1614  data: 0.0052  max mem: 19734
Test:  [ 30/261]  eta: 0:02:44  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.1848  data: 0.0067  max mem: 19734
Test:  [ 40/261]  eta: 0:03:08  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.7294  data: 0.5766  max mem: 19734
Test:  [ 50/261]  eta: 0:02:31  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.7259  data: 0.5792  max mem: 19734
Test:  [ 60/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.1603  data: 0.0141  max mem: 19734
Test:  [ 70/261]  eta: 0:01:59  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.3954  data: 0.2527  max mem: 19734
Test:  [ 80/261]  eta: 0:01:44  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4340  data: 0.2511  max mem: 19734
Test:  [ 90/261]  eta: 0:01:31  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2090  data: 0.0129  max mem: 19734
Test:  [100/261]  eta: 0:01:28  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4176  data: 0.2625  max mem: 19734
Test:  [110/261]  eta: 0:01:17  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.4286  data: 0.2603  max mem: 19734
Test:  [120/261]  eta: 0:01:08  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.1965  data: 0.0089  max mem: 19734
Test:  [130/261]  eta: 0:01:08  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.5492  data: 0.3758  max mem: 19734
Test:  [140/261]  eta: 0:01:00  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.5573  data: 0.3798  max mem: 19734
Test:  [150/261]  eta: 0:00:54  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.2987  data: 0.1017  max mem: 19734
Test:  [160/261]  eta: 0:00:47  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.2953  data: 0.1012  max mem: 19734
Test:  [170/261]  eta: 0:00:42  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.3197  data: 0.1593  max mem: 19734
Test:  [180/261]  eta: 0:00:36  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2846  data: 0.1579  max mem: 19734
Test:  [190/261]  eta: 0:00:30  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.1308  data: 0.0105  max mem: 19734
Test:  [200/261]  eta: 0:00:25  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1437  data: 0.0390  max mem: 19734
Test:  [210/261]  eta: 0:00:20  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1383  data: 0.0590  max mem: 19734
Test:  [220/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.0951  data: 0.0247  max mem: 19734
Test:  [230/261]  eta: 0:00:11  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.0706  data: 0.0002  max mem: 19734
Test:  [240/261]  eta: 0:00:07  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.0708  data: 0.0002  max mem: 19734
Test:  [250/261]  eta: 0:00:03  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.0706  data: 0.0002  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.0687  data: 0.0002  max mem: 19734
Test: Total time: 0:01:29 (0.3436 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.48%
Loss is nan, stopping training this iteration.
Epoch: [13]  [   0/1251]  eta: 6:26:44  lr: 0.000019  loss: 0.0000 (0.0000)  time: 18.5488  data: 14.3719  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  10/1251]  eta: 0:48:40  lr: 0.000019  loss: 0.0000 (0.0000)  time: 2.3536  data: 1.3079  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  20/1251]  eta: 0:29:54  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.6034  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  30/1251]  eta: 0:23:14  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4753  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  40/1251]  eta: 0:19:55  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4928  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  50/1251]  eta: 0:18:08  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5405  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  60/1251]  eta: 0:16:33  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5198  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  70/1251]  eta: 0:15:26  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4754  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  80/1251]  eta: 0:14:33  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4781  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [  90/1251]  eta: 0:13:50  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4703  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 100/1251]  eta: 0:13:15  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4698  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 110/1251]  eta: 0:12:47  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4768  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 120/1251]  eta: 0:12:22  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4790  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 130/1251]  eta: 0:11:59  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4730  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 140/1251]  eta: 0:11:40  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4727  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 150/1251]  eta: 0:11:23  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4833  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 160/1251]  eta: 0:11:07  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4807  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 170/1251]  eta: 0:10:52  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4730  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 180/1251]  eta: 0:10:40  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4852  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 190/1251]  eta: 0:10:29  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5060  data: 0.0007  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 200/1251]  eta: 0:10:19  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5178  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 210/1251]  eta: 0:10:08  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.5019  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 220/1251]  eta: 0:09:57  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4769  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 230/1251]  eta: 0:09:47  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4712  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 240/1251]  eta: 0:09:37  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4783  data: 0.0011  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [13]  [ 250/1251]  eta: 0:09:27  lr: 0.000019  loss: 0.0000 (0.0000)  time: 0.4797  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
