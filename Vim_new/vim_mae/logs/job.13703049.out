CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
CUDA is available
training:
LD_LIBRARY_PATH: /cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/x64:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/lib/oclfpga/host/linux64/lib:/cluster/software/commercial/intel/2023.2/x86_64/compiler/2023.2.0/linux/compiler/lib/intel64_lin
| distributed init (rank 6): env://
| distributed init (rank 5): env://
| distributed init (rank 7): env://
| distributed init (rank 2): env://
| distributed init (rank 3): env://
| distributed init (rank 1): env://
| distributed init (rank 0): env://
| distributed init (rank 4): env://
Namespace(batch_size=128, epochs=100, bce_loss=False, update_freq=1, unscale_lr=False, model='vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2', input_size=224, drop=0.0, drop_path=0.0, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.1, sched='cosine', lr=1e-05, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-06, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='/cluster/work/cvl/guosun/shangye/pretrained/vim_t_midclstok_76p1acc.pth', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, cosub=False, finetune='', attn_only=False, data_path='/scratch/tmp.13703049.guosun/datasets/imagenet_full_size/', data_set='IMNET', inat_category='name', output_dir='/cluster/work/cvl/guosun/shangye/output/Vim_new/vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_300einit_100e_batch_size128_p0.9_lr0.00001_min_lr1e-6_decoder_pruning_loss3stage_weight0.1_mse_weight0.005_sort_keep_policy_pretrain_mae', log_dir=None, device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=32, pin_mem=True, distributed=True, world_size=8, dist_url='env://', if_amp=False, if_continue_inf=True, if_nan2num=False, if_random_cls_token_position=False, if_random_token_rank=False, base_rate=0.9, lr_scale=0.01, ratio_weight=2.0, pruning_weight=0.1, mse_weight=0.005, pretrained_mae_path='/cluster/work/cvl/guosun/shangye/output/pretrain_mae_vim/vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2_mae_300e/checkpoint.pth', local_rank=0, rank=0, gpu=0, dist_backend='nccl')
Creating model: vimpruning_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_clstok_div2
keep_rate [0.9, 0.81, 0.7290000000000001]
VisionMambaPrunning(
  (token_merge_module): TPSModule()
  (score_predictor): ModuleList(
    (0-2): 3 x PredictorLG(
      (in_conv_local): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (in_conv_cls): Sequential(
        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=192, out_features=96, bias=True)
        (2): GELU(approximate='none')
      )
      (out_conv): Sequential(
        (0): Linear(in_features=192, out_features=96, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=96, out_features=48, bias=True)
        (3): GELU(approximate='none')
        (4): Linear(in_features=48, out_features=2, bias=True)
        (5): LogSoftmax(dim=-1)
      )
    )
  )
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (head): Linear(in_features=192, out_features=1000, bias=True)
  (drop_path): Identity()
  (layers): ModuleList(
    (0-23): 24 x Block(
      (mixer): Mamba(
        (in_proj): Linear(in_features=192, out_features=768, bias=False)
        (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (act): SiLU()
        (x_proj): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj): Linear(in_features=12, out_features=384, bias=True)
        (conv1d_b): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
        (x_proj_b): Linear(in_features=384, out_features=44, bias=False)
        (dt_proj_b): Linear(in_features=12, out_features=384, bias=True)
        (out_proj): Linear(in_features=384, out_features=192, bias=False)
      )
      (norm): RMSNorm()
      (drop_path): Identity()
    )
  )
  (decoder): MAE_Decoder(
    (decoder_embed): Linear(in_features=192, out_features=512, bias=True)
    (decoder_blocks): ModuleList(
      (0-7): 8 x Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)
  )
  (norm_f): RMSNorm()
)
Weights of VisionMambaPrunning not initialized from pretrained model: ['score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'decoder.mask_token', 'decoder.decoder_pos_embed', 'decoder.decoder_embed.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_blocks.0.norm1.weight', 'decoder.decoder_blocks.0.norm1.bias', 'decoder.decoder_blocks.0.attn.qkv.weight', 'decoder.decoder_blocks.0.attn.qkv.bias', 'decoder.decoder_blocks.0.attn.proj.weight', 'decoder.decoder_blocks.0.attn.proj.bias', 'decoder.decoder_blocks.0.norm2.weight', 'decoder.decoder_blocks.0.norm2.bias', 'decoder.decoder_blocks.0.mlp.fc1.weight', 'decoder.decoder_blocks.0.mlp.fc1.bias', 'decoder.decoder_blocks.0.mlp.fc2.weight', 'decoder.decoder_blocks.0.mlp.fc2.bias', 'decoder.decoder_blocks.1.norm1.weight', 'decoder.decoder_blocks.1.norm1.bias', 'decoder.decoder_blocks.1.attn.qkv.weight', 'decoder.decoder_blocks.1.attn.qkv.bias', 'decoder.decoder_blocks.1.attn.proj.weight', 'decoder.decoder_blocks.1.attn.proj.bias', 'decoder.decoder_blocks.1.norm2.weight', 'decoder.decoder_blocks.1.norm2.bias', 'decoder.decoder_blocks.1.mlp.fc1.weight', 'decoder.decoder_blocks.1.mlp.fc1.bias', 'decoder.decoder_blocks.1.mlp.fc2.weight', 'decoder.decoder_blocks.1.mlp.fc2.bias', 'decoder.decoder_blocks.2.norm1.weight', 'decoder.decoder_blocks.2.norm1.bias', 'decoder.decoder_blocks.2.attn.qkv.weight', 'decoder.decoder_blocks.2.attn.qkv.bias', 'decoder.decoder_blocks.2.attn.proj.weight', 'decoder.decoder_blocks.2.attn.proj.bias', 'decoder.decoder_blocks.2.norm2.weight', 'decoder.decoder_blocks.2.norm2.bias', 'decoder.decoder_blocks.2.mlp.fc1.weight', 'decoder.decoder_blocks.2.mlp.fc1.bias', 'decoder.decoder_blocks.2.mlp.fc2.weight', 'decoder.decoder_blocks.2.mlp.fc2.bias', 'decoder.decoder_blocks.3.norm1.weight', 'decoder.decoder_blocks.3.norm1.bias', 'decoder.decoder_blocks.3.attn.qkv.weight', 'decoder.decoder_blocks.3.attn.qkv.bias', 'decoder.decoder_blocks.3.attn.proj.weight', 'decoder.decoder_blocks.3.attn.proj.bias', 'decoder.decoder_blocks.3.norm2.weight', 'decoder.decoder_blocks.3.norm2.bias', 'decoder.decoder_blocks.3.mlp.fc1.weight', 'decoder.decoder_blocks.3.mlp.fc1.bias', 'decoder.decoder_blocks.3.mlp.fc2.weight', 'decoder.decoder_blocks.3.mlp.fc2.bias', 'decoder.decoder_blocks.4.norm1.weight', 'decoder.decoder_blocks.4.norm1.bias', 'decoder.decoder_blocks.4.attn.qkv.weight', 'decoder.decoder_blocks.4.attn.qkv.bias', 'decoder.decoder_blocks.4.attn.proj.weight', 'decoder.decoder_blocks.4.attn.proj.bias', 'decoder.decoder_blocks.4.norm2.weight', 'decoder.decoder_blocks.4.norm2.bias', 'decoder.decoder_blocks.4.mlp.fc1.weight', 'decoder.decoder_blocks.4.mlp.fc1.bias', 'decoder.decoder_blocks.4.mlp.fc2.weight', 'decoder.decoder_blocks.4.mlp.fc2.bias', 'decoder.decoder_blocks.5.norm1.weight', 'decoder.decoder_blocks.5.norm1.bias', 'decoder.decoder_blocks.5.attn.qkv.weight', 'decoder.decoder_blocks.5.attn.qkv.bias', 'decoder.decoder_blocks.5.attn.proj.weight', 'decoder.decoder_blocks.5.attn.proj.bias', 'decoder.decoder_blocks.5.norm2.weight', 'decoder.decoder_blocks.5.norm2.bias', 'decoder.decoder_blocks.5.mlp.fc1.weight', 'decoder.decoder_blocks.5.mlp.fc1.bias', 'decoder.decoder_blocks.5.mlp.fc2.weight', 'decoder.decoder_blocks.5.mlp.fc2.bias', 'decoder.decoder_blocks.6.norm1.weight', 'decoder.decoder_blocks.6.norm1.bias', 'decoder.decoder_blocks.6.attn.qkv.weight', 'decoder.decoder_blocks.6.attn.qkv.bias', 'decoder.decoder_blocks.6.attn.proj.weight', 'decoder.decoder_blocks.6.attn.proj.bias', 'decoder.decoder_blocks.6.norm2.weight', 'decoder.decoder_blocks.6.norm2.bias', 'decoder.decoder_blocks.6.mlp.fc1.weight', 'decoder.decoder_blocks.6.mlp.fc1.bias', 'decoder.decoder_blocks.6.mlp.fc2.weight', 'decoder.decoder_blocks.6.mlp.fc2.bias', 'decoder.decoder_blocks.7.norm1.weight', 'decoder.decoder_blocks.7.norm1.bias', 'decoder.decoder_blocks.7.attn.qkv.weight', 'decoder.decoder_blocks.7.attn.qkv.bias', 'decoder.decoder_blocks.7.attn.proj.weight', 'decoder.decoder_blocks.7.attn.proj.bias', 'decoder.decoder_blocks.7.norm2.weight', 'decoder.decoder_blocks.7.norm2.bias', 'decoder.decoder_blocks.7.mlp.fc1.weight', 'decoder.decoder_blocks.7.mlp.fc1.bias', 'decoder.decoder_blocks.7.mlp.fc2.weight', 'decoder.decoder_blocks.7.mlp.fc2.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_pred.bias']
Weights of VisionMambaPrunning not initialized from pretrained model: ['cls_token', 'pos_embed', 'score_predictor.0.in_conv_local.0.weight', 'score_predictor.0.in_conv_local.0.bias', 'score_predictor.0.in_conv_local.1.weight', 'score_predictor.0.in_conv_local.1.bias', 'score_predictor.0.in_conv_cls.0.weight', 'score_predictor.0.in_conv_cls.0.bias', 'score_predictor.0.in_conv_cls.1.weight', 'score_predictor.0.in_conv_cls.1.bias', 'score_predictor.0.out_conv.0.weight', 'score_predictor.0.out_conv.0.bias', 'score_predictor.0.out_conv.2.weight', 'score_predictor.0.out_conv.2.bias', 'score_predictor.0.out_conv.4.weight', 'score_predictor.0.out_conv.4.bias', 'score_predictor.1.in_conv_local.0.weight', 'score_predictor.1.in_conv_local.0.bias', 'score_predictor.1.in_conv_local.1.weight', 'score_predictor.1.in_conv_local.1.bias', 'score_predictor.1.in_conv_cls.0.weight', 'score_predictor.1.in_conv_cls.0.bias', 'score_predictor.1.in_conv_cls.1.weight', 'score_predictor.1.in_conv_cls.1.bias', 'score_predictor.1.out_conv.0.weight', 'score_predictor.1.out_conv.0.bias', 'score_predictor.1.out_conv.2.weight', 'score_predictor.1.out_conv.2.bias', 'score_predictor.1.out_conv.4.weight', 'score_predictor.1.out_conv.4.bias', 'score_predictor.2.in_conv_local.0.weight', 'score_predictor.2.in_conv_local.0.bias', 'score_predictor.2.in_conv_local.1.weight', 'score_predictor.2.in_conv_local.1.bias', 'score_predictor.2.in_conv_cls.0.weight', 'score_predictor.2.in_conv_cls.0.bias', 'score_predictor.2.in_conv_cls.1.weight', 'score_predictor.2.in_conv_cls.1.bias', 'score_predictor.2.out_conv.0.weight', 'score_predictor.2.out_conv.0.bias', 'score_predictor.2.out_conv.2.weight', 'score_predictor.2.out_conv.2.bias', 'score_predictor.2.out_conv.4.weight', 'score_predictor.2.out_conv.4.bias', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'head.weight', 'head.bias', 'layers.0.mixer.A_log', 'layers.0.mixer.D', 'layers.0.mixer.A_b_log', 'layers.0.mixer.D_b', 'layers.0.mixer.in_proj.weight', 'layers.0.mixer.conv1d.weight', 'layers.0.mixer.conv1d.bias', 'layers.0.mixer.x_proj.weight', 'layers.0.mixer.dt_proj.weight', 'layers.0.mixer.dt_proj.bias', 'layers.0.mixer.conv1d_b.weight', 'layers.0.mixer.conv1d_b.bias', 'layers.0.mixer.x_proj_b.weight', 'layers.0.mixer.dt_proj_b.weight', 'layers.0.mixer.dt_proj_b.bias', 'layers.0.mixer.out_proj.weight', 'layers.0.norm.weight', 'layers.1.mixer.A_log', 'layers.1.mixer.D', 'layers.1.mixer.A_b_log', 'layers.1.mixer.D_b', 'layers.1.mixer.in_proj.weight', 'layers.1.mixer.conv1d.weight', 'layers.1.mixer.conv1d.bias', 'layers.1.mixer.x_proj.weight', 'layers.1.mixer.dt_proj.weight', 'layers.1.mixer.dt_proj.bias', 'layers.1.mixer.conv1d_b.weight', 'layers.1.mixer.conv1d_b.bias', 'layers.1.mixer.x_proj_b.weight', 'layers.1.mixer.dt_proj_b.weight', 'layers.1.mixer.dt_proj_b.bias', 'layers.1.mixer.out_proj.weight', 'layers.1.norm.weight', 'layers.2.mixer.A_log', 'layers.2.mixer.D', 'layers.2.mixer.A_b_log', 'layers.2.mixer.D_b', 'layers.2.mixer.in_proj.weight', 'layers.2.mixer.conv1d.weight', 'layers.2.mixer.conv1d.bias', 'layers.2.mixer.x_proj.weight', 'layers.2.mixer.dt_proj.weight', 'layers.2.mixer.dt_proj.bias', 'layers.2.mixer.conv1d_b.weight', 'layers.2.mixer.conv1d_b.bias', 'layers.2.mixer.x_proj_b.weight', 'layers.2.mixer.dt_proj_b.weight', 'layers.2.mixer.dt_proj_b.bias', 'layers.2.mixer.out_proj.weight', 'layers.2.norm.weight', 'layers.3.mixer.A_log', 'layers.3.mixer.D', 'layers.3.mixer.A_b_log', 'layers.3.mixer.D_b', 'layers.3.mixer.in_proj.weight', 'layers.3.mixer.conv1d.weight', 'layers.3.mixer.conv1d.bias', 'layers.3.mixer.x_proj.weight', 'layers.3.mixer.dt_proj.weight', 'layers.3.mixer.dt_proj.bias', 'layers.3.mixer.conv1d_b.weight', 'layers.3.mixer.conv1d_b.bias', 'layers.3.mixer.x_proj_b.weight', 'layers.3.mixer.dt_proj_b.weight', 'layers.3.mixer.dt_proj_b.bias', 'layers.3.mixer.out_proj.weight', 'layers.3.norm.weight', 'layers.4.mixer.A_log', 'layers.4.mixer.D', 'layers.4.mixer.A_b_log', 'layers.4.mixer.D_b', 'layers.4.mixer.in_proj.weight', 'layers.4.mixer.conv1d.weight', 'layers.4.mixer.conv1d.bias', 'layers.4.mixer.x_proj.weight', 'layers.4.mixer.dt_proj.weight', 'layers.4.mixer.dt_proj.bias', 'layers.4.mixer.conv1d_b.weight', 'layers.4.mixer.conv1d_b.bias', 'layers.4.mixer.x_proj_b.weight', 'layers.4.mixer.dt_proj_b.weight', 'layers.4.mixer.dt_proj_b.bias', 'layers.4.mixer.out_proj.weight', 'layers.4.norm.weight', 'layers.5.mixer.A_log', 'layers.5.mixer.D', 'layers.5.mixer.A_b_log', 'layers.5.mixer.D_b', 'layers.5.mixer.in_proj.weight', 'layers.5.mixer.conv1d.weight', 'layers.5.mixer.conv1d.bias', 'layers.5.mixer.x_proj.weight', 'layers.5.mixer.dt_proj.weight', 'layers.5.mixer.dt_proj.bias', 'layers.5.mixer.conv1d_b.weight', 'layers.5.mixer.conv1d_b.bias', 'layers.5.mixer.x_proj_b.weight', 'layers.5.mixer.dt_proj_b.weight', 'layers.5.mixer.dt_proj_b.bias', 'layers.5.mixer.out_proj.weight', 'layers.5.norm.weight', 'layers.6.mixer.A_log', 'layers.6.mixer.D', 'layers.6.mixer.A_b_log', 'layers.6.mixer.D_b', 'layers.6.mixer.in_proj.weight', 'layers.6.mixer.conv1d.weight', 'layers.6.mixer.conv1d.bias', 'layers.6.mixer.x_proj.weight', 'layers.6.mixer.dt_proj.weight', 'layers.6.mixer.dt_proj.bias', 'layers.6.mixer.conv1d_b.weight', 'layers.6.mixer.conv1d_b.bias', 'layers.6.mixer.x_proj_b.weight', 'layers.6.mixer.dt_proj_b.weight', 'layers.6.mixer.dt_proj_b.bias', 'layers.6.mixer.out_proj.weight', 'layers.6.norm.weight', 'layers.7.mixer.A_log', 'layers.7.mixer.D', 'layers.7.mixer.A_b_log', 'layers.7.mixer.D_b', 'layers.7.mixer.in_proj.weight', 'layers.7.mixer.conv1d.weight', 'layers.7.mixer.conv1d.bias', 'layers.7.mixer.x_proj.weight', 'layers.7.mixer.dt_proj.weight', 'layers.7.mixer.dt_proj.bias', 'layers.7.mixer.conv1d_b.weight', 'layers.7.mixer.conv1d_b.bias', 'layers.7.mixer.x_proj_b.weight', 'layers.7.mixer.dt_proj_b.weight', 'layers.7.mixer.dt_proj_b.bias', 'layers.7.mixer.out_proj.weight', 'layers.7.norm.weight', 'layers.8.mixer.A_log', 'layers.8.mixer.D', 'layers.8.mixer.A_b_log', 'layers.8.mixer.D_b', 'layers.8.mixer.in_proj.weight', 'layers.8.mixer.conv1d.weight', 'layers.8.mixer.conv1d.bias', 'layers.8.mixer.x_proj.weight', 'layers.8.mixer.dt_proj.weight', 'layers.8.mixer.dt_proj.bias', 'layers.8.mixer.conv1d_b.weight', 'layers.8.mixer.conv1d_b.bias', 'layers.8.mixer.x_proj_b.weight', 'layers.8.mixer.dt_proj_b.weight', 'layers.8.mixer.dt_proj_b.bias', 'layers.8.mixer.out_proj.weight', 'layers.8.norm.weight', 'layers.9.mixer.A_log', 'layers.9.mixer.D', 'layers.9.mixer.A_b_log', 'layers.9.mixer.D_b', 'layers.9.mixer.in_proj.weight', 'layers.9.mixer.conv1d.weight', 'layers.9.mixer.conv1d.bias', 'layers.9.mixer.x_proj.weight', 'layers.9.mixer.dt_proj.weight', 'layers.9.mixer.dt_proj.bias', 'layers.9.mixer.conv1d_b.weight', 'layers.9.mixer.conv1d_b.bias', 'layers.9.mixer.x_proj_b.weight', 'layers.9.mixer.dt_proj_b.weight', 'layers.9.mixer.dt_proj_b.bias', 'layers.9.mixer.out_proj.weight', 'layers.9.norm.weight', 'layers.10.mixer.A_log', 'layers.10.mixer.D', 'layers.10.mixer.A_b_log', 'layers.10.mixer.D_b', 'layers.10.mixer.in_proj.weight', 'layers.10.mixer.conv1d.weight', 'layers.10.mixer.conv1d.bias', 'layers.10.mixer.x_proj.weight', 'layers.10.mixer.dt_proj.weight', 'layers.10.mixer.dt_proj.bias', 'layers.10.mixer.conv1d_b.weight', 'layers.10.mixer.conv1d_b.bias', 'layers.10.mixer.x_proj_b.weight', 'layers.10.mixer.dt_proj_b.weight', 'layers.10.mixer.dt_proj_b.bias', 'layers.10.mixer.out_proj.weight', 'layers.10.norm.weight', 'layers.11.mixer.A_log', 'layers.11.mixer.D', 'layers.11.mixer.A_b_log', 'layers.11.mixer.D_b', 'layers.11.mixer.in_proj.weight', 'layers.11.mixer.conv1d.weight', 'layers.11.mixer.conv1d.bias', 'layers.11.mixer.x_proj.weight', 'layers.11.mixer.dt_proj.weight', 'layers.11.mixer.dt_proj.bias', 'layers.11.mixer.conv1d_b.weight', 'layers.11.mixer.conv1d_b.bias', 'layers.11.mixer.x_proj_b.weight', 'layers.11.mixer.dt_proj_b.weight', 'layers.11.mixer.dt_proj_b.bias', 'layers.11.mixer.out_proj.weight', 'layers.11.norm.weight', 'layers.12.mixer.A_log', 'layers.12.mixer.D', 'layers.12.mixer.A_b_log', 'layers.12.mixer.D_b', 'layers.12.mixer.in_proj.weight', 'layers.12.mixer.conv1d.weight', 'layers.12.mixer.conv1d.bias', 'layers.12.mixer.x_proj.weight', 'layers.12.mixer.dt_proj.weight', 'layers.12.mixer.dt_proj.bias', 'layers.12.mixer.conv1d_b.weight', 'layers.12.mixer.conv1d_b.bias', 'layers.12.mixer.x_proj_b.weight', 'layers.12.mixer.dt_proj_b.weight', 'layers.12.mixer.dt_proj_b.bias', 'layers.12.mixer.out_proj.weight', 'layers.12.norm.weight', 'layers.13.mixer.A_log', 'layers.13.mixer.D', 'layers.13.mixer.A_b_log', 'layers.13.mixer.D_b', 'layers.13.mixer.in_proj.weight', 'layers.13.mixer.conv1d.weight', 'layers.13.mixer.conv1d.bias', 'layers.13.mixer.x_proj.weight', 'layers.13.mixer.dt_proj.weight', 'layers.13.mixer.dt_proj.bias', 'layers.13.mixer.conv1d_b.weight', 'layers.13.mixer.conv1d_b.bias', 'layers.13.mixer.x_proj_b.weight', 'layers.13.mixer.dt_proj_b.weight', 'layers.13.mixer.dt_proj_b.bias', 'layers.13.mixer.out_proj.weight', 'layers.13.norm.weight', 'layers.14.mixer.A_log', 'layers.14.mixer.D', 'layers.14.mixer.A_b_log', 'layers.14.mixer.D_b', 'layers.14.mixer.in_proj.weight', 'layers.14.mixer.conv1d.weight', 'layers.14.mixer.conv1d.bias', 'layers.14.mixer.x_proj.weight', 'layers.14.mixer.dt_proj.weight', 'layers.14.mixer.dt_proj.bias', 'layers.14.mixer.conv1d_b.weight', 'layers.14.mixer.conv1d_b.bias', 'layers.14.mixer.x_proj_b.weight', 'layers.14.mixer.dt_proj_b.weight', 'layers.14.mixer.dt_proj_b.bias', 'layers.14.mixer.out_proj.weight', 'layers.14.norm.weight', 'layers.15.mixer.A_log', 'layers.15.mixer.D', 'layers.15.mixer.A_b_log', 'layers.15.mixer.D_b', 'layers.15.mixer.in_proj.weight', 'layers.15.mixer.conv1d.weight', 'layers.15.mixer.conv1d.bias', 'layers.15.mixer.x_proj.weight', 'layers.15.mixer.dt_proj.weight', 'layers.15.mixer.dt_proj.bias', 'layers.15.mixer.conv1d_b.weight', 'layers.15.mixer.conv1d_b.bias', 'layers.15.mixer.x_proj_b.weight', 'layers.15.mixer.dt_proj_b.weight', 'layers.15.mixer.dt_proj_b.bias', 'layers.15.mixer.out_proj.weight', 'layers.15.norm.weight', 'layers.16.mixer.A_log', 'layers.16.mixer.D', 'layers.16.mixer.A_b_log', 'layers.16.mixer.D_b', 'layers.16.mixer.in_proj.weight', 'layers.16.mixer.conv1d.weight', 'layers.16.mixer.conv1d.bias', 'layers.16.mixer.x_proj.weight', 'layers.16.mixer.dt_proj.weight', 'layers.16.mixer.dt_proj.bias', 'layers.16.mixer.conv1d_b.weight', 'layers.16.mixer.conv1d_b.bias', 'layers.16.mixer.x_proj_b.weight', 'layers.16.mixer.dt_proj_b.weight', 'layers.16.mixer.dt_proj_b.bias', 'layers.16.mixer.out_proj.weight', 'layers.16.norm.weight', 'layers.17.mixer.A_log', 'layers.17.mixer.D', 'layers.17.mixer.A_b_log', 'layers.17.mixer.D_b', 'layers.17.mixer.in_proj.weight', 'layers.17.mixer.conv1d.weight', 'layers.17.mixer.conv1d.bias', 'layers.17.mixer.x_proj.weight', 'layers.17.mixer.dt_proj.weight', 'layers.17.mixer.dt_proj.bias', 'layers.17.mixer.conv1d_b.weight', 'layers.17.mixer.conv1d_b.bias', 'layers.17.mixer.x_proj_b.weight', 'layers.17.mixer.dt_proj_b.weight', 'layers.17.mixer.dt_proj_b.bias', 'layers.17.mixer.out_proj.weight', 'layers.17.norm.weight', 'layers.18.mixer.A_log', 'layers.18.mixer.D', 'layers.18.mixer.A_b_log', 'layers.18.mixer.D_b', 'layers.18.mixer.in_proj.weight', 'layers.18.mixer.conv1d.weight', 'layers.18.mixer.conv1d.bias', 'layers.18.mixer.x_proj.weight', 'layers.18.mixer.dt_proj.weight', 'layers.18.mixer.dt_proj.bias', 'layers.18.mixer.conv1d_b.weight', 'layers.18.mixer.conv1d_b.bias', 'layers.18.mixer.x_proj_b.weight', 'layers.18.mixer.dt_proj_b.weight', 'layers.18.mixer.dt_proj_b.bias', 'layers.18.mixer.out_proj.weight', 'layers.18.norm.weight', 'layers.19.mixer.A_log', 'layers.19.mixer.D', 'layers.19.mixer.A_b_log', 'layers.19.mixer.D_b', 'layers.19.mixer.in_proj.weight', 'layers.19.mixer.conv1d.weight', 'layers.19.mixer.conv1d.bias', 'layers.19.mixer.x_proj.weight', 'layers.19.mixer.dt_proj.weight', 'layers.19.mixer.dt_proj.bias', 'layers.19.mixer.conv1d_b.weight', 'layers.19.mixer.conv1d_b.bias', 'layers.19.mixer.x_proj_b.weight', 'layers.19.mixer.dt_proj_b.weight', 'layers.19.mixer.dt_proj_b.bias', 'layers.19.mixer.out_proj.weight', 'layers.19.norm.weight', 'layers.20.mixer.A_log', 'layers.20.mixer.D', 'layers.20.mixer.A_b_log', 'layers.20.mixer.D_b', 'layers.20.mixer.in_proj.weight', 'layers.20.mixer.conv1d.weight', 'layers.20.mixer.conv1d.bias', 'layers.20.mixer.x_proj.weight', 'layers.20.mixer.dt_proj.weight', 'layers.20.mixer.dt_proj.bias', 'layers.20.mixer.conv1d_b.weight', 'layers.20.mixer.conv1d_b.bias', 'layers.20.mixer.x_proj_b.weight', 'layers.20.mixer.dt_proj_b.weight', 'layers.20.mixer.dt_proj_b.bias', 'layers.20.mixer.out_proj.weight', 'layers.20.norm.weight', 'layers.21.mixer.A_log', 'layers.21.mixer.D', 'layers.21.mixer.A_b_log', 'layers.21.mixer.D_b', 'layers.21.mixer.in_proj.weight', 'layers.21.mixer.conv1d.weight', 'layers.21.mixer.conv1d.bias', 'layers.21.mixer.x_proj.weight', 'layers.21.mixer.dt_proj.weight', 'layers.21.mixer.dt_proj.bias', 'layers.21.mixer.conv1d_b.weight', 'layers.21.mixer.conv1d_b.bias', 'layers.21.mixer.x_proj_b.weight', 'layers.21.mixer.dt_proj_b.weight', 'layers.21.mixer.dt_proj_b.bias', 'layers.21.mixer.out_proj.weight', 'layers.21.norm.weight', 'layers.22.mixer.A_log', 'layers.22.mixer.D', 'layers.22.mixer.A_b_log', 'layers.22.mixer.D_b', 'layers.22.mixer.in_proj.weight', 'layers.22.mixer.conv1d.weight', 'layers.22.mixer.conv1d.bias', 'layers.22.mixer.x_proj.weight', 'layers.22.mixer.dt_proj.weight', 'layers.22.mixer.dt_proj.bias', 'layers.22.mixer.conv1d_b.weight', 'layers.22.mixer.conv1d_b.bias', 'layers.22.mixer.x_proj_b.weight', 'layers.22.mixer.dt_proj_b.weight', 'layers.22.mixer.dt_proj_b.bias', 'layers.22.mixer.out_proj.weight', 'layers.22.norm.weight', 'layers.23.mixer.A_log', 'layers.23.mixer.D', 'layers.23.mixer.A_b_log', 'layers.23.mixer.D_b', 'layers.23.mixer.in_proj.weight', 'layers.23.mixer.conv1d.weight', 'layers.23.mixer.conv1d.bias', 'layers.23.mixer.x_proj.weight', 'layers.23.mixer.dt_proj.weight', 'layers.23.mixer.dt_proj.bias', 'layers.23.mixer.conv1d_b.weight', 'layers.23.mixer.conv1d_b.bias', 'layers.23.mixer.x_proj_b.weight', 'layers.23.mixer.dt_proj_b.weight', 'layers.23.mixer.dt_proj_b.bias', 'layers.23.mixer.out_proj.weight', 'layers.23.norm.weight', 'norm_f.weight']
number of params: 33044734
ratio_weight= 2.0 reconstruction_weight 0.005 pruning_weight 0.1
Start training for 100 epochs
Epoch: [0]  [   0/1251]  eta: 11:28:09  lr: 0.000001  loss: 7.8776 (7.8776)  time: 33.0055  data: 23.4951  max mem: 19423
Epoch: [0]  [  10/1251]  eta: 1:18:34  lr: 0.000001  loss: 7.8211 (7.8020)  time: 3.7988  data: 2.1363  max mem: 19733
Epoch: [0]  [  20/1251]  eta: 0:48:45  lr: 0.000001  loss: 7.8001 (7.8086)  time: 0.8451  data: 0.0005  max mem: 19733
Epoch: [0]  [  30/1251]  eta: 0:38:10  lr: 0.000001  loss: 7.7787 (7.7870)  time: 0.8187  data: 0.0005  max mem: 19733
Epoch: [0]  [  40/1251]  eta: 0:32:46  lr: 0.000001  loss: 7.7008 (7.7661)  time: 0.8339  data: 0.0005  max mem: 19733
Epoch: [0]  [  50/1251]  eta: 0:29:29  lr: 0.000001  loss: 7.6820 (7.7404)  time: 0.8486  data: 0.0005  max mem: 19733
Epoch: [0]  [  60/1251]  eta: 0:27:20  lr: 0.000001  loss: 7.6347 (7.7234)  time: 0.8724  data: 0.0005  max mem: 19733
Epoch: [0]  [  70/1251]  eta: 0:25:42  lr: 0.000001  loss: 7.6093 (7.7014)  time: 0.8786  data: 0.0005  max mem: 19733
Epoch: [0]  [  80/1251]  eta: 0:24:25  lr: 0.000001  loss: 7.5344 (7.6795)  time: 0.8682  data: 0.0005  max mem: 19733
Epoch: [0]  [  90/1251]  eta: 0:23:25  lr: 0.000001  loss: 7.5218 (7.6649)  time: 0.8743  data: 0.0005  max mem: 19733
loss info: cls_loss=7.0554, ratio_loss=0.8391, pruning_loss=0.2689, mse_loss=1.2840
Epoch: [0]  [ 100/1251]  eta: 0:22:42  lr: 0.000001  loss: 7.4973 (7.6483)  time: 0.9071  data: 0.0005  max mem: 19733
Epoch: [0]  [ 110/1251]  eta: 0:22:01  lr: 0.000001  loss: 7.4661 (7.6293)  time: 0.9180  data: 0.0004  max mem: 19733
Epoch: [0]  [ 120/1251]  eta: 0:21:26  lr: 0.000001  loss: 7.4347 (7.6087)  time: 0.9033  data: 0.0004  max mem: 19733
Epoch: [0]  [ 130/1251]  eta: 0:20:55  lr: 0.000001  loss: 7.3056 (7.5872)  time: 0.9087  data: 0.0004  max mem: 19733
Epoch: [0]  [ 140/1251]  eta: 0:20:28  lr: 0.000001  loss: 7.2733 (7.5613)  time: 0.9150  data: 0.0004  max mem: 19733
Epoch: [0]  [ 150/1251]  eta: 0:20:04  lr: 0.000001  loss: 7.1907 (7.5336)  time: 0.9230  data: 0.0005  max mem: 19733
Epoch: [0]  [ 160/1251]  eta: 0:19:42  lr: 0.000001  loss: 7.1213 (7.5046)  time: 0.9306  data: 0.0004  max mem: 19733
Epoch: [0]  [ 170/1251]  eta: 0:19:22  lr: 0.000001  loss: 7.0804 (7.4761)  time: 0.9356  data: 0.0004  max mem: 19733
Epoch: [0]  [ 180/1251]  eta: 0:19:03  lr: 0.000001  loss: 6.9832 (7.4464)  time: 0.9370  data: 0.0004  max mem: 19733
Epoch: [0]  [ 190/1251]  eta: 0:18:45  lr: 0.000001  loss: 6.9421 (7.4143)  time: 0.9396  data: 0.0004  max mem: 19733
loss info: cls_loss=6.5303, ratio_loss=0.8312, pruning_loss=0.2687, mse_loss=1.3302
Epoch: [0]  [ 200/1251]  eta: 0:18:31  lr: 0.000001  loss: 6.8356 (7.3808)  time: 0.9674  data: 0.0004  max mem: 19733
Epoch: [0]  [ 210/1251]  eta: 0:18:15  lr: 0.000001  loss: 6.8117 (7.3519)  time: 0.9683  data: 0.0005  max mem: 19733
Epoch: [0]  [ 220/1251]  eta: 0:17:59  lr: 0.000001  loss: 6.8268 (7.3231)  time: 0.9438  data: 0.0005  max mem: 19733
Epoch: [0]  [ 230/1251]  eta: 0:17:45  lr: 0.000001  loss: 6.8268 (7.2932)  time: 0.9477  data: 0.0004  max mem: 19733
Epoch: [0]  [ 240/1251]  eta: 0:17:31  lr: 0.000001  loss: 6.6246 (7.2632)  time: 0.9592  data: 0.0005  max mem: 19733
Epoch: [0]  [ 250/1251]  eta: 0:17:18  lr: 0.000001  loss: 6.6246 (7.2298)  time: 0.9729  data: 0.0004  max mem: 19733
Epoch: [0]  [ 260/1251]  eta: 0:17:05  lr: 0.000001  loss: 6.3430 (7.1959)  time: 0.9735  data: 0.0004  max mem: 19733
Epoch: [0]  [ 270/1251]  eta: 0:16:52  lr: 0.000001  loss: 6.2928 (7.1620)  time: 0.9557  data: 0.0004  max mem: 19733
Epoch: [0]  [ 280/1251]  eta: 0:16:39  lr: 0.000001  loss: 6.6265 (7.1416)  time: 0.9548  data: 0.0004  max mem: 19733
Epoch: [0]  [ 290/1251]  eta: 0:16:27  lr: 0.000001  loss: 6.6043 (7.1044)  time: 0.9654  data: 0.0005  max mem: 19733
loss info: cls_loss=5.8711, ratio_loss=0.8246, pruning_loss=0.2769, mse_loss=1.3231
Epoch: [0]  [ 300/1251]  eta: 0:16:14  lr: 0.000001  loss: 5.9796 (7.0705)  time: 0.9597  data: 0.0005  max mem: 19733
Epoch: [0]  [ 310/1251]  eta: 0:16:02  lr: 0.000001  loss: 6.1026 (7.0473)  time: 0.9563  data: 0.0004  max mem: 19733
Epoch: [0]  [ 320/1251]  eta: 0:15:50  lr: 0.000001  loss: 6.7628 (7.0338)  time: 0.9598  data: 0.0005  max mem: 19733
Epoch: [0]  [ 330/1251]  eta: 0:15:38  lr: 0.000001  loss: 6.6570 (7.0102)  time: 0.9659  data: 0.0005  max mem: 19733
Epoch: [0]  [ 340/1251]  eta: 0:15:26  lr: 0.000001  loss: 6.3683 (6.9908)  time: 0.9651  data: 0.0005  max mem: 19733
Epoch: [0]  [ 350/1251]  eta: 0:15:15  lr: 0.000001  loss: 6.3676 (6.9681)  time: 0.9575  data: 0.0005  max mem: 19733
Epoch: [0]  [ 360/1251]  eta: 0:15:03  lr: 0.000001  loss: 6.3676 (6.9434)  time: 0.9506  data: 0.0005  max mem: 19733
Epoch: [0]  [ 370/1251]  eta: 0:14:51  lr: 0.000001  loss: 6.0570 (6.9202)  time: 0.9554  data: 0.0004  max mem: 19733
Epoch: [0]  [ 380/1251]  eta: 0:14:41  lr: 0.000001  loss: 6.0464 (6.8951)  time: 0.9828  data: 0.0004  max mem: 19733
Epoch: [0]  [ 390/1251]  eta: 0:14:30  lr: 0.000001  loss: 5.8951 (6.8681)  time: 0.9857  data: 0.0004  max mem: 19733
loss info: cls_loss=5.6158, ratio_loss=0.8190, pruning_loss=0.2737, mse_loss=1.2795
Epoch: [0]  [ 400/1251]  eta: 0:14:19  lr: 0.000001  loss: 6.1806 (6.8532)  time: 0.9768  data: 0.0004  max mem: 19733
Epoch: [0]  [ 410/1251]  eta: 0:14:08  lr: 0.000001  loss: 6.3431 (6.8405)  time: 0.9622  data: 0.0004  max mem: 19733
Epoch: [0]  [ 420/1251]  eta: 0:13:57  lr: 0.000001  loss: 6.3505 (6.8195)  time: 0.9478  data: 0.0005  max mem: 19733
Epoch: [0]  [ 430/1251]  eta: 0:13:45  lr: 0.000001  loss: 6.3505 (6.8063)  time: 0.9511  data: 0.0005  max mem: 19733
Epoch: [0]  [ 440/1251]  eta: 0:13:35  lr: 0.000001  loss: 6.2952 (6.7919)  time: 0.9630  data: 0.0004  max mem: 19733
Epoch: [0]  [ 450/1251]  eta: 0:13:24  lr: 0.000001  loss: 6.0205 (6.7715)  time: 0.9715  data: 0.0004  max mem: 19733
Epoch: [0]  [ 460/1251]  eta: 0:13:13  lr: 0.000001  loss: 5.7611 (6.7483)  time: 0.9654  data: 0.0005  max mem: 19733
Epoch: [0]  [ 470/1251]  eta: 0:13:03  lr: 0.000001  loss: 5.4774 (6.7202)  time: 0.9651  data: 0.0005  max mem: 19733
Epoch: [0]  [ 480/1251]  eta: 0:12:52  lr: 0.000001  loss: 5.6808 (6.7015)  time: 0.9596  data: 0.0005  max mem: 19733
Epoch: [0]  [ 490/1251]  eta: 0:12:41  lr: 0.000001  loss: 5.9292 (6.6915)  time: 0.9603  data: 0.0004  max mem: 19733
loss info: cls_loss=5.3790, ratio_loss=0.8109, pruning_loss=0.2774, mse_loss=1.2518
Epoch: [0]  [ 500/1251]  eta: 0:12:31  lr: 0.000001  loss: 5.9292 (6.6740)  time: 0.9733  data: 0.0004  max mem: 19733
Epoch: [0]  [ 510/1251]  eta: 0:12:20  lr: 0.000001  loss: 5.9444 (6.6639)  time: 0.9662  data: 0.0004  max mem: 19733
Epoch: [0]  [ 520/1251]  eta: 0:12:09  lr: 0.000001  loss: 6.1419 (6.6493)  time: 0.9460  data: 0.0004  max mem: 19733
Epoch: [0]  [ 530/1251]  eta: 0:11:59  lr: 0.000001  loss: 6.0964 (6.6286)  time: 0.9464  data: 0.0004  max mem: 19733
Epoch: [0]  [ 540/1251]  eta: 0:11:48  lr: 0.000001  loss: 6.0964 (6.6161)  time: 0.9498  data: 0.0004  max mem: 19733
Epoch: [0]  [ 550/1251]  eta: 0:11:38  lr: 0.000001  loss: 5.9640 (6.6011)  time: 0.9459  data: 0.0005  max mem: 19733
Epoch: [0]  [ 560/1251]  eta: 0:11:27  lr: 0.000001  loss: 6.1225 (6.5923)  time: 0.9420  data: 0.0006  max mem: 19733
Epoch: [0]  [ 570/1251]  eta: 0:11:17  lr: 0.000001  loss: 6.2428 (6.5829)  time: 0.9494  data: 0.0006  max mem: 19733
Epoch: [0]  [ 580/1251]  eta: 0:11:07  lr: 0.000001  loss: 6.1525 (6.5705)  time: 0.9760  data: 0.0004  max mem: 19733
Epoch: [0]  [ 590/1251]  eta: 0:10:57  lr: 0.000001  loss: 6.1717 (6.5602)  time: 0.9894  data: 0.0004  max mem: 19733
loss info: cls_loss=5.3536, ratio_loss=0.8060, pruning_loss=0.2779, mse_loss=1.2696
Epoch: [0]  [ 600/1251]  eta: 0:10:46  lr: 0.000001  loss: 6.1176 (6.5457)  time: 0.9616  data: 0.0004  max mem: 19733
Epoch: [0]  [ 610/1251]  eta: 0:10:36  lr: 0.000001  loss: 5.8758 (6.5341)  time: 0.9461  data: 0.0004  max mem: 19733
Epoch: [0]  [ 620/1251]  eta: 0:10:25  lr: 0.000001  loss: 5.8848 (6.5245)  time: 0.9470  data: 0.0004  max mem: 19733
Epoch: [0]  [ 630/1251]  eta: 0:10:15  lr: 0.000001  loss: 6.0991 (6.5136)  time: 0.9458  data: 0.0004  max mem: 19733
Epoch: [0]  [ 640/1251]  eta: 0:10:05  lr: 0.000001  loss: 5.5501 (6.4981)  time: 0.9769  data: 0.0004  max mem: 19733
Epoch: [0]  [ 650/1251]  eta: 0:09:55  lr: 0.000001  loss: 5.6572 (6.4873)  time: 0.9876  data: 0.0004  max mem: 19733
Epoch: [0]  [ 660/1251]  eta: 0:09:45  lr: 0.000001  loss: 5.5591 (6.4686)  time: 0.9571  data: 0.0004  max mem: 19733
Epoch: [0]  [ 670/1251]  eta: 0:09:34  lr: 0.000001  loss: 5.8932 (6.4624)  time: 0.9381  data: 0.0005  max mem: 19733
Epoch: [0]  [ 680/1251]  eta: 0:09:24  lr: 0.000001  loss: 5.9968 (6.4520)  time: 0.9424  data: 0.0004  max mem: 19733
Epoch: [0]  [ 690/1251]  eta: 0:09:14  lr: 0.000001  loss: 5.4181 (6.4368)  time: 0.9459  data: 0.0004  max mem: 19733
loss info: cls_loss=5.1070, ratio_loss=0.8004, pruning_loss=0.2833, mse_loss=1.2394
Epoch: [0]  [ 700/1251]  eta: 0:09:04  lr: 0.000001  loss: 5.4181 (6.4244)  time: 0.9469  data: 0.0004  max mem: 19733
Epoch: [0]  [ 710/1251]  eta: 0:08:53  lr: 0.000001  loss: 6.0858 (6.4122)  time: 0.9449  data: 0.0004  max mem: 19733
Epoch: [0]  [ 720/1251]  eta: 0:08:43  lr: 0.000001  loss: 5.8983 (6.3983)  time: 0.9416  data: 0.0004  max mem: 19733
Epoch: [0]  [ 730/1251]  eta: 0:08:33  lr: 0.000001  loss: 5.5957 (6.3887)  time: 0.9458  data: 0.0005  max mem: 19733
Epoch: [0]  [ 740/1251]  eta: 0:08:23  lr: 0.000001  loss: 5.9222 (6.3851)  time: 0.9596  data: 0.0004  max mem: 19733
Epoch: [0]  [ 750/1251]  eta: 0:08:13  lr: 0.000001  loss: 5.9811 (6.3727)  time: 0.9668  data: 0.0004  max mem: 19733
Epoch: [0]  [ 760/1251]  eta: 0:08:03  lr: 0.000001  loss: 5.9811 (6.3646)  time: 0.9519  data: 0.0004  max mem: 19733
Epoch: [0]  [ 770/1251]  eta: 0:07:53  lr: 0.000001  loss: 5.9972 (6.3555)  time: 0.9397  data: 0.0004  max mem: 19733
Epoch: [0]  [ 780/1251]  eta: 0:07:43  lr: 0.000001  loss: 5.7809 (6.3471)  time: 0.9509  data: 0.0004  max mem: 19733
Epoch: [0]  [ 790/1251]  eta: 0:07:33  lr: 0.000001  loss: 5.5437 (6.3367)  time: 0.9646  data: 0.0004  max mem: 19733
loss info: cls_loss=5.0960, ratio_loss=0.7956, pruning_loss=0.2829, mse_loss=1.2753
Epoch: [0]  [ 800/1251]  eta: 0:07:23  lr: 0.000001  loss: 5.6185 (6.3288)  time: 0.9546  data: 0.0004  max mem: 19733
Epoch: [0]  [ 810/1251]  eta: 0:07:13  lr: 0.000001  loss: 5.2484 (6.3117)  time: 0.9581  data: 0.0004  max mem: 19733
Epoch: [0]  [ 820/1251]  eta: 0:07:03  lr: 0.000001  loss: 5.2742 (6.3056)  time: 0.9567  data: 0.0004  max mem: 19733
Epoch: [0]  [ 830/1251]  eta: 0:06:53  lr: 0.000001  loss: 5.8654 (6.2925)  time: 0.9404  data: 0.0004  max mem: 19733
Epoch: [0]  [ 840/1251]  eta: 0:06:43  lr: 0.000001  loss: 5.0980 (6.2821)  time: 0.9394  data: 0.0005  max mem: 19733
Epoch: [0]  [ 850/1251]  eta: 0:06:33  lr: 0.000001  loss: 5.4468 (6.2720)  time: 0.9509  data: 0.0004  max mem: 19733
Epoch: [0]  [ 860/1251]  eta: 0:06:23  lr: 0.000001  loss: 5.8136 (6.2636)  time: 0.9489  data: 0.0004  max mem: 19733
Epoch: [0]  [ 870/1251]  eta: 0:06:13  lr: 0.000001  loss: 5.5285 (6.2569)  time: 0.9520  data: 0.0005  max mem: 19733
Epoch: [0]  [ 880/1251]  eta: 0:06:03  lr: 0.000001  loss: 5.4946 (6.2449)  time: 0.9554  data: 0.0004  max mem: 19733
Epoch: [0]  [ 890/1251]  eta: 0:05:53  lr: 0.000001  loss: 5.2629 (6.2344)  time: 0.9365  data: 0.0004  max mem: 19733
loss info: cls_loss=4.8558, ratio_loss=0.7891, pruning_loss=0.2887, mse_loss=1.2470
Epoch: [0]  [ 900/1251]  eta: 0:05:43  lr: 0.000001  loss: 5.6193 (6.2276)  time: 0.9304  data: 0.0004  max mem: 19733
Epoch: [0]  [ 910/1251]  eta: 0:05:33  lr: 0.000001  loss: 5.8983 (6.2211)  time: 0.9335  data: 0.0004  max mem: 19733
Epoch: [0]  [ 920/1251]  eta: 0:05:23  lr: 0.000001  loss: 5.8983 (6.2155)  time: 0.9598  data: 0.0005  max mem: 19733
Epoch: [0]  [ 930/1251]  eta: 0:05:13  lr: 0.000001  loss: 5.8832 (6.2125)  time: 0.9604  data: 0.0006  max mem: 19733
Epoch: [0]  [ 940/1251]  eta: 0:05:04  lr: 0.000001  loss: 5.7841 (6.2038)  time: 0.9431  data: 0.0005  max mem: 19733
Epoch: [0]  [ 950/1251]  eta: 0:04:54  lr: 0.000001  loss: 4.9793 (6.1891)  time: 0.9487  data: 0.0004  max mem: 19733
Epoch: [0]  [ 960/1251]  eta: 0:04:44  lr: 0.000001  loss: 5.2816 (6.1833)  time: 0.9596  data: 0.0005  max mem: 19733
Epoch: [0]  [ 970/1251]  eta: 0:04:34  lr: 0.000001  loss: 5.8786 (6.1780)  time: 0.9584  data: 0.0004  max mem: 19733
Epoch: [0]  [ 980/1251]  eta: 0:04:24  lr: 0.000001  loss: 5.7654 (6.1716)  time: 0.9406  data: 0.0005  max mem: 19733
Epoch: [0]  [ 990/1251]  eta: 0:04:14  lr: 0.000001  loss: 5.8848 (6.1653)  time: 0.9354  data: 0.0005  max mem: 19733
loss info: cls_loss=4.9865, ratio_loss=0.7830, pruning_loss=0.2852, mse_loss=1.2760
Epoch: [0]  [1000/1251]  eta: 0:04:04  lr: 0.000001  loss: 5.7259 (6.1583)  time: 0.9467  data: 0.0004  max mem: 19733
Epoch: [0]  [1010/1251]  eta: 0:03:55  lr: 0.000001  loss: 5.6874 (6.1541)  time: 0.9529  data: 0.0004  max mem: 19733
Epoch: [0]  [1020/1251]  eta: 0:03:45  lr: 0.000001  loss: 5.6325 (6.1463)  time: 0.9399  data: 0.0004  max mem: 19733
Epoch: [0]  [1030/1251]  eta: 0:03:35  lr: 0.000001  loss: 5.3602 (6.1358)  time: 0.9328  data: 0.0004  max mem: 19733
Epoch: [0]  [1040/1251]  eta: 0:03:25  lr: 0.000001  loss: 5.3602 (6.1316)  time: 0.9419  data: 0.0005  max mem: 19733
Epoch: [0]  [1050/1251]  eta: 0:03:15  lr: 0.000001  loss: 5.7913 (6.1246)  time: 0.9394  data: 0.0004  max mem: 19733
Epoch: [0]  [1060/1251]  eta: 0:03:06  lr: 0.000001  loss: 5.6746 (6.1206)  time: 0.9515  data: 0.0004  max mem: 19733
Epoch: [0]  [1070/1251]  eta: 0:02:56  lr: 0.000001  loss: 5.6226 (6.1134)  time: 0.9690  data: 0.0004  max mem: 19733
Epoch: [0]  [1080/1251]  eta: 0:02:46  lr: 0.000001  loss: 5.2767 (6.1072)  time: 0.9671  data: 0.0004  max mem: 19733
Epoch: [0]  [1090/1251]  eta: 0:02:36  lr: 0.000001  loss: 5.6211 (6.1010)  time: 0.9531  data: 0.0004  max mem: 19733
loss info: cls_loss=4.8955, ratio_loss=0.7761, pruning_loss=0.2864, mse_loss=1.2519
Epoch: [0]  [1100/1251]  eta: 0:02:26  lr: 0.000001  loss: 5.6211 (6.0944)  time: 0.9331  data: 0.0004  max mem: 19733
Epoch: [0]  [1110/1251]  eta: 0:02:17  lr: 0.000001  loss: 5.6906 (6.0895)  time: 0.9486  data: 0.0004  max mem: 19733
Epoch: [0]  [1120/1251]  eta: 0:02:07  lr: 0.000001  loss: 5.8564 (6.0861)  time: 0.9524  data: 0.0004  max mem: 19733
Epoch: [0]  [1130/1251]  eta: 0:01:57  lr: 0.000001  loss: 5.7326 (6.0820)  time: 0.9374  data: 0.0004  max mem: 19733
Epoch: [0]  [1140/1251]  eta: 0:01:47  lr: 0.000001  loss: 5.5291 (6.0744)  time: 0.9317  data: 0.0004  max mem: 19733
Epoch: [0]  [1150/1251]  eta: 0:01:38  lr: 0.000001  loss: 5.3832 (6.0692)  time: 0.9306  data: 0.0004  max mem: 19733
Epoch: [0]  [1160/1251]  eta: 0:01:28  lr: 0.000001  loss: 5.6639 (6.0633)  time: 0.9481  data: 0.0004  max mem: 19733
Epoch: [0]  [1170/1251]  eta: 0:01:18  lr: 0.000001  loss: 5.6639 (6.0582)  time: 0.9463  data: 0.0004  max mem: 19733
Epoch: [0]  [1180/1251]  eta: 0:01:08  lr: 0.000001  loss: 5.8402 (6.0561)  time: 0.9409  data: 0.0004  max mem: 19733
Epoch: [0]  [1190/1251]  eta: 0:00:59  lr: 0.000001  loss: 5.8402 (6.0528)  time: 0.9471  data: 0.0006  max mem: 19733
loss info: cls_loss=5.0084, ratio_loss=0.7694, pruning_loss=0.2813, mse_loss=1.2338
Epoch: [0]  [1200/1251]  eta: 0:00:49  lr: 0.000001  loss: 5.6934 (6.0493)  time: 0.9352  data: 0.0005  max mem: 19733
Epoch: [0]  [1210/1251]  eta: 0:00:39  lr: 0.000001  loss: 5.5986 (6.0433)  time: 0.9351  data: 0.0001  max mem: 19733
Epoch: [0]  [1220/1251]  eta: 0:00:30  lr: 0.000001  loss: 5.3371 (6.0371)  time: 0.9548  data: 0.0001  max mem: 19733
Epoch: [0]  [1230/1251]  eta: 0:00:20  lr: 0.000001  loss: 5.3371 (6.0319)  time: 0.9487  data: 0.0001  max mem: 19733
Epoch: [0]  [1240/1251]  eta: 0:00:10  lr: 0.000001  loss: 5.3775 (6.0257)  time: 0.9345  data: 0.0001  max mem: 19733
Epoch: [0]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 5.2412 (6.0183)  time: 0.9366  data: 0.0001  max mem: 19733
Epoch: [0] Total time: 0:20:14 (0.9706 s / it)
Averaged stats: lr: 0.000001  loss: 5.2412 (6.0201)
Test:  [  0/261]  eta: 2:33:02  loss: 1.3801 (1.3801)  acc1: 73.9583 (73.9583)  acc5: 90.6250 (90.6250)  time: 35.1828  data: 31.7115  max mem: 19733
Test:  [ 10/261]  eta: 0:14:57  loss: 1.3801 (1.3638)  acc1: 77.6042 (75.3314)  acc5: 93.7500 (91.7140)  time: 3.5747  data: 2.9652  max mem: 19733
Test:  [ 20/261]  eta: 0:08:32  loss: 1.4824 (1.5167)  acc1: 68.2292 (70.1141)  acc5: 90.6250 (90.2778)  time: 0.4736  data: 0.0552  max mem: 19733
Test:  [ 30/261]  eta: 0:06:01  loss: 1.3907 (1.4229)  acc1: 72.9167 (72.9839)  acc5: 90.1042 (90.7090)  time: 0.4610  data: 0.0175  max mem: 19733
Test:  [ 40/261]  eta: 0:04:37  loss: 1.2315 (1.4224)  acc1: 77.0833 (72.8277)  acc5: 92.7083 (90.7520)  time: 0.3384  data: 0.0346  max mem: 19733
Test:  [ 50/261]  eta: 0:03:42  loss: 1.6498 (1.5018)  acc1: 65.1042 (70.0163)  acc5: 86.4583 (89.8284)  time: 0.2579  data: 0.0292  max mem: 19733
Test:  [ 60/261]  eta: 0:03:12  loss: 1.6989 (1.5231)  acc1: 62.5000 (69.1086)  acc5: 88.5417 (89.8395)  time: 0.3521  data: 0.0068  max mem: 19733
Test:  [ 70/261]  eta: 0:02:43  loss: 1.6737 (1.5331)  acc1: 63.5417 (68.4126)  acc5: 90.6250 (90.0675)  time: 0.3508  data: 0.0085  max mem: 19733
Test:  [ 80/261]  eta: 0:02:27  loss: 1.4779 (1.5156)  acc1: 68.7500 (68.9043)  acc5: 92.1875 (90.3357)  time: 0.3748  data: 0.1303  max mem: 19733
Test:  [ 90/261]  eta: 0:02:13  loss: 1.3962 (1.4892)  acc1: 72.9167 (69.4368)  acc5: 92.7083 (90.7109)  time: 0.5047  data: 0.2325  max mem: 19733
Test:  [100/261]  eta: 0:01:56  loss: 1.2607 (1.4779)  acc1: 74.4792 (69.6421)  acc5: 92.1875 (90.7333)  time: 0.3445  data: 0.1120  max mem: 19733
Test:  [110/261]  eta: 0:01:43  loss: 1.3579 (1.4953)  acc1: 71.8750 (69.3741)  acc5: 89.5833 (90.3247)  time: 0.2659  data: 0.0829  max mem: 19733
Test:  [120/261]  eta: 0:01:35  loss: 1.8042 (1.5293)  acc1: 61.4583 (68.5864)  acc5: 84.3750 (89.6866)  time: 0.4673  data: 0.2566  max mem: 19733
Test:  [130/261]  eta: 0:01:24  loss: 1.9634 (1.5706)  acc1: 58.3333 (67.7441)  acc5: 80.7292 (88.9512)  time: 0.3956  data: 0.1826  max mem: 19733
Test:  [140/261]  eta: 0:01:17  loss: 1.8472 (1.5900)  acc1: 58.3333 (67.1321)  acc5: 82.2917 (88.6599)  time: 0.4053  data: 0.1911  max mem: 19733
Test:  [150/261]  eta: 0:01:08  loss: 1.6976 (1.5897)  acc1: 62.5000 (67.2461)  acc5: 86.4583 (88.5244)  time: 0.4701  data: 0.2431  max mem: 19733
Test:  [160/261]  eta: 0:01:01  loss: 1.6397 (1.6120)  acc1: 67.7083 (66.9158)  acc5: 86.4583 (88.0726)  time: 0.4078  data: 0.2165  max mem: 19733
Test:  [170/261]  eta: 0:00:54  loss: 2.0025 (1.6408)  acc1: 57.8125 (66.2311)  acc5: 80.7292 (87.6036)  time: 0.4193  data: 0.1857  max mem: 19733
Test:  [180/261]  eta: 0:00:47  loss: 1.9429 (1.6575)  acc1: 54.6875 (65.8322)  acc5: 81.7708 (87.3331)  time: 0.3418  data: 0.0929  max mem: 19733
Test:  [190/261]  eta: 0:00:41  loss: 1.9429 (1.6730)  acc1: 56.7708 (65.5159)  acc5: 82.2917 (87.0528)  time: 0.5264  data: 0.2969  max mem: 19733
Test:  [200/261]  eta: 0:00:34  loss: 1.9821 (1.6848)  acc1: 58.8542 (65.2182)  acc5: 81.7708 (86.8263)  time: 0.4723  data: 0.2318  max mem: 19733
Test:  [210/261]  eta: 0:00:28  loss: 1.8998 (1.6925)  acc1: 59.3750 (65.0671)  acc5: 82.2917 (86.6706)  time: 0.2736  data: 0.0652  max mem: 19733
Test:  [220/261]  eta: 0:00:22  loss: 1.9485 (1.7170)  acc1: 58.3333 (64.5009)  acc5: 82.2917 (86.2863)  time: 0.2306  data: 0.0625  max mem: 19733
Test:  [230/261]  eta: 0:00:16  loss: 2.0505 (1.7270)  acc1: 52.0833 (64.2339)  acc5: 81.2500 (86.1179)  time: 0.1478  data: 0.0007  max mem: 19733
Test:  [240/261]  eta: 0:00:10  loss: 1.7773 (1.7269)  acc1: 58.8542 (64.1187)  acc5: 82.8125 (86.1428)  time: 0.1428  data: 0.0001  max mem: 19733
Test:  [250/261]  eta: 0:00:05  loss: 1.5572 (1.7137)  acc1: 65.6250 (64.3841)  acc5: 88.5417 (86.3235)  time: 0.1421  data: 0.0001  max mem: 19733
Test:  [260/261]  eta: 0:00:00  loss: 1.4039 (1.7072)  acc1: 68.2292 (64.5520)  acc5: 91.6667 (86.4620)  time: 0.1388  data: 0.0001  max mem: 19733
Test: Total time: 0:02:05 (0.4798 s / it)
* Acc@1 64.552 Acc@5 86.462 loss 1.707
Accuracy of the network on the 50000 test images: 64.6%
Max accuracy: 64.55%
Epoch: [1]  [   0/1251]  eta: 6:49:18  lr: 0.000001  loss: 5.4012 (5.4012)  time: 19.6312  data: 16.3990  max mem: 19734
Epoch: [1]  [  10/1251]  eta: 0:54:05  lr: 0.000001  loss: 5.6640 (5.5184)  time: 2.6153  data: 1.4915  max mem: 19734
Epoch: [1]  [  20/1251]  eta: 0:36:19  lr: 0.000001  loss: 5.6092 (5.4859)  time: 0.8773  data: 0.0006  max mem: 19734
Epoch: [1]  [  30/1251]  eta: 0:29:58  lr: 0.000001  loss: 5.8022 (5.5816)  time: 0.8446  data: 0.0005  max mem: 19734
Epoch: [1]  [  40/1251]  eta: 0:26:42  lr: 0.000001  loss: 5.7583 (5.5448)  time: 0.8545  data: 0.0005  max mem: 19734
loss info: cls_loss=4.8650, ratio_loss=0.7615, pruning_loss=0.2819, mse_loss=1.2292
Epoch: [1]  [  50/1251]  eta: 0:24:42  lr: 0.000001  loss: 5.6244 (5.5296)  time: 0.8649  data: 0.0005  max mem: 19734
Epoch: [1]  [  60/1251]  eta: 0:23:20  lr: 0.000001  loss: 5.4350 (5.4665)  time: 0.8743  data: 0.0004  max mem: 19734
Epoch: [1]  [  70/1251]  eta: 0:22:20  lr: 0.000001  loss: 4.9144 (5.3686)  time: 0.8815  data: 0.0004  max mem: 19734
Epoch: [1]  [  80/1251]  eta: 0:21:36  lr: 0.000001  loss: 5.1960 (5.3936)  time: 0.8947  data: 0.0005  max mem: 19734
Epoch: [1]  [  90/1251]  eta: 0:20:59  lr: 0.000001  loss: 5.7852 (5.3938)  time: 0.9077  data: 0.0005  max mem: 19734
Epoch: [1]  [ 100/1251]  eta: 0:20:31  lr: 0.000001  loss: 5.4280 (5.4067)  time: 0.9201  data: 0.0005  max mem: 19734
Epoch: [1]  [ 110/1251]  eta: 0:20:07  lr: 0.000001  loss: 5.6369 (5.4199)  time: 0.9359  data: 0.0005  max mem: 19734
Epoch: [1]  [ 120/1251]  eta: 0:19:43  lr: 0.000001  loss: 5.6624 (5.4307)  time: 0.9286  data: 0.0005  max mem: 19734
Epoch: [1]  [ 130/1251]  eta: 0:19:22  lr: 0.000001  loss: 5.6532 (5.4364)  time: 0.9192  data: 0.0005  max mem: 19734
Epoch: [1]  [ 140/1251]  eta: 0:19:03  lr: 0.000001  loss: 5.6182 (5.4197)  time: 0.9235  data: 0.0005  max mem: 19734
loss info: cls_loss=4.8236, ratio_loss=0.7527, pruning_loss=0.2810, mse_loss=1.2539
Epoch: [1]  [ 150/1251]  eta: 0:18:46  lr: 0.000001  loss: 5.4459 (5.4215)  time: 0.9353  data: 0.0004  max mem: 19734
Epoch: [1]  [ 160/1251]  eta: 0:18:30  lr: 0.000001  loss: 5.4639 (5.4274)  time: 0.9386  data: 0.0004  max mem: 19734
Epoch: [1]  [ 170/1251]  eta: 0:18:16  lr: 0.000001  loss: 5.4639 (5.4240)  time: 0.9424  data: 0.0004  max mem: 19734
Epoch: [1]  [ 180/1251]  eta: 0:18:00  lr: 0.000001  loss: 5.4452 (5.4146)  time: 0.9366  data: 0.0004  max mem: 19734
Epoch: [1]  [ 190/1251]  eta: 0:17:47  lr: 0.000001  loss: 5.3127 (5.4030)  time: 0.9360  data: 0.0005  max mem: 19734
Epoch: [1]  [ 200/1251]  eta: 0:17:33  lr: 0.000001  loss: 5.4962 (5.3978)  time: 0.9397  data: 0.0005  max mem: 19734
Epoch: [1]  [ 210/1251]  eta: 0:17:19  lr: 0.000001  loss: 5.6748 (5.4035)  time: 0.9309  data: 0.0005  max mem: 19734
Epoch: [1]  [ 220/1251]  eta: 0:17:06  lr: 0.000001  loss: 5.5331 (5.3930)  time: 0.9312  data: 0.0005  max mem: 19734
Epoch: [1]  [ 230/1251]  eta: 0:16:54  lr: 0.000001  loss: 4.8801 (5.3684)  time: 0.9371  data: 0.0004  max mem: 19734
Epoch: [1]  [ 240/1251]  eta: 0:16:41  lr: 0.000001  loss: 4.8115 (5.3456)  time: 0.9357  data: 0.0004  max mem: 19734
loss info: cls_loss=4.6810, ratio_loss=0.7439, pruning_loss=0.2828, mse_loss=1.2232
Epoch: [1]  [ 250/1251]  eta: 0:16:29  lr: 0.000001  loss: 4.9498 (5.3341)  time: 0.9354  data: 0.0004  max mem: 19734
Epoch: [1]  [ 260/1251]  eta: 0:16:18  lr: 0.000001  loss: 5.3400 (5.3446)  time: 0.9525  data: 0.0004  max mem: 19734
Epoch: [1]  [ 270/1251]  eta: 0:16:07  lr: 0.000001  loss: 5.5913 (5.3488)  time: 0.9481  data: 0.0004  max mem: 19734
Epoch: [1]  [ 280/1251]  eta: 0:15:56  lr: 0.000001  loss: 5.4842 (5.3418)  time: 0.9465  data: 0.0004  max mem: 19734
Epoch: [1]  [ 290/1251]  eta: 0:15:44  lr: 0.000001  loss: 5.5534 (5.3419)  time: 0.9435  data: 0.0004  max mem: 19734
Epoch: [1]  [ 300/1251]  eta: 0:15:34  lr: 0.000001  loss: 5.2307 (5.3257)  time: 0.9521  data: 0.0004  max mem: 19734
Epoch: [1]  [ 310/1251]  eta: 0:15:23  lr: 0.000001  loss: 5.2307 (5.3184)  time: 0.9577  data: 0.0004  max mem: 19734
Epoch: [1]  [ 320/1251]  eta: 0:15:13  lr: 0.000001  loss: 5.0747 (5.3093)  time: 0.9537  data: 0.0005  max mem: 19734
Epoch: [1]  [ 330/1251]  eta: 0:15:01  lr: 0.000001  loss: 5.0747 (5.3021)  time: 0.9486  data: 0.0005  max mem: 19734
Epoch: [1]  [ 340/1251]  eta: 0:14:51  lr: 0.000001  loss: 5.4581 (5.3071)  time: 0.9473  data: 0.0005  max mem: 19734
loss info: cls_loss=4.7354, ratio_loss=0.7348, pruning_loss=0.2824, mse_loss=1.2457
Epoch: [1]  [ 350/1251]  eta: 0:14:40  lr: 0.000001  loss: 5.5850 (5.3147)  time: 0.9509  data: 0.0005  max mem: 19734
Epoch: [1]  [ 360/1251]  eta: 0:14:30  lr: 0.000001  loss: 5.6196 (5.3158)  time: 0.9477  data: 0.0005  max mem: 19734
Epoch: [1]  [ 370/1251]  eta: 0:14:19  lr: 0.000001  loss: 5.2965 (5.3099)  time: 0.9477  data: 0.0005  max mem: 19734
Epoch: [1]  [ 380/1251]  eta: 0:14:09  lr: 0.000001  loss: 5.3809 (5.3120)  time: 0.9347  data: 0.0005  max mem: 19734
Epoch: [1]  [ 390/1251]  eta: 0:13:58  lr: 0.000001  loss: 5.3809 (5.3091)  time: 0.9436  data: 0.0005  max mem: 19734
Epoch: [1]  [ 400/1251]  eta: 0:13:48  lr: 0.000001  loss: 5.0673 (5.2884)  time: 0.9496  data: 0.0005  max mem: 19734
Epoch: [1]  [ 410/1251]  eta: 0:13:38  lr: 0.000001  loss: 4.5932 (5.2768)  time: 0.9494  data: 0.0005  max mem: 19734
Epoch: [1]  [ 420/1251]  eta: 0:13:27  lr: 0.000001  loss: 4.8193 (5.2711)  time: 0.9447  data: 0.0004  max mem: 19734
Epoch: [1]  [ 430/1251]  eta: 0:13:18  lr: 0.000001  loss: 5.4906 (5.2703)  time: 0.9546  data: 0.0004  max mem: 19734
Epoch: [1]  [ 440/1251]  eta: 0:13:07  lr: 0.000001  loss: 5.6130 (5.2725)  time: 0.9555  data: 0.0004  max mem: 19734
loss info: cls_loss=4.6249, ratio_loss=0.7269, pruning_loss=0.2818, mse_loss=1.2158
Epoch: [1]  [ 450/1251]  eta: 0:12:57  lr: 0.000001  loss: 5.6204 (5.2778)  time: 0.9330  data: 0.0004  max mem: 19734
Epoch: [1]  [ 460/1251]  eta: 0:12:47  lr: 0.000001  loss: 5.4160 (5.2722)  time: 0.9303  data: 0.0004  max mem: 19734
Epoch: [1]  [ 470/1251]  eta: 0:12:36  lr: 0.000001  loss: 4.9980 (5.2669)  time: 0.9322  data: 0.0004  max mem: 19734
Epoch: [1]  [ 480/1251]  eta: 0:12:26  lr: 0.000001  loss: 5.6124 (5.2722)  time: 0.9377  data: 0.0004  max mem: 19734
Epoch: [1]  [ 490/1251]  eta: 0:12:16  lr: 0.000001  loss: 5.6124 (5.2671)  time: 0.9347  data: 0.0004  max mem: 19734
Epoch: [1]  [ 500/1251]  eta: 0:12:06  lr: 0.000001  loss: 4.7798 (5.2597)  time: 0.9284  data: 0.0004  max mem: 19734
Epoch: [1]  [ 510/1251]  eta: 0:11:56  lr: 0.000001  loss: 4.9273 (5.2572)  time: 0.9386  data: 0.0004  max mem: 19734
Epoch: [1]  [ 520/1251]  eta: 0:11:45  lr: 0.000001  loss: 5.1913 (5.2556)  time: 0.9382  data: 0.0005  max mem: 19734
Epoch: [1]  [ 530/1251]  eta: 0:11:36  lr: 0.000001  loss: 5.5020 (5.2583)  time: 0.9461  data: 0.0005  max mem: 19734
Epoch: [1]  [ 540/1251]  eta: 0:11:26  lr: 0.000001  loss: 5.3266 (5.2539)  time: 0.9577  data: 0.0004  max mem: 19734
loss info: cls_loss=4.6628, ratio_loss=0.7152, pruning_loss=0.2804, mse_loss=1.2351
Epoch: [1]  [ 550/1251]  eta: 0:11:16  lr: 0.000001  loss: 5.2776 (5.2562)  time: 0.9637  data: 0.0004  max mem: 19734
Epoch: [1]  [ 560/1251]  eta: 0:11:07  lr: 0.000001  loss: 5.2923 (5.2527)  time: 0.9714  data: 0.0004  max mem: 19734
Epoch: [1]  [ 570/1251]  eta: 0:10:57  lr: 0.000001  loss: 5.0342 (5.2499)  time: 0.9449  data: 0.0004  max mem: 19734
Epoch: [1]  [ 580/1251]  eta: 0:10:46  lr: 0.000001  loss: 5.4275 (5.2538)  time: 0.9217  data: 0.0004  max mem: 19734
Epoch: [1]  [ 590/1251]  eta: 0:10:37  lr: 0.000001  loss: 5.4264 (5.2488)  time: 0.9339  data: 0.0004  max mem: 19734
Epoch: [1]  [ 600/1251]  eta: 0:10:26  lr: 0.000001  loss: 4.9693 (5.2424)  time: 0.9349  data: 0.0004  max mem: 19734
Epoch: [1]  [ 610/1251]  eta: 0:10:16  lr: 0.000001  loss: 4.9693 (5.2407)  time: 0.9190  data: 0.0005  max mem: 19734
Epoch: [1]  [ 620/1251]  eta: 0:10:06  lr: 0.000001  loss: 4.9539 (5.2334)  time: 0.9141  data: 0.0006  max mem: 19734
Epoch: [1]  [ 630/1251]  eta: 0:09:56  lr: 0.000001  loss: 5.3887 (5.2388)  time: 0.9125  data: 0.0005  max mem: 19734
Epoch: [1]  [ 640/1251]  eta: 0:09:46  lr: 0.000001  loss: 5.3619 (5.2302)  time: 0.9195  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5591, ratio_loss=0.7029, pruning_loss=0.2798, mse_loss=1.2381
Epoch: [1]  [ 650/1251]  eta: 0:09:36  lr: 0.000001  loss: 4.9620 (5.2279)  time: 0.9321  data: 0.0004  max mem: 19734
Epoch: [1]  [ 660/1251]  eta: 0:09:26  lr: 0.000001  loss: 5.2760 (5.2281)  time: 0.9157  data: 0.0004  max mem: 19734
Epoch: [1]  [ 670/1251]  eta: 0:09:16  lr: 0.000001  loss: 4.9180 (5.2180)  time: 0.9115  data: 0.0004  max mem: 19734
Epoch: [1]  [ 680/1251]  eta: 0:09:06  lr: 0.000001  loss: 4.7002 (5.2124)  time: 0.9239  data: 0.0004  max mem: 19734
Epoch: [1]  [ 690/1251]  eta: 0:08:57  lr: 0.000001  loss: 4.8320 (5.2085)  time: 0.9417  data: 0.0004  max mem: 19734
Epoch: [1]  [ 700/1251]  eta: 0:08:47  lr: 0.000001  loss: 5.0650 (5.2034)  time: 0.9427  data: 0.0004  max mem: 19734
Epoch: [1]  [ 710/1251]  eta: 0:08:37  lr: 0.000001  loss: 5.1223 (5.1998)  time: 0.9304  data: 0.0004  max mem: 19734
Epoch: [1]  [ 720/1251]  eta: 0:08:27  lr: 0.000001  loss: 4.9627 (5.1924)  time: 0.9209  data: 0.0004  max mem: 19734
Epoch: [1]  [ 730/1251]  eta: 0:08:17  lr: 0.000001  loss: 4.6103 (5.1872)  time: 0.9004  data: 0.0005  max mem: 19734
Epoch: [1]  [ 740/1251]  eta: 0:08:07  lr: 0.000001  loss: 4.9963 (5.1843)  time: 0.9019  data: 0.0005  max mem: 19734
loss info: cls_loss=4.3842, ratio_loss=0.6919, pruning_loss=0.2888, mse_loss=1.2500
Epoch: [1]  [ 750/1251]  eta: 0:07:58  lr: 0.000001  loss: 5.1143 (5.1800)  time: 0.9138  data: 0.0004  max mem: 19734
Epoch: [1]  [ 760/1251]  eta: 0:07:48  lr: 0.000001  loss: 5.2201 (5.1845)  time: 0.9170  data: 0.0005  max mem: 19734
Epoch: [1]  [ 770/1251]  eta: 0:07:38  lr: 0.000001  loss: 5.3877 (5.1816)  time: 0.9060  data: 0.0005  max mem: 19734
Epoch: [1]  [ 780/1251]  eta: 0:07:28  lr: 0.000001  loss: 5.3979 (5.1828)  time: 0.9078  data: 0.0005  max mem: 19734
Epoch: [1]  [ 790/1251]  eta: 0:07:19  lr: 0.000001  loss: 5.4633 (5.1815)  time: 0.9272  data: 0.0005  max mem: 19734
Epoch: [1]  [ 800/1251]  eta: 0:07:09  lr: 0.000001  loss: 5.0253 (5.1782)  time: 0.9216  data: 0.0004  max mem: 19734
Epoch: [1]  [ 810/1251]  eta: 0:06:59  lr: 0.000001  loss: 4.8656 (5.1722)  time: 0.9043  data: 0.0004  max mem: 19734
Epoch: [1]  [ 820/1251]  eta: 0:06:49  lr: 0.000001  loss: 4.9675 (5.1738)  time: 0.9054  data: 0.0004  max mem: 19734
Epoch: [1]  [ 830/1251]  eta: 0:06:40  lr: 0.000001  loss: 5.0201 (5.1686)  time: 0.9228  data: 0.0004  max mem: 19734
Epoch: [1]  [ 840/1251]  eta: 0:06:30  lr: 0.000001  loss: 5.0428 (5.1708)  time: 0.9341  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5851, ratio_loss=0.6791, pruning_loss=0.2747, mse_loss=1.2052
Epoch: [1]  [ 850/1251]  eta: 0:06:21  lr: 0.000001  loss: 5.2880 (5.1677)  time: 0.9404  data: 0.0004  max mem: 19734
Epoch: [1]  [ 860/1251]  eta: 0:06:11  lr: 0.000001  loss: 5.1432 (5.1665)  time: 0.9313  data: 0.0004  max mem: 19734
Epoch: [1]  [ 870/1251]  eta: 0:06:01  lr: 0.000001  loss: 5.3792 (5.1645)  time: 0.9072  data: 0.0004  max mem: 19734
Epoch: [1]  [ 880/1251]  eta: 0:05:52  lr: 0.000001  loss: 5.2588 (5.1644)  time: 0.9071  data: 0.0005  max mem: 19734
Epoch: [1]  [ 890/1251]  eta: 0:05:42  lr: 0.000001  loss: 5.3860 (5.1655)  time: 0.9139  data: 0.0005  max mem: 19734
Epoch: [1]  [ 900/1251]  eta: 0:05:32  lr: 0.000001  loss: 5.1908 (5.1627)  time: 0.9130  data: 0.0005  max mem: 19734
Epoch: [1]  [ 910/1251]  eta: 0:05:23  lr: 0.000001  loss: 5.1340 (5.1618)  time: 0.9182  data: 0.0004  max mem: 19734
Epoch: [1]  [ 920/1251]  eta: 0:05:13  lr: 0.000001  loss: 5.2344 (5.1612)  time: 0.9318  data: 0.0004  max mem: 19734
Epoch: [1]  [ 930/1251]  eta: 0:05:04  lr: 0.000001  loss: 5.0970 (5.1595)  time: 0.9280  data: 0.0004  max mem: 19734
Epoch: [1]  [ 940/1251]  eta: 0:04:54  lr: 0.000001  loss: 5.3005 (5.1591)  time: 0.9162  data: 0.0004  max mem: 19734
loss info: cls_loss=4.5623, ratio_loss=0.6642, pruning_loss=0.2760, mse_loss=1.2263
Epoch: [1]  [ 950/1251]  eta: 0:04:45  lr: 0.000001  loss: 5.1067 (5.1546)  time: 0.9148  data: 0.0004  max mem: 19734
Epoch: [1]  [ 960/1251]  eta: 0:04:35  lr: 0.000001  loss: 4.8352 (5.1521)  time: 0.9132  data: 0.0004  max mem: 19734
Epoch: [1]  [ 970/1251]  eta: 0:04:25  lr: 0.000001  loss: 4.7401 (5.1470)  time: 0.9288  data: 0.0004  max mem: 19734
Epoch: [1]  [ 980/1251]  eta: 0:04:16  lr: 0.000001  loss: 4.5383 (5.1416)  time: 0.9416  data: 0.0004  max mem: 19734
Epoch: [1]  [ 990/1251]  eta: 0:04:07  lr: 0.000001  loss: 4.7503 (5.1386)  time: 0.9534  data: 0.0004  max mem: 19734
Epoch: [1]  [1000/1251]  eta: 0:03:57  lr: 0.000001  loss: 4.7503 (5.1350)  time: 0.9688  data: 0.0004  max mem: 19734
Epoch: [1]  [1010/1251]  eta: 0:03:48  lr: 0.000001  loss: 4.6892 (5.1314)  time: 0.9366  data: 0.0004  max mem: 19734
Epoch: [1]  [1020/1251]  eta: 0:03:38  lr: 0.000001  loss: 4.7744 (5.1300)  time: 0.9141  data: 0.0005  max mem: 19734
Epoch: [1]  [1030/1251]  eta: 0:03:29  lr: 0.000001  loss: 5.1301 (5.1297)  time: 0.9210  data: 0.0004  max mem: 19734
Epoch: [1]  [1040/1251]  eta: 0:03:19  lr: 0.000001  loss: 5.1301 (5.1271)  time: 0.9174  data: 0.0004  max mem: 19734
loss info: cls_loss=4.3608, ratio_loss=0.6483, pruning_loss=0.2800, mse_loss=1.2343
Epoch: [1]  [1050/1251]  eta: 0:03:10  lr: 0.000001  loss: 4.7083 (5.1214)  time: 0.9229  data: 0.0004  max mem: 19734
Epoch: [1]  [1060/1251]  eta: 0:03:00  lr: 0.000001  loss: 4.8780 (5.1191)  time: 0.9241  data: 0.0004  max mem: 19734
Epoch: [1]  [1070/1251]  eta: 0:02:51  lr: 0.000001  loss: 5.1206 (5.1192)  time: 0.9176  data: 0.0004  max mem: 19734
Epoch: [1]  [1080/1251]  eta: 0:02:41  lr: 0.000001  loss: 5.3139 (5.1200)  time: 0.9201  data: 0.0004  max mem: 19734
Epoch: [1]  [1090/1251]  eta: 0:02:32  lr: 0.000001  loss: 5.1024 (5.1170)  time: 0.9153  data: 0.0004  max mem: 19734
Epoch: [1]  [1100/1251]  eta: 0:02:22  lr: 0.000001  loss: 4.8668 (5.1147)  time: 0.9160  data: 0.0005  max mem: 19734
Epoch: [1]  [1110/1251]  eta: 0:02:13  lr: 0.000001  loss: 4.8405 (5.1111)  time: 0.9346  data: 0.0004  max mem: 19734
Epoch: [1]  [1120/1251]  eta: 0:02:03  lr: 0.000001  loss: 4.7970 (5.1079)  time: 0.9424  data: 0.0004  max mem: 19734
Epoch: [1]  [1130/1251]  eta: 0:01:54  lr: 0.000001  loss: 4.9592 (5.1064)  time: 0.9334  data: 0.0004  max mem: 19734
Epoch: [1]  [1140/1251]  eta: 0:01:44  lr: 0.000001  loss: 4.8807 (5.1018)  time: 0.9418  data: 0.0004  max mem: 19734
loss info: cls_loss=4.3987, ratio_loss=0.6336, pruning_loss=0.2777, mse_loss=1.2268
Epoch: [1]  [1150/1251]  eta: 0:01:35  lr: 0.000001  loss: 4.5603 (5.1005)  time: 0.9342  data: 0.0004  max mem: 19734
Epoch: [1]  [1160/1251]  eta: 0:01:25  lr: 0.000001  loss: 5.2159 (5.0998)  time: 0.9190  data: 0.0004  max mem: 19734
Epoch: [1]  [1170/1251]  eta: 0:01:16  lr: 0.000001  loss: 4.7827 (5.0948)  time: 0.9248  data: 0.0005  max mem: 19734
Epoch: [1]  [1180/1251]  eta: 0:01:06  lr: 0.000001  loss: 4.9626 (5.0933)  time: 0.9208  data: 0.0005  max mem: 19734
Epoch: [1]  [1190/1251]  eta: 0:00:57  lr: 0.000001  loss: 5.0701 (5.0920)  time: 0.9122  data: 0.0008  max mem: 19734
Epoch: [1]  [1200/1251]  eta: 0:00:48  lr: 0.000001  loss: 5.0701 (5.0923)  time: 0.9100  data: 0.0007  max mem: 19734
Epoch: [1]  [1210/1251]  eta: 0:00:38  lr: 0.000001  loss: 5.2388 (5.0895)  time: 0.9161  data: 0.0002  max mem: 19734
Epoch: [1]  [1220/1251]  eta: 0:00:29  lr: 0.000001  loss: 5.2192 (5.0866)  time: 0.9182  data: 0.0002  max mem: 19734
Epoch: [1]  [1230/1251]  eta: 0:00:19  lr: 0.000001  loss: 4.5576 (5.0831)  time: 0.9150  data: 0.0002  max mem: 19734
Epoch: [1]  [1240/1251]  eta: 0:00:10  lr: 0.000001  loss: 5.0631 (5.0839)  time: 0.9179  data: 0.0002  max mem: 19734
loss info: cls_loss=4.4128, ratio_loss=0.6168, pruning_loss=0.2744, mse_loss=1.2092
Epoch: [1]  [1250/1251]  eta: 0:00:00  lr: 0.000001  loss: 5.0483 (5.0792)  time: 0.9198  data: 0.0002  max mem: 19734
Epoch: [1] Total time: 0:19:39 (0.9427 s / it)
Averaged stats: lr: 0.000001  loss: 5.0483 (5.0843)
Test:  [  0/261]  eta: 1:45:26  loss: 1.4689 (1.4689)  acc1: 69.7917 (69.7917)  acc5: 90.1042 (90.1042)  time: 24.2378  data: 23.4751  max mem: 19734
Test:  [ 10/261]  eta: 0:13:36  loss: 1.3673 (1.3923)  acc1: 75.0000 (72.4905)  acc5: 91.1458 (90.1989)  time: 3.2522  data: 2.9488  max mem: 19734
Test:  [ 20/261]  eta: 0:07:35  loss: 1.5332 (1.5714)  acc1: 65.1042 (67.8819)  acc5: 88.5417 (88.0704)  time: 0.7737  data: 0.4539  max mem: 19734
Test:  [ 30/261]  eta: 0:05:30  loss: 1.5105 (1.4832)  acc1: 69.7917 (70.2789)  acc5: 88.0208 (88.8105)  time: 0.4313  data: 0.0138  max mem: 19734
Test:  [ 40/261]  eta: 0:05:26  loss: 1.3129 (1.4829)  acc1: 74.4792 (70.1601)  acc5: 90.6250 (88.8465)  time: 1.0411  data: 0.5870  max mem: 19734
Test:  [ 50/261]  eta: 0:04:29  loss: 1.7384 (1.5731)  acc1: 62.5000 (67.4122)  acc5: 85.9375 (88.1127)  time: 1.0425  data: 0.5876  max mem: 19734
Test:  [ 60/261]  eta: 0:03:50  loss: 1.7983 (1.6044)  acc1: 58.3333 (66.3934)  acc5: 85.4167 (87.9269)  time: 0.4687  data: 0.0162  max mem: 19734
Test:  [ 70/261]  eta: 0:03:25  loss: 1.7778 (1.6140)  acc1: 61.4583 (65.9771)  acc5: 88.5417 (88.1822)  time: 0.5658  data: 0.0150  max mem: 19734
Test:  [ 80/261]  eta: 0:03:00  loss: 1.5710 (1.5958)  acc1: 66.1458 (66.4352)  acc5: 89.5833 (88.4195)  time: 0.5439  data: 0.0164  max mem: 19734
Test:  [ 90/261]  eta: 0:02:41  loss: 1.4293 (1.5666)  acc1: 72.3958 (67.0501)  acc5: 89.5833 (88.8336)  time: 0.4559  data: 0.0148  max mem: 19734
Test:  [100/261]  eta: 0:02:21  loss: 1.3796 (1.5570)  acc1: 71.3542 (67.2906)  acc5: 90.6250 (88.8872)  time: 0.4120  data: 0.0123  max mem: 19734
Test:  [110/261]  eta: 0:02:06  loss: 1.4514 (1.5697)  acc1: 70.8333 (67.0937)  acc5: 87.5000 (88.5651)  time: 0.3473  data: 0.0110  max mem: 19734
Test:  [120/261]  eta: 0:01:59  loss: 1.8202 (1.5978)  acc1: 60.9375 (66.4988)  acc5: 81.7708 (88.0079)  time: 0.6761  data: 0.2217  max mem: 19734
Test:  [130/261]  eta: 0:01:45  loss: 1.9647 (1.6351)  acc1: 55.2083 (65.7045)  acc5: 81.2500 (87.3768)  time: 0.6360  data: 0.2239  max mem: 19734
Test:  [140/261]  eta: 0:01:34  loss: 1.9706 (1.6506)  acc1: 55.2083 (65.2224)  acc5: 82.8125 (87.1602)  time: 0.3569  data: 0.0125  max mem: 19734
Test:  [150/261]  eta: 0:01:22  loss: 1.7033 (1.6480)  acc1: 61.9792 (65.3353)  acc5: 84.3750 (87.0585)  time: 0.3393  data: 0.0169  max mem: 19734
Test:  [160/261]  eta: 0:01:12  loss: 1.6858 (1.6681)  acc1: 65.1042 (64.9780)  acc5: 83.3333 (86.6816)  time: 0.3116  data: 0.0903  max mem: 19734
Test:  [170/261]  eta: 0:01:02  loss: 1.9761 (1.6958)  acc1: 56.2500 (64.3701)  acc5: 80.2083 (86.2512)  time: 0.2787  data: 0.0913  max mem: 19734
Test:  [180/261]  eta: 0:00:53  loss: 2.0121 (1.7135)  acc1: 54.1667 (64.0050)  acc5: 79.1667 (86.0123)  time: 0.1771  data: 0.0254  max mem: 19734
Test:  [190/261]  eta: 0:00:45  loss: 2.0121 (1.7269)  acc1: 57.8125 (63.7407)  acc5: 81.2500 (85.7603)  time: 0.1929  data: 0.0421  max mem: 19734
Test:  [200/261]  eta: 0:00:37  loss: 2.0163 (1.7383)  acc1: 58.3333 (63.4821)  acc5: 80.7292 (85.5385)  time: 0.1913  data: 0.0291  max mem: 19734
Test:  [210/261]  eta: 0:00:30  loss: 1.9337 (1.7447)  acc1: 58.8542 (63.4158)  acc5: 81.2500 (85.3796)  time: 0.1706  data: 0.0004  max mem: 19734
Test:  [220/261]  eta: 0:00:23  loss: 1.9821 (1.7677)  acc1: 58.8542 (62.8394)  acc5: 80.2083 (85.0396)  time: 0.1554  data: 0.0002  max mem: 19734
Test:  [230/261]  eta: 0:00:17  loss: 2.1211 (1.7762)  acc1: 52.6042 (62.6353)  acc5: 78.6458 (84.8846)  time: 0.1419  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:11  loss: 1.8242 (1.7762)  acc1: 58.3333 (62.5562)  acc5: 82.8125 (84.8807)  time: 0.1422  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.5481 (1.7603)  acc1: 65.6250 (62.8528)  acc5: 87.5000 (85.1054)  time: 0.1423  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.3724 (1.7526)  acc1: 66.6667 (63.0400)  acc5: 90.6250 (85.2560)  time: 0.1386  data: 0.0001  max mem: 19734
Test: Total time: 0:02:12 (0.5067 s / it)
* Acc@1 63.040 Acc@5 85.256 loss 1.753
Accuracy of the network on the 50000 test images: 63.0%
Max accuracy: 64.55%
Epoch: [2]  [   0/1251]  eta: 5:30:10  lr: 0.000005  loss: 4.4091 (4.4091)  time: 15.8359  data: 12.9542  max mem: 19734
Epoch: [2]  [  10/1251]  eta: 0:51:56  lr: 0.000005  loss: 4.4091 (4.6942)  time: 2.5109  data: 1.2862  max mem: 19734
Epoch: [2]  [  20/1251]  eta: 0:35:21  lr: 0.000005  loss: 4.8458 (4.7963)  time: 1.0181  data: 0.0599  max mem: 19734
Epoch: [2]  [  30/1251]  eta: 0:29:42  lr: 0.000005  loss: 5.1313 (4.8663)  time: 0.8820  data: 0.0005  max mem: 19734
Epoch: [2]  [  40/1251]  eta: 0:26:36  lr: 0.000005  loss: 5.1303 (4.8381)  time: 0.8926  data: 0.0007  max mem: 19734
Epoch: [2]  [  50/1251]  eta: 0:24:37  lr: 0.000005  loss: 5.1303 (4.8700)  time: 0.8749  data: 0.0006  max mem: 19734
Epoch: [2]  [  60/1251]  eta: 0:23:14  lr: 0.000005  loss: 5.1562 (4.9026)  time: 0.8699  data: 0.0004  max mem: 19734
Epoch: [2]  [  70/1251]  eta: 0:22:15  lr: 0.000005  loss: 5.0894 (4.8896)  time: 0.8766  data: 0.0005  max mem: 19734
Epoch: [2]  [  80/1251]  eta: 0:21:28  lr: 0.000005  loss: 4.7391 (4.8702)  time: 0.8826  data: 0.0005  max mem: 19734
Epoch: [2]  [  90/1251]  eta: 0:20:50  lr: 0.000005  loss: 4.7217 (4.8280)  time: 0.8848  data: 0.0005  max mem: 19734
loss info: cls_loss=4.4017, ratio_loss=0.5597, pruning_loss=0.2666, mse_loss=1.2340
Epoch: [2]  [ 100/1251]  eta: 0:20:18  lr: 0.000005  loss: 4.7233 (4.8150)  time: 0.8904  data: 0.0005  max mem: 19734
Epoch: [2]  [ 110/1251]  eta: 0:19:54  lr: 0.000005  loss: 4.4678 (4.7573)  time: 0.9093  data: 0.0005  max mem: 19734
Epoch: [2]  [ 120/1251]  eta: 0:19:31  lr: 0.000005  loss: 4.4901 (4.7530)  time: 0.9197  data: 0.0004  max mem: 19734
Epoch: [2]  [ 130/1251]  eta: 0:19:09  lr: 0.000005  loss: 4.6339 (4.7338)  time: 0.9072  data: 0.0005  max mem: 19734
Epoch: [2]  [ 140/1251]  eta: 0:18:50  lr: 0.000005  loss: 4.6339 (4.7277)  time: 0.9104  data: 0.0005  max mem: 19734
Epoch: [2]  [ 150/1251]  eta: 0:18:38  lr: 0.000005  loss: 4.6326 (4.6946)  time: 0.9545  data: 0.0005  max mem: 19734
Epoch: [2]  [ 160/1251]  eta: 0:18:20  lr: 0.000005  loss: 4.6326 (4.6890)  time: 0.9476  data: 0.0005  max mem: 19734
Epoch: [2]  [ 170/1251]  eta: 0:18:05  lr: 0.000005  loss: 4.6355 (4.6722)  time: 0.9140  data: 0.0005  max mem: 19734
Epoch: [2]  [ 180/1251]  eta: 0:17:52  lr: 0.000005  loss: 4.6881 (4.6677)  time: 0.9385  data: 0.0004  max mem: 19734
Epoch: [2]  [ 190/1251]  eta: 0:17:38  lr: 0.000005  loss: 4.7434 (4.6572)  time: 0.9441  data: 0.0004  max mem: 19734
loss info: cls_loss=4.1749, ratio_loss=0.4324, pruning_loss=0.2590, mse_loss=1.2039
Epoch: [2]  [ 200/1251]  eta: 0:17:25  lr: 0.000005  loss: 4.5309 (4.6529)  time: 0.9356  data: 0.0006  max mem: 19734
Epoch: [2]  [ 210/1251]  eta: 0:17:12  lr: 0.000005  loss: 4.5239 (4.6470)  time: 0.9330  data: 0.0005  max mem: 19734
Epoch: [2]  [ 220/1251]  eta: 0:16:59  lr: 0.000005  loss: 4.4558 (4.6269)  time: 0.9340  data: 0.0004  max mem: 19734
Epoch: [2]  [ 230/1251]  eta: 0:16:46  lr: 0.000005  loss: 4.4218 (4.6236)  time: 0.9300  data: 0.0004  max mem: 19734
Epoch: [2]  [ 240/1251]  eta: 0:16:34  lr: 0.000005  loss: 4.2071 (4.5927)  time: 0.9261  data: 0.0004  max mem: 19734
Epoch: [2]  [ 250/1251]  eta: 0:16:22  lr: 0.000005  loss: 3.9586 (4.5725)  time: 0.9266  data: 0.0004  max mem: 19734
Epoch: [2]  [ 260/1251]  eta: 0:16:12  lr: 0.000005  loss: 4.1949 (4.5545)  time: 0.9436  data: 0.0005  max mem: 19734
Epoch: [2]  [ 270/1251]  eta: 0:16:00  lr: 0.000005  loss: 4.4650 (4.5507)  time: 0.9426  data: 0.0005  max mem: 19734
Epoch: [2]  [ 280/1251]  eta: 0:15:48  lr: 0.000005  loss: 4.4043 (4.5301)  time: 0.9236  data: 0.0005  max mem: 19734
Epoch: [2]  [ 290/1251]  eta: 0:15:37  lr: 0.000005  loss: 3.7286 (4.4984)  time: 0.9275  data: 0.0006  max mem: 19734
loss info: cls_loss=3.9399, ratio_loss=0.2788, pruning_loss=0.2568, mse_loss=1.2043
Epoch: [2]  [ 300/1251]  eta: 0:15:26  lr: 0.000005  loss: 3.8191 (4.4817)  time: 0.9351  data: 0.0006  max mem: 19734
Epoch: [2]  [ 310/1251]  eta: 0:15:14  lr: 0.000005  loss: 4.1015 (4.4655)  time: 0.9314  data: 0.0005  max mem: 19734
Epoch: [2]  [ 320/1251]  eta: 0:15:04  lr: 0.000005  loss: 4.0944 (4.4533)  time: 0.9342  data: 0.0005  max mem: 19734
Epoch: [2]  [ 330/1251]  eta: 0:14:53  lr: 0.000005  loss: 3.8776 (4.4358)  time: 0.9373  data: 0.0005  max mem: 19734
Epoch: [2]  [ 340/1251]  eta: 0:14:42  lr: 0.000005  loss: 3.6782 (4.4177)  time: 0.9237  data: 0.0004  max mem: 19734
Epoch: [2]  [ 350/1251]  eta: 0:14:32  lr: 0.000005  loss: 3.6782 (4.3980)  time: 0.9287  data: 0.0004  max mem: 19734
Epoch: [2]  [ 360/1251]  eta: 0:14:21  lr: 0.000005  loss: 3.5704 (4.3646)  time: 0.9325  data: 0.0005  max mem: 19734
Epoch: [2]  [ 370/1251]  eta: 0:14:10  lr: 0.000005  loss: 3.3238 (4.3478)  time: 0.9175  data: 0.0004  max mem: 19734
Epoch: [2]  [ 380/1251]  eta: 0:14:00  lr: 0.000005  loss: 3.9531 (4.3346)  time: 0.9278  data: 0.0004  max mem: 19734
Epoch: [2]  [ 390/1251]  eta: 0:13:50  lr: 0.000005  loss: 3.7777 (4.3152)  time: 0.9433  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6516, ratio_loss=0.1445, pruning_loss=0.2534, mse_loss=1.2254
Epoch: [2]  [ 400/1251]  eta: 0:13:39  lr: 0.000005  loss: 3.7837 (4.3029)  time: 0.9242  data: 0.0004  max mem: 19734
Epoch: [2]  [ 410/1251]  eta: 0:13:28  lr: 0.000005  loss: 3.8923 (4.2866)  time: 0.9167  data: 0.0005  max mem: 19734
Epoch: [2]  [ 420/1251]  eta: 0:13:18  lr: 0.000005  loss: 3.8923 (4.2767)  time: 0.9247  data: 0.0005  max mem: 19734
Epoch: [2]  [ 430/1251]  eta: 0:13:08  lr: 0.000005  loss: 3.9939 (4.2694)  time: 0.9234  data: 0.0005  max mem: 19734
Epoch: [2]  [ 440/1251]  eta: 0:12:57  lr: 0.000005  loss: 4.0173 (4.2627)  time: 0.9197  data: 0.0004  max mem: 19734
Epoch: [2]  [ 450/1251]  eta: 0:12:47  lr: 0.000005  loss: 3.8279 (4.2454)  time: 0.9177  data: 0.0004  max mem: 19734
Epoch: [2]  [ 460/1251]  eta: 0:12:37  lr: 0.000005  loss: 3.5781 (4.2314)  time: 0.9159  data: 0.0005  max mem: 19734
Epoch: [2]  [ 470/1251]  eta: 0:12:27  lr: 0.000005  loss: 3.8058 (4.2249)  time: 0.9372  data: 0.0005  max mem: 19734
Epoch: [2]  [ 480/1251]  eta: 0:12:17  lr: 0.000005  loss: 3.8550 (4.2112)  time: 0.9356  data: 0.0005  max mem: 19734
Epoch: [2]  [ 490/1251]  eta: 0:12:07  lr: 0.000005  loss: 3.8856 (4.2072)  time: 0.9182  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6841, ratio_loss=0.0652, pruning_loss=0.2459, mse_loss=1.1972
Epoch: [2]  [ 500/1251]  eta: 0:11:56  lr: 0.000005  loss: 4.0115 (4.1965)  time: 0.9178  data: 0.0005  max mem: 19734
Epoch: [2]  [ 510/1251]  eta: 0:11:46  lr: 0.000005  loss: 3.8253 (4.1891)  time: 0.9170  data: 0.0004  max mem: 19734
Epoch: [2]  [ 520/1251]  eta: 0:11:37  lr: 0.000005  loss: 3.7737 (4.1785)  time: 0.9249  data: 0.0004  max mem: 19734
Epoch: [2]  [ 530/1251]  eta: 0:11:26  lr: 0.000005  loss: 3.7581 (4.1667)  time: 0.9212  data: 0.0004  max mem: 19734
Epoch: [2]  [ 540/1251]  eta: 0:11:16  lr: 0.000005  loss: 3.8791 (4.1588)  time: 0.9110  data: 0.0004  max mem: 19734
Epoch: [2]  [ 550/1251]  eta: 0:11:07  lr: 0.000005  loss: 3.9108 (4.1526)  time: 0.9237  data: 0.0004  max mem: 19734
Epoch: [2]  [ 560/1251]  eta: 0:10:57  lr: 0.000005  loss: 3.6939 (4.1414)  time: 0.9281  data: 0.0004  max mem: 19734
Epoch: [2]  [ 570/1251]  eta: 0:10:47  lr: 0.000005  loss: 3.6077 (4.1328)  time: 0.9215  data: 0.0004  max mem: 19734
Epoch: [2]  [ 580/1251]  eta: 0:10:37  lr: 0.000005  loss: 3.6077 (4.1209)  time: 0.9195  data: 0.0004  max mem: 19734
Epoch: [2]  [ 590/1251]  eta: 0:10:28  lr: 0.000005  loss: 3.6465 (4.1129)  time: 0.9410  data: 0.0004  max mem: 19734
loss info: cls_loss=3.6145, ratio_loss=0.0309, pruning_loss=0.2463, mse_loss=1.2312
Epoch: [2]  [ 600/1251]  eta: 0:10:18  lr: 0.000005  loss: 3.8361 (4.1056)  time: 0.9418  data: 0.0004  max mem: 19734
Epoch: [2]  [ 610/1251]  eta: 0:10:08  lr: 0.000005  loss: 3.7208 (4.0975)  time: 0.9145  data: 0.0004  max mem: 19734
Epoch: [2]  [ 620/1251]  eta: 0:09:58  lr: 0.000005  loss: 3.6710 (4.0921)  time: 0.9225  data: 0.0004  max mem: 19734
Epoch: [2]  [ 630/1251]  eta: 0:09:48  lr: 0.000005  loss: 3.7048 (4.0862)  time: 0.9232  data: 0.0004  max mem: 19734
Epoch: [2]  [ 640/1251]  eta: 0:09:39  lr: 0.000005  loss: 3.3524 (4.0707)  time: 0.9119  data: 0.0004  max mem: 19734
Epoch: [2]  [ 650/1251]  eta: 0:09:29  lr: 0.000005  loss: 3.3518 (4.0631)  time: 0.9113  data: 0.0004  max mem: 19734
Epoch: [2]  [ 660/1251]  eta: 0:09:19  lr: 0.000005  loss: 3.6844 (4.0553)  time: 0.9155  data: 0.0004  max mem: 19734
Epoch: [2]  [ 670/1251]  eta: 0:09:09  lr: 0.000005  loss: 3.5036 (4.0485)  time: 0.9177  data: 0.0004  max mem: 19734
Epoch: [2]  [ 680/1251]  eta: 0:09:00  lr: 0.000005  loss: 3.5971 (4.0415)  time: 0.9245  data: 0.0004  max mem: 19734
Epoch: [2]  [ 690/1251]  eta: 0:08:50  lr: 0.000005  loss: 3.5910 (4.0326)  time: 0.9270  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5086, ratio_loss=0.0163, pruning_loss=0.2445, mse_loss=1.1991
Epoch: [2]  [ 700/1251]  eta: 0:08:41  lr: 0.000005  loss: 3.7506 (4.0286)  time: 0.9275  data: 0.0004  max mem: 19734
Epoch: [2]  [ 710/1251]  eta: 0:08:31  lr: 0.000005  loss: 3.8083 (4.0250)  time: 0.9344  data: 0.0004  max mem: 19734
Epoch: [2]  [ 720/1251]  eta: 0:08:21  lr: 0.000005  loss: 3.7204 (4.0173)  time: 0.9301  data: 0.0004  max mem: 19734
Epoch: [2]  [ 730/1251]  eta: 0:08:12  lr: 0.000005  loss: 3.7336 (4.0155)  time: 0.9390  data: 0.0004  max mem: 19734
Epoch: [2]  [ 740/1251]  eta: 0:08:02  lr: 0.000005  loss: 3.9620 (4.0142)  time: 0.9450  data: 0.0005  max mem: 19734
Epoch: [2]  [ 750/1251]  eta: 0:07:53  lr: 0.000005  loss: 3.9620 (4.0091)  time: 0.9343  data: 0.0005  max mem: 19734
Epoch: [2]  [ 760/1251]  eta: 0:07:43  lr: 0.000005  loss: 3.6056 (4.0059)  time: 0.9282  data: 0.0005  max mem: 19734
Epoch: [2]  [ 770/1251]  eta: 0:07:34  lr: 0.000005  loss: 3.5397 (3.9979)  time: 0.9314  data: 0.0005  max mem: 19734
Epoch: [2]  [ 780/1251]  eta: 0:07:24  lr: 0.000005  loss: 3.3240 (3.9927)  time: 0.9265  data: 0.0005  max mem: 19734
Epoch: [2]  [ 790/1251]  eta: 0:07:15  lr: 0.000005  loss: 3.5691 (3.9881)  time: 0.9120  data: 0.0005  max mem: 19734
loss info: cls_loss=3.6475, ratio_loss=0.0107, pruning_loss=0.2356, mse_loss=1.1168
Epoch: [2]  [ 800/1251]  eta: 0:07:05  lr: 0.000005  loss: 3.8120 (3.9844)  time: 0.9098  data: 0.0004  max mem: 19734
Epoch: [2]  [ 810/1251]  eta: 0:06:55  lr: 0.000005  loss: 3.8120 (3.9808)  time: 0.9104  data: 0.0004  max mem: 19734
Epoch: [2]  [ 820/1251]  eta: 0:06:46  lr: 0.000005  loss: 3.8540 (3.9763)  time: 0.9140  data: 0.0004  max mem: 19734
Epoch: [2]  [ 830/1251]  eta: 0:06:36  lr: 0.000005  loss: 3.8540 (3.9688)  time: 0.9128  data: 0.0004  max mem: 19734
Epoch: [2]  [ 840/1251]  eta: 0:06:27  lr: 0.000005  loss: 3.4025 (3.9615)  time: 0.9147  data: 0.0005  max mem: 19734
Epoch: [2]  [ 850/1251]  eta: 0:06:17  lr: 0.000005  loss: 3.7076 (3.9587)  time: 0.9289  data: 0.0005  max mem: 19734
Epoch: [2]  [ 860/1251]  eta: 0:06:08  lr: 0.000005  loss: 3.7388 (3.9529)  time: 0.9264  data: 0.0005  max mem: 19734
Epoch: [2]  [ 870/1251]  eta: 0:05:58  lr: 0.000005  loss: 3.4928 (3.9472)  time: 0.9084  data: 0.0004  max mem: 19734
Epoch: [2]  [ 880/1251]  eta: 0:05:49  lr: 0.000005  loss: 3.6755 (3.9448)  time: 0.9131  data: 0.0005  max mem: 19734
Epoch: [2]  [ 890/1251]  eta: 0:05:39  lr: 0.000005  loss: 3.8602 (3.9424)  time: 0.9282  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5213, ratio_loss=0.0083, pruning_loss=0.2435, mse_loss=1.1501
Epoch: [2]  [ 900/1251]  eta: 0:05:30  lr: 0.000005  loss: 3.6169 (3.9371)  time: 0.9270  data: 0.0005  max mem: 19734
Epoch: [2]  [ 910/1251]  eta: 0:05:20  lr: 0.000005  loss: 3.6169 (3.9338)  time: 0.9188  data: 0.0005  max mem: 19734
Epoch: [2]  [ 920/1251]  eta: 0:05:11  lr: 0.000005  loss: 3.8845 (3.9317)  time: 0.9142  data: 0.0005  max mem: 19734
Epoch: [2]  [ 930/1251]  eta: 0:05:01  lr: 0.000005  loss: 3.6810 (3.9259)  time: 0.9126  data: 0.0005  max mem: 19734
Epoch: [2]  [ 940/1251]  eta: 0:04:52  lr: 0.000005  loss: 3.3179 (3.9200)  time: 0.9143  data: 0.0005  max mem: 19734
Epoch: [2]  [ 950/1251]  eta: 0:04:42  lr: 0.000005  loss: 3.3179 (3.9155)  time: 0.9133  data: 0.0005  max mem: 19734
Epoch: [2]  [ 960/1251]  eta: 0:04:33  lr: 0.000005  loss: 3.7203 (3.9122)  time: 0.9253  data: 0.0005  max mem: 19734
Epoch: [2]  [ 970/1251]  eta: 0:04:23  lr: 0.000005  loss: 3.3332 (3.9052)  time: 0.9226  data: 0.0005  max mem: 19734
Epoch: [2]  [ 980/1251]  eta: 0:04:14  lr: 0.000005  loss: 3.3611 (3.9003)  time: 0.9250  data: 0.0005  max mem: 19734
Epoch: [2]  [ 990/1251]  eta: 0:04:05  lr: 0.000005  loss: 3.4894 (3.8979)  time: 0.9320  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4576, ratio_loss=0.0067, pruning_loss=0.2411, mse_loss=1.1769
Epoch: [2]  [1000/1251]  eta: 0:03:55  lr: 0.000005  loss: 3.5777 (3.8932)  time: 0.9149  data: 0.0005  max mem: 19734
Epoch: [2]  [1010/1251]  eta: 0:03:46  lr: 0.000005  loss: 3.6063 (3.8907)  time: 0.9103  data: 0.0005  max mem: 19734
Epoch: [2]  [1020/1251]  eta: 0:03:36  lr: 0.000005  loss: 3.4687 (3.8852)  time: 0.9171  data: 0.0004  max mem: 19734
Epoch: [2]  [1030/1251]  eta: 0:03:27  lr: 0.000005  loss: 3.5603 (3.8829)  time: 0.9162  data: 0.0004  max mem: 19734
Epoch: [2]  [1040/1251]  eta: 0:03:17  lr: 0.000005  loss: 3.5221 (3.8780)  time: 0.9121  data: 0.0004  max mem: 19734
Epoch: [2]  [1050/1251]  eta: 0:03:08  lr: 0.000005  loss: 3.5363 (3.8750)  time: 0.9236  data: 0.0004  max mem: 19734
Epoch: [2]  [1060/1251]  eta: 0:02:59  lr: 0.000005  loss: 3.8261 (3.8744)  time: 0.9239  data: 0.0004  max mem: 19734
Epoch: [2]  [1070/1251]  eta: 0:02:49  lr: 0.000005  loss: 3.7592 (3.8720)  time: 0.9114  data: 0.0004  max mem: 19734
Epoch: [2]  [1080/1251]  eta: 0:02:40  lr: 0.000005  loss: 3.6425 (3.8698)  time: 0.9095  data: 0.0004  max mem: 19734
Epoch: [2]  [1090/1251]  eta: 0:02:30  lr: 0.000005  loss: 3.7084 (3.8679)  time: 0.9127  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5574, ratio_loss=0.0058, pruning_loss=0.2356, mse_loss=1.1090
Epoch: [2]  [1100/1251]  eta: 0:02:21  lr: 0.000005  loss: 3.7084 (3.8634)  time: 0.9122  data: 0.0004  max mem: 19734
Epoch: [2]  [1110/1251]  eta: 0:02:12  lr: 0.000005  loss: 3.7126 (3.8628)  time: 0.9173  data: 0.0004  max mem: 19734
Epoch: [2]  [1120/1251]  eta: 0:02:02  lr: 0.000005  loss: 3.7202 (3.8606)  time: 0.9165  data: 0.0004  max mem: 19734
Epoch: [2]  [1130/1251]  eta: 0:01:53  lr: 0.000005  loss: 3.6909 (3.8602)  time: 0.9261  data: 0.0004  max mem: 19734
Epoch: [2]  [1140/1251]  eta: 0:01:43  lr: 0.000005  loss: 3.7651 (3.8574)  time: 0.9362  data: 0.0004  max mem: 19734
Epoch: [2]  [1150/1251]  eta: 0:01:34  lr: 0.000005  loss: 3.7129 (3.8564)  time: 0.9239  data: 0.0004  max mem: 19734
Epoch: [2]  [1160/1251]  eta: 0:01:25  lr: 0.000005  loss: 3.7453 (3.8538)  time: 0.9223  data: 0.0004  max mem: 19734
Epoch: [2]  [1170/1251]  eta: 0:01:15  lr: 0.000005  loss: 3.8865 (3.8530)  time: 0.9329  data: 0.0004  max mem: 19734
Epoch: [2]  [1180/1251]  eta: 0:01:06  lr: 0.000005  loss: 3.6891 (3.8509)  time: 0.9273  data: 0.0004  max mem: 19734
Epoch: [2]  [1190/1251]  eta: 0:00:57  lr: 0.000005  loss: 3.5955 (3.8484)  time: 0.9134  data: 0.0010  max mem: 19734
loss info: cls_loss=3.5903, ratio_loss=0.0052, pruning_loss=0.2368, mse_loss=1.1820
Epoch: [2]  [1200/1251]  eta: 0:00:47  lr: 0.000005  loss: 3.5955 (3.8456)  time: 0.9150  data: 0.0009  max mem: 19734
Epoch: [2]  [1210/1251]  eta: 0:00:38  lr: 0.000005  loss: 3.6573 (3.8429)  time: 0.9184  data: 0.0001  max mem: 19734
Epoch: [2]  [1220/1251]  eta: 0:00:28  lr: 0.000005  loss: 3.7607 (3.8420)  time: 0.9114  data: 0.0001  max mem: 19734
Epoch: [2]  [1230/1251]  eta: 0:00:19  lr: 0.000005  loss: 3.8006 (3.8408)  time: 0.9071  data: 0.0001  max mem: 19734
Epoch: [2]  [1240/1251]  eta: 0:00:10  lr: 0.000005  loss: 3.7605 (3.8394)  time: 0.9281  data: 0.0001  max mem: 19734
Epoch: [2]  [1250/1251]  eta: 0:00:00  lr: 0.000005  loss: 3.6674 (3.8360)  time: 0.9262  data: 0.0001  max mem: 19734
Epoch: [2] Total time: 0:19:30 (0.9353 s / it)
Averaged stats: lr: 0.000005  loss: 3.6674 (3.8156)
Test:  [  0/261]  eta: 2:31:14  loss: 1.0697 (1.0697)  acc1: 75.0000 (75.0000)  acc5: 92.7083 (92.7083)  time: 34.7697  data: 34.1338  max mem: 19734
Test:  [ 10/261]  eta: 0:15:32  loss: 1.0697 (1.1541)  acc1: 75.0000 (74.5265)  acc5: 93.7500 (91.9508)  time: 3.7160  data: 3.2218  max mem: 19734
Test:  [ 20/261]  eta: 0:08:54  loss: 1.3538 (1.3667)  acc1: 69.7917 (68.9732)  acc5: 88.0208 (89.0873)  time: 0.5882  data: 0.0718  max mem: 19734
Test:  [ 30/261]  eta: 0:06:26  loss: 1.2104 (1.2519)  acc1: 73.4375 (72.3454)  acc5: 90.1042 (89.8690)  time: 0.5474  data: 0.0178  max mem: 19734
Test:  [ 40/261]  eta: 0:05:11  loss: 0.9629 (1.2205)  acc1: 78.1250 (73.0183)  acc5: 92.1875 (90.3328)  time: 0.5649  data: 0.1224  max mem: 19734
Test:  [ 50/261]  eta: 0:04:15  loss: 1.4265 (1.2952)  acc1: 67.1875 (70.7516)  acc5: 89.0625 (89.7672)  time: 0.5007  data: 0.1157  max mem: 19734
Test:  [ 60/261]  eta: 0:03:42  loss: 1.5114 (1.3253)  acc1: 64.0625 (69.5953)  acc5: 88.0208 (89.6090)  time: 0.4782  data: 0.0139  max mem: 19734
Test:  [ 70/261]  eta: 0:03:07  loss: 1.3948 (1.3160)  acc1: 65.1042 (69.5643)  acc5: 90.1042 (89.9941)  time: 0.3980  data: 0.0130  max mem: 19734
Test:  [ 80/261]  eta: 0:02:49  loss: 1.2639 (1.2990)  acc1: 72.3958 (70.0939)  acc5: 92.1875 (90.2842)  time: 0.4197  data: 0.1217  max mem: 19734
Test:  [ 90/261]  eta: 0:02:27  loss: 1.1525 (1.2711)  acc1: 74.4792 (70.7704)  acc5: 92.1875 (90.5849)  time: 0.4425  data: 0.1238  max mem: 19734
Test:  [100/261]  eta: 0:02:14  loss: 1.0495 (1.2648)  acc1: 76.0417 (70.9004)  acc5: 91.6667 (90.6611)  time: 0.4154  data: 0.1492  max mem: 19734
Test:  [110/261]  eta: 0:01:58  loss: 1.1793 (1.2729)  acc1: 72.3958 (70.7817)  acc5: 91.1458 (90.4702)  time: 0.4181  data: 0.2079  max mem: 19734
Test:  [120/261]  eta: 0:01:44  loss: 1.4961 (1.2979)  acc1: 65.6250 (70.3125)  acc5: 86.9792 (90.1128)  time: 0.2950  data: 0.0712  max mem: 19734
Test:  [130/261]  eta: 0:01:32  loss: 1.6505 (1.3290)  acc1: 63.0208 (69.7797)  acc5: 85.4167 (89.6788)  time: 0.2681  data: 0.0104  max mem: 19734
Test:  [140/261]  eta: 0:01:22  loss: 1.6039 (1.3445)  acc1: 60.9375 (69.3447)  acc5: 86.4583 (89.5353)  time: 0.2959  data: 0.0218  max mem: 19734
Test:  [150/261]  eta: 0:01:13  loss: 1.3978 (1.3401)  acc1: 68.7500 (69.6468)  acc5: 88.0208 (89.5178)  time: 0.3607  data: 0.0764  max mem: 19734
Test:  [160/261]  eta: 0:01:05  loss: 1.2379 (1.3533)  acc1: 75.0000 (69.5393)  acc5: 89.0625 (89.2890)  time: 0.4107  data: 0.2042  max mem: 19734
Test:  [170/261]  eta: 0:00:57  loss: 1.5661 (1.3766)  acc1: 62.5000 (68.9541)  acc5: 83.8542 (88.9894)  time: 0.4445  data: 0.2771  max mem: 19734
Test:  [180/261]  eta: 0:00:50  loss: 1.6435 (1.3885)  acc1: 61.4583 (68.6781)  acc5: 84.3750 (88.8381)  time: 0.3857  data: 0.2093  max mem: 19734
Test:  [190/261]  eta: 0:00:42  loss: 1.5900 (1.3944)  acc1: 63.0208 (68.6300)  acc5: 86.9792 (88.7680)  time: 0.3191  data: 0.1550  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.5092 (1.4014)  acc1: 67.7083 (68.4831)  acc5: 86.9792 (88.6220)  time: 0.2265  data: 0.0779  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.5818 (1.4093)  acc1: 67.1875 (68.3649)  acc5: 85.4167 (88.4948)  time: 0.1500  data: 0.0009  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.6523 (1.4261)  acc1: 63.5417 (67.9252)  acc5: 83.8542 (88.2942)  time: 0.1451  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.6655 (1.4332)  acc1: 61.4583 (67.7895)  acc5: 84.8958 (88.1854)  time: 0.1420  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.5062 (1.4365)  acc1: 65.6250 (67.7040)  acc5: 85.9375 (88.1548)  time: 0.1421  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.2132 (1.4228)  acc1: 70.8333 (68.0424)  acc5: 90.6250 (88.3259)  time: 0.1422  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0962 (1.4184)  acc1: 74.4792 (68.1420)  acc5: 92.7083 (88.4440)  time: 0.1377  data: 0.0001  max mem: 19734
Test: Total time: 0:02:05 (0.4793 s / it)
* Acc@1 68.142 Acc@5 88.444 loss 1.418
Accuracy of the network on the 50000 test images: 68.1%
Max accuracy: 68.14%
Epoch: [3]  [   0/1251]  eta: 5:29:41  lr: 0.000009  loss: 3.3598 (3.3598)  time: 15.8122  data: 14.9395  max mem: 19734
Epoch: [3]  [  10/1251]  eta: 0:46:49  lr: 0.000009  loss: 3.8663 (3.7608)  time: 2.2637  data: 1.3595  max mem: 19734
Epoch: [3]  [  20/1251]  eta: 0:32:33  lr: 0.000009  loss: 3.7779 (3.6125)  time: 0.8754  data: 0.0010  max mem: 19734
Epoch: [3]  [  30/1251]  eta: 0:27:33  lr: 0.000009  loss: 3.5985 (3.5802)  time: 0.8536  data: 0.0004  max mem: 19734
Epoch: [3]  [  40/1251]  eta: 0:24:51  lr: 0.000009  loss: 3.5985 (3.5937)  time: 0.8594  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5811, ratio_loss=0.0052, pruning_loss=0.2300, mse_loss=1.1438
Epoch: [3]  [  50/1251]  eta: 0:23:17  lr: 0.000009  loss: 3.5312 (3.5988)  time: 0.8694  data: 0.0005  max mem: 19734
Epoch: [3]  [  60/1251]  eta: 0:22:08  lr: 0.000009  loss: 3.3344 (3.5495)  time: 0.8778  data: 0.0005  max mem: 19734
Epoch: [3]  [  70/1251]  eta: 0:21:17  lr: 0.000009  loss: 3.5916 (3.5628)  time: 0.8721  data: 0.0004  max mem: 19734
Epoch: [3]  [  80/1251]  eta: 0:20:44  lr: 0.000009  loss: 3.7955 (3.5702)  time: 0.8993  data: 0.0005  max mem: 19734
Epoch: [3]  [  90/1251]  eta: 0:20:14  lr: 0.000009  loss: 3.7226 (3.5419)  time: 0.9212  data: 0.0005  max mem: 19734
Epoch: [3]  [ 100/1251]  eta: 0:19:46  lr: 0.000009  loss: 3.7167 (3.5654)  time: 0.9050  data: 0.0005  max mem: 19734
Epoch: [3]  [ 110/1251]  eta: 0:19:25  lr: 0.000009  loss: 3.6725 (3.5490)  time: 0.9054  data: 0.0004  max mem: 19734
Epoch: [3]  [ 120/1251]  eta: 0:19:04  lr: 0.000009  loss: 3.4909 (3.5440)  time: 0.9160  data: 0.0005  max mem: 19734
Epoch: [3]  [ 130/1251]  eta: 0:18:49  lr: 0.000009  loss: 3.4474 (3.5267)  time: 0.9347  data: 0.0006  max mem: 19734
Epoch: [3]  [ 140/1251]  eta: 0:18:32  lr: 0.000009  loss: 3.3725 (3.5246)  time: 0.9375  data: 0.0006  max mem: 19734
loss info: cls_loss=3.4259, ratio_loss=0.0042, pruning_loss=0.2377, mse_loss=1.1600
Epoch: [3]  [ 150/1251]  eta: 0:18:16  lr: 0.000009  loss: 3.5668 (3.5093)  time: 0.9163  data: 0.0005  max mem: 19734
Epoch: [3]  [ 160/1251]  eta: 0:18:00  lr: 0.000009  loss: 3.4251 (3.5050)  time: 0.9093  data: 0.0005  max mem: 19734
Epoch: [3]  [ 170/1251]  eta: 0:17:45  lr: 0.000009  loss: 3.4251 (3.5051)  time: 0.9062  data: 0.0005  max mem: 19734
Epoch: [3]  [ 180/1251]  eta: 0:17:32  lr: 0.000009  loss: 3.7094 (3.5054)  time: 0.9199  data: 0.0005  max mem: 19734
Epoch: [3]  [ 190/1251]  eta: 0:17:20  lr: 0.000009  loss: 3.1928 (3.4950)  time: 0.9385  data: 0.0005  max mem: 19734
Epoch: [3]  [ 200/1251]  eta: 0:17:07  lr: 0.000009  loss: 3.2567 (3.4901)  time: 0.9399  data: 0.0004  max mem: 19734
Epoch: [3]  [ 210/1251]  eta: 0:16:55  lr: 0.000009  loss: 3.5714 (3.4973)  time: 0.9259  data: 0.0005  max mem: 19734
Epoch: [3]  [ 220/1251]  eta: 0:16:42  lr: 0.000009  loss: 3.7502 (3.5082)  time: 0.9163  data: 0.0005  max mem: 19734
Epoch: [3]  [ 230/1251]  eta: 0:16:30  lr: 0.000009  loss: 3.6915 (3.5116)  time: 0.9217  data: 0.0005  max mem: 19734
Epoch: [3]  [ 240/1251]  eta: 0:16:19  lr: 0.000009  loss: 3.6855 (3.5205)  time: 0.9252  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4796, ratio_loss=0.0039, pruning_loss=0.2314, mse_loss=1.0986
Epoch: [3]  [ 250/1251]  eta: 0:16:07  lr: 0.000009  loss: 3.6855 (3.5142)  time: 0.9228  data: 0.0005  max mem: 19734
Epoch: [3]  [ 260/1251]  eta: 0:15:56  lr: 0.000009  loss: 3.4388 (3.5070)  time: 0.9221  data: 0.0005  max mem: 19734
Epoch: [3]  [ 270/1251]  eta: 0:15:45  lr: 0.000009  loss: 3.5056 (3.5117)  time: 0.9205  data: 0.0005  max mem: 19734
Epoch: [3]  [ 280/1251]  eta: 0:15:34  lr: 0.000009  loss: 3.5656 (3.5086)  time: 0.9319  data: 0.0005  max mem: 19734
Epoch: [3]  [ 290/1251]  eta: 0:15:24  lr: 0.000009  loss: 3.6774 (3.5192)  time: 0.9380  data: 0.0005  max mem: 19734
Epoch: [3]  [ 300/1251]  eta: 0:15:13  lr: 0.000009  loss: 3.7424 (3.5264)  time: 0.9342  data: 0.0005  max mem: 19734
Epoch: [3]  [ 310/1251]  eta: 0:15:03  lr: 0.000009  loss: 3.7370 (3.5312)  time: 0.9445  data: 0.0005  max mem: 19734
Epoch: [3]  [ 320/1251]  eta: 0:14:53  lr: 0.000009  loss: 3.3360 (3.5164)  time: 0.9387  data: 0.0005  max mem: 19734
Epoch: [3]  [ 330/1251]  eta: 0:14:42  lr: 0.000009  loss: 3.0699 (3.5063)  time: 0.9280  data: 0.0005  max mem: 19734
Epoch: [3]  [ 340/1251]  eta: 0:14:32  lr: 0.000009  loss: 3.2766 (3.5080)  time: 0.9244  data: 0.0007  max mem: 19734
loss info: cls_loss=3.4684, ratio_loss=0.0038, pruning_loss=0.2360, mse_loss=1.1303
Epoch: [3]  [ 350/1251]  eta: 0:14:23  lr: 0.000009  loss: 3.5161 (3.5038)  time: 0.9548  data: 0.0007  max mem: 19734
Epoch: [3]  [ 360/1251]  eta: 0:14:13  lr: 0.000009  loss: 3.5603 (3.5047)  time: 0.9653  data: 0.0005  max mem: 19734
Epoch: [3]  [ 370/1251]  eta: 0:14:03  lr: 0.000009  loss: 3.5603 (3.5046)  time: 0.9337  data: 0.0005  max mem: 19734
Epoch: [3]  [ 380/1251]  eta: 0:13:53  lr: 0.000009  loss: 3.3522 (3.5012)  time: 0.9313  data: 0.0005  max mem: 19734
Epoch: [3]  [ 390/1251]  eta: 0:13:42  lr: 0.000009  loss: 3.4977 (3.5039)  time: 0.9295  data: 0.0005  max mem: 19734
Epoch: [3]  [ 400/1251]  eta: 0:13:32  lr: 0.000009  loss: 3.6228 (3.5045)  time: 0.9247  data: 0.0005  max mem: 19734
Epoch: [3]  [ 410/1251]  eta: 0:13:22  lr: 0.000009  loss: 3.6228 (3.5051)  time: 0.9349  data: 0.0007  max mem: 19734
Epoch: [3]  [ 420/1251]  eta: 0:13:13  lr: 0.000009  loss: 3.7377 (3.5113)  time: 0.9527  data: 0.0007  max mem: 19734
Epoch: [3]  [ 430/1251]  eta: 0:13:03  lr: 0.000009  loss: 3.7377 (3.5162)  time: 0.9450  data: 0.0005  max mem: 19734
Epoch: [3]  [ 440/1251]  eta: 0:12:53  lr: 0.000009  loss: 3.4463 (3.5067)  time: 0.9320  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4855, ratio_loss=0.0037, pruning_loss=0.2305, mse_loss=1.1099
Epoch: [3]  [ 450/1251]  eta: 0:12:43  lr: 0.000009  loss: 3.3862 (3.5092)  time: 0.9381  data: 0.0005  max mem: 19734
Epoch: [3]  [ 460/1251]  eta: 0:12:33  lr: 0.000009  loss: 3.7372 (3.5131)  time: 0.9344  data: 0.0004  max mem: 19734
Epoch: [3]  [ 470/1251]  eta: 0:12:23  lr: 0.000009  loss: 3.7372 (3.5141)  time: 0.9262  data: 0.0004  max mem: 19734
Epoch: [3]  [ 480/1251]  eta: 0:12:13  lr: 0.000009  loss: 3.6415 (3.5088)  time: 0.9300  data: 0.0004  max mem: 19734
Epoch: [3]  [ 490/1251]  eta: 0:12:04  lr: 0.000009  loss: 3.6090 (3.5110)  time: 0.9414  data: 0.0007  max mem: 19734
Epoch: [3]  [ 500/1251]  eta: 0:11:54  lr: 0.000009  loss: 3.6090 (3.5115)  time: 0.9382  data: 0.0007  max mem: 19734
Epoch: [3]  [ 510/1251]  eta: 0:11:44  lr: 0.000009  loss: 3.4297 (3.5088)  time: 0.9274  data: 0.0005  max mem: 19734
Epoch: [3]  [ 520/1251]  eta: 0:11:35  lr: 0.000009  loss: 3.4958 (3.5089)  time: 0.9393  data: 0.0005  max mem: 19734
Epoch: [3]  [ 530/1251]  eta: 0:11:25  lr: 0.000009  loss: 3.5866 (3.5065)  time: 0.9473  data: 0.0004  max mem: 19734
Epoch: [3]  [ 540/1251]  eta: 0:11:15  lr: 0.000009  loss: 3.5866 (3.5074)  time: 0.9335  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4644, ratio_loss=0.0036, pruning_loss=0.2330, mse_loss=1.1824
Epoch: [3]  [ 550/1251]  eta: 0:11:05  lr: 0.000009  loss: 3.5847 (3.5025)  time: 0.9259  data: 0.0005  max mem: 19734
Epoch: [3]  [ 560/1251]  eta: 0:10:56  lr: 0.000009  loss: 3.6489 (3.5042)  time: 0.9351  data: 0.0004  max mem: 19734
Epoch: [3]  [ 570/1251]  eta: 0:10:46  lr: 0.000009  loss: 3.7132 (3.5101)  time: 0.9479  data: 0.0005  max mem: 19734
Epoch: [3]  [ 580/1251]  eta: 0:10:37  lr: 0.000009  loss: 3.6544 (3.5057)  time: 0.9531  data: 0.0005  max mem: 19734
Epoch: [3]  [ 590/1251]  eta: 0:10:27  lr: 0.000009  loss: 3.3805 (3.5017)  time: 0.9409  data: 0.0006  max mem: 19734
Epoch: [3]  [ 600/1251]  eta: 0:10:17  lr: 0.000009  loss: 3.7691 (3.5027)  time: 0.9271  data: 0.0005  max mem: 19734
Epoch: [3]  [ 610/1251]  eta: 0:10:08  lr: 0.000009  loss: 3.7254 (3.5012)  time: 0.9335  data: 0.0004  max mem: 19734
Epoch: [3]  [ 620/1251]  eta: 0:09:58  lr: 0.000009  loss: 3.7263 (3.5056)  time: 0.9469  data: 0.0004  max mem: 19734
Epoch: [3]  [ 630/1251]  eta: 0:09:49  lr: 0.000009  loss: 3.7280 (3.5038)  time: 0.9455  data: 0.0004  max mem: 19734
Epoch: [3]  [ 640/1251]  eta: 0:09:39  lr: 0.000009  loss: 3.6510 (3.5060)  time: 0.9537  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4919, ratio_loss=0.0036, pruning_loss=0.2315, mse_loss=1.1743
Epoch: [3]  [ 650/1251]  eta: 0:09:30  lr: 0.000009  loss: 3.8027 (3.5103)  time: 0.9498  data: 0.0004  max mem: 19734
Epoch: [3]  [ 660/1251]  eta: 0:09:20  lr: 0.000009  loss: 3.7873 (3.5099)  time: 0.9269  data: 0.0004  max mem: 19734
Epoch: [3]  [ 670/1251]  eta: 0:09:11  lr: 0.000009  loss: 3.7180 (3.5109)  time: 0.9327  data: 0.0004  max mem: 19734
Epoch: [3]  [ 680/1251]  eta: 0:09:01  lr: 0.000009  loss: 3.7180 (3.5103)  time: 0.9423  data: 0.0005  max mem: 19734
Epoch: [3]  [ 690/1251]  eta: 0:08:51  lr: 0.000009  loss: 3.6441 (3.5104)  time: 0.9378  data: 0.0004  max mem: 19734
Epoch: [3]  [ 700/1251]  eta: 0:08:42  lr: 0.000009  loss: 3.8117 (3.5096)  time: 0.9330  data: 0.0005  max mem: 19734
Epoch: [3]  [ 710/1251]  eta: 0:08:32  lr: 0.000009  loss: 3.7593 (3.5086)  time: 0.9396  data: 0.0005  max mem: 19734
Epoch: [3]  [ 720/1251]  eta: 0:08:23  lr: 0.000009  loss: 3.7892 (3.5148)  time: 0.9348  data: 0.0004  max mem: 19734
Epoch: [3]  [ 730/1251]  eta: 0:08:13  lr: 0.000009  loss: 3.7575 (3.5151)  time: 0.9243  data: 0.0004  max mem: 19734
Epoch: [3]  [ 740/1251]  eta: 0:08:03  lr: 0.000009  loss: 3.4103 (3.5105)  time: 0.9317  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4902, ratio_loss=0.0037, pruning_loss=0.2295, mse_loss=1.1445
Epoch: [3]  [ 750/1251]  eta: 0:07:54  lr: 0.000009  loss: 3.3083 (3.5103)  time: 0.9312  data: 0.0004  max mem: 19734
Epoch: [3]  [ 760/1251]  eta: 0:07:44  lr: 0.000009  loss: 3.7155 (3.5125)  time: 0.9297  data: 0.0005  max mem: 19734
Epoch: [3]  [ 770/1251]  eta: 0:07:35  lr: 0.000009  loss: 3.7155 (3.5121)  time: 0.9417  data: 0.0004  max mem: 19734
Epoch: [3]  [ 780/1251]  eta: 0:07:25  lr: 0.000009  loss: 3.6188 (3.5121)  time: 0.9447  data: 0.0004  max mem: 19734
Epoch: [3]  [ 790/1251]  eta: 0:07:16  lr: 0.000009  loss: 3.6188 (3.5115)  time: 0.9344  data: 0.0005  max mem: 19734
Epoch: [3]  [ 800/1251]  eta: 0:07:06  lr: 0.000009  loss: 3.5335 (3.5107)  time: 0.9323  data: 0.0004  max mem: 19734
Epoch: [3]  [ 810/1251]  eta: 0:06:57  lr: 0.000009  loss: 3.6005 (3.5128)  time: 0.9339  data: 0.0004  max mem: 19734
Epoch: [3]  [ 820/1251]  eta: 0:06:47  lr: 0.000009  loss: 3.6592 (3.5130)  time: 0.9462  data: 0.0004  max mem: 19734
Epoch: [3]  [ 830/1251]  eta: 0:06:38  lr: 0.000009  loss: 3.6592 (3.5135)  time: 0.9612  data: 0.0004  max mem: 19734
Epoch: [3]  [ 840/1251]  eta: 0:06:28  lr: 0.000009  loss: 3.7890 (3.5160)  time: 0.9432  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5194, ratio_loss=0.0035, pruning_loss=0.2273, mse_loss=1.1289
Epoch: [3]  [ 850/1251]  eta: 0:06:19  lr: 0.000009  loss: 3.6171 (3.5168)  time: 0.9269  data: 0.0004  max mem: 19734
Epoch: [3]  [ 860/1251]  eta: 0:06:09  lr: 0.000009  loss: 3.5095 (3.5146)  time: 0.9303  data: 0.0005  max mem: 19734
Epoch: [3]  [ 870/1251]  eta: 0:06:00  lr: 0.000009  loss: 3.4497 (3.5159)  time: 0.9383  data: 0.0005  max mem: 19734
Epoch: [3]  [ 880/1251]  eta: 0:05:50  lr: 0.000009  loss: 3.6692 (3.5158)  time: 0.9327  data: 0.0005  max mem: 19734
Epoch: [3]  [ 890/1251]  eta: 0:05:41  lr: 0.000009  loss: 3.7230 (3.5174)  time: 0.9271  data: 0.0004  max mem: 19734
Epoch: [3]  [ 900/1251]  eta: 0:05:31  lr: 0.000009  loss: 3.7230 (3.5184)  time: 0.9325  data: 0.0004  max mem: 19734
Epoch: [3]  [ 910/1251]  eta: 0:05:22  lr: 0.000009  loss: 3.6296 (3.5193)  time: 0.9426  data: 0.0005  max mem: 19734
Epoch: [3]  [ 920/1251]  eta: 0:05:12  lr: 0.000009  loss: 3.6296 (3.5204)  time: 0.9440  data: 0.0005  max mem: 19734
Epoch: [3]  [ 930/1251]  eta: 0:05:03  lr: 0.000009  loss: 3.6766 (3.5212)  time: 0.9442  data: 0.0006  max mem: 19734
Epoch: [3]  [ 940/1251]  eta: 0:04:53  lr: 0.000009  loss: 3.7841 (3.5236)  time: 0.9406  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5800, ratio_loss=0.0037, pruning_loss=0.2226, mse_loss=1.0732
Epoch: [3]  [ 950/1251]  eta: 0:04:44  lr: 0.000009  loss: 3.7236 (3.5259)  time: 0.9249  data: 0.0004  max mem: 19734
Epoch: [3]  [ 960/1251]  eta: 0:04:34  lr: 0.000009  loss: 3.5347 (3.5250)  time: 0.9294  data: 0.0004  max mem: 19734
Epoch: [3]  [ 970/1251]  eta: 0:04:25  lr: 0.000009  loss: 3.6857 (3.5264)  time: 0.9520  data: 0.0004  max mem: 19734
Epoch: [3]  [ 980/1251]  eta: 0:04:16  lr: 0.000009  loss: 3.6922 (3.5245)  time: 0.9475  data: 0.0004  max mem: 19734
Epoch: [3]  [ 990/1251]  eta: 0:04:06  lr: 0.000009  loss: 3.6036 (3.5249)  time: 0.9348  data: 0.0005  max mem: 19734
Epoch: [3]  [1000/1251]  eta: 0:03:57  lr: 0.000009  loss: 3.6982 (3.5254)  time: 0.9309  data: 0.0005  max mem: 19734
Epoch: [3]  [1010/1251]  eta: 0:03:47  lr: 0.000009  loss: 3.6982 (3.5256)  time: 0.9241  data: 0.0004  max mem: 19734
Epoch: [3]  [1020/1251]  eta: 0:03:38  lr: 0.000009  loss: 3.6219 (3.5226)  time: 0.9268  data: 0.0004  max mem: 19734
Epoch: [3]  [1030/1251]  eta: 0:03:28  lr: 0.000009  loss: 3.4973 (3.5222)  time: 0.9237  data: 0.0004  max mem: 19734
Epoch: [3]  [1040/1251]  eta: 0:03:19  lr: 0.000009  loss: 3.6549 (3.5226)  time: 0.9243  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4638, ratio_loss=0.0035, pruning_loss=0.2276, mse_loss=1.1374
Epoch: [3]  [1050/1251]  eta: 0:03:09  lr: 0.000009  loss: 3.6985 (3.5231)  time: 0.9271  data: 0.0004  max mem: 19734
Epoch: [3]  [1060/1251]  eta: 0:03:00  lr: 0.000009  loss: 3.4813 (3.5228)  time: 0.9295  data: 0.0004  max mem: 19734
Epoch: [3]  [1070/1251]  eta: 0:02:50  lr: 0.000009  loss: 3.3852 (3.5207)  time: 0.9470  data: 0.0004  max mem: 19734
Epoch: [3]  [1080/1251]  eta: 0:02:41  lr: 0.000009  loss: 3.6239 (3.5215)  time: 0.9596  data: 0.0004  max mem: 19734
Epoch: [3]  [1090/1251]  eta: 0:02:31  lr: 0.000009  loss: 3.6350 (3.5217)  time: 0.9493  data: 0.0004  max mem: 19734
Epoch: [3]  [1100/1251]  eta: 0:02:22  lr: 0.000009  loss: 3.5508 (3.5197)  time: 0.9292  data: 0.0004  max mem: 19734
Epoch: [3]  [1110/1251]  eta: 0:02:13  lr: 0.000009  loss: 3.3545 (3.5164)  time: 0.9328  data: 0.0004  max mem: 19734
Epoch: [3]  [1120/1251]  eta: 0:02:03  lr: 0.000009  loss: 3.3545 (3.5148)  time: 0.9741  data: 0.0005  max mem: 19734
Epoch: [3]  [1130/1251]  eta: 0:01:54  lr: 0.000009  loss: 3.4642 (3.5142)  time: 0.9782  data: 0.0005  max mem: 19734
Epoch: [3]  [1140/1251]  eta: 0:01:44  lr: 0.000009  loss: 3.5899 (3.5150)  time: 0.9373  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3967, ratio_loss=0.0035, pruning_loss=0.2289, mse_loss=1.1442
Epoch: [3]  [1150/1251]  eta: 0:01:35  lr: 0.000009  loss: 3.5379 (3.5140)  time: 0.9190  data: 0.0004  max mem: 19734
Epoch: [3]  [1160/1251]  eta: 0:01:25  lr: 0.000009  loss: 3.3745 (3.5136)  time: 0.9219  data: 0.0004  max mem: 19734
Epoch: [3]  [1170/1251]  eta: 0:01:16  lr: 0.000009  loss: 3.6301 (3.5142)  time: 0.9343  data: 0.0004  max mem: 19734
Epoch: [3]  [1180/1251]  eta: 0:01:06  lr: 0.000009  loss: 3.6833 (3.5144)  time: 0.9330  data: 0.0004  max mem: 19734
Epoch: [3]  [1190/1251]  eta: 0:00:57  lr: 0.000009  loss: 3.7029 (3.5156)  time: 0.9223  data: 0.0006  max mem: 19734
Epoch: [3]  [1200/1251]  eta: 0:00:48  lr: 0.000009  loss: 3.6151 (3.5163)  time: 0.9390  data: 0.0004  max mem: 19734
Epoch: [3]  [1210/1251]  eta: 0:00:38  lr: 0.000009  loss: 3.4991 (3.5140)  time: 0.9361  data: 0.0001  max mem: 19734
Epoch: [3]  [1220/1251]  eta: 0:00:29  lr: 0.000009  loss: 3.5383 (3.5152)  time: 0.9303  data: 0.0001  max mem: 19734
Epoch: [3]  [1230/1251]  eta: 0:00:19  lr: 0.000009  loss: 3.8558 (3.5184)  time: 0.9428  data: 0.0001  max mem: 19734
Epoch: [3]  [1240/1251]  eta: 0:00:10  lr: 0.000009  loss: 3.8177 (3.5178)  time: 0.9330  data: 0.0001  max mem: 19734
loss info: cls_loss=3.5538, ratio_loss=0.0037, pruning_loss=0.2235, mse_loss=1.0874
Epoch: [3]  [1250/1251]  eta: 0:00:00  lr: 0.000009  loss: 3.7209 (3.5197)  time: 0.9228  data: 0.0001  max mem: 19734
Epoch: [3] Total time: 0:19:40 (0.9436 s / it)
Averaged stats: lr: 0.000009  loss: 3.7209 (3.5254)
Test:  [  0/261]  eta: 0:49:25  loss: 0.7762 (0.7762)  acc1: 82.8125 (82.8125)  acc5: 96.3542 (96.3542)  time: 11.3610  data: 11.0093  max mem: 19734
Test:  [ 10/261]  eta: 0:12:23  loss: 0.7666 (0.7931)  acc1: 86.4583 (83.3333)  acc5: 96.8750 (96.0701)  time: 2.9632  data: 2.6811  max mem: 19734
Test:  [ 20/261]  eta: 0:06:45  loss: 1.0183 (0.9741)  acc1: 78.1250 (78.1498)  acc5: 93.7500 (94.1964)  time: 1.1994  data: 0.9324  max mem: 19734
Test:  [ 30/261]  eta: 0:04:42  loss: 0.8791 (0.8882)  acc1: 81.7708 (81.0148)  acc5: 93.7500 (94.7077)  time: 0.2691  data: 0.0132  max mem: 19734
Test:  [ 40/261]  eta: 0:04:08  loss: 0.6274 (0.8562)  acc1: 87.5000 (81.8979)  acc5: 96.8750 (94.9949)  time: 0.5338  data: 0.1620  max mem: 19734
Test:  [ 50/261]  eta: 0:03:18  loss: 1.0064 (0.9314)  acc1: 75.0000 (79.6467)  acc5: 93.7500 (94.4240)  time: 0.5054  data: 0.1638  max mem: 19734
Test:  [ 60/261]  eta: 0:02:50  loss: 1.1412 (0.9514)  acc1: 72.3958 (78.8849)  acc5: 93.2292 (94.5014)  time: 0.2891  data: 0.0129  max mem: 19734
Test:  [ 70/261]  eta: 0:02:29  loss: 1.0705 (0.9535)  acc1: 73.9583 (78.3818)  acc5: 96.3542 (94.7330)  time: 0.3796  data: 0.0144  max mem: 19734
Test:  [ 80/261]  eta: 0:02:11  loss: 0.9195 (0.9483)  acc1: 78.6458 (78.5880)  acc5: 96.3542 (94.8367)  time: 0.3564  data: 0.0200  max mem: 19734
Test:  [ 90/261]  eta: 0:01:59  loss: 0.8462 (0.9296)  acc1: 81.7708 (78.9950)  acc5: 95.8333 (94.9863)  time: 0.3892  data: 0.0183  max mem: 19734
Test:  [100/261]  eta: 0:01:45  loss: 0.8448 (0.9296)  acc1: 81.7708 (79.0274)  acc5: 95.3125 (95.0547)  time: 0.3705  data: 0.0109  max mem: 19734
Test:  [110/261]  eta: 0:01:41  loss: 0.8512 (0.9478)  acc1: 75.5208 (78.6271)  acc5: 94.2708 (94.7823)  time: 0.5727  data: 0.2674  max mem: 19734
Test:  [120/261]  eta: 0:01:31  loss: 1.2882 (0.9852)  acc1: 70.3125 (77.8108)  acc5: 89.5833 (94.2321)  time: 0.6007  data: 0.2739  max mem: 19734
Test:  [130/261]  eta: 0:01:22  loss: 1.4818 (1.0286)  acc1: 68.2292 (76.9283)  acc5: 86.9792 (93.6307)  time: 0.3552  data: 0.0137  max mem: 19734
Test:  [140/261]  eta: 0:01:13  loss: 1.4124 (1.0540)  acc1: 66.6667 (76.2707)  acc5: 88.0208 (93.3437)  time: 0.3459  data: 0.0090  max mem: 19734
Test:  [150/261]  eta: 0:01:06  loss: 1.2186 (1.0564)  acc1: 70.8333 (76.3693)  acc5: 90.6250 (93.2188)  time: 0.4222  data: 0.0749  max mem: 19734
Test:  [160/261]  eta: 0:01:00  loss: 1.0341 (1.0752)  acc1: 77.0833 (76.0773)  acc5: 92.1875 (92.9315)  time: 0.5495  data: 0.0755  max mem: 19734
Test:  [170/261]  eta: 0:00:53  loss: 1.3825 (1.1042)  acc1: 65.6250 (75.2985)  acc5: 87.5000 (92.5956)  time: 0.4982  data: 0.0169  max mem: 19734
Test:  [180/261]  eta: 0:00:47  loss: 1.5016 (1.1207)  acc1: 63.0208 (74.9194)  acc5: 87.5000 (92.4378)  time: 0.4887  data: 0.0980  max mem: 19734
Test:  [190/261]  eta: 0:00:40  loss: 1.4487 (1.1335)  acc1: 66.1458 (74.6782)  acc5: 90.1042 (92.2584)  time: 0.4614  data: 0.0974  max mem: 19734
Test:  [200/261]  eta: 0:00:34  loss: 1.3983 (1.1470)  acc1: 69.2708 (74.3626)  acc5: 89.5833 (92.0476)  time: 0.3190  data: 0.0134  max mem: 19734
Test:  [210/261]  eta: 0:00:27  loss: 1.4036 (1.1579)  acc1: 68.2292 (74.1435)  acc5: 88.0208 (91.9086)  time: 0.2387  data: 0.0278  max mem: 19734
Test:  [220/261]  eta: 0:00:21  loss: 1.4432 (1.1768)  acc1: 66.6667 (73.6590)  acc5: 87.5000 (91.6902)  time: 0.1723  data: 0.0242  max mem: 19734
Test:  [230/261]  eta: 0:00:15  loss: 1.4807 (1.1871)  acc1: 65.6250 (73.4217)  acc5: 88.5417 (91.5675)  time: 0.1432  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.4082 (1.1957)  acc1: 66.1458 (73.1976)  acc5: 89.5833 (91.4808)  time: 0.1422  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0818 (1.1872)  acc1: 75.0000 (73.4334)  acc5: 91.6667 (91.6065)  time: 0.1422  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9826 (1.1869)  acc1: 76.5625 (73.4340)  acc5: 94.7917 (91.6700)  time: 0.1385  data: 0.0001  max mem: 19734
Test: Total time: 0:02:01 (0.4663 s / it)
* Acc@1 73.434 Acc@5 91.670 loss 1.187
Accuracy of the network on the 50000 test images: 73.4%
Max accuracy: 73.43%
Epoch: [4]  [   0/1251]  eta: 6:14:54  lr: 0.000012  loss: 4.1953 (4.1953)  time: 17.9813  data: 17.1611  max mem: 19734
Epoch: [4]  [  10/1251]  eta: 0:51:29  lr: 0.000012  loss: 3.6494 (3.6212)  time: 2.4896  data: 1.5605  max mem: 19734
Epoch: [4]  [  20/1251]  eta: 0:35:01  lr: 0.000012  loss: 3.6494 (3.5303)  time: 0.8937  data: 0.0005  max mem: 19734
Epoch: [4]  [  30/1251]  eta: 0:29:12  lr: 0.000012  loss: 3.5467 (3.4328)  time: 0.8549  data: 0.0006  max mem: 19734
Epoch: [4]  [  40/1251]  eta: 0:26:11  lr: 0.000012  loss: 3.5467 (3.4797)  time: 0.8678  data: 0.0005  max mem: 19734
Epoch: [4]  [  50/1251]  eta: 0:24:16  lr: 0.000012  loss: 3.6050 (3.4694)  time: 0.8686  data: 0.0006  max mem: 19734
Epoch: [4]  [  60/1251]  eta: 0:22:57  lr: 0.000012  loss: 3.6937 (3.4974)  time: 0.8679  data: 0.0005  max mem: 19734
Epoch: [4]  [  70/1251]  eta: 0:22:00  lr: 0.000012  loss: 3.6517 (3.4660)  time: 0.8751  data: 0.0004  max mem: 19734
Epoch: [4]  [  80/1251]  eta: 0:21:18  lr: 0.000012  loss: 3.5065 (3.4542)  time: 0.8944  data: 0.0004  max mem: 19734
Epoch: [4]  [  90/1251]  eta: 0:20:41  lr: 0.000012  loss: 3.4132 (3.4389)  time: 0.8988  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4077, ratio_loss=0.0038, pruning_loss=0.2265, mse_loss=1.1158
Epoch: [4]  [ 100/1251]  eta: 0:20:11  lr: 0.000012  loss: 3.5176 (3.4579)  time: 0.8935  data: 0.0004  max mem: 19734
Epoch: [4]  [ 110/1251]  eta: 0:19:45  lr: 0.000012  loss: 3.6156 (3.4576)  time: 0.8979  data: 0.0004  max mem: 19734
Epoch: [4]  [ 120/1251]  eta: 0:19:22  lr: 0.000012  loss: 3.3061 (3.4523)  time: 0.9037  data: 0.0004  max mem: 19734
Epoch: [4]  [ 130/1251]  eta: 0:19:01  lr: 0.000012  loss: 3.5495 (3.4671)  time: 0.9058  data: 0.0003  max mem: 19734
Epoch: [4]  [ 140/1251]  eta: 0:18:46  lr: 0.000012  loss: 3.7738 (3.4857)  time: 0.9273  data: 0.0004  max mem: 19734
Epoch: [4]  [ 150/1251]  eta: 0:18:30  lr: 0.000012  loss: 3.6152 (3.4796)  time: 0.9440  data: 0.0005  max mem: 19734
Epoch: [4]  [ 160/1251]  eta: 0:18:15  lr: 0.000012  loss: 3.5806 (3.4971)  time: 0.9339  data: 0.0004  max mem: 19734
Epoch: [4]  [ 170/1251]  eta: 0:17:59  lr: 0.000012  loss: 3.5936 (3.4888)  time: 0.9218  data: 0.0005  max mem: 19734
Epoch: [4]  [ 180/1251]  eta: 0:17:45  lr: 0.000012  loss: 3.6253 (3.4961)  time: 0.9262  data: 0.0005  max mem: 19734
Epoch: [4]  [ 190/1251]  eta: 0:17:31  lr: 0.000012  loss: 3.5259 (3.4849)  time: 0.9320  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5124, ratio_loss=0.0039, pruning_loss=0.2215, mse_loss=1.1091
Epoch: [4]  [ 200/1251]  eta: 0:17:18  lr: 0.000012  loss: 3.5522 (3.4969)  time: 0.9194  data: 0.0005  max mem: 19734
Epoch: [4]  [ 210/1251]  eta: 0:17:05  lr: 0.000012  loss: 3.9064 (3.5064)  time: 0.9293  data: 0.0006  max mem: 19734
Epoch: [4]  [ 220/1251]  eta: 0:16:53  lr: 0.000012  loss: 3.7738 (3.5106)  time: 0.9359  data: 0.0005  max mem: 19734
Epoch: [4]  [ 230/1251]  eta: 0:16:41  lr: 0.000012  loss: 3.6535 (3.5034)  time: 0.9268  data: 0.0005  max mem: 19734
Epoch: [4]  [ 240/1251]  eta: 0:16:28  lr: 0.000012  loss: 3.8019 (3.5204)  time: 0.9216  data: 0.0005  max mem: 19734
Epoch: [4]  [ 250/1251]  eta: 0:16:17  lr: 0.000012  loss: 3.8720 (3.5236)  time: 0.9306  data: 0.0006  max mem: 19734
Epoch: [4]  [ 260/1251]  eta: 0:16:05  lr: 0.000012  loss: 3.7120 (3.5251)  time: 0.9327  data: 0.0006  max mem: 19734
Epoch: [4]  [ 270/1251]  eta: 0:15:55  lr: 0.000012  loss: 3.7120 (3.5266)  time: 0.9364  data: 0.0004  max mem: 19734
Epoch: [4]  [ 280/1251]  eta: 0:15:43  lr: 0.000012  loss: 3.8256 (3.5317)  time: 0.9376  data: 0.0004  max mem: 19734
Epoch: [4]  [ 290/1251]  eta: 0:15:33  lr: 0.000012  loss: 3.7342 (3.5290)  time: 0.9428  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5887, ratio_loss=0.0035, pruning_loss=0.2196, mse_loss=1.0516
Epoch: [4]  [ 300/1251]  eta: 0:15:23  lr: 0.000012  loss: 3.6015 (3.5364)  time: 0.9551  data: 0.0004  max mem: 19734
Epoch: [4]  [ 310/1251]  eta: 0:15:12  lr: 0.000012  loss: 3.6015 (3.5283)  time: 0.9409  data: 0.0004  max mem: 19734
Epoch: [4]  [ 320/1251]  eta: 0:15:01  lr: 0.000012  loss: 3.3694 (3.5233)  time: 0.9238  data: 0.0004  max mem: 19734
Epoch: [4]  [ 330/1251]  eta: 0:14:49  lr: 0.000012  loss: 3.3478 (3.5159)  time: 0.9158  data: 0.0004  max mem: 19734
Epoch: [4]  [ 340/1251]  eta: 0:14:39  lr: 0.000012  loss: 3.6811 (3.5204)  time: 0.9188  data: 0.0004  max mem: 19734
Epoch: [4]  [ 350/1251]  eta: 0:14:28  lr: 0.000012  loss: 3.8257 (3.5241)  time: 0.9312  data: 0.0004  max mem: 19734
Epoch: [4]  [ 360/1251]  eta: 0:14:18  lr: 0.000012  loss: 3.6485 (3.5222)  time: 0.9317  data: 0.0004  max mem: 19734
Epoch: [4]  [ 370/1251]  eta: 0:14:08  lr: 0.000012  loss: 3.4168 (3.5207)  time: 0.9340  data: 0.0004  max mem: 19734
Epoch: [4]  [ 380/1251]  eta: 0:13:57  lr: 0.000012  loss: 3.4196 (3.5170)  time: 0.9310  data: 0.0004  max mem: 19734
Epoch: [4]  [ 390/1251]  eta: 0:13:47  lr: 0.000012  loss: 3.5657 (3.5150)  time: 0.9342  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4483, ratio_loss=0.0036, pruning_loss=0.2263, mse_loss=1.1110
Epoch: [4]  [ 400/1251]  eta: 0:13:37  lr: 0.000012  loss: 3.7238 (3.5187)  time: 0.9461  data: 0.0004  max mem: 19734
Epoch: [4]  [ 410/1251]  eta: 0:13:27  lr: 0.000012  loss: 3.6631 (3.5209)  time: 0.9446  data: 0.0004  max mem: 19734
Epoch: [4]  [ 420/1251]  eta: 0:13:17  lr: 0.000012  loss: 3.5484 (3.5162)  time: 0.9309  data: 0.0004  max mem: 19734
Epoch: [4]  [ 430/1251]  eta: 0:13:06  lr: 0.000012  loss: 3.6310 (3.5166)  time: 0.9208  data: 0.0004  max mem: 19734
Epoch: [4]  [ 440/1251]  eta: 0:12:56  lr: 0.000012  loss: 3.4245 (3.5148)  time: 0.9303  data: 0.0005  max mem: 19734
Epoch: [4]  [ 450/1251]  eta: 0:12:47  lr: 0.000012  loss: 3.4245 (3.5154)  time: 0.9447  data: 0.0005  max mem: 19734
Epoch: [4]  [ 460/1251]  eta: 0:12:37  lr: 0.000012  loss: 3.7369 (3.5192)  time: 0.9373  data: 0.0005  max mem: 19734
Epoch: [4]  [ 470/1251]  eta: 0:12:26  lr: 0.000012  loss: 3.8355 (3.5220)  time: 0.9195  data: 0.0004  max mem: 19734
Epoch: [4]  [ 480/1251]  eta: 0:12:16  lr: 0.000012  loss: 3.3979 (3.5155)  time: 0.9200  data: 0.0004  max mem: 19734
Epoch: [4]  [ 490/1251]  eta: 0:12:06  lr: 0.000012  loss: 3.5156 (3.5189)  time: 0.9197  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4778, ratio_loss=0.0038, pruning_loss=0.2235, mse_loss=1.1285
Epoch: [4]  [ 500/1251]  eta: 0:11:56  lr: 0.000012  loss: 3.6254 (3.5191)  time: 0.9287  data: 0.0004  max mem: 19734
Epoch: [4]  [ 510/1251]  eta: 0:11:46  lr: 0.000012  loss: 3.3147 (3.5150)  time: 0.9311  data: 0.0004  max mem: 19734
Epoch: [4]  [ 520/1251]  eta: 0:11:36  lr: 0.000012  loss: 3.6073 (3.5171)  time: 0.9266  data: 0.0004  max mem: 19734
Epoch: [4]  [ 530/1251]  eta: 0:11:27  lr: 0.000012  loss: 3.4194 (3.5097)  time: 0.9283  data: 0.0004  max mem: 19734
Epoch: [4]  [ 540/1251]  eta: 0:11:18  lr: 0.000012  loss: 3.3766 (3.5085)  time: 0.9615  data: 0.0005  max mem: 19734
Epoch: [4]  [ 550/1251]  eta: 0:11:08  lr: 0.000012  loss: 3.6778 (3.5063)  time: 0.9689  data: 0.0005  max mem: 19734
Epoch: [4]  [ 560/1251]  eta: 0:10:58  lr: 0.000012  loss: 3.6029 (3.5073)  time: 0.9346  data: 0.0004  max mem: 19734
Epoch: [4]  [ 570/1251]  eta: 0:10:49  lr: 0.000012  loss: 3.4172 (3.5043)  time: 0.9480  data: 0.0004  max mem: 19734
Epoch: [4]  [ 580/1251]  eta: 0:10:39  lr: 0.000012  loss: 3.5862 (3.5054)  time: 0.9540  data: 0.0004  max mem: 19734
Epoch: [4]  [ 590/1251]  eta: 0:10:29  lr: 0.000012  loss: 3.7400 (3.5095)  time: 0.9381  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4437, ratio_loss=0.0036, pruning_loss=0.2215, mse_loss=1.0463
Epoch: [4]  [ 600/1251]  eta: 0:10:19  lr: 0.000012  loss: 3.7175 (3.5063)  time: 0.9244  data: 0.0004  max mem: 19734
Epoch: [4]  [ 610/1251]  eta: 0:10:09  lr: 0.000012  loss: 3.5678 (3.5058)  time: 0.9142  data: 0.0004  max mem: 19734
Epoch: [4]  [ 620/1251]  eta: 0:10:00  lr: 0.000012  loss: 3.6520 (3.5060)  time: 0.9162  data: 0.0005  max mem: 19734
Epoch: [4]  [ 630/1251]  eta: 0:09:50  lr: 0.000012  loss: 3.8146 (3.5115)  time: 0.9174  data: 0.0005  max mem: 19734
Epoch: [4]  [ 640/1251]  eta: 0:09:40  lr: 0.000012  loss: 3.8146 (3.5110)  time: 0.9203  data: 0.0005  max mem: 19734
Epoch: [4]  [ 650/1251]  eta: 0:09:30  lr: 0.000012  loss: 3.5924 (3.5104)  time: 0.9234  data: 0.0004  max mem: 19734
Epoch: [4]  [ 660/1251]  eta: 0:09:21  lr: 0.000012  loss: 3.3304 (3.5083)  time: 0.9346  data: 0.0004  max mem: 19734
Epoch: [4]  [ 670/1251]  eta: 0:09:11  lr: 0.000012  loss: 3.3304 (3.5068)  time: 0.9374  data: 0.0004  max mem: 19734
Epoch: [4]  [ 680/1251]  eta: 0:09:01  lr: 0.000012  loss: 3.4901 (3.5086)  time: 0.9272  data: 0.0004  max mem: 19734
Epoch: [4]  [ 690/1251]  eta: 0:08:52  lr: 0.000012  loss: 3.6125 (3.5087)  time: 0.9236  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4751, ratio_loss=0.0037, pruning_loss=0.2200, mse_loss=1.0690
Epoch: [4]  [ 700/1251]  eta: 0:08:42  lr: 0.000012  loss: 3.7508 (3.5105)  time: 0.9242  data: 0.0004  max mem: 19734
Epoch: [4]  [ 710/1251]  eta: 0:08:32  lr: 0.000012  loss: 3.7387 (3.5083)  time: 0.9257  data: 0.0004  max mem: 19734
Epoch: [4]  [ 720/1251]  eta: 0:08:23  lr: 0.000012  loss: 3.2672 (3.5046)  time: 0.9444  data: 0.0005  max mem: 19734
Epoch: [4]  [ 730/1251]  eta: 0:08:14  lr: 0.000012  loss: 3.2989 (3.5034)  time: 0.9629  data: 0.0005  max mem: 19734
Epoch: [4]  [ 740/1251]  eta: 0:08:04  lr: 0.000012  loss: 3.4411 (3.5022)  time: 0.9549  data: 0.0005  max mem: 19734
Epoch: [4]  [ 750/1251]  eta: 0:07:54  lr: 0.000012  loss: 3.5680 (3.5038)  time: 0.9405  data: 0.0005  max mem: 19734
Epoch: [4]  [ 760/1251]  eta: 0:07:45  lr: 0.000012  loss: 3.7668 (3.5061)  time: 0.9317  data: 0.0005  max mem: 19734
Epoch: [4]  [ 770/1251]  eta: 0:07:35  lr: 0.000012  loss: 3.5349 (3.5053)  time: 0.9213  data: 0.0005  max mem: 19734
Epoch: [4]  [ 780/1251]  eta: 0:07:26  lr: 0.000012  loss: 3.5349 (3.5048)  time: 0.9341  data: 0.0005  max mem: 19734
Epoch: [4]  [ 790/1251]  eta: 0:07:16  lr: 0.000012  loss: 3.4644 (3.5016)  time: 0.9361  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4188, ratio_loss=0.0039, pruning_loss=0.2189, mse_loss=1.0834
Epoch: [4]  [ 800/1251]  eta: 0:07:07  lr: 0.000012  loss: 3.4644 (3.5017)  time: 0.9291  data: 0.0005  max mem: 19734
Epoch: [4]  [ 810/1251]  eta: 0:06:57  lr: 0.000012  loss: 3.5460 (3.5014)  time: 0.9371  data: 0.0005  max mem: 19734
Epoch: [4]  [ 820/1251]  eta: 0:06:48  lr: 0.000012  loss: 3.5984 (3.5032)  time: 0.9494  data: 0.0004  max mem: 19734
Epoch: [4]  [ 830/1251]  eta: 0:06:38  lr: 0.000012  loss: 3.6810 (3.5046)  time: 0.9624  data: 0.0004  max mem: 19734
Epoch: [4]  [ 840/1251]  eta: 0:06:29  lr: 0.000012  loss: 3.6475 (3.5046)  time: 0.9596  data: 0.0005  max mem: 19734
Epoch: [4]  [ 850/1251]  eta: 0:06:19  lr: 0.000012  loss: 3.5240 (3.5030)  time: 0.9380  data: 0.0004  max mem: 19734
Epoch: [4]  [ 860/1251]  eta: 0:06:10  lr: 0.000012  loss: 3.7531 (3.5074)  time: 0.9262  data: 0.0004  max mem: 19734
Epoch: [4]  [ 870/1251]  eta: 0:06:00  lr: 0.000012  loss: 3.8475 (3.5109)  time: 0.9261  data: 0.0004  max mem: 19734
Epoch: [4]  [ 880/1251]  eta: 0:05:51  lr: 0.000012  loss: 3.7929 (3.5113)  time: 0.9208  data: 0.0004  max mem: 19734
Epoch: [4]  [ 890/1251]  eta: 0:05:41  lr: 0.000012  loss: 3.5538 (3.5098)  time: 0.9235  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5036, ratio_loss=0.0037, pruning_loss=0.2164, mse_loss=1.0677
Epoch: [4]  [ 900/1251]  eta: 0:05:31  lr: 0.000012  loss: 3.2304 (3.5070)  time: 0.9242  data: 0.0004  max mem: 19734
Epoch: [4]  [ 910/1251]  eta: 0:05:22  lr: 0.000012  loss: 3.5616 (3.5092)  time: 0.9317  data: 0.0004  max mem: 19734
Epoch: [4]  [ 920/1251]  eta: 0:05:12  lr: 0.000012  loss: 3.8124 (3.5101)  time: 0.9359  data: 0.0004  max mem: 19734
Epoch: [4]  [ 930/1251]  eta: 0:05:03  lr: 0.000012  loss: 3.7835 (3.5109)  time: 0.9280  data: 0.0004  max mem: 19734
Epoch: [4]  [ 940/1251]  eta: 0:04:53  lr: 0.000012  loss: 3.7090 (3.5119)  time: 0.9237  data: 0.0005  max mem: 19734
Epoch: [4]  [ 950/1251]  eta: 0:04:44  lr: 0.000012  loss: 3.6589 (3.5112)  time: 0.9554  data: 0.0005  max mem: 19734
Epoch: [4]  [ 960/1251]  eta: 0:04:35  lr: 0.000012  loss: 3.6374 (3.5097)  time: 0.9733  data: 0.0004  max mem: 19734
Epoch: [4]  [ 970/1251]  eta: 0:04:25  lr: 0.000012  loss: 3.7025 (3.5107)  time: 0.9659  data: 0.0003  max mem: 19734
Epoch: [4]  [ 980/1251]  eta: 0:04:16  lr: 0.000012  loss: 3.7025 (3.5115)  time: 0.9673  data: 0.0005  max mem: 19734
Epoch: [4]  [ 990/1251]  eta: 0:04:06  lr: 0.000012  loss: 3.4483 (3.5109)  time: 0.9415  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5348, ratio_loss=0.0036, pruning_loss=0.2158, mse_loss=1.0930
Epoch: [4]  [1000/1251]  eta: 0:03:57  lr: 0.000012  loss: 3.5978 (3.5114)  time: 0.9251  data: 0.0005  max mem: 19734
Epoch: [4]  [1010/1251]  eta: 0:03:47  lr: 0.000012  loss: 3.6781 (3.5134)  time: 0.9264  data: 0.0005  max mem: 19734
Epoch: [4]  [1020/1251]  eta: 0:03:38  lr: 0.000012  loss: 3.7641 (3.5125)  time: 0.9220  data: 0.0005  max mem: 19734
Epoch: [4]  [1030/1251]  eta: 0:03:28  lr: 0.000012  loss: 3.1746 (3.5095)  time: 0.9361  data: 0.0005  max mem: 19734
Epoch: [4]  [1040/1251]  eta: 0:03:19  lr: 0.000012  loss: 3.3922 (3.5089)  time: 0.9445  data: 0.0004  max mem: 19734
Epoch: [4]  [1050/1251]  eta: 0:03:09  lr: 0.000012  loss: 3.4700 (3.5086)  time: 0.9379  data: 0.0004  max mem: 19734
Epoch: [4]  [1060/1251]  eta: 0:03:00  lr: 0.000012  loss: 3.6690 (3.5093)  time: 0.9531  data: 0.0003  max mem: 19734
Epoch: [4]  [1070/1251]  eta: 0:02:51  lr: 0.000012  loss: 3.6772 (3.5107)  time: 0.9661  data: 0.0004  max mem: 19734
Epoch: [4]  [1080/1251]  eta: 0:02:41  lr: 0.000012  loss: 3.6300 (3.5107)  time: 0.9419  data: 0.0004  max mem: 19734
Epoch: [4]  [1090/1251]  eta: 0:02:32  lr: 0.000012  loss: 3.6117 (3.5108)  time: 0.9224  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4754, ratio_loss=0.0037, pruning_loss=0.2160, mse_loss=1.0729
Epoch: [4]  [1100/1251]  eta: 0:02:22  lr: 0.000012  loss: 3.5320 (3.5110)  time: 0.9206  data: 0.0004  max mem: 19734
Epoch: [4]  [1110/1251]  eta: 0:02:13  lr: 0.000012  loss: 3.2381 (3.5078)  time: 0.9300  data: 0.0004  max mem: 19734
Epoch: [4]  [1120/1251]  eta: 0:02:03  lr: 0.000012  loss: 3.3168 (3.5073)  time: 0.9433  data: 0.0005  max mem: 19734
Epoch: [4]  [1130/1251]  eta: 0:01:54  lr: 0.000012  loss: 3.4749 (3.5068)  time: 0.9341  data: 0.0005  max mem: 19734
Epoch: [4]  [1140/1251]  eta: 0:01:44  lr: 0.000012  loss: 3.2440 (3.5035)  time: 0.9192  data: 0.0005  max mem: 19734
Epoch: [4]  [1150/1251]  eta: 0:01:35  lr: 0.000012  loss: 3.2973 (3.5034)  time: 0.9158  data: 0.0004  max mem: 19734
Epoch: [4]  [1160/1251]  eta: 0:01:25  lr: 0.000012  loss: 3.5459 (3.5022)  time: 0.9191  data: 0.0005  max mem: 19734
Epoch: [4]  [1170/1251]  eta: 0:01:16  lr: 0.000012  loss: 3.5479 (3.5040)  time: 0.9218  data: 0.0005  max mem: 19734
Epoch: [4]  [1180/1251]  eta: 0:01:06  lr: 0.000012  loss: 3.5985 (3.5034)  time: 0.9212  data: 0.0004  max mem: 19734
Epoch: [4]  [1190/1251]  eta: 0:00:57  lr: 0.000012  loss: 3.5806 (3.5032)  time: 0.9194  data: 0.0012  max mem: 19734
loss info: cls_loss=3.4007, ratio_loss=0.0037, pruning_loss=0.2203, mse_loss=1.0729
Epoch: [4]  [1200/1251]  eta: 0:00:48  lr: 0.000012  loss: 3.5882 (3.5042)  time: 0.9190  data: 0.0011  max mem: 19734
Epoch: [4]  [1210/1251]  eta: 0:00:38  lr: 0.000012  loss: 3.2947 (3.5001)  time: 0.9242  data: 0.0002  max mem: 19734
Epoch: [4]  [1220/1251]  eta: 0:00:29  lr: 0.000012  loss: 3.3211 (3.5006)  time: 0.9301  data: 0.0002  max mem: 19734
Epoch: [4]  [1230/1251]  eta: 0:00:19  lr: 0.000012  loss: 3.4617 (3.4994)  time: 0.9253  data: 0.0001  max mem: 19734
Epoch: [4]  [1240/1251]  eta: 0:00:10  lr: 0.000012  loss: 3.7557 (3.5007)  time: 0.9178  data: 0.0001  max mem: 19734
Epoch: [4]  [1250/1251]  eta: 0:00:00  lr: 0.000012  loss: 3.7640 (3.5031)  time: 0.9121  data: 0.0001  max mem: 19734
Epoch: [4] Total time: 0:19:39 (0.9431 s / it)
Averaged stats: lr: 0.000012  loss: 3.7640 (3.5008)
Test:  [  0/261]  eta: 2:35:14  loss: 0.7255 (0.7255)  acc1: 82.8125 (82.8125)  acc5: 96.3542 (96.3542)  time: 35.6879  data: 35.0026  max mem: 19734
Test:  [ 10/261]  eta: 0:15:19  loss: 0.7255 (0.7742)  acc1: 85.4167 (83.6648)  acc5: 96.3542 (95.9280)  time: 3.6629  data: 3.2433  max mem: 19734
Test:  [ 20/261]  eta: 0:08:21  loss: 1.0053 (0.9284)  acc1: 78.1250 (78.7450)  acc5: 94.2708 (94.6925)  time: 0.4013  data: 0.0424  max mem: 19734
Test:  [ 30/261]  eta: 0:05:43  loss: 0.8076 (0.8402)  acc1: 83.3333 (81.6700)  acc5: 94.7917 (95.2117)  time: 0.2888  data: 0.0160  max mem: 19734
Test:  [ 40/261]  eta: 0:05:17  loss: 0.5950 (0.8121)  acc1: 88.0208 (82.7871)  acc5: 96.3542 (95.3379)  time: 0.7552  data: 0.5050  max mem: 19734
Test:  [ 50/261]  eta: 0:04:18  loss: 0.9831 (0.8782)  acc1: 77.0833 (80.9130)  acc5: 94.2708 (94.8223)  time: 0.8151  data: 0.5082  max mem: 19734
Test:  [ 60/261]  eta: 0:03:39  loss: 1.0290 (0.8917)  acc1: 76.0417 (80.4474)  acc5: 94.2708 (94.8685)  time: 0.3958  data: 0.0169  max mem: 19734
Test:  [ 70/261]  eta: 0:03:12  loss: 0.9828 (0.8948)  acc1: 76.0417 (79.8636)  acc5: 95.3125 (95.1218)  time: 0.4519  data: 0.0137  max mem: 19734
Test:  [ 80/261]  eta: 0:02:50  loss: 0.8588 (0.8950)  acc1: 78.6458 (80.0476)  acc5: 96.8750 (95.2032)  time: 0.4723  data: 0.1194  max mem: 19734
Test:  [ 90/261]  eta: 0:02:29  loss: 0.8578 (0.8813)  acc1: 83.8542 (80.4258)  acc5: 95.8333 (95.3068)  time: 0.4021  data: 0.1166  max mem: 19734
Test:  [100/261]  eta: 0:02:28  loss: 0.8578 (0.8859)  acc1: 83.3333 (80.3991)  acc5: 95.3125 (95.3434)  time: 0.8402  data: 0.5500  max mem: 19734
Test:  [110/261]  eta: 0:02:12  loss: 0.9083 (0.9111)  acc1: 75.5208 (79.9033)  acc5: 94.2708 (94.9794)  time: 0.8955  data: 0.5571  max mem: 19734
Test:  [120/261]  eta: 0:01:58  loss: 1.2486 (0.9504)  acc1: 71.8750 (79.0849)  acc5: 89.5833 (94.4731)  time: 0.4433  data: 0.0402  max mem: 19734
Test:  [130/261]  eta: 0:01:44  loss: 1.3994 (0.9976)  acc1: 67.7083 (78.0813)  acc5: 87.5000 (93.8812)  time: 0.3814  data: 0.0377  max mem: 19734
Test:  [140/261]  eta: 0:01:32  loss: 1.3518 (1.0241)  acc1: 68.7500 (77.3862)  acc5: 89.0625 (93.6170)  time: 0.3147  data: 0.0705  max mem: 19734
Test:  [150/261]  eta: 0:01:21  loss: 1.2792 (1.0327)  acc1: 71.8750 (77.3524)  acc5: 91.1458 (93.4499)  time: 0.2619  data: 0.0702  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: 1.0984 (1.0555)  acc1: 76.5625 (76.9992)  acc5: 91.1458 (93.1224)  time: 0.1938  data: 0.0208  max mem: 19734
Test:  [170/261]  eta: 0:01:00  loss: 1.4005 (1.0885)  acc1: 64.5833 (76.1513)  acc5: 86.9792 (92.7632)  time: 0.1665  data: 0.0141  max mem: 19734
Test:  [180/261]  eta: 0:00:51  loss: 1.5281 (1.1064)  acc1: 65.1042 (75.7453)  acc5: 87.5000 (92.5904)  time: 0.1488  data: 0.0021  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: 1.4482 (1.1201)  acc1: 67.7083 (75.4745)  acc5: 90.1042 (92.4302)  time: 0.1472  data: 0.0011  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.4046 (1.1345)  acc1: 69.7917 (75.1399)  acc5: 89.0625 (92.2316)  time: 0.1659  data: 0.0011  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.4374 (1.1501)  acc1: 68.7500 (74.8988)  acc5: 88.5417 (92.0295)  time: 0.1642  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.4841 (1.1699)  acc1: 67.1875 (74.4203)  acc5: 88.5417 (91.8293)  time: 0.1448  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.5170 (1.1804)  acc1: 64.5833 (74.2244)  acc5: 88.0208 (91.6892)  time: 0.1424  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.3961 (1.1898)  acc1: 68.7500 (73.9735)  acc5: 89.0625 (91.5997)  time: 0.1425  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.0559 (1.1805)  acc1: 76.0417 (74.1741)  acc5: 92.7083 (91.7331)  time: 0.1426  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 0.9403 (1.1792)  acc1: 76.0417 (74.1880)  acc5: 95.3125 (91.8080)  time: 0.1386  data: 0.0001  max mem: 19734
Test: Total time: 0:02:07 (0.4870 s / it)
* Acc@1 74.188 Acc@5 91.808 loss 1.179
Accuracy of the network on the 50000 test images: 74.2%
Max accuracy: 74.19%
Epoch: [5]  [   0/1251]  eta: 6:15:08  lr: 0.000016  loss: 3.6150 (3.6150)  time: 17.9920  data: 17.1434  max mem: 19734
Epoch: [5]  [  10/1251]  eta: 0:49:52  lr: 0.000016  loss: 3.7738 (3.7351)  time: 2.4114  data: 1.5593  max mem: 19734
Epoch: [5]  [  20/1251]  eta: 0:34:14  lr: 0.000016  loss: 3.7707 (3.6454)  time: 0.8529  data: 0.0007  max mem: 19734
Epoch: [5]  [  30/1251]  eta: 0:28:35  lr: 0.000016  loss: 3.6574 (3.5945)  time: 0.8510  data: 0.0005  max mem: 19734
Epoch: [5]  [  40/1251]  eta: 0:25:35  lr: 0.000016  loss: 3.6574 (3.6480)  time: 0.8471  data: 0.0006  max mem: 19734
loss info: cls_loss=3.5272, ratio_loss=0.0043, pruning_loss=0.2148, mse_loss=1.0627
Epoch: [5]  [  50/1251]  eta: 0:23:53  lr: 0.000016  loss: 3.7352 (3.6653)  time: 0.8660  data: 0.0006  max mem: 19734
Epoch: [5]  [  60/1251]  eta: 0:22:45  lr: 0.000016  loss: 3.7352 (3.6593)  time: 0.8968  data: 0.0004  max mem: 19734
Epoch: [5]  [  70/1251]  eta: 0:21:49  lr: 0.000016  loss: 3.7952 (3.6573)  time: 0.8941  data: 0.0005  max mem: 19734
Epoch: [5]  [  80/1251]  eta: 0:21:04  lr: 0.000016  loss: 3.7201 (3.6385)  time: 0.8759  data: 0.0006  max mem: 19734
Epoch: [5]  [  90/1251]  eta: 0:20:29  lr: 0.000016  loss: 3.5620 (3.6311)  time: 0.8817  data: 0.0007  max mem: 19734
Epoch: [5]  [ 100/1251]  eta: 0:20:01  lr: 0.000016  loss: 3.4742 (3.5909)  time: 0.9014  data: 0.0005  max mem: 19734
Epoch: [5]  [ 110/1251]  eta: 0:19:37  lr: 0.000016  loss: 3.5556 (3.5905)  time: 0.9065  data: 0.0005  max mem: 19734
Epoch: [5]  [ 120/1251]  eta: 0:19:14  lr: 0.000016  loss: 3.7638 (3.5955)  time: 0.9019  data: 0.0005  max mem: 19734
Epoch: [5]  [ 130/1251]  eta: 0:18:54  lr: 0.000016  loss: 3.7060 (3.5899)  time: 0.9009  data: 0.0005  max mem: 19734
Epoch: [5]  [ 140/1251]  eta: 0:18:36  lr: 0.000016  loss: 3.4671 (3.5812)  time: 0.9062  data: 0.0005  max mem: 19734
loss info: cls_loss=3.5221, ratio_loss=0.0036, pruning_loss=0.2130, mse_loss=1.0235
Epoch: [5]  [ 150/1251]  eta: 0:18:20  lr: 0.000016  loss: 3.6514 (3.5798)  time: 0.9232  data: 0.0005  max mem: 19734
Epoch: [5]  [ 160/1251]  eta: 0:18:05  lr: 0.000016  loss: 3.6551 (3.5767)  time: 0.9240  data: 0.0005  max mem: 19734
Epoch: [5]  [ 170/1251]  eta: 0:17:51  lr: 0.000016  loss: 3.6582 (3.5805)  time: 0.9225  data: 0.0005  max mem: 19734
Epoch: [5]  [ 180/1251]  eta: 0:17:36  lr: 0.000016  loss: 3.4437 (3.5656)  time: 0.9247  data: 0.0005  max mem: 19734
Epoch: [5]  [ 190/1251]  eta: 0:17:23  lr: 0.000016  loss: 3.3319 (3.5571)  time: 0.9169  data: 0.0004  max mem: 19734
Epoch: [5]  [ 200/1251]  eta: 0:17:10  lr: 0.000016  loss: 3.5687 (3.5448)  time: 0.9229  data: 0.0004  max mem: 19734
Epoch: [5]  [ 210/1251]  eta: 0:16:57  lr: 0.000016  loss: 3.4937 (3.5296)  time: 0.9269  data: 0.0005  max mem: 19734
Epoch: [5]  [ 220/1251]  eta: 0:16:46  lr: 0.000016  loss: 3.1883 (3.5186)  time: 0.9311  data: 0.0005  max mem: 19734
Epoch: [5]  [ 230/1251]  eta: 0:16:34  lr: 0.000016  loss: 3.4041 (3.5135)  time: 0.9353  data: 0.0006  max mem: 19734
Epoch: [5]  [ 240/1251]  eta: 0:16:22  lr: 0.000016  loss: 3.5835 (3.5121)  time: 0.9304  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3472, ratio_loss=0.0042, pruning_loss=0.2178, mse_loss=1.0724
Epoch: [5]  [ 250/1251]  eta: 0:16:10  lr: 0.000016  loss: 3.6745 (3.5049)  time: 0.9210  data: 0.0004  max mem: 19734
Epoch: [5]  [ 260/1251]  eta: 0:15:59  lr: 0.000016  loss: 3.6835 (3.5111)  time: 0.9185  data: 0.0005  max mem: 19734
Epoch: [5]  [ 270/1251]  eta: 0:15:48  lr: 0.000016  loss: 3.6177 (3.5084)  time: 0.9302  data: 0.0006  max mem: 19734
Epoch: [5]  [ 280/1251]  eta: 0:15:39  lr: 0.000016  loss: 3.6380 (3.5091)  time: 0.9578  data: 0.0005  max mem: 19734
Epoch: [5]  [ 290/1251]  eta: 0:15:27  lr: 0.000016  loss: 3.6380 (3.5133)  time: 0.9472  data: 0.0006  max mem: 19734
Epoch: [5]  [ 300/1251]  eta: 0:15:17  lr: 0.000016  loss: 3.5965 (3.5123)  time: 0.9302  data: 0.0005  max mem: 19734
Epoch: [5]  [ 310/1251]  eta: 0:15:06  lr: 0.000016  loss: 3.5558 (3.5120)  time: 0.9321  data: 0.0005  max mem: 19734
Epoch: [5]  [ 320/1251]  eta: 0:14:56  lr: 0.000016  loss: 3.5745 (3.5113)  time: 0.9281  data: 0.0004  max mem: 19734
Epoch: [5]  [ 330/1251]  eta: 0:14:45  lr: 0.000016  loss: 3.6052 (3.5081)  time: 0.9298  data: 0.0004  max mem: 19734
Epoch: [5]  [ 340/1251]  eta: 0:14:35  lr: 0.000016  loss: 3.6350 (3.5117)  time: 0.9354  data: 0.0004  max mem: 19734
loss info: cls_loss=3.5234, ratio_loss=0.0036, pruning_loss=0.2102, mse_loss=1.0251
Epoch: [5]  [ 350/1251]  eta: 0:14:25  lr: 0.000016  loss: 3.5562 (3.5041)  time: 0.9539  data: 0.0004  max mem: 19734
Epoch: [5]  [ 360/1251]  eta: 0:14:17  lr: 0.000016  loss: 3.5230 (3.5036)  time: 0.9763  data: 0.0005  max mem: 19734
Epoch: [5]  [ 370/1251]  eta: 0:14:07  lr: 0.000016  loss: 3.6179 (3.5117)  time: 0.9711  data: 0.0004  max mem: 19734
Epoch: [5]  [ 380/1251]  eta: 0:13:57  lr: 0.000016  loss: 3.6854 (3.5080)  time: 0.9482  data: 0.0004  max mem: 19734
Epoch: [5]  [ 390/1251]  eta: 0:13:47  lr: 0.000016  loss: 3.5881 (3.5082)  time: 0.9421  data: 0.0005  max mem: 19734
Epoch: [5]  [ 400/1251]  eta: 0:13:37  lr: 0.000016  loss: 3.4037 (3.5021)  time: 0.9421  data: 0.0004  max mem: 19734
Epoch: [5]  [ 410/1251]  eta: 0:13:27  lr: 0.000016  loss: 3.4037 (3.5008)  time: 0.9506  data: 0.0005  max mem: 19734
Epoch: [5]  [ 420/1251]  eta: 0:13:17  lr: 0.000016  loss: 3.4879 (3.4948)  time: 0.9374  data: 0.0004  max mem: 19734
Epoch: [5]  [ 430/1251]  eta: 0:13:06  lr: 0.000016  loss: 3.2459 (3.4881)  time: 0.9214  data: 0.0004  max mem: 19734
Epoch: [5]  [ 440/1251]  eta: 0:12:56  lr: 0.000016  loss: 3.4047 (3.4853)  time: 0.9329  data: 0.0004  max mem: 19734
loss info: cls_loss=3.3804, ratio_loss=0.0041, pruning_loss=0.2154, mse_loss=1.0971
Epoch: [5]  [ 450/1251]  eta: 0:12:46  lr: 0.000016  loss: 3.7276 (3.4913)  time: 0.9400  data: 0.0005  max mem: 19734
Epoch: [5]  [ 460/1251]  eta: 0:12:36  lr: 0.000016  loss: 3.6426 (3.4941)  time: 0.9284  data: 0.0005  max mem: 19734
Epoch: [5]  [ 470/1251]  eta: 0:12:27  lr: 0.000016  loss: 3.6426 (3.4985)  time: 0.9354  data: 0.0005  max mem: 19734
Epoch: [5]  [ 480/1251]  eta: 0:12:17  lr: 0.000016  loss: 3.7020 (3.4988)  time: 0.9482  data: 0.0005  max mem: 19734
Epoch: [5]  [ 490/1251]  eta: 0:12:07  lr: 0.000016  loss: 3.3872 (3.4930)  time: 0.9505  data: 0.0006  max mem: 19734
Epoch: [5]  [ 500/1251]  eta: 0:11:57  lr: 0.000016  loss: 3.3872 (3.4923)  time: 0.9504  data: 0.0007  max mem: 19734
Epoch: [5]  [ 510/1251]  eta: 0:11:48  lr: 0.000016  loss: 3.4444 (3.4885)  time: 0.9369  data: 0.0007  max mem: 19734
Epoch: [5]  [ 520/1251]  eta: 0:11:37  lr: 0.000016  loss: 3.2711 (3.4901)  time: 0.9250  data: 0.0005  max mem: 19734
Epoch: [5]  [ 530/1251]  eta: 0:11:28  lr: 0.000016  loss: 3.7998 (3.4911)  time: 0.9236  data: 0.0005  max mem: 19734
Epoch: [5]  [ 540/1251]  eta: 0:11:17  lr: 0.000016  loss: 3.6945 (3.4961)  time: 0.9204  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4719, ratio_loss=0.0042, pruning_loss=0.2131, mse_loss=1.0650
Epoch: [5]  [ 550/1251]  eta: 0:11:08  lr: 0.000016  loss: 3.6263 (3.4896)  time: 0.9389  data: 0.0004  max mem: 19734
Epoch: [5]  [ 560/1251]  eta: 0:10:58  lr: 0.000016  loss: 3.5397 (3.4896)  time: 0.9499  data: 0.0005  max mem: 19734
Epoch: [5]  [ 570/1251]  eta: 0:10:49  lr: 0.000016  loss: 3.5397 (3.4902)  time: 0.9350  data: 0.0005  max mem: 19734
Epoch: [5]  [ 580/1251]  eta: 0:10:39  lr: 0.000016  loss: 3.4589 (3.4875)  time: 0.9252  data: 0.0004  max mem: 19734
Epoch: [5]  [ 590/1251]  eta: 0:10:29  lr: 0.000016  loss: 3.5263 (3.4895)  time: 0.9330  data: 0.0004  max mem: 19734
Epoch: [5]  [ 600/1251]  eta: 0:10:20  lr: 0.000016  loss: 3.3219 (3.4829)  time: 0.9572  data: 0.0004  max mem: 19734
Epoch: [5]  [ 610/1251]  eta: 0:10:10  lr: 0.000016  loss: 3.1573 (3.4824)  time: 0.9387  data: 0.0004  max mem: 19734
Epoch: [5]  [ 620/1251]  eta: 0:10:00  lr: 0.000016  loss: 3.4824 (3.4856)  time: 0.9126  data: 0.0004  max mem: 19734
Epoch: [5]  [ 630/1251]  eta: 0:09:50  lr: 0.000016  loss: 3.7410 (3.4845)  time: 0.9235  data: 0.0004  max mem: 19734
Epoch: [5]  [ 640/1251]  eta: 0:09:40  lr: 0.000016  loss: 3.6561 (3.4844)  time: 0.9240  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4087, ratio_loss=0.0044, pruning_loss=0.2144, mse_loss=1.0099
Epoch: [5]  [ 650/1251]  eta: 0:09:31  lr: 0.000016  loss: 3.4867 (3.4790)  time: 0.9236  data: 0.0006  max mem: 19734
Epoch: [5]  [ 660/1251]  eta: 0:09:21  lr: 0.000016  loss: 3.2972 (3.4797)  time: 0.9357  data: 0.0005  max mem: 19734
Epoch: [5]  [ 670/1251]  eta: 0:09:11  lr: 0.000016  loss: 3.6384 (3.4782)  time: 0.9226  data: 0.0005  max mem: 19734
Epoch: [5]  [ 680/1251]  eta: 0:09:01  lr: 0.000016  loss: 3.5349 (3.4817)  time: 0.9087  data: 0.0005  max mem: 19734
Epoch: [5]  [ 690/1251]  eta: 0:08:52  lr: 0.000016  loss: 3.4240 (3.4771)  time: 0.9202  data: 0.0005  max mem: 19734
Epoch: [5]  [ 700/1251]  eta: 0:08:42  lr: 0.000016  loss: 3.2255 (3.4736)  time: 0.9269  data: 0.0005  max mem: 19734
Epoch: [5]  [ 710/1251]  eta: 0:08:32  lr: 0.000016  loss: 3.5139 (3.4743)  time: 0.9333  data: 0.0005  max mem: 19734
Epoch: [5]  [ 720/1251]  eta: 0:08:23  lr: 0.000016  loss: 3.5742 (3.4730)  time: 0.9296  data: 0.0005  max mem: 19734
Epoch: [5]  [ 730/1251]  eta: 0:08:13  lr: 0.000016  loss: 3.3660 (3.4726)  time: 0.9209  data: 0.0005  max mem: 19734
Epoch: [5]  [ 740/1251]  eta: 0:08:03  lr: 0.000016  loss: 3.4282 (3.4705)  time: 0.9140  data: 0.0006  max mem: 19734
loss info: cls_loss=3.3427, ratio_loss=0.0044, pruning_loss=0.2153, mse_loss=1.0334
Epoch: [5]  [ 750/1251]  eta: 0:07:54  lr: 0.000016  loss: 3.0451 (3.4649)  time: 0.9128  data: 0.0005  max mem: 19734
Epoch: [5]  [ 760/1251]  eta: 0:07:44  lr: 0.000016  loss: 3.3674 (3.4663)  time: 0.9120  data: 0.0005  max mem: 19734
Epoch: [5]  [ 770/1251]  eta: 0:07:34  lr: 0.000016  loss: 3.6879 (3.4659)  time: 0.9041  data: 0.0004  max mem: 19734
Epoch: [5]  [ 780/1251]  eta: 0:07:25  lr: 0.000016  loss: 3.6268 (3.4675)  time: 0.9138  data: 0.0004  max mem: 19734
Epoch: [5]  [ 790/1251]  eta: 0:07:15  lr: 0.000016  loss: 3.5344 (3.4638)  time: 0.9350  data: 0.0004  max mem: 19734
Epoch: [5]  [ 800/1251]  eta: 0:07:06  lr: 0.000016  loss: 3.5107 (3.4627)  time: 0.9557  data: 0.0004  max mem: 19734
Epoch: [5]  [ 810/1251]  eta: 0:06:56  lr: 0.000016  loss: 3.7530 (3.4659)  time: 0.9283  data: 0.0004  max mem: 19734
Epoch: [5]  [ 820/1251]  eta: 0:06:46  lr: 0.000016  loss: 3.5652 (3.4644)  time: 0.9041  data: 0.0004  max mem: 19734
Epoch: [5]  [ 830/1251]  eta: 0:06:37  lr: 0.000016  loss: 3.1414 (3.4615)  time: 0.9083  data: 0.0004  max mem: 19734
Epoch: [5]  [ 840/1251]  eta: 0:06:27  lr: 0.000016  loss: 3.4387 (3.4631)  time: 0.9170  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4071, ratio_loss=0.0045, pruning_loss=0.2155, mse_loss=1.0871
Epoch: [5]  [ 850/1251]  eta: 0:06:18  lr: 0.000016  loss: 3.5456 (3.4640)  time: 0.9257  data: 0.0005  max mem: 19734
Epoch: [5]  [ 860/1251]  eta: 0:06:08  lr: 0.000016  loss: 3.4270 (3.4626)  time: 0.9113  data: 0.0005  max mem: 19734
Epoch: [5]  [ 870/1251]  eta: 0:05:59  lr: 0.000016  loss: 3.4395 (3.4635)  time: 0.9147  data: 0.0005  max mem: 19734
Epoch: [5]  [ 880/1251]  eta: 0:05:49  lr: 0.000016  loss: 3.5787 (3.4642)  time: 0.9207  data: 0.0004  max mem: 19734
Epoch: [5]  [ 890/1251]  eta: 0:05:40  lr: 0.000016  loss: 3.5787 (3.4650)  time: 0.9166  data: 0.0004  max mem: 19734
Epoch: [5]  [ 900/1251]  eta: 0:05:30  lr: 0.000016  loss: 3.8853 (3.4699)  time: 0.9131  data: 0.0004  max mem: 19734
Epoch: [5]  [ 910/1251]  eta: 0:05:21  lr: 0.000016  loss: 3.8688 (3.4713)  time: 0.9098  data: 0.0004  max mem: 19734
Epoch: [5]  [ 920/1251]  eta: 0:05:11  lr: 0.000016  loss: 3.8125 (3.4745)  time: 0.9112  data: 0.0004  max mem: 19734
Epoch: [5]  [ 930/1251]  eta: 0:05:02  lr: 0.000016  loss: 3.6697 (3.4705)  time: 0.9175  data: 0.0004  max mem: 19734
Epoch: [5]  [ 940/1251]  eta: 0:04:52  lr: 0.000016  loss: 3.3885 (3.4702)  time: 0.9237  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4914, ratio_loss=0.0043, pruning_loss=0.2112, mse_loss=1.0110
Epoch: [5]  [ 950/1251]  eta: 0:04:43  lr: 0.000016  loss: 3.3970 (3.4702)  time: 0.9230  data: 0.0004  max mem: 19734
Epoch: [5]  [ 960/1251]  eta: 0:04:33  lr: 0.000016  loss: 3.4664 (3.4692)  time: 0.9273  data: 0.0004  max mem: 19734
Epoch: [5]  [ 970/1251]  eta: 0:04:24  lr: 0.000016  loss: 3.5157 (3.4705)  time: 0.9136  data: 0.0004  max mem: 19734
Epoch: [5]  [ 980/1251]  eta: 0:04:14  lr: 0.000016  loss: 3.5971 (3.4707)  time: 0.8982  data: 0.0004  max mem: 19734
Epoch: [5]  [ 990/1251]  eta: 0:04:05  lr: 0.000016  loss: 3.6470 (3.4721)  time: 0.8997  data: 0.0003  max mem: 19734
Epoch: [5]  [1000/1251]  eta: 0:03:55  lr: 0.000016  loss: 3.6470 (3.4715)  time: 0.8996  data: 0.0004  max mem: 19734
Epoch: [5]  [1010/1251]  eta: 0:03:46  lr: 0.000016  loss: 3.7525 (3.4746)  time: 0.9000  data: 0.0004  max mem: 19734
Epoch: [5]  [1020/1251]  eta: 0:03:36  lr: 0.000016  loss: 3.7525 (3.4747)  time: 0.9138  data: 0.0003  max mem: 19734
Epoch: [5]  [1030/1251]  eta: 0:03:27  lr: 0.000016  loss: 3.1260 (3.4718)  time: 0.9163  data: 0.0003  max mem: 19734
Epoch: [5]  [1040/1251]  eta: 0:03:17  lr: 0.000016  loss: 3.3629 (3.4718)  time: 0.9147  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4491, ratio_loss=0.0044, pruning_loss=0.2085, mse_loss=1.0064
Epoch: [5]  [1050/1251]  eta: 0:03:08  lr: 0.000016  loss: 3.5029 (3.4730)  time: 0.9151  data: 0.0003  max mem: 19734
Epoch: [5]  [1060/1251]  eta: 0:02:59  lr: 0.000016  loss: 3.5029 (3.4731)  time: 0.9053  data: 0.0003  max mem: 19734
Epoch: [5]  [1070/1251]  eta: 0:02:49  lr: 0.000016  loss: 3.4049 (3.4726)  time: 0.9099  data: 0.0004  max mem: 19734
Epoch: [5]  [1080/1251]  eta: 0:02:40  lr: 0.000016  loss: 3.3475 (3.4708)  time: 0.9165  data: 0.0004  max mem: 19734
Epoch: [5]  [1090/1251]  eta: 0:02:30  lr: 0.000016  loss: 3.5684 (3.4737)  time: 0.9256  data: 0.0003  max mem: 19734
Epoch: [5]  [1100/1251]  eta: 0:02:21  lr: 0.000016  loss: 3.6532 (3.4739)  time: 0.9347  data: 0.0003  max mem: 19734
Epoch: [5]  [1110/1251]  eta: 0:02:12  lr: 0.000016  loss: 3.6681 (3.4741)  time: 0.9232  data: 0.0003  max mem: 19734
Epoch: [5]  [1120/1251]  eta: 0:02:02  lr: 0.000016  loss: 3.6681 (3.4731)  time: 0.9206  data: 0.0003  max mem: 19734
Epoch: [5]  [1130/1251]  eta: 0:01:53  lr: 0.000016  loss: 3.4418 (3.4730)  time: 0.9193  data: 0.0004  max mem: 19734
Epoch: [5]  [1140/1251]  eta: 0:01:43  lr: 0.000016  loss: 3.5591 (3.4735)  time: 0.9096  data: 0.0004  max mem: 19734
loss info: cls_loss=3.4424, ratio_loss=0.0045, pruning_loss=0.2113, mse_loss=0.9916
Epoch: [5]  [1150/1251]  eta: 0:01:34  lr: 0.000016  loss: 3.4141 (3.4715)  time: 0.9095  data: 0.0004  max mem: 19734
Epoch: [5]  [1160/1251]  eta: 0:01:25  lr: 0.000016  loss: 3.4595 (3.4714)  time: 0.9057  data: 0.0004  max mem: 19734
Epoch: [5]  [1170/1251]  eta: 0:01:15  lr: 0.000016  loss: 3.5973 (3.4732)  time: 0.9119  data: 0.0004  max mem: 19734
Epoch: [5]  [1180/1251]  eta: 0:01:06  lr: 0.000016  loss: 3.5062 (3.4717)  time: 0.9164  data: 0.0004  max mem: 19734
Epoch: [5]  [1190/1251]  eta: 0:00:57  lr: 0.000016  loss: 3.3613 (3.4692)  time: 0.9111  data: 0.0006  max mem: 19734
Epoch: [5]  [1200/1251]  eta: 0:00:47  lr: 0.000016  loss: 3.4340 (3.4693)  time: 0.9100  data: 0.0005  max mem: 19734
Epoch: [5]  [1210/1251]  eta: 0:00:38  lr: 0.000016  loss: 3.6691 (3.4704)  time: 0.9090  data: 0.0001  max mem: 19734
Epoch: [5]  [1220/1251]  eta: 0:00:28  lr: 0.000016  loss: 3.8237 (3.4715)  time: 0.9086  data: 0.0001  max mem: 19734
Epoch: [5]  [1230/1251]  eta: 0:00:19  lr: 0.000016  loss: 3.4594 (3.4714)  time: 0.9139  data: 0.0001  max mem: 19734
Epoch: [5]  [1240/1251]  eta: 0:00:10  lr: 0.000016  loss: 3.3924 (3.4700)  time: 0.9115  data: 0.0001  max mem: 19734
loss info: cls_loss=3.4176, ratio_loss=0.0045, pruning_loss=0.2100, mse_loss=1.0243
Epoch: [5]  [1250/1251]  eta: 0:00:00  lr: 0.000016  loss: 3.5074 (3.4697)  time: 0.9064  data: 0.0001  max mem: 19734
Epoch: [5] Total time: 0:19:29 (0.9346 s / it)
Averaged stats: lr: 0.000016  loss: 3.5074 (3.4757)
Test:  [  0/261]  eta: 1:59:12  loss: 0.7472 (0.7472)  acc1: 83.3333 (83.3333)  acc5: 96.8750 (96.8750)  time: 27.4024  data: 27.2143  max mem: 19734
Test:  [ 10/261]  eta: 0:15:12  loss: 0.7472 (0.7933)  acc1: 85.4167 (83.5701)  acc5: 96.3542 (95.8807)  time: 3.6344  data: 3.2712  max mem: 19734
Test:  [ 20/261]  eta: 0:08:12  loss: 0.9556 (0.9559)  acc1: 78.6458 (78.7202)  acc5: 93.7500 (94.5685)  time: 0.7745  data: 0.4451  max mem: 19734
Test:  [ 30/261]  eta: 0:05:42  loss: 0.8526 (0.8723)  acc1: 81.7708 (81.5524)  acc5: 94.7917 (95.0437)  time: 0.2988  data: 0.0168  max mem: 19734
Test:  [ 40/261]  eta: 0:04:25  loss: 0.6543 (0.8381)  acc1: 87.5000 (82.5838)  acc5: 96.3542 (95.2744)  time: 0.3164  data: 0.0486  max mem: 19734
Test:  [ 50/261]  eta: 0:03:31  loss: 0.9560 (0.9048)  acc1: 78.1250 (80.7190)  acc5: 94.7917 (94.7406)  time: 0.2596  data: 0.0407  max mem: 19734
Test:  [ 60/261]  eta: 0:02:54  loss: 1.0618 (0.9204)  acc1: 75.5208 (80.1315)  acc5: 93.2292 (94.7319)  time: 0.1861  data: 0.0047  max mem: 19734
Test:  [ 70/261]  eta: 0:03:05  loss: 0.9989 (0.9243)  acc1: 76.5625 (79.6655)  acc5: 95.3125 (94.9237)  time: 0.8942  data: 0.6840  max mem: 19734
Test:  [ 80/261]  eta: 0:02:41  loss: 0.8753 (0.9271)  acc1: 78.6458 (79.7840)  acc5: 96.3542 (95.0232)  time: 0.9665  data: 0.6935  max mem: 19734
Test:  [ 90/261]  eta: 0:02:22  loss: 0.8753 (0.9124)  acc1: 82.8125 (80.1740)  acc5: 95.8333 (95.1122)  time: 0.3312  data: 0.0172  max mem: 19734
Test:  [100/261]  eta: 0:02:05  loss: 0.9017 (0.9152)  acc1: 82.8125 (80.2599)  acc5: 94.2708 (95.1526)  time: 0.3080  data: 0.0107  max mem: 19734
Test:  [110/261]  eta: 0:01:56  loss: 0.9865 (0.9456)  acc1: 78.1250 (79.6969)  acc5: 93.2292 (94.7729)  time: 0.5118  data: 0.1352  max mem: 19734
Test:  [120/261]  eta: 0:01:43  loss: 1.3089 (0.9896)  acc1: 68.7500 (78.7276)  acc5: 88.5417 (94.1761)  time: 0.5139  data: 0.1391  max mem: 19734
Test:  [130/261]  eta: 0:01:32  loss: 1.5720 (1.0398)  acc1: 67.1875 (77.7393)  acc5: 86.4583 (93.5393)  time: 0.3080  data: 0.0592  max mem: 19734
Test:  [140/261]  eta: 0:01:26  loss: 1.4480 (1.0693)  acc1: 68.2292 (77.0575)  acc5: 89.0625 (93.2255)  time: 0.6058  data: 0.3273  max mem: 19734
Test:  [150/261]  eta: 0:01:19  loss: 1.3332 (1.0794)  acc1: 71.8750 (77.0212)  acc5: 90.1042 (93.0602)  time: 0.8201  data: 0.4953  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: 1.1983 (1.1006)  acc1: 75.0000 (76.6919)  acc5: 90.6250 (92.7860)  time: 0.5470  data: 0.2203  max mem: 19734
Test:  [170/261]  eta: 0:01:00  loss: 1.4692 (1.1342)  acc1: 64.5833 (75.8894)  acc5: 86.9792 (92.4464)  time: 0.2551  data: 0.0070  max mem: 19734
Test:  [180/261]  eta: 0:00:51  loss: 1.5785 (1.1529)  acc1: 63.5417 (75.4662)  acc5: 88.5417 (92.2537)  time: 0.1606  data: 0.0048  max mem: 19734
Test:  [190/261]  eta: 0:00:43  loss: 1.5315 (1.1681)  acc1: 65.1042 (75.1663)  acc5: 89.5833 (92.0757)  time: 0.1429  data: 0.0011  max mem: 19734
Test:  [200/261]  eta: 0:00:35  loss: 1.5170 (1.1831)  acc1: 69.7917 (74.8497)  acc5: 89.5833 (91.8532)  time: 0.1428  data: 0.0011  max mem: 19734
Test:  [210/261]  eta: 0:00:28  loss: 1.4548 (1.1980)  acc1: 68.2292 (74.5582)  acc5: 88.5417 (91.6593)  time: 0.1425  data: 0.0004  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: 1.5711 (1.2170)  acc1: 65.6250 (74.0903)  acc5: 85.9375 (91.4451)  time: 0.1423  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: 1.4612 (1.2265)  acc1: 66.1458 (73.8479)  acc5: 88.0208 (91.3307)  time: 0.1425  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: 1.4595 (1.2375)  acc1: 66.6667 (73.5477)  acc5: 89.5833 (91.2258)  time: 0.1425  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: 1.1342 (1.2289)  acc1: 73.4375 (73.7363)  acc5: 93.2292 (91.3533)  time: 0.1426  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: 1.0663 (1.2278)  acc1: 76.0417 (73.7500)  acc5: 94.7917 (91.4100)  time: 0.1389  data: 0.0001  max mem: 19734
Test: Total time: 0:02:06 (0.4846 s / it)
* Acc@1 73.750 Acc@5 91.410 loss 1.228
Accuracy of the network on the 50000 test images: 73.8%
Max accuracy: 74.19%
Epoch: [6]  [   0/1251]  eta: 6:03:01  lr: 0.000020  loss: 2.7794 (2.7794)  time: 17.4110  data: 9.6905  max mem: 19734
Epoch: [6]  [  10/1251]  eta: 0:52:13  lr: 0.000020  loss: 3.3381 (3.3367)  time: 2.5248  data: 0.9459  max mem: 19734
Epoch: [6]  [  20/1251]  eta: 0:35:13  lr: 0.000020  loss: 3.4199 (3.3505)  time: 0.9323  data: 0.0361  max mem: 19734
Epoch: [6]  [  30/1251]  eta: 0:29:18  lr: 0.000020  loss: 3.4958 (3.3581)  time: 0.8441  data: 0.0006  max mem: 19734
Epoch: [6]  [  40/1251]  eta: 0:26:15  lr: 0.000020  loss: 3.4958 (3.4073)  time: 0.8634  data: 0.0005  max mem: 19734
Epoch: [6]  [  50/1251]  eta: 0:24:23  lr: 0.000020  loss: 3.6066 (3.3719)  time: 0.8745  data: 0.0005  max mem: 19734
Epoch: [6]  [  60/1251]  eta: 0:23:02  lr: 0.000020  loss: 3.0311 (3.3168)  time: 0.8736  data: 0.0005  max mem: 19734
Epoch: [6]  [  70/1251]  eta: 0:22:06  lr: 0.000020  loss: 3.3568 (3.3428)  time: 0.8787  data: 0.0004  max mem: 19734
Epoch: [6]  [  80/1251]  eta: 0:21:19  lr: 0.000020  loss: 3.5650 (3.3764)  time: 0.8848  data: 0.0005  max mem: 19734
Epoch: [6]  [  90/1251]  eta: 0:20:47  lr: 0.000020  loss: 3.5650 (3.3762)  time: 0.9039  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3556, ratio_loss=0.0049, pruning_loss=0.2100, mse_loss=1.0176
Epoch: [6]  [ 100/1251]  eta: 0:20:15  lr: 0.000020  loss: 3.6567 (3.3918)  time: 0.9079  data: 0.0007  max mem: 19734
Epoch: [6]  [ 110/1251]  eta: 0:19:53  lr: 0.000020  loss: 3.6066 (3.3782)  time: 0.9133  data: 0.0007  max mem: 19734
Epoch: [6]  [ 120/1251]  eta: 0:19:29  lr: 0.000020  loss: 3.5085 (3.3872)  time: 0.9260  data: 0.0005  max mem: 19734
Epoch: [6]  [ 130/1251]  eta: 0:19:08  lr: 0.000020  loss: 3.5816 (3.3989)  time: 0.9092  data: 0.0005  max mem: 19734
Epoch: [6]  [ 140/1251]  eta: 0:18:49  lr: 0.000020  loss: 3.4953 (3.4035)  time: 0.9080  data: 0.0005  max mem: 19734
Epoch: [6]  [ 150/1251]  eta: 0:18:31  lr: 0.000020  loss: 3.4953 (3.4189)  time: 0.9104  data: 0.0005  max mem: 19734
Epoch: [6]  [ 160/1251]  eta: 0:18:15  lr: 0.000020  loss: 3.7499 (3.4315)  time: 0.9147  data: 0.0005  max mem: 19734
Epoch: [6]  [ 170/1251]  eta: 0:18:02  lr: 0.000020  loss: 3.5684 (3.4228)  time: 0.9364  data: 0.0005  max mem: 19734
Epoch: [6]  [ 180/1251]  eta: 0:17:47  lr: 0.000020  loss: 3.4679 (3.4268)  time: 0.9367  data: 0.0005  max mem: 19734
Epoch: [6]  [ 190/1251]  eta: 0:17:32  lr: 0.000020  loss: 3.5558 (3.4360)  time: 0.9160  data: 0.0005  max mem: 19734
loss info: cls_loss=3.4739, ratio_loss=0.0050, pruning_loss=0.2065, mse_loss=1.0106
Epoch: [6]  [ 200/1251]  eta: 0:17:18  lr: 0.000020  loss: 3.7262 (3.4513)  time: 0.9144  data: 0.0005  max mem: 19734
Epoch: [6]  [ 210/1251]  eta: 0:17:04  lr: 0.000020  loss: 3.5096 (3.4484)  time: 0.9114  data: 0.0005  max mem: 19734
Epoch: [6]  [ 220/1251]  eta: 0:16:51  lr: 0.000020  loss: 3.2151 (3.4258)  time: 0.9104  data: 0.0005  max mem: 19734
Epoch: [6]  [ 230/1251]  eta: 0:16:39  lr: 0.000020  loss: 3.3119 (3.4288)  time: 0.9165  data: 0.0005  max mem: 19734
Epoch: [6]  [ 240/1251]  eta: 0:16:26  lr: 0.000020  loss: 3.4355 (3.4306)  time: 0.9207  data: 0.0005  max mem: 19734
Epoch: [6]  [ 250/1251]  eta: 0:16:16  lr: 0.000020  loss: 3.4330 (3.4272)  time: 0.9369  data: 0.0005  max mem: 19734
Epoch: [6]  [ 260/1251]  eta: 0:16:05  lr: 0.000020  loss: 3.4198 (3.4293)  time: 0.9449  data: 0.0005  max mem: 19734
Epoch: [6]  [ 270/1251]  eta: 0:15:55  lr: 0.000020  loss: 3.6035 (3.4318)  time: 0.9546  data: 0.0005  max mem: 19734
Epoch: [6]  [ 280/1251]  eta: 0:15:43  lr: 0.000020  loss: 3.2743 (3.4297)  time: 0.9504  data: 0.0005  max mem: 19734
Epoch: [6]  [ 290/1251]  eta: 0:15:33  lr: 0.000020  loss: 3.4464 (3.4323)  time: 0.9352  data: 0.0005  max mem: 19734
loss info: cls_loss=3.3660, ratio_loss=0.0053, pruning_loss=0.2110, mse_loss=0.9941
Epoch: [6]  [ 300/1251]  eta: 0:15:22  lr: 0.000020  loss: 3.4688 (3.4338)  time: 0.9376  data: 0.0005  max mem: 19734
Epoch: [6]  [ 310/1251]  eta: 0:15:11  lr: 0.000020  loss: 3.5912 (3.4353)  time: 0.9271  data: 0.0005  max mem: 19734
Epoch: [6]  [ 320/1251]  eta: 0:15:00  lr: 0.000020  loss: 3.5361 (3.4298)  time: 0.9195  data: 0.0005  max mem: 19734
Epoch: [6]  [ 330/1251]  eta: 0:14:49  lr: 0.000020  loss: 3.5361 (3.4392)  time: 0.9338  data: 0.0005  max mem: 19734
Epoch: [6]  [ 340/1251]  eta: 0:14:39  lr: 0.000020  loss: 3.7243 (3.4397)  time: 0.9436  data: 0.0005  max mem: 19734
Epoch: [6]  [ 350/1251]  eta: 0:14:28  lr: 0.000020  loss: 3.5773 (3.4345)  time: 0.9229  data: 0.0005  max mem: 19734
Epoch: [6]  [ 360/1251]  eta: 0:14:18  lr: 0.000020  loss: 3.6835 (3.4407)  time: 0.9352  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 370/1251]  eta: 0:14:05  lr: 0.000020  loss: 3.1954 (3.3696)  time: 0.8946  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 380/1251]  eta: 0:13:52  lr: 0.000020  loss: 0.0000 (3.2812)  time: 0.8187  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 390/1251]  eta: 0:13:40  lr: 0.000020  loss: 0.0000 (3.1973)  time: 0.8296  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 400/1251]  eta: 0:13:28  lr: 0.000020  loss: 0.0000 (3.1175)  time: 0.8452  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 410/1251]  eta: 0:13:17  lr: 0.000020  loss: 0.0000 (3.0417)  time: 0.8438  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 420/1251]  eta: 0:13:05  lr: 0.000020  loss: 0.0000 (2.9694)  time: 0.8334  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 430/1251]  eta: 0:12:53  lr: 0.000020  loss: 0.0000 (2.9005)  time: 0.8167  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 440/1251]  eta: 0:12:41  lr: 0.000020  loss: 0.0000 (2.8348)  time: 0.8106  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 450/1251]  eta: 0:12:29  lr: 0.000020  loss: 0.0000 (2.7719)  time: 0.8066  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 460/1251]  eta: 0:12:18  lr: 0.000020  loss: 0.0000 (2.7118)  time: 0.8089  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 470/1251]  eta: 0:12:06  lr: 0.000020  loss: 0.0000 (2.6542)  time: 0.8127  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 480/1251]  eta: 0:11:55  lr: 0.000020  loss: 0.0000 (2.5990)  time: 0.8133  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 490/1251]  eta: 0:11:44  lr: 0.000020  loss: 0.0000 (2.5461)  time: 0.8236  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 500/1251]  eta: 0:11:34  lr: 0.000020  loss: 0.0000 (2.4953)  time: 0.8267  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 510/1251]  eta: 0:11:23  lr: 0.000020  loss: 0.0000 (2.4465)  time: 0.8214  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 520/1251]  eta: 0:11:12  lr: 0.000020  loss: 0.0000 (2.3995)  time: 0.8170  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 530/1251]  eta: 0:11:02  lr: 0.000020  loss: 0.0000 (2.3543)  time: 0.8211  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 540/1251]  eta: 0:10:51  lr: 0.000020  loss: 0.0000 (2.3108)  time: 0.8261  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 550/1251]  eta: 0:10:41  lr: 0.000020  loss: 0.0000 (2.2689)  time: 0.8292  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 560/1251]  eta: 0:10:31  lr: 0.000020  loss: 0.0000 (2.2284)  time: 0.8331  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 570/1251]  eta: 0:10:21  lr: 0.000020  loss: 0.0000 (2.1894)  time: 0.8267  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 580/1251]  eta: 0:10:10  lr: 0.000020  loss: 0.0000 (2.1517)  time: 0.8228  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 590/1251]  eta: 0:10:00  lr: 0.000020  loss: 0.0000 (2.1153)  time: 0.8138  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 600/1251]  eta: 0:09:50  lr: 0.000020  loss: 0.0000 (2.0801)  time: 0.8049  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 610/1251]  eta: 0:09:40  lr: 0.000020  loss: 0.0000 (2.0461)  time: 0.8087  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 620/1251]  eta: 0:09:30  lr: 0.000020  loss: 0.0000 (2.0131)  time: 0.8133  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 630/1251]  eta: 0:09:20  lr: 0.000020  loss: 0.0000 (1.9812)  time: 0.8139  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 640/1251]  eta: 0:09:10  lr: 0.000020  loss: 0.0000 (1.9503)  time: 0.8267  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 650/1251]  eta: 0:09:01  lr: 0.000020  loss: 0.0000 (1.9203)  time: 0.8321  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 660/1251]  eta: 0:08:51  lr: 0.000020  loss: 0.0000 (1.8913)  time: 0.8230  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 670/1251]  eta: 0:08:41  lr: 0.000020  loss: 0.0000 (1.8631)  time: 0.8151  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 680/1251]  eta: 0:08:32  lr: 0.000020  loss: 0.0000 (1.8357)  time: 0.8138  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 690/1251]  eta: 0:08:22  lr: 0.000020  loss: 0.0000 (1.8092)  time: 0.8228  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 700/1251]  eta: 0:08:13  lr: 0.000020  loss: 0.0000 (1.7834)  time: 0.8388  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 710/1251]  eta: 0:08:03  lr: 0.000020  loss: 0.0000 (1.7583)  time: 0.8354  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 720/1251]  eta: 0:07:54  lr: 0.000020  loss: 0.0000 (1.7339)  time: 0.8223  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 730/1251]  eta: 0:07:44  lr: 0.000020  loss: 0.0000 (1.7102)  time: 0.8198  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 740/1251]  eta: 0:07:35  lr: 0.000020  loss: 0.0000 (1.6871)  time: 0.8098  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 750/1251]  eta: 0:07:25  lr: 0.000020  loss: 0.0000 (1.6646)  time: 0.8084  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 760/1251]  eta: 0:07:16  lr: 0.000020  loss: 0.0000 (1.6428)  time: 0.8144  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 770/1251]  eta: 0:07:06  lr: 0.000020  loss: 0.0000 (1.6214)  time: 0.8108  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 780/1251]  eta: 0:06:57  lr: 0.000020  loss: 0.0000 (1.6007)  time: 0.8130  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 790/1251]  eta: 0:06:48  lr: 0.000020  loss: 0.0000 (1.5805)  time: 0.8181  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 800/1251]  eta: 0:06:39  lr: 0.000020  loss: 0.0000 (1.5607)  time: 0.8172  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 810/1251]  eta: 0:06:29  lr: 0.000020  loss: 0.0000 (1.5415)  time: 0.8203  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 820/1251]  eta: 0:06:20  lr: 0.000020  loss: 0.0000 (1.5227)  time: 0.8195  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 830/1251]  eta: 0:06:11  lr: 0.000020  loss: 0.0000 (1.5044)  time: 0.8227  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 840/1251]  eta: 0:06:02  lr: 0.000020  loss: 0.0000 (1.4865)  time: 0.8291  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 850/1251]  eta: 0:05:53  lr: 0.000020  loss: 0.0000 (1.4690)  time: 0.8377  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 860/1251]  eta: 0:05:44  lr: 0.000020  loss: 0.0000 (1.4520)  time: 0.8476  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 870/1251]  eta: 0:05:35  lr: 0.000020  loss: 0.0000 (1.4353)  time: 0.8372  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 880/1251]  eta: 0:05:26  lr: 0.000020  loss: 0.0000 (1.4190)  time: 0.8151  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 890/1251]  eta: 0:05:17  lr: 0.000020  loss: 0.0000 (1.4031)  time: 0.8062  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 900/1251]  eta: 0:05:08  lr: 0.000020  loss: 0.0000 (1.3875)  time: 0.8095  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 910/1251]  eta: 0:04:59  lr: 0.000020  loss: 0.0000 (1.3723)  time: 0.8129  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 920/1251]  eta: 0:04:50  lr: 0.000020  loss: 0.0000 (1.3574)  time: 0.8098  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 930/1251]  eta: 0:04:41  lr: 0.000020  loss: 0.0000 (1.3428)  time: 0.8164  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 940/1251]  eta: 0:04:32  lr: 0.000020  loss: 0.0000 (1.3285)  time: 0.8231  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 950/1251]  eta: 0:04:23  lr: 0.000020  loss: 0.0000 (1.3145)  time: 0.8211  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 960/1251]  eta: 0:04:14  lr: 0.000020  loss: 0.0000 (1.3009)  time: 0.8188  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 970/1251]  eta: 0:04:05  lr: 0.000020  loss: 0.0000 (1.2875)  time: 0.8137  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 980/1251]  eta: 0:03:56  lr: 0.000020  loss: 0.0000 (1.2743)  time: 0.8196  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [ 990/1251]  eta: 0:03:47  lr: 0.000020  loss: 0.0000 (1.2615)  time: 0.8396  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1000/1251]  eta: 0:03:39  lr: 0.000020  loss: 0.0000 (1.2489)  time: 0.8316  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1010/1251]  eta: 0:03:30  lr: 0.000020  loss: 0.0000 (1.2365)  time: 0.8296  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1020/1251]  eta: 0:03:21  lr: 0.000020  loss: 0.0000 (1.2244)  time: 0.8292  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1030/1251]  eta: 0:03:12  lr: 0.000020  loss: 0.0000 (1.2125)  time: 0.8099  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1040/1251]  eta: 0:03:03  lr: 0.000020  loss: 0.0000 (1.2009)  time: 0.8111  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1050/1251]  eta: 0:02:54  lr: 0.000020  loss: 0.0000 (1.1895)  time: 0.8171  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1060/1251]  eta: 0:02:46  lr: 0.000020  loss: 0.0000 (1.1783)  time: 0.8208  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1070/1251]  eta: 0:02:37  lr: 0.000020  loss: 0.0000 (1.1673)  time: 0.8232  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1080/1251]  eta: 0:02:28  lr: 0.000020  loss: 0.0000 (1.1565)  time: 0.8197  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1090/1251]  eta: 0:02:19  lr: 0.000020  loss: 0.0000 (1.1459)  time: 0.8158  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1100/1251]  eta: 0:02:11  lr: 0.000020  loss: 0.0000 (1.1355)  time: 0.8180  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1110/1251]  eta: 0:02:02  lr: 0.000020  loss: 0.0000 (1.1252)  time: 0.8145  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1120/1251]  eta: 0:01:53  lr: 0.000020  loss: 0.0000 (1.1152)  time: 0.8133  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1130/1251]  eta: 0:01:44  lr: 0.000020  loss: 0.0000 (1.1053)  time: 0.8219  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1140/1251]  eta: 0:01:36  lr: 0.000020  loss: 0.0000 (1.0957)  time: 0.8436  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1150/1251]  eta: 0:01:27  lr: 0.000020  loss: 0.0000 (1.0861)  time: 0.8371  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1160/1251]  eta: 0:01:18  lr: 0.000020  loss: 0.0000 (1.0768)  time: 0.8238  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1170/1251]  eta: 0:01:10  lr: 0.000020  loss: 0.0000 (1.0676)  time: 0.8221  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1180/1251]  eta: 0:01:01  lr: 0.000020  loss: 0.0000 (1.0585)  time: 0.8193  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1190/1251]  eta: 0:00:52  lr: 0.000020  loss: 0.0000 (1.0497)  time: 0.8173  data: 0.0010  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1200/1251]  eta: 0:00:44  lr: 0.000020  loss: 0.0000 (1.0409)  time: 0.8071  data: 0.0008  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1210/1251]  eta: 0:00:35  lr: 0.000020  loss: 0.0000 (1.0323)  time: 0.8043  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1220/1251]  eta: 0:00:26  lr: 0.000020  loss: 0.0000 (1.0239)  time: 0.8072  data: 0.0001  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1230/1251]  eta: 0:00:18  lr: 0.000020  loss: 0.0000 (1.0155)  time: 0.8115  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1240/1251]  eta: 0:00:09  lr: 0.000020  loss: 0.0000 (1.0074)  time: 0.8067  data: 0.0002  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [6]  [1250/1251]  eta: 0:00:00  lr: 0.000020  loss: 0.0000 (0.9993)  time: 0.7998  data: 0.0002  max mem: 19734
Epoch: [6] Total time: 0:17:58 (0.8625 s / it)
Averaged stats: lr: 0.000020  loss: 0.0000 (1.0065)
Test:  [  0/261]  eta: 2:49:58  loss: nan (nan)  acc1: 26.0417 (26.0417)  acc5: 100.0000 (100.0000)  time: 39.0730  data: 38.8760  max mem: 19734
Test:  [ 10/261]  eta: 0:16:59  loss: nan (nan)  acc1: 0.0000 (2.3674)  acc5: 0.0000 (11.8371)  time: 4.0607  data: 3.6594  max mem: 19734
Test:  [ 20/261]  eta: 0:09:02  loss: nan (nan)  acc1: 0.0000 (1.2401)  acc5: 0.0000 (6.2004)  time: 0.4118  data: 0.0747  max mem: 19734
Test:  [ 30/261]  eta: 0:06:14  loss: nan (nan)  acc1: 0.0000 (0.8401)  acc5: 0.0000 (4.2003)  time: 0.2764  data: 0.0119  max mem: 19734
Test:  [ 40/261]  eta: 0:05:01  loss: nan (nan)  acc1: 0.0000 (0.6352)  acc5: 0.0000 (3.1758)  time: 0.4338  data: 0.0259  max mem: 19734
Test:  [ 50/261]  eta: 0:04:02  loss: nan (nan)  acc1: 0.0000 (0.5106)  acc5: 0.0000 (2.5531)  time: 0.4169  data: 0.0261  max mem: 19734
Test:  [ 60/261]  eta: 0:03:23  loss: nan (nan)  acc1: 0.0000 (0.4269)  acc5: 0.0000 (2.1346)  time: 0.2859  data: 0.0141  max mem: 19734
Test:  [ 70/261]  eta: 0:03:02  loss: nan (nan)  acc1: 0.0000 (0.3668)  acc5: 0.0000 (1.8339)  time: 0.4617  data: 0.1844  max mem: 19734
Test:  [ 80/261]  eta: 0:02:36  loss: nan (nan)  acc1: 0.0000 (0.3215)  acc5: 0.0000 (1.6075)  time: 0.4187  data: 0.1867  max mem: 19734
Test:  [ 90/261]  eta: 0:02:16  loss: nan (nan)  acc1: 0.0000 (0.2862)  acc5: 0.0000 (1.4309)  time: 0.2341  data: 0.0152  max mem: 19734
Test:  [100/261]  eta: 0:02:05  loss: nan (nan)  acc1: 0.0000 (0.2578)  acc5: 0.0000 (1.2892)  time: 0.4332  data: 0.2386  max mem: 19734
Test:  [110/261]  eta: 0:01:58  loss: nan (nan)  acc1: 0.0000 (0.2346)  acc5: 0.0000 (1.1730)  time: 0.7174  data: 0.4816  max mem: 19734
Test:  [120/261]  eta: 0:01:44  loss: nan (nan)  acc1: 0.0000 (0.2152)  acc5: 0.0000 (1.0761)  time: 0.5254  data: 0.2748  max mem: 19734
Test:  [130/261]  eta: 0:01:32  loss: nan (nan)  acc1: 0.0000 (0.1988)  acc5: 0.0000 (0.9940)  time: 0.2708  data: 0.0364  max mem: 19734
Test:  [140/261]  eta: 0:01:23  loss: nan (nan)  acc1: 0.0000 (0.1847)  acc5: 0.0000 (0.9235)  time: 0.3983  data: 0.1285  max mem: 19734
Test:  [150/261]  eta: 0:01:20  loss: nan (nan)  acc1: 0.0000 (0.1725)  acc5: 0.0000 (0.8623)  time: 0.8840  data: 0.5403  max mem: 19734
Test:  [160/261]  eta: 0:01:10  loss: nan (nan)  acc1: 0.0000 (0.1617)  acc5: 0.0000 (0.8087)  time: 0.7881  data: 0.4311  max mem: 19734
Test:  [170/261]  eta: 0:01:01  loss: nan (nan)  acc1: 0.0000 (0.1523)  acc5: 0.0000 (0.7615)  time: 0.2483  data: 0.0128  max mem: 19734
Test:  [180/261]  eta: 0:00:52  loss: nan (nan)  acc1: 0.0000 (0.1439)  acc5: 0.0000 (0.7194)  time: 0.2417  data: 0.0667  max mem: 19734
Test:  [190/261]  eta: 0:00:44  loss: nan (nan)  acc1: 0.0000 (0.1363)  acc5: 0.0000 (0.6817)  time: 0.2159  data: 0.0636  max mem: 19734
Test:  [200/261]  eta: 0:00:36  loss: nan (nan)  acc1: 0.0000 (0.1296)  acc5: 0.0000 (0.6478)  time: 0.1355  data: 0.0004  max mem: 19734
Test:  [210/261]  eta: 0:00:29  loss: nan (nan)  acc1: 0.0000 (0.1234)  acc5: 0.0000 (0.6171)  time: 0.1355  data: 0.0002  max mem: 19734
Test:  [220/261]  eta: 0:00:22  loss: nan (nan)  acc1: 0.0000 (0.1178)  acc5: 0.0000 (0.5892)  time: 0.1354  data: 0.0001  max mem: 19734
Test:  [230/261]  eta: 0:00:16  loss: nan (nan)  acc1: 0.0000 (0.1127)  acc5: 0.0000 (0.5637)  time: 0.1338  data: 0.0001  max mem: 19734
Test:  [240/261]  eta: 0:00:10  loss: nan (nan)  acc1: 0.0000 (0.1081)  acc5: 0.0000 (0.5403)  time: 0.1336  data: 0.0001  max mem: 19734
Test:  [250/261]  eta: 0:00:05  loss: nan (nan)  acc1: 0.0000 (0.1038)  acc5: 0.0000 (0.5188)  time: 0.1336  data: 0.0001  max mem: 19734
Test:  [260/261]  eta: 0:00:00  loss: nan (nan)  acc1: 0.0000 (0.1000)  acc5: 0.0000 (0.5000)  time: 0.1300  data: 0.0001  max mem: 19734
Test: Total time: 0:02:08 (0.4936 s / it)
* Acc@1 0.100 Acc@5 0.500 loss nan
Accuracy of the network on the 50000 test images: 0.1%
Max accuracy: 74.19%
Loss is nan, stopping training this iteration.
Epoch: [7]  [   0/1251]  eta: 5:48:56  lr: 0.000020  loss: 0.0000 (0.0000)  time: 16.7362  data: 14.9804  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  10/1251]  eta: 0:53:26  lr: 0.000020  loss: 0.0000 (0.0000)  time: 2.5836  data: 1.3635  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  20/1251]  eta: 0:35:40  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.9887  data: 0.0012  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  30/1251]  eta: 0:29:19  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8122  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  40/1251]  eta: 0:25:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8132  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
loss info: cls_loss=nan, ratio_loss=nan, pruning_loss=nan, mse_loss=nan
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  50/1251]  eta: 0:23:55  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8143  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  60/1251]  eta: 0:22:27  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8119  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  70/1251]  eta: 0:21:22  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8090  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  80/1251]  eta: 0:20:31  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8100  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [  90/1251]  eta: 0:19:50  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8086  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 100/1251]  eta: 0:19:16  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8134  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 110/1251]  eta: 0:18:47  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8177  data: 0.0006  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 120/1251]  eta: 0:18:21  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8186  data: 0.0005  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Epoch: [7]  [ 130/1251]  eta: 0:17:58  lr: 0.000020  loss: 0.0000 (0.0000)  time: 0.8179  data: 0.0004  max mem: 19734
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
Loss is nan, stopping training this iteration.
